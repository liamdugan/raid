import torch
import math
from tqdm import tqdm
from torch.utils.data import DataLoader
from transformers import AutoTokenizer

from .dataset import EncodeEvalData
from .detector import RobertaForTextGenClassification

class Conda:

    def __init__(self):
        torch.manual_seed(1000)

        # loss_mode = 'simclr'  ## only simclr supported for now
        # transformation = '_syn_rep'  ## only synonym replacement supported for now
        # src = "fair_wmt19"
        # tgt = "ctrl"

        self.model_name = "/nlp/data/ldugan/raid/detectors/models/conda/fair_wmt19_chatgpt_syn_rep_loss1.pt"
        self.device = f'cuda:{0}' if torch.cuda.is_available() else 'cpu'
        self.kn_priors = [0.5, 0.5]
        self.batch_size = 1
        self.max_sequence_length = 256
        self.kn_large = False

        self.tokenizer = AutoTokenizer.from_pretrained('roberta-base')
        self.model = RobertaForTextGenClassification.from_pretrained('roberta-base').to(self.device)

        kwargs = {'map_location': 'cpu'} if self.device == 'cpu' else {}
        state_dict = torch.load(self.model_name, **kwargs)['model_state_dict']
        state_dict = {k: v for k, v in state_dict.items() if k != "roberta.embeddings.position_ids"}
        self.model.load_state_dict(state_dict)
    
    def run_inference(self, input_texts: str):
        # Encapsulate the inputs
        eval_dataset = EncodeEvalData(input_texts, self.tokenizer, self.max_sequence_length)
        eval_loader = DataLoader(eval_dataset)

        # Dictionary will contain all the scores and evidences generated by the model
        results = {"cls": [], "LLR_score": [], "prob_score": {"cls_0": [], "cls_1": []}, "generator": None}

        self.model.eval()

        with torch.no_grad():
            for texts, masks in eval_loader:

                texts, masks = texts.to(self.device), masks.to(self.device)

                output_dict = self.model(texts, attention_mask=masks)
                disc_out = output_dict["logits"]

                cls0_prob = disc_out[:, 0].tolist()
                cls1_prob = disc_out[:, 1].tolist()

                results["prob_score"]["cls_0"].extend(cls0_prob)
                results["prob_score"]["cls_1"].extend(cls1_prob)

                prior_llr = math.log10(self.kn_priors[0]/self.kn_priors[1])

                results["LLR_score"].extend([math.log10(prob/(1-prob)) + prior_llr for prob in cls1_prob])

                _, predicted = torch.max(disc_out, 1)

                results["cls"].extend(predicted.tolist())
                      
        return results
        

    def inference(self, texts: list) -> list:
        predictions = []
        for text in tqdm(texts):
            results = self.run_inference([text])
            predictions.append(results["LLR_score"][0])
        return predictions