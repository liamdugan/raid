{
  "_autogen_note": "This file is automatically generated by the RAID leaderboard submission script. Do not edit this file manually or include it in new submissions' PRs.",
  "_submission_hash": "a65d27d2d543b9e63cac607f525c0506847d6c16ee2480051c2239902993e6f1",
  "_results_hash": "65a3769bfa8250e7727401e647654a1c030560a84957becbaf90a70b86eabf2f",
  "date_released": "2025-11-06",
  "detector_name": "debarta-text-classifier",
  "contact_info": "kishan.sutex.kp@gmail.com",
  "website": null,
  "paper_link": null,
  "huggingface_link": "https://huggingface.co/kishankachhadiya/debarta-text-classifier",
  "github_link": null,
  "additional_metadata": null,
  "score_agg": {
    "all": {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 426392,
          "fn": 226408,
          "accuracy": 0.6531740196078432
        },
        "0.01": null
      },
      "auroc": 0.8241536372564977
    },
    "no_adversarial": {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 44199,
          "fn": 10201,
          "accuracy": 0.8124816176470588
        },
        "0.01": null
      },
      "auroc": 0.925997380993413
    }
  },
  "scores": [
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9986197916666668
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9926479166666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9956338541666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9968427083333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9786604166666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9877515625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.99773125
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9856541666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 774,
          "fn": 26,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 707,
          "fn": 93,
          "accuracy": 0.88375
        }
      },
      "auroc": 0.9916927083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.8828125
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        }
      },
      "auroc": 0.94140625
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        }
      },
      "auroc": 0.94140625
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 737,
          "fn": 63,
          "accuracy": 0.92125
        },
        "0.01": {
          "tp": 697,
          "fn": 103,
          "accuracy": 0.87125
        }
      },
      "auroc": 0.9707031249999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9826833333333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9978625
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9902729166666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9826895833333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9994739583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9910817708333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9826864583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9986682291666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 778,
          "fn": 22,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 724,
          "fn": 76,
          "accuracy": 0.905
        }
      },
      "auroc": 0.99067734375
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9962354166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9981177083333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.8593291666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9899010416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9246151041666668
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9296645833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9930682291666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 705,
          "fn": 95,
          "accuracy": 0.88125
        },
        "0.01": {
          "tp": 641,
          "fn": 159,
          "accuracy": 0.80125
        }
      },
      "auroc": 0.9613664062499999
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9994229166666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9997114583333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.9051364583333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9525682291666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9525682291666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9997114583333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 749,
          "fn": 51,
          "accuracy": 0.93625
        },
        "0.01": {
          "tp": 732,
          "fn": 68,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9761398437500001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9927489583333332
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.980646875
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        }
      },
      "auroc": 0.9866979166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9723999999999999
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9677812499999999
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        }
      },
      "auroc": 0.970090625
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        }
      },
      "auroc": 0.9825744791666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        }
      },
      "auroc": 0.9742140625
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 738,
          "fn": 62,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 618,
          "fn": 182,
          "accuracy": 0.7725
        }
      },
      "auroc": 0.9783942708333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8903833333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8903833333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.8671916666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.8671916666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 154,
          "fn": 246,
          "accuracy": 0.385
        }
      },
      "auroc": 0.8787874999999999
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 154,
          "fn": 246,
          "accuracy": 0.385
        }
      },
      "auroc": 0.8787874999999999
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.7142697916666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.7142697916666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.6763739583333332
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.6763739583333332
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 67,
          "fn": 333,
          "accuracy": 0.1675
        }
      },
      "auroc": 0.695321875
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 67,
          "fn": 333,
          "accuracy": 0.1675
        }
      },
      "auroc": 0.695321875
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9877979166666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9877979166666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.976915625
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.976915625
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9823567708333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9823567708333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9920375
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9920375
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.879303125
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.879303125
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9356703124999999
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9356703124999999
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.90663125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.90663125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.8657052083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.8657052083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        }
      },
      "auroc": 0.8861682291666668
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        }
      },
      "auroc": 0.8861682291666668
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1912,
          "fn": 288,
          "accuracy": 0.8690909090909091
        },
        "0.01": {
          "tp": 1743,
          "fn": 457,
          "accuracy": 0.7922727272727272
        }
      },
      "auroc": 0.9513792613636363
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1178,
          "fn": 22,
          "accuracy": 0.9816666666666667
        },
        "0.01": {
          "tp": 1109,
          "fn": 91,
          "accuracy": 0.9241666666666667
        }
      },
      "auroc": 0.9944692708333334
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3090,
          "fn": 310,
          "accuracy": 0.9088235294117647
        },
        "0.01": {
          "tp": 2852,
          "fn": 548,
          "accuracy": 0.8388235294117647
        }
      },
      "auroc": 0.9665875
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1576,
          "fn": 624,
          "accuracy": 0.7163636363636363
        },
        "0.01": {
          "tp": 1211,
          "fn": 989,
          "accuracy": 0.5504545454545454
        }
      },
      "auroc": 0.8967909090909091
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1153,
          "fn": 47,
          "accuracy": 0.9608333333333333
        },
        "0.01": {
          "tp": 1075,
          "fn": 125,
          "accuracy": 0.8958333333333334
        }
      },
      "auroc": 0.9893027777777778
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2729,
          "fn": 671,
          "accuracy": 0.8026470588235294
        },
        "0.01": {
          "tp": 2286,
          "fn": 1114,
          "accuracy": 0.6723529411764706
        }
      },
      "auroc": 0.9294421568627451
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3488,
          "fn": 912,
          "accuracy": 0.7927272727272727
        },
        "0.01": {
          "tp": 2954,
          "fn": 1446,
          "accuracy": 0.6713636363636364
        }
      },
      "auroc": 0.9240850852272728
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2331,
          "fn": 69,
          "accuracy": 0.97125
        },
        "0.01": {
          "tp": 2184,
          "fn": 216,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9918860243055557
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 5819,
          "fn": 981,
          "accuracy": 0.855735294117647
        },
        "0.01": {
          "tp": 5138,
          "fn": 1662,
          "accuracy": 0.7555882352941177
        }
      },
      "auroc": 0.9480148284313725
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9986197916666668
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9926468749999999
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9956333333333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9968427083333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9786604166666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9877515625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.99773125
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9856536458333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 774,
          "fn": 26,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 707,
          "fn": 93,
          "accuracy": 0.88375
        }
      },
      "auroc": 0.9916924479166668
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.8828125
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        }
      },
      "auroc": 0.94140625
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        }
      },
      "auroc": 0.94140625
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 737,
          "fn": 63,
          "accuracy": 0.92125
        },
        "0.01": {
          "tp": 697,
          "fn": 103,
          "accuracy": 0.87125
        }
      },
      "auroc": 0.9707031249999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9826833333333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9978635416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9902734375000001
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9826885416666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9994739583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9910812499999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9826859375
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.99866875
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 778,
          "fn": 22,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 724,
          "fn": 76,
          "accuracy": 0.905
        }
      },
      "auroc": 0.99067734375
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.996225
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9981125000000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.860046875
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9899010416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9249739583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9300234374999999
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9930630208333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 705,
          "fn": 95,
          "accuracy": 0.88125
        },
        "0.01": {
          "tp": 641,
          "fn": 159,
          "accuracy": 0.80125
        }
      },
      "auroc": 0.9615432291666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9994229166666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9997114583333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.9052427083333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9526213541666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9526213541666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9997114583333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 749,
          "fn": 51,
          "accuracy": 0.93625
        },
        "0.01": {
          "tp": 732,
          "fn": 68,
          "accuracy": 0.915
        }
      },
      "auroc": 0.97616640625
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9927489583333332
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9806458333333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        }
      },
      "auroc": 0.9866973958333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9722708333333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9677812499999999
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        }
      },
      "auroc": 0.9700260416666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        }
      },
      "auroc": 0.9825098958333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        }
      },
      "auroc": 0.9742135416666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 738,
          "fn": 62,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 618,
          "fn": 182,
          "accuracy": 0.7725
        }
      },
      "auroc": 0.97836171875
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8904572916666668
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8904572916666668
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.8671916666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.8671916666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 154,
          "fn": 246,
          "accuracy": 0.385
        }
      },
      "auroc": 0.8788244791666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 154,
          "fn": 246,
          "accuracy": 0.385
        }
      },
      "auroc": 0.8788244791666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.7142697916666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.7142697916666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.6763739583333332
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.6763739583333332
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 67,
          "fn": 333,
          "accuracy": 0.1675
        }
      },
      "auroc": 0.695321875
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 67,
          "fn": 333,
          "accuracy": 0.1675
        }
      },
      "auroc": 0.695321875
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9877979166666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9877979166666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.976915625
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.976915625
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9823567708333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9823567708333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9920375
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9920375
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.8796229166666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.8796229166666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9358302083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9358302083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.90663125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.90663125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.8657052083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.8657052083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        }
      },
      "auroc": 0.8861682291666668
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        }
      },
      "auroc": 0.8861682291666668
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1912,
          "fn": 288,
          "accuracy": 0.8690909090909091
        },
        "0.01": {
          "tp": 1743,
          "fn": 457,
          "accuracy": 0.7922727272727272
        }
      },
      "auroc": 0.9513859848484848
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1178,
          "fn": 22,
          "accuracy": 0.9816666666666667
        },
        "0.01": {
          "tp": 1109,
          "fn": 91,
          "accuracy": 0.9241666666666667
        }
      },
      "auroc": 0.994467361111111
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3090,
          "fn": 310,
          "accuracy": 0.9088235294117647
        },
        "0.01": {
          "tp": 2852,
          "fn": 548,
          "accuracy": 0.8388235294117647
        }
      },
      "auroc": 0.9665911764705882
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1576,
          "fn": 624,
          "accuracy": 0.7163636363636363
        },
        "0.01": {
          "tp": 1211,
          "fn": 989,
          "accuracy": 0.5504545454545454
        }
      },
      "auroc": 0.8968830492424241
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1153,
          "fn": 47,
          "accuracy": 0.9608333333333333
        },
        "0.01": {
          "tp": 1075,
          "fn": 125,
          "accuracy": 0.8958333333333334
        }
      },
      "auroc": 0.9893027777777778
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2729,
          "fn": 671,
          "accuracy": 0.8026470588235294
        },
        "0.01": {
          "tp": 2286,
          "fn": 1114,
          "accuracy": 0.6723529411764706
        }
      },
      "auroc": 0.9295017769607842
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3488,
          "fn": 912,
          "accuracy": 0.7927272727272727
        },
        "0.01": {
          "tp": 2954,
          "fn": 1446,
          "accuracy": 0.6713636363636364
        }
      },
      "auroc": 0.9241345170454545
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2331,
          "fn": 69,
          "accuracy": 0.97125
        },
        "0.01": {
          "tp": 2184,
          "fn": 216,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9918850694444444
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 5819,
          "fn": 981,
          "accuracy": 0.855735294117647
        },
        "0.01": {
          "tp": 5138,
          "fn": 1662,
          "accuracy": 0.7555882352941177
        }
      },
      "auroc": 0.9480464767156864
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9958812499999999
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.986725
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        }
      },
      "auroc": 0.991303125
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9922385416666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.9710322916666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        }
      },
      "auroc": 0.9816354166666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9940598958333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        }
      },
      "auroc": 0.9788786458333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 757,
          "fn": 43,
          "accuracy": 0.94625
        },
        "0.01": {
          "tp": 649,
          "fn": 151,
          "accuracy": 0.81125
        }
      },
      "auroc": 0.9864692708333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        }
      },
      "auroc": 0.8677531249999999
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        }
      },
      "auroc": 0.9338765625
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        }
      },
      "auroc": 0.9338765625
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 728,
          "fn": 72,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 693,
          "fn": 107,
          "accuracy": 0.86625
        }
      },
      "auroc": 0.9669382812499999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9752406250000001
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9968520833333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9860463541666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9773364583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.999246875
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9882916666666668
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        }
      },
      "auroc": 0.9762885416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9980494791666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 768,
          "fn": 32,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 691,
          "fn": 109,
          "accuracy": 0.86375
        }
      },
      "auroc": 0.9871690104166666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9923458333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        }
      },
      "auroc": 0.9961729166666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.839378125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9930583333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9162182291666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        }
      },
      "auroc": 0.9196890624999999
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9927020833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 693,
          "fn": 107,
          "accuracy": 0.86625
        },
        "0.01": {
          "tp": 627,
          "fn": 173,
          "accuracy": 0.78375
        }
      },
      "auroc": 0.9561955729166667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.99934375
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.999671875
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        }
      },
      "auroc": 0.8933916666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        }
      },
      "auroc": 0.9466958333333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        }
      },
      "auroc": 0.9466958333333335
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.999671875
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 745,
          "fn": 55,
          "accuracy": 0.93125
        },
        "0.01": {
          "tp": 729,
          "fn": 71,
          "accuracy": 0.91125
        }
      },
      "auroc": 0.9731838541666668
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9857010416666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9730510416666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        }
      },
      "auroc": 0.9793760416666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.9568947916666668
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9627729166666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        }
      },
      "auroc": 0.9598338541666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        }
      },
      "auroc": 0.9712979166666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9679119791666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 703,
          "fn": 97,
          "accuracy": 0.87875
        },
        "0.01": {
          "tp": 581,
          "fn": 219,
          "accuracy": 0.72625
        }
      },
      "auroc": 0.9696049479166667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.8712958333333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.8712958333333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.84891875
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.84891875
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        },
        "0.01": {
          "tp": 127,
          "fn": 273,
          "accuracy": 0.3175
        }
      },
      "auroc": 0.8601072916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        },
        "0.01": {
          "tp": 127,
          "fn": 273,
          "accuracy": 0.3175
        }
      },
      "auroc": 0.8601072916666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        },
        "0.01": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        }
      },
      "auroc": 0.6828218749999999
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        },
        "0.01": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        }
      },
      "auroc": 0.6828218749999999
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.6455479166666668
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.6455479166666668
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 305,
          "accuracy": 0.2375
        },
        "0.01": {
          "tp": 55,
          "fn": 345,
          "accuracy": 0.1375
        }
      },
      "auroc": 0.6641848958333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 305,
          "accuracy": 0.2375
        },
        "0.01": {
          "tp": 55,
          "fn": 345,
          "accuracy": 0.1375
        }
      },
      "auroc": 0.6641848958333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9781041666666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9781041666666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        }
      },
      "auroc": 0.9625979166666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        }
      },
      "auroc": 0.9625979166666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 259,
          "fn": 141,
          "accuracy": 0.6475
        }
      },
      "auroc": 0.9703510416666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 259,
          "fn": 141,
          "accuracy": 0.6475
        }
      },
      "auroc": 0.9703510416666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9865177083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9865177083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.8459843749999999
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.8459843749999999
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        }
      },
      "auroc": 0.9162510416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        }
      },
      "auroc": 0.9162510416666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        }
      },
      "auroc": 0.8915520833333332
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        }
      },
      "auroc": 0.8915520833333332
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.84860625
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.84860625
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        },
        "0.01": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        }
      },
      "auroc": 0.8700791666666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        },
        "0.01": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        }
      },
      "auroc": 0.8700791666666667
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1863,
          "fn": 337,
          "accuracy": 0.8468181818181818
        },
        "0.01": {
          "tp": 1630,
          "fn": 570,
          "accuracy": 0.740909090909091
        }
      },
      "auroc": 0.9424649621212121
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1160,
          "fn": 40,
          "accuracy": 0.9666666666666667
        },
        "0.01": {
          "tp": 1062,
          "fn": 138,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9913862847222221
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3023,
          "fn": 377,
          "accuracy": 0.8891176470588236
        },
        "0.01": {
          "tp": 2692,
          "fn": 708,
          "accuracy": 0.7917647058823529
        }
      },
      "auroc": 0.9597313112745098
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1486,
          "fn": 714,
          "accuracy": 0.6754545454545454
        },
        "0.01": {
          "tp": 1082,
          "fn": 1118,
          "accuracy": 0.4918181818181818
        }
      },
      "auroc": 0.8798770833333333
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1142,
          "fn": 58,
          "accuracy": 0.9516666666666667
        },
        "0.01": {
          "tp": 1053,
          "fn": 147,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9876850694444445
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2628,
          "fn": 772,
          "accuracy": 0.7729411764705882
        },
        "0.01": {
          "tp": 2135,
          "fn": 1265,
          "accuracy": 0.6279411764705882
        }
      },
      "auroc": 0.9179269607843138
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3349,
          "fn": 1051,
          "accuracy": 0.7611363636363636
        },
        "0.01": {
          "tp": 2712,
          "fn": 1688,
          "accuracy": 0.6163636363636363
        }
      },
      "auroc": 0.9111710227272728
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2302,
          "fn": 98,
          "accuracy": 0.9591666666666666
        },
        "0.01": {
          "tp": 2115,
          "fn": 285,
          "accuracy": 0.88125
        }
      },
      "auroc": 0.9895356770833335
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 5651,
          "fn": 1149,
          "accuracy": 0.8310294117647059
        },
        "0.01": {
          "tp": 4827,
          "fn": 1973,
          "accuracy": 0.7098529411764706
        }
      },
      "auroc": 0.9388291360294116
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.9690135416666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        }
      },
      "auroc": 0.9444541666666668
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        }
      },
      "auroc": 0.9567338541666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        }
      },
      "auroc": 0.961096875
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.9118635416666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        }
      },
      "auroc": 0.9364802083333335
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        }
      },
      "auroc": 0.9650552083333332
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        },
        "0.01": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        }
      },
      "auroc": 0.9281588541666665
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 640,
          "fn": 160,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 428,
          "fn": 372,
          "accuracy": 0.535
        }
      },
      "auroc": 0.9466070312499999
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999979166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999989583333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8408177083333332
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        }
      },
      "auroc": 0.9204088541666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        }
      },
      "auroc": 0.9204078125
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 725,
          "fn": 75,
          "accuracy": 0.90625
        },
        "0.01": {
          "tp": 685,
          "fn": 115,
          "accuracy": 0.85625
        }
      },
      "auroc": 0.96020390625
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        }
      },
      "auroc": 0.9310114583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9923447916666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9616781249999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.9371739583333332
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9976614583333332
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        }
      },
      "auroc": 0.9674177083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 320,
          "fn": 80,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        }
      },
      "auroc": 0.9340927083333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.995003125
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 714,
          "fn": 86,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 559,
          "fn": 241,
          "accuracy": 0.69875
        }
      },
      "auroc": 0.9645479166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9788270833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9894135416666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.8068645833333332
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9939583333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        }
      },
      "auroc": 0.9004114583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        },
        "0.01": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        }
      },
      "auroc": 0.9034322916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        }
      },
      "auroc": 0.9863927083333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 675,
          "fn": 125,
          "accuracy": 0.84375
        },
        "0.01": {
          "tp": 605,
          "fn": 195,
          "accuracy": 0.75625
        }
      },
      "auroc": 0.9449125
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.99914375
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9995718750000001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        }
      },
      "auroc": 0.8836552083333332
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        }
      },
      "auroc": 0.9418276041666668
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        }
      },
      "auroc": 0.9418276041666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.999571875
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 744,
          "fn": 56,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 728,
          "fn": 72,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9706997395833332
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.9410187500000001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.9289552083333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        },
        "0.01": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        }
      },
      "auroc": 0.9349869791666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.8921031249999999
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        }
      },
      "auroc": 0.9332447916666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        }
      },
      "auroc": 0.9126739583333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        }
      },
      "auroc": 0.9165609375000001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.9311
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 597,
          "fn": 203,
          "accuracy": 0.74625
        },
        "0.01": {
          "tp": 393,
          "fn": 407,
          "accuracy": 0.49125
        }
      },
      "auroc": 0.92383046875
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        }
      },
      "auroc": 0.8236729166666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        }
      },
      "auroc": 0.8236729166666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.7804322916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.7804322916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        },
        "0.01": {
          "tp": 81,
          "fn": 319,
          "accuracy": 0.2025
        }
      },
      "auroc": 0.8020526041666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        },
        "0.01": {
          "tp": 81,
          "fn": 319,
          "accuracy": 0.2025
        }
      },
      "auroc": 0.8020526041666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        },
        "0.01": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        }
      },
      "auroc": 0.601484375
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        },
        "0.01": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        }
      },
      "auroc": 0.601484375
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.5705104166666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.5705104166666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 77,
          "fn": 323,
          "accuracy": 0.1925
        },
        "0.01": {
          "tp": 41,
          "fn": 359,
          "accuracy": 0.1025
        }
      },
      "auroc": 0.5859973958333334
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 77,
          "fn": 323,
          "accuracy": 0.1925
        },
        "0.01": {
          "tp": 41,
          "fn": 359,
          "accuracy": 0.1025
        }
      },
      "auroc": 0.5859973958333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.9094677083333332
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.9094677083333332
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        }
      },
      "auroc": 0.8863791666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        }
      },
      "auroc": 0.8863791666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        }
      },
      "auroc": 0.8979234375000001
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        }
      },
      "auroc": 0.8979234375000001
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        }
      },
      "auroc": 0.932365625
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        }
      },
      "auroc": 0.932365625
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.7705395833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.7705395833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        },
        "0.01": {
          "tp": 104,
          "fn": 296,
          "accuracy": 0.26
        }
      },
      "auroc": 0.8514526041666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        },
        "0.01": {
          "tp": 104,
          "fn": 296,
          "accuracy": 0.26
        }
      },
      "auroc": 0.8514526041666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.8478791666666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.8478791666666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.7930770833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.7930770833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        }
      },
      "auroc": 0.8204781250000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        }
      },
      "auroc": 0.8204781250000001
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1645,
          "fn": 555,
          "accuracy": 0.7477272727272727
        },
        "0.01": {
          "tp": 1264,
          "fn": 936,
          "accuracy": 0.5745454545454546
        }
      },
      "auroc": 0.9050828598484849
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1088,
          "fn": 112,
          "accuracy": 0.9066666666666666
        },
        "0.01": {
          "tp": 928,
          "fn": 272,
          "accuracy": 0.7733333333333333
        }
      },
      "auroc": 0.9739541666666667
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2733,
          "fn": 667,
          "accuracy": 0.8038235294117647
        },
        "0.01": {
          "tp": 2192,
          "fn": 1208,
          "accuracy": 0.6447058823529411
        }
      },
      "auroc": 0.9293903799019609
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1219,
          "fn": 981,
          "accuracy": 0.5540909090909091
        },
        "0.01": {
          "tp": 759,
          "fn": 1441,
          "accuracy": 0.345
        }
      },
      "auroc": 0.8293318181818182
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1085,
          "fn": 115,
          "accuracy": 0.9041666666666667
        },
        "0.01": {
          "tp": 963,
          "fn": 237,
          "accuracy": 0.8025
        }
      },
      "auroc": 0.9727880208333334
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2304,
          "fn": 1096,
          "accuracy": 0.6776470588235294
        },
        "0.01": {
          "tp": 1722,
          "fn": 1678,
          "accuracy": 0.5064705882352941
        }
      },
      "auroc": 0.879963419117647
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2864,
          "fn": 1536,
          "accuracy": 0.6509090909090909
        },
        "0.01": {
          "tp": 2023,
          "fn": 2377,
          "accuracy": 0.4597727272727273
        }
      },
      "auroc": 0.8672073390151515
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2173,
          "fn": 227,
          "accuracy": 0.9054166666666666
        },
        "0.01": {
          "tp": 1891,
          "fn": 509,
          "accuracy": 0.7879166666666667
        }
      },
      "auroc": 0.9733710937499999
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 5037,
          "fn": 1763,
          "accuracy": 0.740735294117647
        },
        "0.01": {
          "tp": 3914,
          "fn": 2886,
          "accuracy": 0.5755882352941176
        }
      },
      "auroc": 0.9046768995098039
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9962479166666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9889302083333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        }
      },
      "auroc": 0.9925890625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9950729166666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9717510416666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        }
      },
      "auroc": 0.9834119791666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9956604166666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        }
      },
      "auroc": 0.980340625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 761,
          "fn": 39,
          "accuracy": 0.95125
        },
        "0.01": {
          "tp": 682,
          "fn": 118,
          "accuracy": 0.8525
        }
      },
      "auroc": 0.9880005208333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        }
      },
      "auroc": 0.8536177083333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9268088541666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9268088541666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 723,
          "fn": 77,
          "accuracy": 0.90375
        },
        "0.01": {
          "tp": 688,
          "fn": 112,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9634044270833332
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9796947916666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9968645833333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        }
      },
      "auroc": 0.9882796875000001
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9807302083333332
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9992770833333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9900036458333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9802125
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9980708333333332
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 773,
          "fn": 27,
          "accuracy": 0.96625
        },
        "0.01": {
          "tp": 721,
          "fn": 79,
          "accuracy": 0.90125
        }
      },
      "auroc": 0.9891416666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9935093749999999
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9967546875000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.83134375
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9887729166666668
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        }
      },
      "auroc": 0.9100583333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        },
        "0.01": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        }
      },
      "auroc": 0.915671875
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9911411458333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 692,
          "fn": 108,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 628,
          "fn": 172,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9534065104166667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.999221875
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9996109375000001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.895621875
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999979166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        }
      },
      "auroc": 0.9478098958333332
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        }
      },
      "auroc": 0.9478109374999999
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9996098958333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 746,
          "fn": 54,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 731,
          "fn": 69,
          "accuracy": 0.91375
        }
      },
      "auroc": 0.9737104166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9899822916666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.973971875
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9819770833333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9611760416666668
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.958125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9596505208333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        }
      },
      "auroc": 0.9755791666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 289,
          "fn": 111,
          "accuracy": 0.7225
        }
      },
      "auroc": 0.9660484375
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 720,
          "fn": 80,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 596,
          "fn": 204,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9708138020833335
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.8830843749999999
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.8830843749999999
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        }
      },
      "auroc": 0.8575645833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        }
      },
      "auroc": 0.8575645833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        },
        "0.01": {
          "tp": 144,
          "fn": 256,
          "accuracy": 0.36
        }
      },
      "auroc": 0.8703244791666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        },
        "0.01": {
          "tp": 144,
          "fn": 256,
          "accuracy": 0.36
        }
      },
      "auroc": 0.8703244791666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.68406875
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.68406875
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.6434343750000001
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.6434343750000001
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 305,
          "accuracy": 0.2375
        },
        "0.01": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        }
      },
      "auroc": 0.6637515625
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 305,
          "accuracy": 0.2375
        },
        "0.01": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        }
      },
      "auroc": 0.6637515625
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9841979166666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9841979166666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9684104166666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9684104166666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        }
      },
      "auroc": 0.9763041666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        }
      },
      "auroc": 0.9763041666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9904572916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9904572916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.8584145833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.8584145833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 232,
          "fn": 168,
          "accuracy": 0.58
        }
      },
      "auroc": 0.9244359375
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 232,
          "fn": 168,
          "accuracy": 0.58
        }
      },
      "auroc": 0.9244359375
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        }
      },
      "auroc": 0.8960375000000002
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        }
      },
      "auroc": 0.8960375000000002
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.8492895833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.8492895833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        }
      },
      "auroc": 0.8726635416666668
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        }
      },
      "auroc": 0.8726635416666668
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1888,
          "fn": 312,
          "accuracy": 0.8581818181818182
        },
        "0.01": {
          "tp": 1712,
          "fn": 488,
          "accuracy": 0.7781818181818182
        }
      },
      "auroc": 0.9457973484848485
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1167,
          "fn": 33,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 1092,
          "fn": 108,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9920829861111111
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3055,
          "fn": 345,
          "accuracy": 0.8985294117647059
        },
        "0.01": {
          "tp": 2804,
          "fn": 596,
          "accuracy": 0.8247058823529412
        }
      },
      "auroc": 0.962133455882353
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1506,
          "fn": 694,
          "accuracy": 0.6845454545454546
        },
        "0.01": {
          "tp": 1148,
          "fn": 1052,
          "accuracy": 0.5218181818181818
        }
      },
      "auroc": 0.8813341856060606
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1142,
          "fn": 58,
          "accuracy": 0.9516666666666667
        },
        "0.01": {
          "tp": 1056,
          "fn": 144,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9863206597222223
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2648,
          "fn": 752,
          "accuracy": 0.7788235294117647
        },
        "0.01": {
          "tp": 2204,
          "fn": 1196,
          "accuracy": 0.648235294117647
        }
      },
      "auroc": 0.9183882352941177
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3394,
          "fn": 1006,
          "accuracy": 0.7713636363636364
        },
        "0.01": {
          "tp": 2860,
          "fn": 1540,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9135657670454544
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2309,
          "fn": 91,
          "accuracy": 0.9620833333333333
        },
        "0.01": {
          "tp": 2148,
          "fn": 252,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9892018229166667
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 5703,
          "fn": 1097,
          "accuracy": 0.8386764705882352
        },
        "0.01": {
          "tp": 5008,
          "fn": 1792,
          "accuracy": 0.7364705882352941
        }
      },
      "auroc": 0.9402608455882353
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.974128125
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.9426947916666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.9584114583333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        }
      },
      "auroc": 0.9660958333333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.9141104166666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.940103125
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        }
      },
      "auroc": 0.9701119791666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        }
      },
      "auroc": 0.9284026041666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 660,
          "fn": 140,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 442,
          "fn": 358,
          "accuracy": 0.5525
        }
      },
      "auroc": 0.9492572916666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9987427083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9706791666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9847109374999999
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.8367270833333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9801260416666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 291,
          "fn": 109,
          "accuracy": 0.7275
        },
        "0.01": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        }
      },
      "auroc": 0.9084265625
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        }
      },
      "auroc": 0.9177348958333332
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        }
      },
      "auroc": 0.9754026041666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 670,
          "fn": 130,
          "accuracy": 0.8375
        },
        "0.01": {
          "tp": 546,
          "fn": 254,
          "accuracy": 0.6825
        }
      },
      "auroc": 0.9465687500000001
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.9260677083333332
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.8749052083333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 151,
          "fn": 249,
          "accuracy": 0.3775
        }
      },
      "auroc": 0.9004864583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.9217604166666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.8624895833333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 121,
          "fn": 279,
          "accuracy": 0.3025
        }
      },
      "auroc": 0.892125
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 291,
          "fn": 109,
          "accuracy": 0.7275
        },
        "0.01": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        }
      },
      "auroc": 0.9239140625000001
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        },
        "0.01": {
          "tp": 99,
          "fn": 301,
          "accuracy": 0.2475
        }
      },
      "auroc": 0.8686973958333335
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 522,
          "fn": 278,
          "accuracy": 0.6525
        },
        "0.01": {
          "tp": 272,
          "fn": 528,
          "accuracy": 0.34
        }
      },
      "auroc": 0.8963057291666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9997583333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.9439479166666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        }
      },
      "auroc": 0.971853125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.83488125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.79265625
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        },
        "0.01": {
          "tp": 76,
          "fn": 324,
          "accuracy": 0.19
        }
      },
      "auroc": 0.81376875
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        }
      },
      "auroc": 0.9173197916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 134,
          "fn": 266,
          "accuracy": 0.335
        }
      },
      "auroc": 0.8683020833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 534,
          "fn": 266,
          "accuracy": 0.6675
        },
        "0.01": {
          "tp": 374,
          "fn": 426,
          "accuracy": 0.4675
        }
      },
      "auroc": 0.8928109374999998
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9933208333333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.768978125
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        }
      },
      "auroc": 0.8811494791666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.743225
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        }
      },
      "auroc": 0.8495614583333332
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        },
        "0.01": {
          "tp": 116,
          "fn": 284,
          "accuracy": 0.29
        }
      },
      "auroc": 0.7963932291666665
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        }
      },
      "auroc": 0.8682729166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 116,
          "fn": 284,
          "accuracy": 0.29
        }
      },
      "auroc": 0.8092697916666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 449,
          "fn": 351,
          "accuracy": 0.56125
        },
        "0.01": {
          "tp": 334,
          "fn": 466,
          "accuracy": 0.4175
        }
      },
      "auroc": 0.8387713541666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        }
      },
      "auroc": 0.9515458333333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.9117291666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        }
      },
      "auroc": 0.9316375
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.9206677083333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        }
      },
      "auroc": 0.8419218749999998
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": {
          "tp": 140,
          "fn": 260,
          "accuracy": 0.35
        }
      },
      "auroc": 0.8812947916666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        }
      },
      "auroc": 0.9361067708333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 236,
          "fn": 164,
          "accuracy": 0.59
        },
        "0.01": {
          "tp": 140,
          "fn": 260,
          "accuracy": 0.35
        }
      },
      "auroc": 0.8768255208333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 549,
          "fn": 251,
          "accuracy": 0.68625
        },
        "0.01": {
          "tp": 344,
          "fn": 456,
          "accuracy": 0.43
        }
      },
      "auroc": 0.9064661458333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.8227885416666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.8227885416666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.820615625
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.820615625
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 93,
          "fn": 307,
          "accuracy": 0.2325
        }
      },
      "auroc": 0.8217020833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 93,
          "fn": 307,
          "accuracy": 0.2325
        }
      },
      "auroc": 0.8217020833333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.671875
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.671875
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.6490447916666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.6490447916666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 318,
          "accuracy": 0.205
        },
        "0.01": {
          "tp": 36,
          "fn": 364,
          "accuracy": 0.09
        }
      },
      "auroc": 0.6604598958333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 318,
          "accuracy": 0.205
        },
        "0.01": {
          "tp": 36,
          "fn": 364,
          "accuracy": 0.09
        }
      },
      "auroc": 0.6604598958333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.9170604166666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.9170604166666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.8944041666666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.8944041666666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 267,
          "fn": 133,
          "accuracy": 0.6675
        },
        "0.01": {
          "tp": 147,
          "fn": 253,
          "accuracy": 0.3675
        }
      },
      "auroc": 0.9057322916666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 267,
          "fn": 133,
          "accuracy": 0.6675
        },
        "0.01": {
          "tp": 147,
          "fn": 253,
          "accuracy": 0.3675
        }
      },
      "auroc": 0.9057322916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.932015625
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.932015625
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.8143947916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.8143947916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 100,
          "fn": 300,
          "accuracy": 0.25
        }
      },
      "auroc": 0.8732052083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 100,
          "fn": 300,
          "accuracy": 0.25
        }
      },
      "auroc": 0.8732052083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.8829864583333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.8829864583333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.8206854166666666
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.8206854166666666
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 158,
          "fn": 242,
          "accuracy": 0.395
        }
      },
      "auroc": 0.8518359375000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 158,
          "fn": 242,
          "accuracy": 0.395
        }
      },
      "auroc": 0.8518359375000001
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1653,
          "fn": 547,
          "accuracy": 0.7513636363636363
        },
        "0.01": {
          "tp": 1252,
          "fn": 948,
          "accuracy": 0.5690909090909091
        }
      },
      "auroc": 0.9154808712121213
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 822,
          "fn": 378,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 515,
          "fn": 685,
          "accuracy": 0.42916666666666664
        }
      },
      "auroc": 0.9021557291666666
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2475,
          "fn": 925,
          "accuracy": 0.7279411764705882
        },
        "0.01": {
          "tp": 1767,
          "fn": 1633,
          "accuracy": 0.5197058823529411
        }
      },
      "auroc": 0.9107778799019608
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1184,
          "fn": 1016,
          "accuracy": 0.5381818181818182
        },
        "0.01": {
          "tp": 629,
          "fn": 1571,
          "accuracy": 0.2859090909090909
        }
      },
      "auroc": 0.8384092803030303
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 736,
          "fn": 464,
          "accuracy": 0.6133333333333333
        },
        "0.01": {
          "tp": 450,
          "fn": 750,
          "accuracy": 0.375
        }
      },
      "auroc": 0.8734776041666666
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1920,
          "fn": 1480,
          "accuracy": 0.5647058823529412
        },
        "0.01": {
          "tp": 1079,
          "fn": 2321,
          "accuracy": 0.3173529411764706
        }
      },
      "auroc": 0.8507863357843137
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2837,
          "fn": 1563,
          "accuracy": 0.6447727272727273
        },
        "0.01": {
          "tp": 1881,
          "fn": 2519,
          "accuracy": 0.4275
        }
      },
      "auroc": 0.8769450757575757
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1558,
          "fn": 842,
          "accuracy": 0.6491666666666667
        },
        "0.01": {
          "tp": 965,
          "fn": 1435,
          "accuracy": 0.40208333333333335
        }
      },
      "auroc": 0.8878166666666668
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 4395,
          "fn": 2405,
          "accuracy": 0.6463235294117647
        },
        "0.01": {
          "tp": 2846,
          "fn": 3954,
          "accuracy": 0.41852941176470587
        }
      },
      "auroc": 0.8807821078431374
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9985937500000001
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9926010416666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9955973958333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9964885416666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9784541666666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        }
      },
      "auroc": 0.9874713541666668
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9975411458333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9855276041666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 774,
          "fn": 26,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 706,
          "fn": 94,
          "accuracy": 0.8825
        }
      },
      "auroc": 0.991534375
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.8824218749999999
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        }
      },
      "auroc": 0.9412109375
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        }
      },
      "auroc": 0.9412109375
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 736,
          "fn": 64,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 697,
          "fn": 103,
          "accuracy": 0.87125
        }
      },
      "auroc": 0.97060546875
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9824885416666665
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9978635416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9901760416666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9826239583333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9994739583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9910489583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        }
      },
      "auroc": 0.98255625
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.99866875
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 778,
          "fn": 22,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 724,
          "fn": 76,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9906125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.99618125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9980906249999999
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.8585208333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9899010416666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9242109374999999
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9292604166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9930411458333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 705,
          "fn": 95,
          "accuracy": 0.88125
        },
        "0.01": {
          "tp": 641,
          "fn": 159,
          "accuracy": 0.80125
        }
      },
      "auroc": 0.96115078125
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9994229166666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9997114583333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.9049510416666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999989583333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9524750000000001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9524755208333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9997109375000001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 748,
          "fn": 52,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 732,
          "fn": 68,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9760932291666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.992678125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9805375000000001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9866078125000001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9718145833333335
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9677822916666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        }
      },
      "auroc": 0.9697984374999999
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9822463541666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        }
      },
      "auroc": 0.9741598958333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 738,
          "fn": 62,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 617,
          "fn": 183,
          "accuracy": 0.77125
        }
      },
      "auroc": 0.978203125
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8904260416666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8904260416666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        }
      },
      "auroc": 0.8667489583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        }
      },
      "auroc": 0.8667489583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 153,
          "fn": 247,
          "accuracy": 0.3825
        }
      },
      "auroc": 0.8785875000000001
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 153,
          "fn": 247,
          "accuracy": 0.3825
        }
      },
      "auroc": 0.8785875000000001
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.7136625
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.7136625
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.6758614583333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.6758614583333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 287,
          "accuracy": 0.2825
        },
        "0.01": {
          "tp": 66,
          "fn": 334,
          "accuracy": 0.165
        }
      },
      "auroc": 0.6947619791666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 287,
          "accuracy": 0.2825
        },
        "0.01": {
          "tp": 66,
          "fn": 334,
          "accuracy": 0.165
        }
      },
      "auroc": 0.6947619791666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9876395833333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9876395833333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9767583333333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9767583333333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        }
      },
      "auroc": 0.9821989583333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        }
      },
      "auroc": 0.9821989583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9920375
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9920375
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.8795322916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.8795322916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9357848958333332
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9357848958333332
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.9061166666666668
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.9061166666666668
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.865325
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.865325
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.8857208333333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.8857208333333334
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1912,
          "fn": 288,
          "accuracy": 0.8690909090909091
        },
        "0.01": {
          "tp": 1740,
          "fn": 460,
          "accuracy": 0.7909090909090909
        }
      },
      "auroc": 0.9512402462121212
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1178,
          "fn": 22,
          "accuracy": 0.9816666666666667
        },
        "0.01": {
          "tp": 1109,
          "fn": 91,
          "accuracy": 0.9241666666666667
        }
      },
      "auroc": 0.994434375
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3090,
          "fn": 310,
          "accuracy": 0.9088235294117647
        },
        "0.01": {
          "tp": 2849,
          "fn": 551,
          "accuracy": 0.8379411764705882
        }
      },
      "auroc": 0.9664852328431373
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1572,
          "fn": 628,
          "accuracy": 0.7145454545454546
        },
        "0.01": {
          "tp": 1208,
          "fn": 992,
          "accuracy": 0.5490909090909091
        }
      },
      "auroc": 0.8964588068181818
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1153,
          "fn": 47,
          "accuracy": 0.9608333333333333
        },
        "0.01": {
          "tp": 1075,
          "fn": 125,
          "accuracy": 0.8958333333333334
        }
      },
      "auroc": 0.9892684027777777
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2725,
          "fn": 675,
          "accuracy": 0.8014705882352942
        },
        "0.01": {
          "tp": 2283,
          "fn": 1117,
          "accuracy": 0.6714705882352942
        }
      },
      "auroc": 0.9292151348039215
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3484,
          "fn": 916,
          "accuracy": 0.7918181818181819
        },
        "0.01": {
          "tp": 2948,
          "fn": 1452,
          "accuracy": 0.67
        }
      },
      "auroc": 0.9238495265151515
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2331,
          "fn": 69,
          "accuracy": 0.97125
        },
        "0.01": {
          "tp": 2184,
          "fn": 216,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9918513888888889
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 5815,
          "fn": 985,
          "accuracy": 0.8551470588235294
        },
        "0.01": {
          "tp": 5132,
          "fn": 1668,
          "accuracy": 0.7547058823529412
        }
      },
      "auroc": 0.9478501838235295
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9986197916666668
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9926468749999999
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9956333333333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9968427083333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9786604166666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9877515625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.99773125
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9856536458333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 774,
          "fn": 26,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 707,
          "fn": 93,
          "accuracy": 0.88375
        }
      },
      "auroc": 0.9916924479166668
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.8828104166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        }
      },
      "auroc": 0.9414052083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        }
      },
      "auroc": 0.9414052083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 737,
          "fn": 63,
          "accuracy": 0.92125
        },
        "0.01": {
          "tp": 697,
          "fn": 103,
          "accuracy": 0.87125
        }
      },
      "auroc": 0.9707026041666665
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9826833333333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9978645833333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9902739583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9826885416666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9994739583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9910812499999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9826859375
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9986692708333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 778,
          "fn": 22,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 724,
          "fn": 76,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9906776041666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.996225
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9981125000000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.8599552083333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9899010416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        }
      },
      "auroc": 0.924928125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9299776041666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9930630208333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 705,
          "fn": 95,
          "accuracy": 0.88125
        },
        "0.01": {
          "tp": 641,
          "fn": 159,
          "accuracy": 0.80125
        }
      },
      "auroc": 0.9615203124999999
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9994229166666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9997114583333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.9052427083333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9526213541666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9526213541666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9997114583333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 749,
          "fn": 51,
          "accuracy": 0.93625
        },
        "0.01": {
          "tp": 732,
          "fn": 68,
          "accuracy": 0.915
        }
      },
      "auroc": 0.97616640625
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9927489583333332
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9806458333333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        }
      },
      "auroc": 0.9866973958333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9722708333333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9677812499999999
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        }
      },
      "auroc": 0.9700260416666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        }
      },
      "auroc": 0.9825098958333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        }
      },
      "auroc": 0.9742135416666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 738,
          "fn": 62,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 618,
          "fn": 182,
          "accuracy": 0.7725
        }
      },
      "auroc": 0.97836171875
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8904583333333335
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8904583333333335
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.8671718749999999
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.8671718749999999
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 154,
          "fn": 246,
          "accuracy": 0.385
        }
      },
      "auroc": 0.8788151041666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 154,
          "fn": 246,
          "accuracy": 0.385
        }
      },
      "auroc": 0.8788151041666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.7142697916666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.7142697916666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.6763739583333332
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.6763739583333332
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 67,
          "fn": 333,
          "accuracy": 0.1675
        }
      },
      "auroc": 0.695321875
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 67,
          "fn": 333,
          "accuracy": 0.1675
        }
      },
      "auroc": 0.695321875
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9877979166666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9877979166666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9769156250000001
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9769156250000001
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9823567708333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9823567708333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9920447916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9920447916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.8794916666666668
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.8794916666666668
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9357682291666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9357682291666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.90663125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.90663125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.8657781250000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.8657781250000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        }
      },
      "auroc": 0.8862046875
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        }
      },
      "auroc": 0.8862046875
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1912,
          "fn": 288,
          "accuracy": 0.8690909090909091
        },
        "0.01": {
          "tp": 1743,
          "fn": 457,
          "accuracy": 0.7922727272727272
        }
      },
      "auroc": 0.9513867424242425
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1178,
          "fn": 22,
          "accuracy": 0.9816666666666667
        },
        "0.01": {
          "tp": 1109,
          "fn": 91,
          "accuracy": 0.9241666666666667
        }
      },
      "auroc": 0.9944675347222223
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3090,
          "fn": 310,
          "accuracy": 0.9088235294117647
        },
        "0.01": {
          "tp": 2852,
          "fn": 548,
          "accuracy": 0.8388235294117647
        }
      },
      "auroc": 0.9665917279411765
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1576,
          "fn": 624,
          "accuracy": 0.7163636363636363
        },
        "0.01": {
          "tp": 1211,
          "fn": 989,
          "accuracy": 0.5504545454545454
        }
      },
      "auroc": 0.8968674242424242
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1153,
          "fn": 47,
          "accuracy": 0.9608333333333333
        },
        "0.01": {
          "tp": 1075,
          "fn": 125,
          "accuracy": 0.8958333333333334
        }
      },
      "auroc": 0.9893027777777778
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2729,
          "fn": 671,
          "accuracy": 0.8026470588235294
        },
        "0.01": {
          "tp": 2286,
          "fn": 1114,
          "accuracy": 0.6723529411764706
        }
      },
      "auroc": 0.9294916666666668
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3488,
          "fn": 912,
          "accuracy": 0.7927272727272727
        },
        "0.01": {
          "tp": 2954,
          "fn": 1446,
          "accuracy": 0.6713636363636364
        }
      },
      "auroc": 0.9241270833333335
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2331,
          "fn": 69,
          "accuracy": 0.97125
        },
        "0.01": {
          "tp": 2184,
          "fn": 216,
          "accuracy": 0.91
        }
      },
      "auroc": 0.99188515625
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 5819,
          "fn": 981,
          "accuracy": 0.855735294117647
        },
        "0.01": {
          "tp": 5138,
          "fn": 1662,
          "accuracy": 0.7555882352941177
        }
      },
      "auroc": 0.9480416973039216
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7886114583333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7780697916666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 369,
          "accuracy": 0.0775
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.783340625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.792809375
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7579312500000001
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7753703125
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 52,
          "fn": 348,
          "accuracy": 0.13
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7907104166666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 371,
          "accuracy": 0.0725
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7680005208333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 719,
          "accuracy": 0.10125
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7793554687500001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.9412520833333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9204802083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        },
        "0.01": {
          "tp": 82,
          "fn": 318,
          "accuracy": 0.205
        }
      },
      "auroc": 0.9308661458333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.8035010416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.936546875
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        },
        "0.01": {
          "tp": 42,
          "fn": 358,
          "accuracy": 0.105
        }
      },
      "auroc": 0.8700239583333335
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": {
          "tp": 91,
          "fn": 309,
          "accuracy": 0.2275
        }
      },
      "auroc": 0.8723765625000001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 33,
          "fn": 367,
          "accuracy": 0.0825
        }
      },
      "auroc": 0.9285135416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 475,
          "fn": 325,
          "accuracy": 0.59375
        },
        "0.01": {
          "tp": 124,
          "fn": 676,
          "accuracy": 0.155
        }
      },
      "auroc": 0.9004450520833334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7289729166666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6925864583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 386,
          "accuracy": 0.035
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7107796875
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7291479166666668
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7057760416666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 381,
          "accuracy": 0.0475
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7174619791666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7290604166666668
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6991812500000001
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 767,
          "accuracy": 0.04125
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7141208333333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9818166666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.8431677083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        },
        "0.01": {
          "tp": 152,
          "fn": 248,
          "accuracy": 0.38
        }
      },
      "auroc": 0.9124921874999998
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.8068135416666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.8400125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 313,
          "accuracy": 0.2175
        },
        "0.01": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        }
      },
      "auroc": 0.8234130208333335
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        },
        "0.01": {
          "tp": 152,
          "fn": 248,
          "accuracy": 0.38
        }
      },
      "auroc": 0.8943151041666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 89,
          "fn": 311,
          "accuracy": 0.2225
        },
        "0.01": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        }
      },
      "auroc": 0.8415901041666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 482,
          "accuracy": 0.3975
        },
        "0.01": {
          "tp": 162,
          "fn": 638,
          "accuracy": 0.2025
        }
      },
      "auroc": 0.8679526041666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9977854166666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9940489583333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9959171875
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        }
      },
      "auroc": 0.8756614583333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.9101072916666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        },
        "0.01": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        }
      },
      "auroc": 0.892884375
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        },
        "0.01": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        }
      },
      "auroc": 0.9367234375
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 259,
          "fn": 141,
          "accuracy": 0.6475
        }
      },
      "auroc": 0.9520781250000001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 613,
          "fn": 187,
          "accuracy": 0.76625
        },
        "0.01": {
          "tp": 536,
          "fn": 264,
          "accuracy": 0.67
        }
      },
      "auroc": 0.94440078125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.81789375
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7662989583333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 332,
          "accuracy": 0.17
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.7920963541666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.7716645833333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.7498583333333332
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 358,
          "accuracy": 0.105
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.7607614583333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 321,
          "accuracy": 0.1975
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.7947791666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 369,
          "accuracy": 0.0775
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.7580786458333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 690,
          "accuracy": 0.1375
        },
        "0.01": {
          "tp": 3,
          "fn": 797,
          "accuracy": 0.00375
        }
      },
      "auroc": 0.77642890625
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.8461541666666668
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.8461541666666668
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.80853125
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.80853125
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 11,
          "fn": 389,
          "accuracy": 0.0275
        }
      },
      "auroc": 0.8273427083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 11,
          "fn": 389,
          "accuracy": 0.0275
        }
      },
      "auroc": 0.8273427083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7215750000000001
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7215750000000001
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7368291666666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7368291666666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 24,
          "fn": 376,
          "accuracy": 0.06
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7292020833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 24,
          "fn": 376,
          "accuracy": 0.06
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7292020833333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7405593749999999
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7405593749999999
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7247802083333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7247802083333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 371,
          "accuracy": 0.0725
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7326697916666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 371,
          "accuracy": 0.0725
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7326697916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6337072916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6337072916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5750802083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5750802083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6043937500000001
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6043937500000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.7932031250000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.7932031250000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.7817125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.7817125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 332,
          "accuracy": 0.17
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.7874578125000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 332,
          "accuracy": 0.17
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.7874578125000001
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 761,
          "fn": 1439,
          "accuracy": 0.3459090909090909
        },
        "0.01": {
          "tp": 437,
          "fn": 1763,
          "accuracy": 0.19863636363636364
        }
      },
      "auroc": 0.8174119318181817
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 802,
          "accuracy": 0.33166666666666667
        },
        "0.01": {
          "tp": 198,
          "fn": 1002,
          "accuracy": 0.165
        }
      },
      "auroc": 0.8324420138888889
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1159,
          "fn": 2241,
          "accuracy": 0.34088235294117647
        },
        "0.01": {
          "tp": 635,
          "fn": 2765,
          "accuracy": 0.18676470588235294
        }
      },
      "auroc": 0.8227166666666668
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 1834,
          "accuracy": 0.16636363636363635
        },
        "0.01": {
          "tp": 99,
          "fn": 2101,
          "accuracy": 0.045
        }
      },
      "auroc": 0.7642301136363636
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 859,
          "accuracy": 0.2841666666666667
        },
        "0.01": {
          "tp": 105,
          "fn": 1095,
          "accuracy": 0.0875
        }
      },
      "auroc": 0.8167053819444445
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 707,
          "fn": 2693,
          "accuracy": 0.20794117647058824
        },
        "0.01": {
          "tp": 204,
          "fn": 3196,
          "accuracy": 0.06
        }
      },
      "auroc": 0.7827507965686274
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1127,
          "fn": 3273,
          "accuracy": 0.25613636363636366
        },
        "0.01": {
          "tp": 536,
          "fn": 3864,
          "accuracy": 0.12181818181818181
        }
      },
      "auroc": 0.7908210227272727
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 739,
          "fn": 1661,
          "accuracy": 0.30791666666666667
        },
        "0.01": {
          "tp": 303,
          "fn": 2097,
          "accuracy": 0.12625
        }
      },
      "auroc": 0.8245736979166666
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1866,
          "fn": 4934,
          "accuracy": 0.27441176470588236
        },
        "0.01": {
          "tp": 839,
          "fn": 5961,
          "accuracy": 0.12338235294117647
        }
      },
      "auroc": 0.8027337316176472
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.9876104166666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.974859375
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        }
      },
      "auroc": 0.9812348958333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.9804562500000001
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9510208333333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        }
      },
      "auroc": 0.9657385416666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        }
      },
      "auroc": 0.9840333333333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.9629401041666668
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 719,
          "fn": 81,
          "accuracy": 0.89875
        },
        "0.01": {
          "tp": 526,
          "fn": 274,
          "accuracy": 0.6575
        }
      },
      "auroc": 0.97348671875
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999979166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999989583333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        }
      },
      "auroc": 0.8957791666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        }
      },
      "auroc": 0.9478895833333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        }
      },
      "auroc": 0.9478885416666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 738,
          "fn": 62,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 687,
          "fn": 113,
          "accuracy": 0.85875
        }
      },
      "auroc": 0.9739442708333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        }
      },
      "auroc": 0.9637395833333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9958354166666668
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        }
      },
      "auroc": 0.9797875
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        }
      },
      "auroc": 0.9635197916666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9992833333333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        }
      },
      "auroc": 0.9814015625
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        }
      },
      "auroc": 0.9636296875
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.997559375
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 756,
          "fn": 44,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 619,
          "fn": 181,
          "accuracy": 0.77375
        }
      },
      "auroc": 0.98059453125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9863052083333332
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        }
      },
      "auroc": 0.9931526041666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        }
      },
      "auroc": 0.8715020833333332
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.997546875
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        }
      },
      "auroc": 0.9345244791666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9357510416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9919260416666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 694,
          "fn": 106,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 618,
          "fn": 182,
          "accuracy": 0.7725
        }
      },
      "auroc": 0.9638385416666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.99938125
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.999690625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        }
      },
      "auroc": 0.915253125
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        }
      },
      "auroc": 0.9576265625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        }
      },
      "auroc": 0.9576265625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9996906249999999
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 748,
          "fn": 52,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 733,
          "fn": 67,
          "accuracy": 0.91625
        }
      },
      "auroc": 0.97865859375
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.9681947916666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9570947916666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        }
      },
      "auroc": 0.9626447916666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        }
      },
      "auroc": 0.9363291666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9672833333333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        }
      },
      "auroc": 0.9518062500000001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        }
      },
      "auroc": 0.9522619791666668
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        }
      },
      "auroc": 0.9621890625
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 669,
          "fn": 131,
          "accuracy": 0.83625
        },
        "0.01": {
          "tp": 489,
          "fn": 311,
          "accuracy": 0.61125
        }
      },
      "auroc": 0.9572255208333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.8354041666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.8354041666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.8212135416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.8212135416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 74,
          "fn": 326,
          "accuracy": 0.185
        }
      },
      "auroc": 0.8283088541666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 74,
          "fn": 326,
          "accuracy": 0.185
        }
      },
      "auroc": 0.8283088541666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.705225
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.705225
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.6817104166666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.6817104166666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 315,
          "accuracy": 0.2125
        },
        "0.01": {
          "tp": 44,
          "fn": 356,
          "accuracy": 0.11
        }
      },
      "auroc": 0.6934677083333334
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 315,
          "accuracy": 0.2125
        },
        "0.01": {
          "tp": 44,
          "fn": 356,
          "accuracy": 0.11
        }
      },
      "auroc": 0.6934677083333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.9540947916666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.9540947916666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.933209375
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.933209375
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        }
      },
      "auroc": 0.9436520833333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        }
      },
      "auroc": 0.9436520833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.9723489583333335
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.9723489583333335
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.8481375000000001
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.8481375000000001
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        },
        "0.01": {
          "tp": 156,
          "fn": 244,
          "accuracy": 0.39
        }
      },
      "auroc": 0.9102432291666668
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        },
        "0.01": {
          "tp": 156,
          "fn": 244,
          "accuracy": 0.39
        }
      },
      "auroc": 0.9102432291666668
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.8928083333333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.8928083333333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        }
      },
      "auroc": 0.8494010416666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        }
      },
      "auroc": 0.8494010416666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        }
      },
      "auroc": 0.8711046875
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        }
      },
      "auroc": 0.8711046875
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1769,
          "fn": 431,
          "accuracy": 0.8040909090909091
        },
        "0.01": {
          "tp": 1372,
          "fn": 828,
          "accuracy": 0.6236363636363637
        }
      },
      "auroc": 0.9344930871212122
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1133,
          "fn": 67,
          "accuracy": 0.9441666666666667
        },
        "0.01": {
          "tp": 1003,
          "fn": 197,
          "accuracy": 0.8358333333333333
        }
      },
      "auroc": 0.9855793402777778
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2902,
          "fn": 498,
          "accuracy": 0.8535294117647059
        },
        "0.01": {
          "tp": 2375,
          "fn": 1025,
          "accuracy": 0.6985294117647058
        }
      },
      "auroc": 0.9525235294117647
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1392,
          "fn": 808,
          "accuracy": 0.6327272727272727
        },
        "0.01": {
          "tp": 883,
          "fn": 1317,
          "accuracy": 0.40136363636363637
        }
      },
      "auroc": 0.8815010416666668
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1136,
          "fn": 64,
          "accuracy": 0.9466666666666667
        },
        "0.01": {
          "tp": 1037,
          "fn": 163,
          "accuracy": 0.8641666666666666
        }
      },
      "auroc": 0.9858557291666666
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2528,
          "fn": 872,
          "accuracy": 0.7435294117647059
        },
        "0.01": {
          "tp": 1920,
          "fn": 1480,
          "accuracy": 0.5647058823529412
        }
      },
      "auroc": 0.9183321078431372
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3161,
          "fn": 1239,
          "accuracy": 0.7184090909090909
        },
        "0.01": {
          "tp": 2255,
          "fn": 2145,
          "accuracy": 0.5125
        }
      },
      "auroc": 0.9079970643939393
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2269,
          "fn": 131,
          "accuracy": 0.9454166666666667
        },
        "0.01": {
          "tp": 2040,
          "fn": 360,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9857175347222222
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 5430,
          "fn": 1370,
          "accuracy": 0.7985294117647059
        },
        "0.01": {
          "tp": 4295,
          "fn": 2505,
          "accuracy": 0.6316176470588235
        }
      },
      "auroc": 0.935427818627451
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9976052083333332
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9899145833333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        }
      },
      "auroc": 0.9937598958333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9954
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9722187500000001
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        }
      },
      "auroc": 0.9838093749999999
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.9965026041666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        }
      },
      "auroc": 0.9810666666666668
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 763,
          "fn": 37,
          "accuracy": 0.95375
        },
        "0.01": {
          "tp": 682,
          "fn": 118,
          "accuracy": 0.8525
        }
      },
      "auroc": 0.9887846354166668
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        }
      },
      "auroc": 0.8660895833333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 295,
          "fn": 105,
          "accuracy": 0.7375
        }
      },
      "auroc": 0.9330447916666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 295,
          "fn": 105,
          "accuracy": 0.7375
        }
      },
      "auroc": 0.9330447916666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 728,
          "fn": 72,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 695,
          "fn": 105,
          "accuracy": 0.86875
        }
      },
      "auroc": 0.9665223958333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9790760416666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.99669375
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9878848958333335
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9804822916666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9988927083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9896874999999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9797791666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9977932291666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 771,
          "fn": 29,
          "accuracy": 0.96375
        },
        "0.01": {
          "tp": 708,
          "fn": 92,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9887861979166667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9945770833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9972885416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.8374895833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9871166666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        }
      },
      "auroc": 0.912303125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        }
      },
      "auroc": 0.9187447916666668
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        }
      },
      "auroc": 0.990846875
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 698,
          "fn": 102,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 634,
          "fn": 166,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9547958333333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9993979166666668
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9996989583333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.8944031250000001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        }
      },
      "auroc": 0.9472015625000001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        }
      },
      "auroc": 0.9472015625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9996989583333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 744,
          "fn": 56,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 731,
          "fn": 69,
          "accuracy": 0.91375
        }
      },
      "auroc": 0.9734502604166666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.990784375
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.97664375
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        }
      },
      "auroc": 0.9837140625
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.965078125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        }
      },
      "auroc": 0.9588791666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        }
      },
      "auroc": 0.9619786458333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        }
      },
      "auroc": 0.9779312500000001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 291,
          "fn": 109,
          "accuracy": 0.7275
        }
      },
      "auroc": 0.9677614583333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 722,
          "fn": 78,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 598,
          "fn": 202,
          "accuracy": 0.7475
        }
      },
      "auroc": 0.9728463541666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.8821333333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.8821333333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        }
      },
      "auroc": 0.8556385416666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        }
      },
      "auroc": 0.8556385416666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": {
          "tp": 149,
          "fn": 251,
          "accuracy": 0.3725
        }
      },
      "auroc": 0.8688859375
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": {
          "tp": 149,
          "fn": 251,
          "accuracy": 0.3725
        }
      },
      "auroc": 0.8688859375
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.689059375
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.689059375
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.6520791666666668
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.6520791666666668
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 295,
          "accuracy": 0.2625
        },
        "0.01": {
          "tp": 58,
          "fn": 342,
          "accuracy": 0.145
        }
      },
      "auroc": 0.6705692708333334
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 295,
          "accuracy": 0.2625
        },
        "0.01": {
          "tp": 58,
          "fn": 342,
          "accuracy": 0.145
        }
      },
      "auroc": 0.6705692708333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9855114583333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9855114583333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9709729166666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9709729166666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9782421875
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9782421875
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9888947916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9888947916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.8597114583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.8597114583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        }
      },
      "auroc": 0.9243031250000001
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        }
      },
      "auroc": 0.9243031250000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        }
      },
      "auroc": 0.8969843750000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        }
      },
      "auroc": 0.8969843750000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.8514406250000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.8514406250000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        },
        "0.01": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        }
      },
      "auroc": 0.8742125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        },
        "0.01": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        }
      },
      "auroc": 0.8742125
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1889,
          "fn": 311,
          "accuracy": 0.8586363636363636
        },
        "0.01": {
          "tp": 1706,
          "fn": 494,
          "accuracy": 0.7754545454545455
        }
      },
      "auroc": 0.9463680871212122
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1167,
          "fn": 33,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 1086,
          "fn": 114,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9928711805555556
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3056,
          "fn": 344,
          "accuracy": 0.8988235294117647
        },
        "0.01": {
          "tp": 2792,
          "fn": 608,
          "accuracy": 0.8211764705882353
        }
      },
      "auroc": 0.962780943627451
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1523,
          "fn": 677,
          "accuracy": 0.6922727272727273
        },
        "0.01": {
          "tp": 1157,
          "fn": 1043,
          "accuracy": 0.5259090909090909
        }
      },
      "auroc": 0.8844350378787879
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1144,
          "fn": 56,
          "accuracy": 0.9533333333333334
        },
        "0.01": {
          "tp": 1060,
          "fn": 140,
          "accuracy": 0.8833333333333333
        }
      },
      "auroc": 0.986184548611111
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2667,
          "fn": 733,
          "accuracy": 0.7844117647058824
        },
        "0.01": {
          "tp": 2217,
          "fn": 1183,
          "accuracy": 0.6520588235294118
        }
      },
      "auroc": 0.9203466299019607
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3412,
          "fn": 988,
          "accuracy": 0.7754545454545455
        },
        "0.01": {
          "tp": 2863,
          "fn": 1537,
          "accuracy": 0.6506818181818181
        }
      },
      "auroc": 0.9154015624999999
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2311,
          "fn": 89,
          "accuracy": 0.9629166666666666
        },
        "0.01": {
          "tp": 2146,
          "fn": 254,
          "accuracy": 0.8941666666666667
        }
      },
      "auroc": 0.9895278645833335
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 5723,
          "fn": 1077,
          "accuracy": 0.8416176470588236
        },
        "0.01": {
          "tp": 5009,
          "fn": 1791,
          "accuracy": 0.7366176470588235
        }
      },
      "auroc": 0.9415637867647059
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9171822916666665
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9106583333333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9139203124999999
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9172989583333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9086624999999999
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.9129807291666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.9172406249999999
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9096604166666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 457,
          "fn": 343,
          "accuracy": 0.57125
        },
        "0.01": {
          "tp": 5,
          "fn": 795,
          "accuracy": 0.00625
        }
      },
      "auroc": 0.9134505208333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.9562385416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9627
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 73,
          "fn": 327,
          "accuracy": 0.1825
        }
      },
      "auroc": 0.9594692708333332
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.9124260416666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.9704114583333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 301,
          "fn": 99,
          "accuracy": 0.7525
        },
        "0.01": {
          "tp": 33,
          "fn": 367,
          "accuracy": 0.0825
        }
      },
      "auroc": 0.94141875
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        },
        "0.01": {
          "tp": 88,
          "fn": 312,
          "accuracy": 0.22
        }
      },
      "auroc": 0.9343322916666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        }
      },
      "auroc": 0.9665557291666668
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 659,
          "fn": 141,
          "accuracy": 0.82375
        },
        "0.01": {
          "tp": 106,
          "fn": 694,
          "accuracy": 0.1325
        }
      },
      "auroc": 0.9504440104166667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.8968666666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.874178125
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 249,
          "accuracy": 0.3775
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.8855223958333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.8950625
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8829260416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 238,
          "accuracy": 0.405
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.8889942708333335
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.8959645833333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 272,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8785520833333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 487,
          "accuracy": 0.39125
        },
        "0.01": {
          "tp": 2,
          "fn": 798,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.8872583333333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        }
      },
      "auroc": 0.9815562499999999
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.9006166666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 127,
          "fn": 273,
          "accuracy": 0.3175
        }
      },
      "auroc": 0.9410864583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.9162489583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.9283260416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 13,
          "fn": 387,
          "accuracy": 0.0325
        }
      },
      "auroc": 0.9222874999999999
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": {
          "tp": 126,
          "fn": 274,
          "accuracy": 0.315
        }
      },
      "auroc": 0.9489026041666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": {
          "tp": 14,
          "fn": 386,
          "accuracy": 0.035
        }
      },
      "auroc": 0.9144713541666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 552,
          "fn": 248,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 140,
          "fn": 660,
          "accuracy": 0.175
        }
      },
      "auroc": 0.9316869791666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.998178125
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9969822916666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9975802083333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.9493947916666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        }
      },
      "auroc": 0.9590885416666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        }
      },
      "auroc": 0.9542416666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        }
      },
      "auroc": 0.9737864583333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        }
      },
      "auroc": 0.9780354166666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 710,
          "fn": 90,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 534,
          "fn": 266,
          "accuracy": 0.6675
        }
      },
      "auroc": 0.9759109375
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.9208208333333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.88490625
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.9028635416666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.904859375
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.8884729166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.8966661458333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        }
      },
      "auroc": 0.9128401041666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 255,
          "accuracy": 0.3625
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.8866895833333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 425,
          "accuracy": 0.46875
        },
        "0.01": {
          "tp": 6,
          "fn": 794,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.89976484375
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.9212333333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.9212333333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9048052083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9048052083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        },
        "0.01": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        }
      },
      "auroc": 0.9130192708333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        },
        "0.01": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        }
      },
      "auroc": 0.9130192708333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8995760416666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8995760416666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.902684375
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.902684375
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9011302083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9011302083333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9094
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9094
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9011333333333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9011333333333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9052666666666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.9052666666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.861940625
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.861940625
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8683083333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8683083333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 320,
          "accuracy": 0.2
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8651244791666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 320,
          "accuracy": 0.2
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8651244791666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9173000000000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.9173000000000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.9188083333333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.9188083333333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        },
        "0.01": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        }
      },
      "auroc": 0.9180541666666666
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        },
        "0.01": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        }
      },
      "auroc": 0.9180541666666666
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1370,
          "fn": 830,
          "accuracy": 0.6227272727272727
        },
        "0.01": {
          "tp": 407,
          "fn": 1793,
          "accuracy": 0.185
        }
      },
      "auroc": 0.9254811553030302
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 725,
          "fn": 475,
          "accuracy": 0.6041666666666666
        },
        "0.01": {
          "tp": 201,
          "fn": 999,
          "accuracy": 0.1675
        }
      },
      "auroc": 0.921673611111111
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2095,
          "fn": 1305,
          "accuracy": 0.6161764705882353
        },
        "0.01": {
          "tp": 608,
          "fn": 2792,
          "accuracy": 0.17882352941176471
        }
      },
      "auroc": 0.9241373161764704
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1163,
          "fn": 1037,
          "accuracy": 0.5286363636363637
        },
        "0.01": {
          "tp": 122,
          "fn": 2078,
          "accuracy": 0.05545454545454546
        }
      },
      "auroc": 0.9082754734848486
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 761,
          "fn": 439,
          "accuracy": 0.6341666666666667
        },
        "0.01": {
          "tp": 91,
          "fn": 1109,
          "accuracy": 0.07583333333333334
        }
      },
      "auroc": 0.92298125
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1924,
          "fn": 1476,
          "accuracy": 0.5658823529411765
        },
        "0.01": {
          "tp": 213,
          "fn": 3187,
          "accuracy": 0.06264705882352942
        }
      },
      "auroc": 0.9134657475490197
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2533,
          "fn": 1867,
          "accuracy": 0.5756818181818182
        },
        "0.01": {
          "tp": 529,
          "fn": 3871,
          "accuracy": 0.12022727272727272
        }
      },
      "auroc": 0.9168783143939394
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1486,
          "fn": 914,
          "accuracy": 0.6191666666666666
        },
        "0.01": {
          "tp": 292,
          "fn": 2108,
          "accuracy": 0.12166666666666667
        }
      },
      "auroc": 0.9223274305555555
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4019,
          "fn": 2781,
          "accuracy": 0.5910294117647059
        },
        "0.01": {
          "tp": 821,
          "fn": 5979,
          "accuracy": 0.12073529411764707
        }
      },
      "auroc": 0.9188015318627452
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2078,
          "fn": 322,
          "accuracy": 0.8658333333333333
        },
        "0.01": {
          "tp": 1746,
          "fn": 654,
          "accuracy": 0.7275
        }
      },
      "auroc": 0.9683944444444443
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1970,
          "fn": 430,
          "accuracy": 0.8208333333333333
        },
        "0.01": {
          "tp": 1541,
          "fn": 859,
          "accuracy": 0.6420833333333333
        }
      },
      "auroc": 0.9572374131944444
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4048,
          "fn": 752,
          "accuracy": 0.8433333333333334
        },
        "0.01": {
          "tp": 3287,
          "fn": 1513,
          "accuracy": 0.6847916666666667
        }
      },
      "auroc": 0.9628159288194443
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2076,
          "fn": 324,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 1664,
          "fn": 736,
          "accuracy": 0.6933333333333334
        }
      },
      "auroc": 0.9656237847222223
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1810,
          "fn": 590,
          "accuracy": 0.7541666666666667
        },
        "0.01": {
          "tp": 1290,
          "fn": 1110,
          "accuracy": 0.5375
        }
      },
      "auroc": 0.9394188368055555
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3886,
          "fn": 914,
          "accuracy": 0.8095833333333333
        },
        "0.01": {
          "tp": 2954,
          "fn": 1846,
          "accuracy": 0.6154166666666666
        }
      },
      "auroc": 0.952521310763889
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4154,
          "fn": 646,
          "accuracy": 0.8654166666666666
        },
        "0.01": {
          "tp": 3410,
          "fn": 1390,
          "accuracy": 0.7104166666666667
        }
      },
      "auroc": 0.9670091145833334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3780,
          "fn": 1020,
          "accuracy": 0.7875
        },
        "0.01": {
          "tp": 2831,
          "fn": 1969,
          "accuracy": 0.5897916666666667
        }
      },
      "auroc": 0.9483281250000001
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7934,
          "fn": 1666,
          "accuracy": 0.8264583333333333
        },
        "0.01": {
          "tp": 6241,
          "fn": 3359,
          "accuracy": 0.6501041666666667
        }
      },
      "auroc": 0.9576686197916665
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2312,
          "fn": 88,
          "accuracy": 0.9633333333333334
        },
        "0.01": {
          "tp": 2146,
          "fn": 254,
          "accuracy": 0.8941666666666667
        }
      },
      "auroc": 0.9913524305555556
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2305,
          "fn": 95,
          "accuracy": 0.9604166666666667
        },
        "0.01": {
          "tp": 1941,
          "fn": 459,
          "accuracy": 0.80875
        }
      },
      "auroc": 0.9878216145833333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4617,
          "fn": 183,
          "accuracy": 0.961875
        },
        "0.01": {
          "tp": 4087,
          "fn": 713,
          "accuracy": 0.8514583333333333
        }
      },
      "auroc": 0.9895870225694444
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1454,
          "fn": 946,
          "accuracy": 0.6058333333333333
        },
        "0.01": {
          "tp": 916,
          "fn": 1484,
          "accuracy": 0.38166666666666665
        }
      },
      "auroc": 0.8672973958333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2322,
          "fn": 78,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 2009,
          "fn": 391,
          "accuracy": 0.8370833333333333
        }
      },
      "auroc": 0.9905903645833334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3776,
          "fn": 1024,
          "accuracy": 0.7866666666666666
        },
        "0.01": {
          "tp": 2925,
          "fn": 1875,
          "accuracy": 0.609375
        }
      },
      "auroc": 0.9289438802083333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3766,
          "fn": 1034,
          "accuracy": 0.7845833333333333
        },
        "0.01": {
          "tp": 3062,
          "fn": 1738,
          "accuracy": 0.6379166666666667
        }
      },
      "auroc": 0.9293249131944443
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4627,
          "fn": 173,
          "accuracy": 0.9639583333333334
        },
        "0.01": {
          "tp": 3950,
          "fn": 850,
          "accuracy": 0.8229166666666666
        }
      },
      "auroc": 0.9892059895833334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8393,
          "fn": 1207,
          "accuracy": 0.8742708333333333
        },
        "0.01": {
          "tp": 7012,
          "fn": 2588,
          "accuracy": 0.7304166666666667
        }
      },
      "auroc": 0.9592654513888889
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1897,
          "fn": 503,
          "accuracy": 0.7904166666666667
        },
        "0.01": {
          "tp": 1485,
          "fn": 915,
          "accuracy": 0.61875
        }
      },
      "auroc": 0.9426006944444446
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1964,
          "fn": 436,
          "accuracy": 0.8183333333333334
        },
        "0.01": {
          "tp": 1720,
          "fn": 680,
          "accuracy": 0.7166666666666667
        }
      },
      "auroc": 0.9509762152777779
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3861,
          "fn": 939,
          "accuracy": 0.804375
        },
        "0.01": {
          "tp": 3205,
          "fn": 1595,
          "accuracy": 0.6677083333333333
        }
      },
      "auroc": 0.9467884548611112
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1916,
          "fn": 484,
          "accuracy": 0.7983333333333333
        },
        "0.01": {
          "tp": 1458,
          "fn": 942,
          "accuracy": 0.6075
        }
      },
      "auroc": 0.942992013888889
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1985,
          "fn": 415,
          "accuracy": 0.8270833333333333
        },
        "0.01": {
          "tp": 1805,
          "fn": 595,
          "accuracy": 0.7520833333333333
        }
      },
      "auroc": 0.9536207465277778
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3901,
          "fn": 899,
          "accuracy": 0.8127083333333334
        },
        "0.01": {
          "tp": 3263,
          "fn": 1537,
          "accuracy": 0.6797916666666667
        }
      },
      "auroc": 0.9483063802083334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3813,
          "fn": 987,
          "accuracy": 0.794375
        },
        "0.01": {
          "tp": 2943,
          "fn": 1857,
          "accuracy": 0.613125
        }
      },
      "auroc": 0.9427963541666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3949,
          "fn": 851,
          "accuracy": 0.8227083333333334
        },
        "0.01": {
          "tp": 3525,
          "fn": 1275,
          "accuracy": 0.734375
        }
      },
      "auroc": 0.9522984809027779
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7762,
          "fn": 1838,
          "accuracy": 0.8085416666666667
        },
        "0.01": {
          "tp": 6468,
          "fn": 3132,
          "accuracy": 0.67375
        }
      },
      "auroc": 0.9475474175347223
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2378,
          "fn": 22,
          "accuracy": 0.9908333333333333
        },
        "0.01": {
          "tp": 2268,
          "fn": 132,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9969276041666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2044,
          "fn": 356,
          "accuracy": 0.8516666666666667
        },
        "0.01": {
          "tp": 1688,
          "fn": 712,
          "accuracy": 0.7033333333333334
        }
      },
      "auroc": 0.9681802951388888
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4422,
          "fn": 378,
          "accuracy": 0.92125
        },
        "0.01": {
          "tp": 3956,
          "fn": 844,
          "accuracy": 0.8241666666666667
        }
      },
      "auroc": 0.9825539496527776
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1234,
          "fn": 1166,
          "accuracy": 0.5141666666666667
        },
        "0.01": {
          "tp": 647,
          "fn": 1753,
          "accuracy": 0.26958333333333334
        }
      },
      "auroc": 0.8485311631944444
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2020,
          "fn": 380,
          "accuracy": 0.8416666666666667
        },
        "0.01": {
          "tp": 1749,
          "fn": 651,
          "accuracy": 0.72875
        }
      },
      "auroc": 0.9567543402777778
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3254,
          "fn": 1546,
          "accuracy": 0.6779166666666666
        },
        "0.01": {
          "tp": 2396,
          "fn": 2404,
          "accuracy": 0.49916666666666665
        }
      },
      "auroc": 0.9026427517361111
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3612,
          "fn": 1188,
          "accuracy": 0.7525
        },
        "0.01": {
          "tp": 2915,
          "fn": 1885,
          "accuracy": 0.6072916666666667
        }
      },
      "auroc": 0.9227293836805556
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4064,
          "fn": 736,
          "accuracy": 0.8466666666666667
        },
        "0.01": {
          "tp": 3437,
          "fn": 1363,
          "accuracy": 0.7160416666666667
        }
      },
      "auroc": 0.9624673177083332
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7676,
          "fn": 1924,
          "accuracy": 0.7995833333333333
        },
        "0.01": {
          "tp": 6352,
          "fn": 3248,
          "accuracy": 0.6616666666666666
        }
      },
      "auroc": 0.9425983506944445
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2390,
          "fn": 10,
          "accuracy": 0.9958333333333333
        },
        "0.01": {
          "tp": 2373,
          "fn": 27,
          "accuracy": 0.98875
        }
      },
      "auroc": 0.99910703125
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2255,
          "fn": 145,
          "accuracy": 0.9395833333333333
        },
        "0.01": {
          "tp": 2200,
          "fn": 200,
          "accuracy": 0.9166666666666666
        }
      },
      "auroc": 0.9795157986111112
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4645,
          "fn": 155,
          "accuracy": 0.9677083333333333
        },
        "0.01": {
          "tp": 4573,
          "fn": 227,
          "accuracy": 0.9527083333333334
        }
      },
      "auroc": 0.9893114149305555
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1645,
          "fn": 755,
          "accuracy": 0.6854166666666667
        },
        "0.01": {
          "tp": 1398,
          "fn": 1002,
          "accuracy": 0.5825
        }
      },
      "auroc": 0.8892649305555554
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2204,
          "fn": 196,
          "accuracy": 0.9183333333333333
        },
        "0.01": {
          "tp": 2013,
          "fn": 387,
          "accuracy": 0.83875
        }
      },
      "auroc": 0.9765628472222222
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3849,
          "fn": 951,
          "accuracy": 0.801875
        },
        "0.01": {
          "tp": 3411,
          "fn": 1389,
          "accuracy": 0.710625
        }
      },
      "auroc": 0.9329138888888888
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4035,
          "fn": 765,
          "accuracy": 0.840625
        },
        "0.01": {
          "tp": 3771,
          "fn": 1029,
          "accuracy": 0.785625
        }
      },
      "auroc": 0.9441859809027778
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4459,
          "fn": 341,
          "accuracy": 0.9289583333333333
        },
        "0.01": {
          "tp": 4213,
          "fn": 587,
          "accuracy": 0.8777083333333333
        }
      },
      "auroc": 0.9780393229166666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8494,
          "fn": 1106,
          "accuracy": 0.8847916666666666
        },
        "0.01": {
          "tp": 7984,
          "fn": 1616,
          "accuracy": 0.8316666666666667
        }
      },
      "auroc": 0.9611126519097223
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2020,
          "fn": 380,
          "accuracy": 0.8416666666666667
        },
        "0.01": {
          "tp": 1587,
          "fn": 813,
          "accuracy": 0.66125
        }
      },
      "auroc": 0.9614055555555555
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1834,
          "fn": 566,
          "accuracy": 0.7641666666666667
        },
        "0.01": {
          "tp": 1431,
          "fn": 969,
          "accuracy": 0.59625
        }
      },
      "auroc": 0.9412605902777778
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3854,
          "fn": 946,
          "accuracy": 0.8029166666666666
        },
        "0.01": {
          "tp": 3018,
          "fn": 1782,
          "accuracy": 0.62875
        }
      },
      "auroc": 0.9513330729166666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1785,
          "fn": 615,
          "accuracy": 0.74375
        },
        "0.01": {
          "tp": 1214,
          "fn": 1186,
          "accuracy": 0.5058333333333334
        }
      },
      "auroc": 0.9331274305555557
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1758,
          "fn": 642,
          "accuracy": 0.7325
        },
        "0.01": {
          "tp": 1249,
          "fn": 1151,
          "accuracy": 0.5204166666666666
        }
      },
      "auroc": 0.9276403645833334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3543,
          "fn": 1257,
          "accuracy": 0.738125
        },
        "0.01": {
          "tp": 2463,
          "fn": 2337,
          "accuracy": 0.513125
        }
      },
      "auroc": 0.9303838975694444
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3805,
          "fn": 995,
          "accuracy": 0.7927083333333333
        },
        "0.01": {
          "tp": 2801,
          "fn": 1999,
          "accuracy": 0.5835416666666666
        }
      },
      "auroc": 0.9472664930555555
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3592,
          "fn": 1208,
          "accuracy": 0.7483333333333333
        },
        "0.01": {
          "tp": 2680,
          "fn": 2120,
          "accuracy": 0.5583333333333333
        }
      },
      "auroc": 0.9344504774305555
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7397,
          "fn": 2203,
          "accuracy": 0.7705208333333333
        },
        "0.01": {
          "tp": 5481,
          "fn": 4119,
          "accuracy": 0.5709375
        }
      },
      "auroc": 0.9408584852430555
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1379,
          "fn": 1021,
          "accuracy": 0.5745833333333333
        },
        "0.01": {
          "tp": 747,
          "fn": 1653,
          "accuracy": 0.31125
        }
      },
      "auroc": 0.8706243055555556
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1379,
          "fn": 1021,
          "accuracy": 0.5745833333333333
        },
        "0.01": {
          "tp": 747,
          "fn": 1653,
          "accuracy": 0.31125
        }
      },
      "auroc": 0.8706243055555556
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1213,
          "fn": 1187,
          "accuracy": 0.5054166666666666
        },
        "0.01": {
          "tp": 562,
          "fn": 1838,
          "accuracy": 0.23416666666666666
        }
      },
      "auroc": 0.8471686631944445
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1213,
          "fn": 1187,
          "accuracy": 0.5054166666666666
        },
        "0.01": {
          "tp": 562,
          "fn": 1838,
          "accuracy": 0.23416666666666666
        }
      },
      "auroc": 0.8471686631944445
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2592,
          "fn": 2208,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 1309,
          "fn": 3491,
          "accuracy": 0.27270833333333333
        }
      },
      "auroc": 0.858896484375
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2592,
          "fn": 2208,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 1309,
          "fn": 3491,
          "accuracy": 0.27270833333333333
        }
      },
      "auroc": 0.858896484375
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 632,
          "fn": 1768,
          "accuracy": 0.2633333333333333
        },
        "0.01": {
          "tp": 327,
          "fn": 2073,
          "accuracy": 0.13625
        }
      },
      "auroc": 0.7093464409722223
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 632,
          "fn": 1768,
          "accuracy": 0.2633333333333333
        },
        "0.01": {
          "tp": 327,
          "fn": 2073,
          "accuracy": 0.13625
        }
      },
      "auroc": 0.7093464409722223
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 583,
          "fn": 1817,
          "accuracy": 0.24291666666666667
        },
        "0.01": {
          "tp": 238,
          "fn": 2162,
          "accuracy": 0.09916666666666667
        }
      },
      "auroc": 0.6822353298611111
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 583,
          "fn": 1817,
          "accuracy": 0.24291666666666667
        },
        "0.01": {
          "tp": 238,
          "fn": 2162,
          "accuracy": 0.09916666666666667
        }
      },
      "auroc": 0.6822353298611111
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1215,
          "fn": 3585,
          "accuracy": 0.253125
        },
        "0.01": {
          "tp": 565,
          "fn": 4235,
          "accuracy": 0.11770833333333333
        }
      },
      "auroc": 0.6957908854166668
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1215,
          "fn": 3585,
          "accuracy": 0.253125
        },
        "0.01": {
          "tp": 565,
          "fn": 4235,
          "accuracy": 0.11770833333333333
        }
      },
      "auroc": 0.6957908854166668
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1902,
          "fn": 498,
          "accuracy": 0.7925
        },
        "0.01": {
          "tp": 1375,
          "fn": 1025,
          "accuracy": 0.5729166666666666
        }
      },
      "auroc": 0.9441190972222222
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1902,
          "fn": 498,
          "accuracy": 0.7925
        },
        "0.01": {
          "tp": 1375,
          "fn": 1025,
          "accuracy": 0.5729166666666666
        }
      },
      "auroc": 0.9441190972222222
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1756,
          "fn": 644,
          "accuracy": 0.7316666666666667
        },
        "0.01": {
          "tp": 1166,
          "fn": 1234,
          "accuracy": 0.48583333333333334
        }
      },
      "auroc": 0.9291160590277779
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1756,
          "fn": 644,
          "accuracy": 0.7316666666666667
        },
        "0.01": {
          "tp": 1166,
          "fn": 1234,
          "accuracy": 0.48583333333333334
        }
      },
      "auroc": 0.9291160590277779
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3658,
          "fn": 1142,
          "accuracy": 0.7620833333333333
        },
        "0.01": {
          "tp": 2541,
          "fn": 2259,
          "accuracy": 0.529375
        }
      },
      "auroc": 0.9366175781249999
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3658,
          "fn": 1142,
          "accuracy": 0.7620833333333333
        },
        "0.01": {
          "tp": 2541,
          "fn": 2259,
          "accuracy": 0.529375
        }
      },
      "auroc": 0.9366175781249999
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1863,
          "fn": 537,
          "accuracy": 0.77625
        },
        "0.01": {
          "tp": 1506,
          "fn": 894,
          "accuracy": 0.6275
        }
      },
      "auroc": 0.9388671006944446
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1863,
          "fn": 537,
          "accuracy": 0.77625
        },
        "0.01": {
          "tp": 1506,
          "fn": 894,
          "accuracy": 0.6275
        }
      },
      "auroc": 0.9388671006944446
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1074,
          "fn": 1326,
          "accuracy": 0.4475
        },
        "0.01": {
          "tp": 503,
          "fn": 1897,
          "accuracy": 0.20958333333333334
        }
      },
      "auroc": 0.8298767361111111
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1074,
          "fn": 1326,
          "accuracy": 0.4475
        },
        "0.01": {
          "tp": 503,
          "fn": 1897,
          "accuracy": 0.20958333333333334
        }
      },
      "auroc": 0.8298767361111111
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2937,
          "fn": 1863,
          "accuracy": 0.611875
        },
        "0.01": {
          "tp": 2009,
          "fn": 2791,
          "accuracy": 0.41854166666666665
        }
      },
      "auroc": 0.884371918402778
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2937,
          "fn": 1863,
          "accuracy": 0.611875
        },
        "0.01": {
          "tp": 2009,
          "fn": 2791,
          "accuracy": 0.41854166666666665
        }
      },
      "auroc": 0.884371918402778
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1635,
          "fn": 765,
          "accuracy": 0.68125
        },
        "0.01": {
          "tp": 1189,
          "fn": 1211,
          "accuracy": 0.49541666666666667
        }
      },
      "auroc": 0.8870634548611112
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1635,
          "fn": 765,
          "accuracy": 0.68125
        },
        "0.01": {
          "tp": 1189,
          "fn": 1211,
          "accuracy": 0.49541666666666667
        }
      },
      "auroc": 0.8870634548611112
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1403,
          "fn": 997,
          "accuracy": 0.5845833333333333
        },
        "0.01": {
          "tp": 954,
          "fn": 1446,
          "accuracy": 0.3975
        }
      },
      "auroc": 0.8479611979166667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1403,
          "fn": 997,
          "accuracy": 0.5845833333333333
        },
        "0.01": {
          "tp": 954,
          "fn": 1446,
          "accuracy": 0.3975
        }
      },
      "auroc": 0.8479611979166667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3038,
          "fn": 1762,
          "accuracy": 0.6329166666666667
        },
        "0.01": {
          "tp": 2143,
          "fn": 2657,
          "accuracy": 0.44645833333333335
        }
      },
      "auroc": 0.8675123263888888
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3038,
          "fn": 1762,
          "accuracy": 0.6329166666666667
        },
        "0.01": {
          "tp": 2143,
          "fn": 2657,
          "accuracy": 0.44645833333333335
        }
      },
      "auroc": 0.8675123263888888
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 20486,
          "fn": 5914,
          "accuracy": 0.7759848484848485
        },
        "0.01": {
          "tp": 16749,
          "fn": 9651,
          "accuracy": 0.6344318181818182
        }
      },
      "auroc": 0.9281643781565655
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 12372,
          "fn": 2028,
          "accuracy": 0.8591666666666666
        },
        "0.01": {
          "tp": 10521,
          "fn": 3879,
          "accuracy": 0.730625
        }
      },
      "auroc": 0.9641653211805556
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 32858,
          "fn": 7942,
          "accuracy": 0.8053431372549019
        },
        "0.01": {
          "tp": 27270,
          "fn": 13530,
          "accuracy": 0.6683823529411764
        }
      },
      "auroc": 0.9408705933415032
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 16139,
          "fn": 10261,
          "accuracy": 0.6113257575757576
        },
        "0.01": {
          "tp": 10720,
          "fn": 15680,
          "accuracy": 0.40606060606060607
        }
      },
      "auroc": 0.8711995186237375
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 12099,
          "fn": 2301,
          "accuracy": 0.8402083333333333
        },
        "0.01": {
          "tp": 10115,
          "fn": 4285,
          "accuracy": 0.7024305555555556
        }
      },
      "auroc": 0.95743125
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 28238,
          "fn": 12562,
          "accuracy": 0.6921078431372549
        },
        "0.01": {
          "tp": 20835,
          "fn": 19965,
          "accuracy": 0.5106617647058823
        }
      },
      "auroc": 0.9016342473447712
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 36625,
          "fn": 16175,
          "accuracy": 0.693655303030303
        },
        "0.01": {
          "tp": 27469,
          "fn": 25331,
          "accuracy": 0.5202462121212121
        }
      },
      "auroc": 0.8996819483901515
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 24471,
          "fn": 4329,
          "accuracy": 0.8496875
        },
        "0.01": {
          "tp": 20636,
          "fn": 8164,
          "accuracy": 0.7165277777777778
        }
      },
      "auroc": 0.9607982855902777
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 61096,
          "fn": 20504,
          "accuracy": 0.7487254901960785
        },
        "0.01": {
          "tp": 48105,
          "fn": 33495,
          "accuracy": 0.5895220588235294
        }
      },
      "auroc": 0.9212524203431374
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.999871875
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999359375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999359375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9999679687499999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.999875
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999708333333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999229166666668
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.8545927083333335
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999614583333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 256,
          "fn": 144,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        }
      },
      "auroc": 0.9272770833333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 256,
          "fn": 144,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        }
      },
      "auroc": 0.9272338541666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999661458333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 656,
          "fn": 144,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 618,
          "fn": 182,
          "accuracy": 0.7725
        }
      },
      "auroc": 0.9636
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999947916666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9997604166666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9998776041666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999166666666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.999178125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9995473958333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999557291666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9994692708333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 788,
          "fn": 12,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9997125
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9990333333333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9995166666666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.9111739583333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.9406260416666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.9259000000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        },
        "0.01": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        }
      },
      "auroc": 0.9555869791666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        }
      },
      "auroc": 0.9698296875
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 589,
          "fn": 211,
          "accuracy": 0.73625
        },
        "0.01": {
          "tp": 443,
          "fn": 357,
          "accuracy": 0.55375
        }
      },
      "auroc": 0.9627083333333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999406249999999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999703124999999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.8513312500000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9752177083333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        }
      },
      "auroc": 0.9132744791666669
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        },
        "0.01": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        }
      },
      "auroc": 0.9256359375
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9876088541666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 650,
          "fn": 150,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 599,
          "fn": 201,
          "accuracy": 0.74875
        }
      },
      "auroc": 0.9566223958333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999895833333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999947916666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999895833333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9996291666666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9998093749999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999947916666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9998093749999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        }
      },
      "auroc": 0.9999020833333332
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.99696875
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.99696875
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9911145833333332
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9911145833333332
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9940416666666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9940416666666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.937178125
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.937178125
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.8616635416666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.8616635416666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        }
      },
      "auroc": 0.8994208333333332
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        }
      },
      "auroc": 0.8994208333333332
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999947916666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999947916666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9998281249999998
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9998281249999998
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999114583333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999114583333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999895833333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999895833333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9909635416666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9909635416666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        }
      },
      "auroc": 0.9954765625000002
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        }
      },
      "auroc": 0.9954765625000002
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9911572916666668
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9911572916666668
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9843479166666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9843479166666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        }
      },
      "auroc": 0.9877526041666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        }
      },
      "auroc": 0.9877526041666667
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2088,
          "fn": 112,
          "accuracy": 0.9490909090909091
        },
        "0.01": {
          "tp": 2003,
          "fn": 197,
          "accuracy": 0.9104545454545454
        }
      },
      "auroc": 0.9931908143939394
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1185,
          "fn": 15,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9997923611111111
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3288,
          "fn": 112,
          "accuracy": 0.9670588235294117
        },
        "0.01": {
          "tp": 3188,
          "fn": 212,
          "accuracy": 0.9376470588235294
        }
      },
      "auroc": 0.9955207720588236
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1625,
          "fn": 575,
          "accuracy": 0.7386363636363636
        },
        "0.01": {
          "tp": 1378,
          "fn": 822,
          "accuracy": 0.6263636363636363
        }
      },
      "auroc": 0.9495383522727273
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1076,
          "fn": 124,
          "accuracy": 0.8966666666666666
        },
        "0.01": {
          "tp": 966,
          "fn": 234,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9857473958333333
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2701,
          "fn": 699,
          "accuracy": 0.7944117647058824
        },
        "0.01": {
          "tp": 2344,
          "fn": 1056,
          "accuracy": 0.6894117647058824
        }
      },
      "auroc": 0.9623180147058822
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3713,
          "fn": 687,
          "accuracy": 0.8438636363636364
        },
        "0.01": {
          "tp": 3381,
          "fn": 1019,
          "accuracy": 0.7684090909090909
        }
      },
      "auroc": 0.9713645833333333
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2276,
          "fn": 124,
          "accuracy": 0.9483333333333334
        },
        "0.01": {
          "tp": 2151,
          "fn": 249,
          "accuracy": 0.89625
        }
      },
      "auroc": 0.9927698784722222
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 5989,
          "fn": 811,
          "accuracy": 0.8807352941176471
        },
        "0.01": {
          "tp": 5532,
          "fn": 1268,
          "accuracy": 0.8135294117647058
        }
      },
      "auroc": 0.9789193933823528
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.999871875
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999359375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999359375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9999679687499999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.999875
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999708333333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999229166666668
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.8545927083333335
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999614583333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 256,
          "fn": 144,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        }
      },
      "auroc": 0.9272770833333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 256,
          "fn": 144,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        }
      },
      "auroc": 0.9272338541666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999661458333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 656,
          "fn": 144,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 618,
          "fn": 182,
          "accuracy": 0.7725
        }
      },
      "auroc": 0.9636
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999947916666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9997604166666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9998776041666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999166666666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.999178125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9995473958333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999557291666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9994692708333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 788,
          "fn": 12,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9997125
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9990333333333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9995166666666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.9111739583333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.9406260416666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.9259000000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        },
        "0.01": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        }
      },
      "auroc": 0.9555869791666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        }
      },
      "auroc": 0.9698296875
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 589,
          "fn": 211,
          "accuracy": 0.73625
        },
        "0.01": {
          "tp": 443,
          "fn": 357,
          "accuracy": 0.55375
        }
      },
      "auroc": 0.9627083333333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999406249999999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999703124999999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.8513312500000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9752177083333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        }
      },
      "auroc": 0.9132744791666669
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        },
        "0.01": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        }
      },
      "auroc": 0.9256359375
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9876088541666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 650,
          "fn": 150,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 599,
          "fn": 201,
          "accuracy": 0.74875
        }
      },
      "auroc": 0.9566223958333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999895833333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999947916666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999895833333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9996291666666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9998093749999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999947916666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9998093749999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        }
      },
      "auroc": 0.9999020833333332
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.99696875
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.99696875
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9911145833333332
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9911145833333332
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9940416666666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9940416666666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.937178125
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.937178125
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.8616635416666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.8616635416666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        }
      },
      "auroc": 0.8994208333333332
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        }
      },
      "auroc": 0.8994208333333332
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999947916666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999947916666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9998281249999998
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9998281249999998
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999114583333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999114583333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999895833333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999895833333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9909635416666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9909635416666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        }
      },
      "auroc": 0.9954765625000002
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        }
      },
      "auroc": 0.9954765625000002
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9911572916666668
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9911572916666668
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9843479166666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9843479166666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        }
      },
      "auroc": 0.9877526041666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        }
      },
      "auroc": 0.9877526041666667
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2088,
          "fn": 112,
          "accuracy": 0.9490909090909091
        },
        "0.01": {
          "tp": 2003,
          "fn": 197,
          "accuracy": 0.9104545454545454
        }
      },
      "auroc": 0.9931908143939394
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1185,
          "fn": 15,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9997923611111111
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3288,
          "fn": 112,
          "accuracy": 0.9670588235294117
        },
        "0.01": {
          "tp": 3188,
          "fn": 212,
          "accuracy": 0.9376470588235294
        }
      },
      "auroc": 0.9955207720588236
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1625,
          "fn": 575,
          "accuracy": 0.7386363636363636
        },
        "0.01": {
          "tp": 1378,
          "fn": 822,
          "accuracy": 0.6263636363636363
        }
      },
      "auroc": 0.9495383522727273
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1076,
          "fn": 124,
          "accuracy": 0.8966666666666666
        },
        "0.01": {
          "tp": 966,
          "fn": 234,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9857473958333333
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2701,
          "fn": 699,
          "accuracy": 0.7944117647058824
        },
        "0.01": {
          "tp": 2344,
          "fn": 1056,
          "accuracy": 0.6894117647058824
        }
      },
      "auroc": 0.9623180147058822
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3713,
          "fn": 687,
          "accuracy": 0.8438636363636364
        },
        "0.01": {
          "tp": 3381,
          "fn": 1019,
          "accuracy": 0.7684090909090909
        }
      },
      "auroc": 0.9713645833333333
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2276,
          "fn": 124,
          "accuracy": 0.9483333333333334
        },
        "0.01": {
          "tp": 2151,
          "fn": 249,
          "accuracy": 0.89625
        }
      },
      "auroc": 0.9927698784722222
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 5989,
          "fn": 811,
          "accuracy": 0.8807352941176471
        },
        "0.01": {
          "tp": 5532,
          "fn": 1268,
          "accuracy": 0.8135294117647058
        }
      },
      "auroc": 0.9789193933823528
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999947916666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9994770833333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9997359375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999973958333332
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9997385416666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9998679687500001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9994770833333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999708333333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9997239583333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.7888447916666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999562499999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        }
      },
      "auroc": 0.8944005208333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        }
      },
      "auroc": 0.8941609374999998
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999635416666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 640,
          "fn": 160,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 606,
          "fn": 194,
          "accuracy": 0.7575
        }
      },
      "auroc": 0.9470622395833335
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999260416666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.99929375
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9996098958333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9996697916666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9985520833333332
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9991109375
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9997979166666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9989229166666668
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 775,
          "fn": 25,
          "accuracy": 0.96875
        }
      },
      "auroc": 0.9993604166666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9968812499999999
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.998440625
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.8641083333333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        }
      },
      "auroc": 0.9243885416666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 251,
          "accuracy": 0.3725
        },
        "0.01": {
          "tp": 47,
          "fn": 353,
          "accuracy": 0.1175
        }
      },
      "auroc": 0.8942484375
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 210,
          "fn": 190,
          "accuracy": 0.525
        }
      },
      "auroc": 0.9320541666666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        },
        "0.01": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        }
      },
      "auroc": 0.9606348958333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 547,
          "fn": 253,
          "accuracy": 0.68375
        },
        "0.01": {
          "tp": 413,
          "fn": 387,
          "accuracy": 0.51625
        }
      },
      "auroc": 0.94634453125
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9998260416666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999130208333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.7903989583333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9705812500000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        }
      },
      "auroc": 0.8804901041666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        }
      },
      "auroc": 0.8951125
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.985290625
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 638,
          "fn": 162,
          "accuracy": 0.7975
        },
        "0.01": {
          "tp": 594,
          "fn": 206,
          "accuracy": 0.7425
        }
      },
      "auroc": 0.9402015624999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999458333333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999729166666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999479166666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9988510416666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9993994791666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999739583333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9993984375
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        }
      },
      "auroc": 0.9996861979166667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9924041666666665
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9924041666666665
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        }
      },
      "auroc": 0.9811864583333332
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        }
      },
      "auroc": 0.9811864583333332
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        }
      },
      "auroc": 0.9867953125000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        }
      },
      "auroc": 0.9867953125000001
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        }
      },
      "auroc": 0.88839375
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        }
      },
      "auroc": 0.88839375
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.7931895833333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.7931895833333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 268,
          "accuracy": 0.33
        },
        "0.01": {
          "tp": 85,
          "fn": 315,
          "accuracy": 0.2125
        }
      },
      "auroc": 0.8407916666666668
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 268,
          "accuracy": 0.33
        },
        "0.01": {
          "tp": 85,
          "fn": 315,
          "accuracy": 0.2125
        }
      },
      "auroc": 0.8407916666666668
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999843749999999
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999843749999999
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9994645833333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9994645833333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9997244791666666
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9997244791666666
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9998677083333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9998677083333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.9806489583333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.9806489583333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9902583333333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9902583333333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9836260416666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9836260416666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.97101875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.97101875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": {
          "tp": 291,
          "fn": 109,
          "accuracy": 0.7275
        }
      },
      "auroc": 0.9773223958333332
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": {
          "tp": 291,
          "fn": 109,
          "accuracy": 0.7275
        }
      },
      "auroc": 0.9773223958333332
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2042,
          "fn": 158,
          "accuracy": 0.9281818181818182
        },
        "0.01": {
          "tp": 1943,
          "fn": 257,
          "accuracy": 0.8831818181818182
        }
      },
      "auroc": 0.9875913825757576
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        },
        "0.01": {
          "tp": 1157,
          "fn": 43,
          "accuracy": 0.9641666666666666
        }
      },
      "auroc": 0.9993486111111112
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3240,
          "fn": 160,
          "accuracy": 0.9529411764705882
        },
        "0.01": {
          "tp": 3100,
          "fn": 300,
          "accuracy": 0.9117647058823529
        }
      },
      "auroc": 0.9917409926470587
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1493,
          "fn": 707,
          "accuracy": 0.6786363636363636
        },
        "0.01": {
          "tp": 1273,
          "fn": 927,
          "accuracy": 0.5786363636363636
        }
      },
      "auroc": 0.9244066287878788
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1060,
          "fn": 140,
          "accuracy": 0.8833333333333333
        },
        "0.01": {
          "tp": 946,
          "fn": 254,
          "accuracy": 0.7883333333333333
        }
      },
      "auroc": 0.9819677083333334
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2553,
          "fn": 847,
          "accuracy": 0.7508823529411764
        },
        "0.01": {
          "tp": 2219,
          "fn": 1181,
          "accuracy": 0.6526470588235294
        }
      },
      "auroc": 0.9447223039215685
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3535,
          "fn": 865,
          "accuracy": 0.803409090909091
        },
        "0.01": {
          "tp": 3216,
          "fn": 1184,
          "accuracy": 0.730909090909091
        }
      },
      "auroc": 0.9559990056818182
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2258,
          "fn": 142,
          "accuracy": 0.9408333333333333
        },
        "0.01": {
          "tp": 2103,
          "fn": 297,
          "accuracy": 0.87625
        }
      },
      "auroc": 0.9906581597222223
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 5793,
          "fn": 1007,
          "accuracy": 0.8519117647058824
        },
        "0.01": {
          "tp": 5319,
          "fn": 1481,
          "accuracy": 0.7822058823529412
        }
      },
      "auroc": 0.9682316482843136
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.997959375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9965906250000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.997275
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9979447916666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.991653125
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9947989583333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9979520833333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        }
      },
      "auroc": 0.994121875
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 775,
          "fn": 25,
          "accuracy": 0.96875
        },
        "0.01": {
          "tp": 692,
          "fn": 108,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9960369791666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9942520833333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999708333333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9971114583333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.6926708333333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999510416666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        },
        "0.01": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        }
      },
      "auroc": 0.8463109375
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        },
        "0.01": {
          "tp": 166,
          "fn": 234,
          "accuracy": 0.415
        }
      },
      "auroc": 0.8434614583333335
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999609374999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 619,
          "fn": 181,
          "accuracy": 0.77375
        },
        "0.01": {
          "tp": 565,
          "fn": 235,
          "accuracy": 0.70625
        }
      },
      "auroc": 0.9217111979166668
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9963166666666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.996721875
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9965192708333332
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.995909375
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9942885416666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9950989583333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        }
      },
      "auroc": 0.9961130208333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        }
      },
      "auroc": 0.9955052083333332
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 767,
          "fn": 33,
          "accuracy": 0.95875
        },
        "0.01": {
          "tp": 670,
          "fn": 130,
          "accuracy": 0.8375
        }
      },
      "auroc": 0.9958091145833333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.9774229166666668
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        }
      },
      "auroc": 0.9887114583333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.8101385416666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.9124322916666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 263,
          "accuracy": 0.3425
        },
        "0.01": {
          "tp": 47,
          "fn": 353,
          "accuracy": 0.1175
        }
      },
      "auroc": 0.8612854166666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        },
        "0.01": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        }
      },
      "auroc": 0.9050692708333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        }
      },
      "auroc": 0.9449276041666668
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 486,
          "fn": 314,
          "accuracy": 0.6075
        },
        "0.01": {
          "tp": 332,
          "fn": 468,
          "accuracy": 0.415
        }
      },
      "auroc": 0.9249984375
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9979947916666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9989973958333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        },
        "0.01": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        }
      },
      "auroc": 0.7370812500000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9739260416666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        },
        "0.01": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        }
      },
      "auroc": 0.8555036458333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        },
        "0.01": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        }
      },
      "auroc": 0.8675380208333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9869630208333332
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 624,
          "fn": 176,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 575,
          "fn": 225,
          "accuracy": 0.71875
        }
      },
      "auroc": 0.9272505208333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9991729166666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9973208333333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        }
      },
      "auroc": 0.998246875
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9962833333333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.9913322916666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9938078125
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.997728125
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9943265625
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 764,
          "fn": 36,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 684,
          "fn": 116,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9960273437499999
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9642583333333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9642583333333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        }
      },
      "auroc": 0.9270895833333332
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        }
      },
      "auroc": 0.9270895833333332
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 95,
          "fn": 305,
          "accuracy": 0.2375
        }
      },
      "auroc": 0.9456739583333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 95,
          "fn": 305,
          "accuracy": 0.2375
        }
      },
      "auroc": 0.9456739583333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.7851010416666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.7851010416666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.676834375
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.676834375
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 324,
          "accuracy": 0.19
        },
        "0.01": {
          "tp": 43,
          "fn": 357,
          "accuracy": 0.1075
        }
      },
      "auroc": 0.7309677083333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 324,
          "accuracy": 0.19
        },
        "0.01": {
          "tp": 43,
          "fn": 357,
          "accuracy": 0.1075
        }
      },
      "auroc": 0.7309677083333334
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.998734375
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.998734375
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9935229166666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9935229166666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9961286458333334
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9961286458333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.99539375
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.99539375
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.9362583333333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.9362583333333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        }
      },
      "auroc": 0.9658260416666666
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        }
      },
      "auroc": 0.9658260416666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        }
      },
      "auroc": 0.9551739583333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        }
      },
      "auroc": 0.9551739583333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        }
      },
      "auroc": 0.9216270833333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        }
      },
      "auroc": 0.9216270833333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        }
      },
      "auroc": 0.9384005208333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        }
      },
      "auroc": 0.9384005208333334
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1881,
          "fn": 319,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 1647,
          "fn": 553,
          "accuracy": 0.7486363636363637
        }
      },
      "auroc": 0.9713052083333332
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1137,
          "fn": 63,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 1001,
          "fn": 199,
          "accuracy": 0.8341666666666666
        }
      },
      "auroc": 0.9946711805555556
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3018,
          "fn": 382,
          "accuracy": 0.8876470588235295
        },
        "0.01": {
          "tp": 2648,
          "fn": 752,
          "accuracy": 0.7788235294117647
        }
      },
      "auroc": 0.9795520220588234
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1255,
          "fn": 945,
          "accuracy": 0.5704545454545454
        },
        "0.01": {
          "tp": 966,
          "fn": 1234,
          "accuracy": 0.4390909090909091
        }
      },
      "auroc": 0.8804873106060606
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1018,
          "fn": 182,
          "accuracy": 0.8483333333333334
        },
        "0.01": {
          "tp": 840,
          "fn": 360,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9772638888888887
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2273,
          "fn": 1127,
          "accuracy": 0.6685294117647059
        },
        "0.01": {
          "tp": 1806,
          "fn": 1594,
          "accuracy": 0.5311764705882352
        }
      },
      "auroc": 0.9146437500000001
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3136,
          "fn": 1264,
          "accuracy": 0.7127272727272728
        },
        "0.01": {
          "tp": 2613,
          "fn": 1787,
          "accuracy": 0.5938636363636364
        }
      },
      "auroc": 0.925896259469697
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2155,
          "fn": 245,
          "accuracy": 0.8979166666666667
        },
        "0.01": {
          "tp": 1841,
          "fn": 559,
          "accuracy": 0.7670833333333333
        }
      },
      "auroc": 0.9859675347222223
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 5291,
          "fn": 1509,
          "accuracy": 0.7780882352941176
        },
        "0.01": {
          "tp": 4454,
          "fn": 2346,
          "accuracy": 0.655
        }
      },
      "auroc": 0.9470978860294118
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999791666666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999895833333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999843749999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9993624999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9996734375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999817708333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.99968125
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9998315104166666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9997375
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999604166666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9998489583333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.7653677083333332
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999614583333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        }
      },
      "auroc": 0.8826645833333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": {
          "tp": 210,
          "fn": 190,
          "accuracy": 0.525
        }
      },
      "auroc": 0.8825526041666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999609374999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 639,
          "fn": 161,
          "accuracy": 0.79875
        },
        "0.01": {
          "tp": 609,
          "fn": 191,
          "accuracy": 0.76125
        }
      },
      "auroc": 0.9412567708333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999635416666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.99935625
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9996598958333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9998510416666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9983124999999999
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9990817708333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999072916666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9988343749999999
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        }
      },
      "auroc": 0.9993708333333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9959791666666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9979895833333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.8286562499999998
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.8559802083333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 291,
          "accuracy": 0.2725
        },
        "0.01": {
          "tp": 32,
          "fn": 368,
          "accuracy": 0.08
        }
      },
      "auroc": 0.8423182291666668
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        }
      },
      "auroc": 0.9143281250000002
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        },
        "0.01": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        }
      },
      "auroc": 0.9259796875000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 503,
          "fn": 297,
          "accuracy": 0.62875
        },
        "0.01": {
          "tp": 392,
          "fn": 408,
          "accuracy": 0.49
        }
      },
      "auroc": 0.9201539062499999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.999846875
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999234375
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        },
        "0.01": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        }
      },
      "auroc": 0.7759239583333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.95055
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        },
        "0.01": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        }
      },
      "auroc": 0.8632369791666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        }
      },
      "auroc": 0.8878854166666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        }
      },
      "auroc": 0.975275
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 625,
          "fn": 175,
          "accuracy": 0.78125
        },
        "0.01": {
          "tp": 579,
          "fn": 221,
          "accuracy": 0.72375
        }
      },
      "auroc": 0.9315802083333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999895833333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999291666666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.999959375
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.99988125
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9983510416666665
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9991161458333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999354166666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9991401041666668
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 781,
          "fn": 19,
          "accuracy": 0.97625
        }
      },
      "auroc": 0.9995377604166666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.993890625
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.993890625
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9839822916666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9839822916666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        }
      },
      "auroc": 0.9889364583333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        }
      },
      "auroc": 0.9889364583333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.8894666666666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.8894666666666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.7927562499999999
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.7927562499999999
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 87,
          "fn": 313,
          "accuracy": 0.2175
        }
      },
      "auroc": 0.8411114583333332
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 87,
          "fn": 313,
          "accuracy": 0.2175
        }
      },
      "auroc": 0.8411114583333332
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999770833333332
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999770833333332
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9995802083333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9995802083333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9997786458333334
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9997786458333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999145833333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999145833333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.9773947916666668
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.9773947916666668
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9886546875
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9886546875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9827312499999998
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9827312499999998
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.9672125
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.9672125
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        }
      },
      "auroc": 0.974971875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        }
      },
      "auroc": 0.974971875
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2047,
          "fn": 153,
          "accuracy": 0.9304545454545454
        },
        "0.01": {
          "tp": 1947,
          "fn": 253,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9877724431818182
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1194,
          "fn": 6,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 1153,
          "fn": 47,
          "accuracy": 0.9608333333333333
        }
      },
      "auroc": 0.9992041666666668
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3241,
          "fn": 159,
          "accuracy": 0.9532352941176471
        },
        "0.01": {
          "tp": 3100,
          "fn": 300,
          "accuracy": 0.9117647058823529
        }
      },
      "auroc": 0.991807169117647
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1484,
          "fn": 716,
          "accuracy": 0.6745454545454546
        },
        "0.01": {
          "tp": 1274,
          "fn": 926,
          "accuracy": 0.5790909090909091
        }
      },
      "auroc": 0.9173264204545454
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1018,
          "fn": 182,
          "accuracy": 0.8483333333333334
        },
        "0.01": {
          "tp": 915,
          "fn": 285,
          "accuracy": 0.7625
        }
      },
      "auroc": 0.9670862847222222
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2502,
          "fn": 898,
          "accuracy": 0.7358823529411764
        },
        "0.01": {
          "tp": 2189,
          "fn": 1211,
          "accuracy": 0.6438235294117647
        }
      },
      "auroc": 0.9348887254901961
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3531,
          "fn": 869,
          "accuracy": 0.8025
        },
        "0.01": {
          "tp": 3221,
          "fn": 1179,
          "accuracy": 0.7320454545454546
        }
      },
      "auroc": 0.9525494318181817
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2212,
          "fn": 188,
          "accuracy": 0.9216666666666666
        },
        "0.01": {
          "tp": 2068,
          "fn": 332,
          "accuracy": 0.8616666666666667
        }
      },
      "auroc": 0.9831452256944445
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 5743,
          "fn": 1057,
          "accuracy": 0.8445588235294118
        },
        "0.01": {
          "tp": 5289,
          "fn": 1511,
          "accuracy": 0.7777941176470589
        }
      },
      "auroc": 0.9633479473039215
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.983546875
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        }
      },
      "auroc": 0.9786020833333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        }
      },
      "auroc": 0.9810744791666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        }
      },
      "auroc": 0.9865802083333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9677416666666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        },
        "0.01": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        }
      },
      "auroc": 0.9771609375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        }
      },
      "auroc": 0.9850635416666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.9731718749999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 656,
          "fn": 144,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 447,
          "fn": 353,
          "accuracy": 0.55875
        }
      },
      "auroc": 0.9791177083333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.9936697916666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.907096875
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        },
        "0.01": {
          "tp": 172,
          "fn": 228,
          "accuracy": 0.43
        }
      },
      "auroc": 0.9503833333333332
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.8023541666666668
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.9354395833333332
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 261,
          "accuracy": 0.3475
        },
        "0.01": {
          "tp": 41,
          "fn": 359,
          "accuracy": 0.1025
        }
      },
      "auroc": 0.8688968749999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        },
        "0.01": {
          "tp": 160,
          "fn": 240,
          "accuracy": 0.4
        }
      },
      "auroc": 0.8980119791666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        },
        "0.01": {
          "tp": 53,
          "fn": 347,
          "accuracy": 0.1325
        }
      },
      "auroc": 0.9212682291666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 406,
          "accuracy": 0.4925
        },
        "0.01": {
          "tp": 213,
          "fn": 587,
          "accuracy": 0.26625
        }
      },
      "auroc": 0.9096401041666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        }
      },
      "auroc": 0.9670552083333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        }
      },
      "auroc": 0.9404979166666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.9537765625
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.9625510416666665
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.8976843750000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9301177083333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        }
      },
      "auroc": 0.964803125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 119,
          "fn": 281,
          "accuracy": 0.2975
        }
      },
      "auroc": 0.9190911458333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 526,
          "fn": 274,
          "accuracy": 0.6575
        },
        "0.01": {
          "tp": 315,
          "fn": 485,
          "accuracy": 0.39375
        }
      },
      "auroc": 0.9419471354166666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9997874999999999
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.98228125
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        }
      },
      "auroc": 0.991034375
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.8931104166666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.8325916666666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 283,
          "accuracy": 0.2925
        },
        "0.01": {
          "tp": 27,
          "fn": 373,
          "accuracy": 0.0675
        }
      },
      "auroc": 0.8628510416666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 274,
          "fn": 126,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        }
      },
      "auroc": 0.9464489583333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 210,
          "fn": 190,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        }
      },
      "auroc": 0.9074364583333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 484,
          "fn": 316,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 325,
          "fn": 475,
          "accuracy": 0.40625
        }
      },
      "auroc": 0.9269427083333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9946947916666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.9411166666666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        },
        "0.01": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        }
      },
      "auroc": 0.9679057291666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.8114010416666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.7713885416666668
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 72,
          "fn": 328,
          "accuracy": 0.18
        },
        "0.01": {
          "tp": 16,
          "fn": 384,
          "accuracy": 0.04
        }
      },
      "auroc": 0.7913947916666668
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 236,
          "fn": 164,
          "accuracy": 0.59
        },
        "0.01": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        }
      },
      "auroc": 0.9030479166666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 279,
          "accuracy": 0.3025
        },
        "0.01": {
          "tp": 41,
          "fn": 359,
          "accuracy": 0.1025
        }
      },
      "auroc": 0.8562526041666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 443,
          "accuracy": 0.44625
        },
        "0.01": {
          "tp": 214,
          "fn": 586,
          "accuracy": 0.2675
        }
      },
      "auroc": 0.8796502604166666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9885645833333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9805697916666665
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 267,
          "fn": 133,
          "accuracy": 0.6675
        }
      },
      "auroc": 0.9845671874999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9823000000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        }
      },
      "auroc": 0.9576072916666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        }
      },
      "auroc": 0.9699536458333332
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        }
      },
      "auroc": 0.9854322916666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        }
      },
      "auroc": 0.9690885416666668
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 661,
          "fn": 139,
          "accuracy": 0.82625
        },
        "0.01": {
          "tp": 481,
          "fn": 319,
          "accuracy": 0.60125
        }
      },
      "auroc": 0.9772604166666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.9568927083333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.9568927083333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        }
      },
      "auroc": 0.9416979166666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        }
      },
      "auroc": 0.9416979166666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 119,
          "fn": 281,
          "accuracy": 0.2975
        }
      },
      "auroc": 0.9492953125
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 119,
          "fn": 281,
          "accuracy": 0.2975
        }
      },
      "auroc": 0.9492953125
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.8430479166666668
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.8430479166666668
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.7808927083333332
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.7808927083333332
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 295,
          "accuracy": 0.2625
        },
        "0.01": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        }
      },
      "auroc": 0.8119703125
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 295,
          "accuracy": 0.2625
        },
        "0.01": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        }
      },
      "auroc": 0.8119703125
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.97608125
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.97608125
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        }
      },
      "auroc": 0.963
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        }
      },
      "auroc": 0.963
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        }
      },
      "auroc": 0.9695406249999999
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        }
      },
      "auroc": 0.9695406249999999
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9718635416666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9718635416666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.8895645833333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.8895645833333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": {
          "tp": 150,
          "fn": 250,
          "accuracy": 0.375
        }
      },
      "auroc": 0.9307140625
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": {
          "tp": 150,
          "fn": 250,
          "accuracy": 0.375
        }
      },
      "auroc": 0.9307140625
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.9493229166666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.9493229166666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.9127854166666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.9127854166666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        }
      },
      "auroc": 0.9310541666666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        }
      },
      "auroc": 0.9310541666666666
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1764,
          "fn": 436,
          "accuracy": 0.8018181818181818
        },
        "0.01": {
          "tp": 1303,
          "fn": 897,
          "accuracy": 0.5922727272727273
        }
      },
      "auroc": 0.9658660984848485
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 782,
          "fn": 418,
          "accuracy": 0.6516666666666666
        },
        "0.01": {
          "tp": 475,
          "fn": 725,
          "accuracy": 0.3958333333333333
        }
      },
      "auroc": 0.9550274305555556
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2546,
          "fn": 854,
          "accuracy": 0.7488235294117647
        },
        "0.01": {
          "tp": 1778,
          "fn": 1622,
          "accuracy": 0.5229411764705882
        }
      },
      "auroc": 0.9620406862745098
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1140,
          "fn": 1060,
          "accuracy": 0.5181818181818182
        },
        "0.01": {
          "tp": 671,
          "fn": 1529,
          "accuracy": 0.305
        }
      },
      "auroc": 0.9023852272727273
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 561,
          "fn": 639,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 265,
          "fn": 935,
          "accuracy": 0.22083333333333333
        }
      },
      "auroc": 0.8937421875
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1701,
          "fn": 1699,
          "accuracy": 0.5002941176470588
        },
        "0.01": {
          "tp": 936,
          "fn": 2464,
          "accuracy": 0.2752941176470588
        }
      },
      "auroc": 0.8993347426470588
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2904,
          "fn": 1496,
          "accuracy": 0.66
        },
        "0.01": {
          "tp": 1974,
          "fn": 2426,
          "accuracy": 0.4486363636363636
        }
      },
      "auroc": 0.9341256628787878
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1343,
          "fn": 1057,
          "accuracy": 0.5595833333333333
        },
        "0.01": {
          "tp": 740,
          "fn": 1660,
          "accuracy": 0.30833333333333335
        }
      },
      "auroc": 0.9243848090277778
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 4247,
          "fn": 2553,
          "accuracy": 0.6245588235294117
        },
        "0.01": {
          "tp": 2714,
          "fn": 4086,
          "accuracy": 0.3991176470588235
        }
      },
      "auroc": 0.9306877144607844
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.999871875
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999359375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999359375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9999679687499999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9998541666666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999708333333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999125
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.85125
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999614583333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        },
        "0.01": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        }
      },
      "auroc": 0.9256057291666668
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        },
        "0.01": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        }
      },
      "auroc": 0.9255520833333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999661458333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 655,
          "fn": 145,
          "accuracy": 0.81875
        },
        "0.01": {
          "tp": 617,
          "fn": 183,
          "accuracy": 0.77125
        }
      },
      "auroc": 0.9627591145833333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999947916666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9997604166666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9998776041666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999166666666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9991760416666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9995463541666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999557291666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9994682291666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 788,
          "fn": 12,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9997119791666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.998975
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9994875
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.9083166666666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.9402739583333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        },
        "0.01": {
          "tp": 55,
          "fn": 345,
          "accuracy": 0.1375
        }
      },
      "auroc": 0.9242953125
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        }
      },
      "auroc": 0.9541583333333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        }
      },
      "auroc": 0.9696244791666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 588,
          "fn": 212,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 442,
          "fn": 358,
          "accuracy": 0.5525
        }
      },
      "auroc": 0.9618914062499999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999354166666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999677083333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        }
      },
      "auroc": 0.8491166666666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9750687500000002
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.9120927083333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9245260416666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        }
      },
      "auroc": 0.987534375
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 649,
          "fn": 151,
          "accuracy": 0.81125
        },
        "0.01": {
          "tp": 600,
          "fn": 200,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9560302083333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999895833333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999947916666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999895833333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9996291666666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9998093749999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999947916666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9998093749999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        }
      },
      "auroc": 0.9999020833333332
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9968552083333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9968552083333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9908270833333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9908270833333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9938411458333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9938411458333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        }
      },
      "auroc": 0.935875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        }
      },
      "auroc": 0.935875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        },
        "0.01": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        }
      },
      "auroc": 0.8602729166666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        },
        "0.01": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        }
      },
      "auroc": 0.8602729166666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        },
        "0.01": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        }
      },
      "auroc": 0.8980739583333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        },
        "0.01": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        }
      },
      "auroc": 0.8980739583333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999947916666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999947916666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9998281249999998
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9998281249999998
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999114583333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999114583333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999895833333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999895833333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9909385416666666
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9909385416666666
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        }
      },
      "auroc": 0.9954640625
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        }
      },
      "auroc": 0.9954640625
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9911375
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9911375
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9842239583333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9842239583333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        }
      },
      "auroc": 0.9876807291666665
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        }
      },
      "auroc": 0.9876807291666665
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2086,
          "fn": 114,
          "accuracy": 0.9481818181818182
        },
        "0.01": {
          "tp": 1999,
          "fn": 201,
          "accuracy": 0.9086363636363637
        }
      },
      "auroc": 0.9930578598484848
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1185,
          "fn": 15,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9997826388888889
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3286,
          "fn": 114,
          "accuracy": 0.9664705882352941
        },
        "0.01": {
          "tp": 3184,
          "fn": 216,
          "accuracy": 0.9364705882352942
        }
      },
      "auroc": 0.9954313112745098
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1617,
          "fn": 583,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 1374,
          "fn": 826,
          "accuracy": 0.6245454545454545
        }
      },
      "auroc": 0.9486072916666667
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1078,
          "fn": 122,
          "accuracy": 0.8983333333333333
        },
        "0.01": {
          "tp": 968,
          "fn": 232,
          "accuracy": 0.8066666666666666
        }
      },
      "auroc": 0.9856635416666668
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2695,
          "fn": 705,
          "accuracy": 0.7926470588235294
        },
        "0.01": {
          "tp": 2342,
          "fn": 1058,
          "accuracy": 0.6888235294117647
        }
      },
      "auroc": 0.9616859681372548
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3703,
          "fn": 697,
          "accuracy": 0.8415909090909091
        },
        "0.01": {
          "tp": 3373,
          "fn": 1027,
          "accuracy": 0.7665909090909091
        }
      },
      "auroc": 0.9708325757575758
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2278,
          "fn": 122,
          "accuracy": 0.9491666666666667
        },
        "0.01": {
          "tp": 2153,
          "fn": 247,
          "accuracy": 0.8970833333333333
        }
      },
      "auroc": 0.9927230902777777
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 5981,
          "fn": 819,
          "accuracy": 0.8795588235294117
        },
        "0.01": {
          "tp": 5526,
          "fn": 1274,
          "accuracy": 0.8126470588235294
        }
      },
      "auroc": 0.9785586397058823
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.999871875
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999359375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999359375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9999679687499999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.999875
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999708333333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999229166666668
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.8544781250000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999614583333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 256,
          "fn": 144,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        }
      },
      "auroc": 0.9272197916666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 256,
          "fn": 144,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        }
      },
      "auroc": 0.9271765625000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999661458333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 656,
          "fn": 144,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 618,
          "fn": 182,
          "accuracy": 0.7725
        }
      },
      "auroc": 0.9635713541666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999947916666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9997604166666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9998776041666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999166666666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.999178125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9995473958333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999557291666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9994692708333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 788,
          "fn": 12,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9997125
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9990333333333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9995166666666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.9112052083333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.9404989583333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.9258520833333332
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        },
        "0.01": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        }
      },
      "auroc": 0.9556026041666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        }
      },
      "auroc": 0.9697661458333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 588,
          "fn": 212,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 443,
          "fn": 357,
          "accuracy": 0.55375
        }
      },
      "auroc": 0.962684375
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999406249999999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999703124999999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.8513312500000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9751447916666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        }
      },
      "auroc": 0.9132380208333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        },
        "0.01": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        }
      },
      "auroc": 0.9256359375
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9875723958333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 650,
          "fn": 150,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 599,
          "fn": 201,
          "accuracy": 0.74875
        }
      },
      "auroc": 0.9566041666666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999895833333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999947916666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999895833333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9996291666666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9998093749999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999947916666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9998093749999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        }
      },
      "auroc": 0.9999020833333332
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.99696875
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.99696875
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9911145833333332
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9911145833333332
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9940416666666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9940416666666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.937178125
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.937178125
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.8616635416666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.8616635416666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        }
      },
      "auroc": 0.8994208333333332
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        }
      },
      "auroc": 0.8994208333333332
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999947916666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999947916666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9998281249999998
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9998281249999998
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999114583333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999114583333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999895833333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999895833333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9909635416666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9909635416666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        }
      },
      "auroc": 0.9954765625000002
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        }
      },
      "auroc": 0.9954765625000002
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9911572916666668
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9911572916666668
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9843479166666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9843479166666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        }
      },
      "auroc": 0.9877526041666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        }
      },
      "auroc": 0.9877526041666667
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2088,
          "fn": 112,
          "accuracy": 0.9490909090909091
        },
        "0.01": {
          "tp": 2003,
          "fn": 197,
          "accuracy": 0.9104545454545454
        }
      },
      "auroc": 0.9931908143939394
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 1185,
          "fn": 15,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9997923611111111
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3288,
          "fn": 112,
          "accuracy": 0.9670588235294117
        },
        "0.01": {
          "tp": 3188,
          "fn": 212,
          "accuracy": 0.9376470588235294
        }
      },
      "auroc": 0.9955207720588236
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1625,
          "fn": 575,
          "accuracy": 0.7386363636363636
        },
        "0.01": {
          "tp": 1378,
          "fn": 822,
          "accuracy": 0.6263636363636363
        }
      },
      "auroc": 0.9495307765151515
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1075,
          "fn": 125,
          "accuracy": 0.8958333333333334
        },
        "0.01": {
          "tp": 966,
          "fn": 234,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9857140624999999
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2700,
          "fn": 700,
          "accuracy": 0.7941176470588235
        },
        "0.01": {
          "tp": 2344,
          "fn": 1056,
          "accuracy": 0.6894117647058824
        }
      },
      "auroc": 0.9623013480392155
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3713,
          "fn": 687,
          "accuracy": 0.8438636363636364
        },
        "0.01": {
          "tp": 3381,
          "fn": 1019,
          "accuracy": 0.7684090909090909
        }
      },
      "auroc": 0.9713607954545453
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2275,
          "fn": 125,
          "accuracy": 0.9479166666666666
        },
        "0.01": {
          "tp": 2151,
          "fn": 249,
          "accuracy": 0.89625
        }
      },
      "auroc": 0.9927532118055555
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 5988,
          "fn": 812,
          "accuracy": 0.8805882352941177
        },
        "0.01": {
          "tp": 5532,
          "fn": 1268,
          "accuracy": 0.8135294117647058
        }
      },
      "auroc": 0.9789110600490195
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.6631572916666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6554802083333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.65931875
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6756208333333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6160854166666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.645853125
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.6693890625
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6357828124999998
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 795,
          "accuracy": 0.00625
        },
        "0.01": {
          "tp": 1,
          "fn": 799,
          "accuracy": 0.00125
        }
      },
      "auroc": 0.6525859375
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.7513458333333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8979937499999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 378,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 13,
          "fn": 387,
          "accuracy": 0.0325
        }
      },
      "auroc": 0.8246697916666665
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.616940625
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.8821083333333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 381,
          "accuracy": 0.0475
        },
        "0.01": {
          "tp": 13,
          "fn": 387,
          "accuracy": 0.0325
        }
      },
      "auroc": 0.7495244791666668
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        },
        "0.01": {
          "tp": 19,
          "fn": 381,
          "accuracy": 0.0475
        }
      },
      "auroc": 0.6841432291666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 384,
          "accuracy": 0.04
        },
        "0.01": {
          "tp": 7,
          "fn": 393,
          "accuracy": 0.0175
        }
      },
      "auroc": 0.8900510416666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 41,
          "fn": 759,
          "accuracy": 0.05125
        },
        "0.01": {
          "tp": 26,
          "fn": 774,
          "accuracy": 0.0325
        }
      },
      "auroc": 0.7870971354166667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6170145833333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5691041666666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5930593749999999
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6119614583333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.580784375
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5963729166666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6144880208333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5749442708333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 799,
          "accuracy": 0.00125
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5947161458333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.9366531250000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8210364583333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 277,
          "accuracy": 0.3075
        },
        "0.01": {
          "tp": 86,
          "fn": 314,
          "accuracy": 0.215
        }
      },
      "auroc": 0.8788447916666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.6515770833333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7142010416666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.6828890625
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        },
        "0.01": {
          "tp": 87,
          "fn": 313,
          "accuracy": 0.2175
        }
      },
      "auroc": 0.7941151041666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.76761875
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 671,
          "accuracy": 0.16125
        },
        "0.01": {
          "tp": 87,
          "fn": 713,
          "accuracy": 0.10875
        }
      },
      "auroc": 0.7808669270833333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.8763083333333332
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        },
        "0.01": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9381541666666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.7198072916666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        },
        "0.01": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        }
      },
      "auroc": 0.8158885416666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 110,
          "fn": 290,
          "accuracy": 0.275
        }
      },
      "auroc": 0.7678479166666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 270,
          "accuracy": 0.325
        },
        "0.01": {
          "tp": 117,
          "fn": 283,
          "accuracy": 0.2925
        }
      },
      "auroc": 0.7980578125
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        },
        "0.01": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        }
      },
      "auroc": 0.9079442708333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 405,
          "accuracy": 0.49375
        },
        "0.01": {
          "tp": 382,
          "fn": 418,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.8530010416666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.686059375
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.62124375
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6536515625
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6451010416666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6057468749999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6254239583333332
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6655802083333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6134953125
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6395377604166668
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.6440135416666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.6440135416666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6082895833333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6082895833333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.6261515625
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.6261515625
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.5648052083333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.5648052083333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5213666666666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5213666666666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.5430859375
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.5430859375
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5037145833333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5037145833333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.4627927083333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.4627927083333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.4832536458333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.4832536458333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6149281249999999
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6149281249999999
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.4455166666666666
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.4455166666666666
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5302223958333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5302223958333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.7067291666666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.7067291666666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.6666572916666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.6666572916666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.6866932291666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.6866932291666666
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 232,
          "fn": 1968,
          "accuracy": 0.10545454545454545
        },
        "0.01": {
          "tp": 176,
          "fn": 2024,
          "accuracy": 0.08
        }
      },
      "auroc": 0.6877026515151515
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 208,
          "fn": 992,
          "accuracy": 0.17333333333333334
        },
        "0.01": {
          "tp": 200,
          "fn": 1000,
          "accuracy": 0.16666666666666666
        }
      },
      "auroc": 0.7608097222222223
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 440,
          "fn": 2960,
          "accuracy": 0.12941176470588237
        },
        "0.01": {
          "tp": 376,
          "fn": 3024,
          "accuracy": 0.11058823529411765
        }
      },
      "auroc": 0.7135051470588235
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 67,
          "fn": 2133,
          "accuracy": 0.030454545454545453
        },
        "0.01": {
          "tp": 53,
          "fn": 2147,
          "accuracy": 0.02409090909090909
        }
      },
      "auroc": 0.6023301136363636
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 1121,
          "accuracy": 0.06583333333333333
        },
        "0.01": {
          "tp": 72,
          "fn": 1128,
          "accuracy": 0.06
        }
      },
      "auroc": 0.7024690972222223
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 3254,
          "accuracy": 0.04294117647058823
        },
        "0.01": {
          "tp": 125,
          "fn": 3275,
          "accuracy": 0.03676470588235294
        }
      },
      "auroc": 0.6376732843137254
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 299,
          "fn": 4101,
          "accuracy": 0.06795454545454545
        },
        "0.01": {
          "tp": 229,
          "fn": 4171,
          "accuracy": 0.05204545454545455
        }
      },
      "auroc": 0.6450163825757577
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 287,
          "fn": 2113,
          "accuracy": 0.11958333333333333
        },
        "0.01": {
          "tp": 272,
          "fn": 2128,
          "accuracy": 0.11333333333333333
        }
      },
      "auroc": 0.7316394097222222
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 586,
          "fn": 6214,
          "accuracy": 0.0861764705882353
        },
        "0.01": {
          "tp": 501,
          "fn": 6299,
          "accuracy": 0.07367647058823529
        }
      },
      "auroc": 0.6755892156862746
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9998239583333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9998604166666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9998421875
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9997729166666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9989583333333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9993656250000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9997984375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9994093750000002
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 781,
          "fn": 19,
          "accuracy": 0.97625
        }
      },
      "auroc": 0.99960390625
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.99695
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999708333333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9984604166666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        },
        "0.01": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        }
      },
      "auroc": 0.8020072916666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999614583333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.900984375
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 228,
          "fn": 172,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        }
      },
      "auroc": 0.8994786458333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999661458333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 628,
          "fn": 172,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 574,
          "fn": 226,
          "accuracy": 0.7175
        }
      },
      "auroc": 0.9497223958333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9992916666666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9992281249999999
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9992598958333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9991802083333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9987916666666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9989859375000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9992359375
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9990098958333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 763,
          "fn": 37,
          "accuracy": 0.95375
        }
      },
      "auroc": 0.9991229166666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.995209375
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9976046875
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.8760666666666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        }
      },
      "auroc": 0.9499270833333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 237,
          "accuracy": 0.4075
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.912996875
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        }
      },
      "auroc": 0.9380333333333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.9725682291666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 559,
          "fn": 241,
          "accuracy": 0.69875
        },
        "0.01": {
          "tp": 404,
          "fn": 396,
          "accuracy": 0.505
        }
      },
      "auroc": 0.95530078125
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9989958333333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9994979166666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.8135927083333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9792020833333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        }
      },
      "auroc": 0.8963973958333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        }
      },
      "auroc": 0.9062942708333332
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9896010416666665
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 647,
          "fn": 153,
          "accuracy": 0.80875
        },
        "0.01": {
          "tp": 593,
          "fn": 207,
          "accuracy": 0.74125
        }
      },
      "auroc": 0.9479476562500001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.999953125
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9997739583333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9998635416666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.99945
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9986333333333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9990416666666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9997015625
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9992036458333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 776,
          "fn": 24,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9994526041666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        }
      },
      "auroc": 0.9822375
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        }
      },
      "auroc": 0.9822375
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.9686864583333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.9686864583333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 120,
          "fn": 280,
          "accuracy": 0.3
        }
      },
      "auroc": 0.9754619791666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 120,
          "fn": 280,
          "accuracy": 0.3
        }
      },
      "auroc": 0.9754619791666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        }
      },
      "auroc": 0.8695427083333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        }
      },
      "auroc": 0.8695427083333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        }
      },
      "auroc": 0.7564427083333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        }
      },
      "auroc": 0.7564427083333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 294,
          "accuracy": 0.265
        },
        "0.01": {
          "tp": 58,
          "fn": 342,
          "accuracy": 0.145
        }
      },
      "auroc": 0.8129927083333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 294,
          "accuracy": 0.265
        },
        "0.01": {
          "tp": 58,
          "fn": 342,
          "accuracy": 0.145
        }
      },
      "auroc": 0.8129927083333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999354166666666
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999354166666666
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9982666666666666
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9982666666666666
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9991010416666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9991010416666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9994166666666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9994166666666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.9770302083333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.9770302083333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        }
      },
      "auroc": 0.9882234375000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        }
      },
      "auroc": 0.9882234375000001
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9819072916666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9819072916666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.9657760416666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.9657760416666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        }
      },
      "auroc": 0.9738416666666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        }
      },
      "auroc": 0.9738416666666667
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1992,
          "fn": 208,
          "accuracy": 0.9054545454545454
        },
        "0.01": {
          "tp": 1780,
          "fn": 420,
          "accuracy": 0.8090909090909091
        }
      },
      "auroc": 0.9843685606060606
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1196,
          "fn": 4,
          "accuracy": 0.9966666666666667
        },
        "0.01": {
          "tp": 1137,
          "fn": 63,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9990071180555556
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3188,
          "fn": 212,
          "accuracy": 0.9376470588235294
        },
        "0.01": {
          "tp": 2917,
          "fn": 483,
          "accuracy": 0.8579411764705882
        }
      },
      "auroc": 0.9895351102941177
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1431,
          "fn": 769,
          "accuracy": 0.6504545454545455
        },
        "0.01": {
          "tp": 1122,
          "fn": 1078,
          "accuracy": 0.51
        }
      },
      "auroc": 0.9232974431818183
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1090,
          "fn": 110,
          "accuracy": 0.9083333333333333
        },
        "0.01": {
          "tp": 967,
          "fn": 233,
          "accuracy": 0.8058333333333333
        }
      },
      "auroc": 0.9875789930555555
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2521,
          "fn": 879,
          "accuracy": 0.7414705882352941
        },
        "0.01": {
          "tp": 2089,
          "fn": 1311,
          "accuracy": 0.6144117647058823
        }
      },
      "auroc": 0.9459850490196078
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3423,
          "fn": 977,
          "accuracy": 0.7779545454545455
        },
        "0.01": {
          "tp": 2902,
          "fn": 1498,
          "accuracy": 0.6595454545454545
        }
      },
      "auroc": 0.9538330018939394
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2286,
          "fn": 114,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 2104,
          "fn": 296,
          "accuracy": 0.8766666666666667
        }
      },
      "auroc": 0.9932930555555557
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 5709,
          "fn": 1091,
          "accuracy": 0.8395588235294118
        },
        "0.01": {
          "tp": 5006,
          "fn": 1794,
          "accuracy": 0.7361764705882353
        }
      },
      "auroc": 0.9677600796568627
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9997406249999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9998703124999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9998703124999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        }
      },
      "auroc": 0.99993515625
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9996979166666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999708333333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9998343749999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.828071875
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999614583333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        }
      },
      "auroc": 0.9140166666666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        }
      },
      "auroc": 0.9138848958333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999661458333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 649,
          "fn": 151,
          "accuracy": 0.81125
        },
        "0.01": {
          "tp": 608,
          "fn": 192,
          "accuracy": 0.76
        }
      },
      "auroc": 0.9569255208333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999739583333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9996020833333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9997880208333332
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9998041666666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.99888125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9993427083333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9998890625
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9992416666666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 784,
          "fn": 16,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9995653645833333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.99845625
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.999228125
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.8794770833333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        },
        "0.01": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        }
      },
      "auroc": 0.9037416666666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 250,
          "accuracy": 0.375
        },
        "0.01": {
          "tp": 51,
          "fn": 349,
          "accuracy": 0.1275
        }
      },
      "auroc": 0.8916093750000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        },
        "0.01": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        }
      },
      "auroc": 0.9397385416666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        }
      },
      "auroc": 0.9510989583333332
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 549,
          "fn": 251,
          "accuracy": 0.68625
        },
        "0.01": {
          "tp": 434,
          "fn": 366,
          "accuracy": 0.5425
        }
      },
      "auroc": 0.94541875
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9998052083333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999026041666668
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        }
      },
      "auroc": 0.825915625
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9673677083333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.8966416666666668
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        },
        "0.01": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9128604166666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9836838541666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 641,
          "fn": 159,
          "accuracy": 0.80125
        },
        "0.01": {
          "tp": 590,
          "fn": 210,
          "accuracy": 0.7375
        }
      },
      "auroc": 0.9482721354166667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99996875
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999843749999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999614583333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9993479166666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9996546875000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999807291666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9996583333333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        }
      },
      "auroc": 0.99981953125
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.99580625
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.99580625
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9875062499999999
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9875062499999999
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        }
      },
      "auroc": 0.99165625
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        }
      },
      "auroc": 0.99165625
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.9207249999999999
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.9207249999999999
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        },
        "0.01": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        }
      },
      "auroc": 0.8324468749999999
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        },
        "0.01": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        }
      },
      "auroc": 0.8324468749999999
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 235,
          "accuracy": 0.4125
        },
        "0.01": {
          "tp": 100,
          "fn": 300,
          "accuracy": 0.25
        }
      },
      "auroc": 0.8765859375000001
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 235,
          "accuracy": 0.4125
        },
        "0.01": {
          "tp": 100,
          "fn": 300,
          "accuracy": 0.25
        }
      },
      "auroc": 0.8765859375000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999947916666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9999947916666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9998072916666666
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9998072916666666
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999010416666666
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9999010416666666
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999177083333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9999177083333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.9814020833333332
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.9814020833333332
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        }
      },
      "auroc": 0.9906598958333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        }
      },
      "auroc": 0.9906598958333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9856791666666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9856791666666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9807104166666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9807104166666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        }
      },
      "auroc": 0.9831947916666668
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        }
      },
      "auroc": 0.9831947916666668
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2071,
          "fn": 129,
          "accuracy": 0.9413636363636364
        },
        "0.01": {
          "tp": 1982,
          "fn": 218,
          "accuracy": 0.9009090909090909
        }
      },
      "auroc": 0.9910545454545454
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 1178,
          "fn": 22,
          "accuracy": 0.9816666666666667
        }
      },
      "auroc": 0.9996663194444444
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3270,
          "fn": 130,
          "accuracy": 0.961764705882353
        },
        "0.01": {
          "tp": 3160,
          "fn": 240,
          "accuracy": 0.9294117647058824
        }
      },
      "auroc": 0.9940939950980392
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1570,
          "fn": 630,
          "accuracy": 0.7136363636363636
        },
        "0.01": {
          "tp": 1320,
          "fn": 880,
          "accuracy": 0.6
        }
      },
      "auroc": 0.9377366477272728
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1045,
          "fn": 155,
          "accuracy": 0.8708333333333333
        },
        "0.01": {
          "tp": 952,
          "fn": 248,
          "accuracy": 0.7933333333333333
        }
      },
      "auroc": 0.9781734375
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2615,
          "fn": 785,
          "accuracy": 0.7691176470588236
        },
        "0.01": {
          "tp": 2272,
          "fn": 1128,
          "accuracy": 0.668235294117647
        }
      },
      "auroc": 0.9520084558823529
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3641,
          "fn": 759,
          "accuracy": 0.8275
        },
        "0.01": {
          "tp": 3302,
          "fn": 1098,
          "accuracy": 0.7504545454545455
        }
      },
      "auroc": 0.9643955965909091
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2244,
          "fn": 156,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 2130,
          "fn": 270,
          "accuracy": 0.8875
        }
      },
      "auroc": 0.9889198784722223
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 5885,
          "fn": 915,
          "accuracy": 0.8654411764705883
        },
        "0.01": {
          "tp": 5432,
          "fn": 1368,
          "accuracy": 0.7988235294117647
        }
      },
      "auroc": 0.9730512254901962
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.706209375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7140041666666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7101067708333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7134583333333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7144177083333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7139380208333332
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7098338541666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7142109375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7120223958333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.7218489583333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.9135385416666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        }
      },
      "auroc": 0.81769375
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.7604604166666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.9203947916666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 381,
          "accuracy": 0.0475
        },
        "0.01": {
          "tp": 13,
          "fn": 387,
          "accuracy": 0.0325
        }
      },
      "auroc": 0.8404276041666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 387,
          "accuracy": 0.0325
        },
        "0.01": {
          "tp": 11,
          "fn": 389,
          "accuracy": 0.0275
        }
      },
      "auroc": 0.7411546875
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 386,
          "accuracy": 0.035
        },
        "0.01": {
          "tp": 7,
          "fn": 393,
          "accuracy": 0.0175
        }
      },
      "auroc": 0.9169666666666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 773,
          "accuracy": 0.03375
        },
        "0.01": {
          "tp": 18,
          "fn": 782,
          "accuracy": 0.0225
        }
      },
      "auroc": 0.8290606770833333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6899020833333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.736015625
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7129588541666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7158197916666665
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.754078125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7349489583333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7028609375000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.745046875
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 799,
          "accuracy": 0.00125
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7239539062499999
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.856396875
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7661937500000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        },
        "0.01": {
          "tp": 33,
          "fn": 367,
          "accuracy": 0.0825
        }
      },
      "auroc": 0.8112953125
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.776953125
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.8288375
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 393,
          "accuracy": 0.0175
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.8028953125
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 341,
          "accuracy": 0.1475
        },
        "0.01": {
          "tp": 34,
          "fn": 366,
          "accuracy": 0.085
        }
      },
      "auroc": 0.816675
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7975156250000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 739,
          "accuracy": 0.07625
        },
        "0.01": {
          "tp": 34,
          "fn": 766,
          "accuracy": 0.0425
        }
      },
      "auroc": 0.8070953125
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.8284083333333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 1.0
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        }
      },
      "auroc": 0.9142041666666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        },
        "0.01": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        }
      },
      "auroc": 0.8194145833333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        },
        "0.01": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        }
      },
      "auroc": 0.8925770833333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 284,
          "accuracy": 0.29
        },
        "0.01": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        }
      },
      "auroc": 0.8559958333333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 283,
          "accuracy": 0.2925
        },
        "0.01": {
          "tp": 110,
          "fn": 290,
          "accuracy": 0.275
        }
      },
      "auroc": 0.8239114583333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 267,
          "fn": 133,
          "accuracy": 0.6675
        },
        "0.01": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        }
      },
      "auroc": 0.9462885416666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 416,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 374,
          "fn": 426,
          "accuracy": 0.4675
        }
      },
      "auroc": 0.8851
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7125875
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6906677083333332
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7016276041666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7244291666666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7280906250000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7262598958333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7185083333333332
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7093791666666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7139437499999999
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7061729166666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7061729166666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7009364583333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7009364583333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7035546875
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7035546875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7134072916666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7134072916666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.710634375
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.710634375
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7120208333333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7120208333333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6477916666666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6477916666666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.664878125
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.664878125
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6563348958333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.6563348958333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7107270833333332
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7107270833333332
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7029885416666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7029885416666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7068578125
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7068578125
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7624270833333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7624270833333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7573989583333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7573989583333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7599130208333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.7599130208333333
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 2070,
          "accuracy": 0.05909090909090909
        },
        "0.01": {
          "tp": 104,
          "fn": 2096,
          "accuracy": 0.04727272727272727
        }
      },
      "auroc": 0.7323526515151515
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 998,
          "accuracy": 0.16833333333333333
        },
        "0.01": {
          "tp": 200,
          "fn": 1000,
          "accuracy": 0.16666666666666666
        }
      },
      "auroc": 0.8034032986111111
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 3068,
          "accuracy": 0.0976470588235294
        },
        "0.01": {
          "tp": 304,
          "fn": 3096,
          "accuracy": 0.08941176470588236
        }
      },
      "auroc": 0.7574293504901961
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 2138,
          "accuracy": 0.028181818181818183
        },
        "0.01": {
          "tp": 51,
          "fn": 2149,
          "accuracy": 0.023181818181818182
        }
      },
      "auroc": 0.7315792613636365
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 1118,
          "accuracy": 0.06833333333333333
        },
        "0.01": {
          "tp": 71,
          "fn": 1129,
          "accuracy": 0.059166666666666666
        }
      },
      "auroc": 0.8063993055555554
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 3256,
          "accuracy": 0.042352941176470586
        },
        "0.01": {
          "tp": 122,
          "fn": 3278,
          "accuracy": 0.03588235294117647
        }
      },
      "auroc": 0.7579863357843137
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 4208,
          "accuracy": 0.04363636363636364
        },
        "0.01": {
          "tp": 155,
          "fn": 4245,
          "accuracy": 0.035227272727272725
        }
      },
      "auroc": 0.7319659564393939
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 2116,
          "accuracy": 0.11833333333333333
        },
        "0.01": {
          "tp": 271,
          "fn": 2129,
          "accuracy": 0.11291666666666667
        }
      },
      "auroc": 0.8049013020833333
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 476,
          "fn": 6324,
          "accuracy": 0.07
        },
        "0.01": {
          "tp": 426,
          "fn": 6374,
          "accuracy": 0.06264705882352942
        }
      },
      "auroc": 0.7577078431372548
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1977,
          "fn": 423,
          "accuracy": 0.82375
        },
        "0.01": {
          "tp": 1899,
          "fn": 501,
          "accuracy": 0.79125
        }
      },
      "auroc": 0.9458896701388888
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1957,
          "fn": 443,
          "accuracy": 0.8154166666666667
        },
        "0.01": {
          "tp": 1889,
          "fn": 511,
          "accuracy": 0.7870833333333334
        }
      },
      "auroc": 0.9453781250000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3934,
          "fn": 866,
          "accuracy": 0.8195833333333333
        },
        "0.01": {
          "tp": 3788,
          "fn": 1012,
          "accuracy": 0.7891666666666667
        }
      },
      "auroc": 0.9456338975694445
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1970,
          "fn": 430,
          "accuracy": 0.8208333333333333
        },
        "0.01": {
          "tp": 1901,
          "fn": 499,
          "accuracy": 0.7920833333333334
        }
      },
      "auroc": 0.9477796875
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1930,
          "fn": 470,
          "accuracy": 0.8041666666666667
        },
        "0.01": {
          "tp": 1813,
          "fn": 587,
          "accuracy": 0.7554166666666666
        }
      },
      "auroc": 0.9405769965277778
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3900,
          "fn": 900,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 3714,
          "fn": 1086,
          "accuracy": 0.77375
        }
      },
      "auroc": 0.9441783420138888
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3947,
          "fn": 853,
          "accuracy": 0.8222916666666666
        },
        "0.01": {
          "tp": 3800,
          "fn": 1000,
          "accuracy": 0.7916666666666666
        }
      },
      "auroc": 0.9468346788194444
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3887,
          "fn": 913,
          "accuracy": 0.8097916666666667
        },
        "0.01": {
          "tp": 3702,
          "fn": 1098,
          "accuracy": 0.77125
        }
      },
      "auroc": 0.9429775607638888
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7834,
          "fn": 1766,
          "accuracy": 0.8160416666666667
        },
        "0.01": {
          "tp": 7502,
          "fn": 2098,
          "accuracy": 0.7814583333333334
        }
      },
      "auroc": 0.9449061197916668
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1996,
          "fn": 404,
          "accuracy": 0.8316666666666667
        },
        "0.01": {
          "tp": 1874,
          "fn": 526,
          "accuracy": 0.7808333333333334
        }
      },
      "auroc": 0.9547048611111111
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1872,
          "fn": 528,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 1820,
          "fn": 580,
          "accuracy": 0.7583333333333333
        }
      },
      "auroc": 0.9765296875
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3868,
          "fn": 932,
          "accuracy": 0.8058333333333333
        },
        "0.01": {
          "tp": 3694,
          "fn": 1106,
          "accuracy": 0.7695833333333333
        }
      },
      "auroc": 0.9656172743055556
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 467,
          "fn": 1933,
          "accuracy": 0.19458333333333333
        },
        "0.01": {
          "tp": 158,
          "fn": 2242,
          "accuracy": 0.06583333333333333
        }
      },
      "auroc": 0.7893026041666668
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1925,
          "fn": 475,
          "accuracy": 0.8020833333333334
        },
        "0.01": {
          "tp": 1838,
          "fn": 562,
          "accuracy": 0.7658333333333334
        }
      },
      "auroc": 0.9781316840277778
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2392,
          "fn": 2408,
          "accuracy": 0.49833333333333335
        },
        "0.01": {
          "tp": 1996,
          "fn": 2804,
          "accuracy": 0.41583333333333333
        }
      },
      "auroc": 0.8837171440972222
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2463,
          "fn": 2337,
          "accuracy": 0.513125
        },
        "0.01": {
          "tp": 2032,
          "fn": 2768,
          "accuracy": 0.42333333333333334
        }
      },
      "auroc": 0.8720037326388891
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3797,
          "fn": 1003,
          "accuracy": 0.7910416666666666
        },
        "0.01": {
          "tp": 3658,
          "fn": 1142,
          "accuracy": 0.7620833333333333
        }
      },
      "auroc": 0.9773306857638889
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 6260,
          "fn": 3340,
          "accuracy": 0.6520833333333333
        },
        "0.01": {
          "tp": 5690,
          "fn": 3910,
          "accuracy": 0.5927083333333333
        }
      },
      "auroc": 0.9246672092013888
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1947,
          "fn": 453,
          "accuracy": 0.81125
        },
        "0.01": {
          "tp": 1859,
          "fn": 541,
          "accuracy": 0.7745833333333333
        }
      },
      "auroc": 0.9391185763888888
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1921,
          "fn": 479,
          "accuracy": 0.8004166666666667
        },
        "0.01": {
          "tp": 1803,
          "fn": 597,
          "accuracy": 0.75125
        }
      },
      "auroc": 0.9365717881944446
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3868,
          "fn": 932,
          "accuracy": 0.8058333333333333
        },
        "0.01": {
          "tp": 3662,
          "fn": 1138,
          "accuracy": 0.7629166666666667
        }
      },
      "auroc": 0.9378451822916668
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1935,
          "fn": 465,
          "accuracy": 0.80625
        },
        "0.01": {
          "tp": 1855,
          "fn": 545,
          "accuracy": 0.7729166666666667
        }
      },
      "auroc": 0.9403677951388889
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1878,
          "fn": 522,
          "accuracy": 0.7825
        },
        "0.01": {
          "tp": 1719,
          "fn": 681,
          "accuracy": 0.71625
        }
      },
      "auroc": 0.9348402777777779
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3813,
          "fn": 987,
          "accuracy": 0.794375
        },
        "0.01": {
          "tp": 3574,
          "fn": 1226,
          "accuracy": 0.7445833333333334
        }
      },
      "auroc": 0.9376040364583333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3882,
          "fn": 918,
          "accuracy": 0.80875
        },
        "0.01": {
          "tp": 3714,
          "fn": 1086,
          "accuracy": 0.77375
        }
      },
      "auroc": 0.9397431857638888
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3799,
          "fn": 1001,
          "accuracy": 0.7914583333333334
        },
        "0.01": {
          "tp": 3522,
          "fn": 1278,
          "accuracy": 0.73375
        }
      },
      "auroc": 0.9357060329861111
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7681,
          "fn": 1919,
          "accuracy": 0.8001041666666666
        },
        "0.01": {
          "tp": 7236,
          "fn": 2364,
          "accuracy": 0.75375
        }
      },
      "auroc": 0.9377246093749998
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2174,
          "fn": 226,
          "accuracy": 0.9058333333333334
        },
        "0.01": {
          "tp": 2115,
          "fn": 285,
          "accuracy": 0.88125
        }
      },
      "auroc": 0.9827364583333332
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1906,
          "fn": 494,
          "accuracy": 0.7941666666666667
        },
        "0.01": {
          "tp": 1592,
          "fn": 808,
          "accuracy": 0.6633333333333333
        }
      },
      "auroc": 0.9607946180555555
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4080,
          "fn": 720,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 3707,
          "fn": 1093,
          "accuracy": 0.7722916666666667
        }
      },
      "auroc": 0.9717655381944444
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 641,
          "fn": 1759,
          "accuracy": 0.26708333333333334
        },
        "0.01": {
          "tp": 137,
          "fn": 2263,
          "accuracy": 0.05708333333333333
        }
      },
      "auroc": 0.8518297743055558
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 951,
          "fn": 1449,
          "accuracy": 0.39625
        },
        "0.01": {
          "tp": 348,
          "fn": 2052,
          "accuracy": 0.145
        }
      },
      "auroc": 0.8903437500000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1592,
          "fn": 3208,
          "accuracy": 0.33166666666666667
        },
        "0.01": {
          "tp": 485,
          "fn": 4315,
          "accuracy": 0.10104166666666667
        }
      },
      "auroc": 0.8710867621527778
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2815,
          "fn": 1985,
          "accuracy": 0.5864583333333333
        },
        "0.01": {
          "tp": 2252,
          "fn": 2548,
          "accuracy": 0.4691666666666667
        }
      },
      "auroc": 0.9172831163194446
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2857,
          "fn": 1943,
          "accuracy": 0.5952083333333333
        },
        "0.01": {
          "tp": 1940,
          "fn": 2860,
          "accuracy": 0.4041666666666667
        }
      },
      "auroc": 0.9255691840277779
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5672,
          "fn": 3928,
          "accuracy": 0.5908333333333333
        },
        "0.01": {
          "tp": 4192,
          "fn": 5408,
          "accuracy": 0.43666666666666665
        }
      },
      "auroc": 0.9214261501736111
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2136,
          "fn": 264,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 2061,
          "fn": 339,
          "accuracy": 0.85875
        }
      },
      "auroc": 0.9746364583333332
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2293,
          "fn": 107,
          "accuracy": 0.9554166666666667
        },
        "0.01": {
          "tp": 2238,
          "fn": 162,
          "accuracy": 0.9325
        }
      },
      "auroc": 0.9950930555555556
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4429,
          "fn": 371,
          "accuracy": 0.9227083333333334
        },
        "0.01": {
          "tp": 4299,
          "fn": 501,
          "accuracy": 0.895625
        }
      },
      "auroc": 0.9848647569444445
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 822,
          "fn": 1578,
          "accuracy": 0.3425
        },
        "0.01": {
          "tp": 579,
          "fn": 1821,
          "accuracy": 0.24125
        }
      },
      "auroc": 0.8080538194444445
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1659,
          "fn": 741,
          "accuracy": 0.69125
        },
        "0.01": {
          "tp": 1420,
          "fn": 980,
          "accuracy": 0.5916666666666667
        }
      },
      "auroc": 0.9351775173611111
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2481,
          "fn": 2319,
          "accuracy": 0.516875
        },
        "0.01": {
          "tp": 1999,
          "fn": 2801,
          "accuracy": 0.4164583333333333
        }
      },
      "auroc": 0.8716156684027777
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2958,
          "fn": 1842,
          "accuracy": 0.61625
        },
        "0.01": {
          "tp": 2640,
          "fn": 2160,
          "accuracy": 0.55
        }
      },
      "auroc": 0.8913451388888889
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3952,
          "fn": 848,
          "accuracy": 0.8233333333333334
        },
        "0.01": {
          "tp": 3658,
          "fn": 1142,
          "accuracy": 0.7620833333333333
        }
      },
      "auroc": 0.9651352864583332
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 6910,
          "fn": 2690,
          "accuracy": 0.7197916666666667
        },
        "0.01": {
          "tp": 6298,
          "fn": 3302,
          "accuracy": 0.6560416666666666
        }
      },
      "auroc": 0.9282402126736111
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1977,
          "fn": 423,
          "accuracy": 0.82375
        },
        "0.01": {
          "tp": 1934,
          "fn": 466,
          "accuracy": 0.8058333333333333
        }
      },
      "auroc": 0.9488605902777776
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1967,
          "fn": 433,
          "accuracy": 0.8195833333333333
        },
        "0.01": {
          "tp": 1899,
          "fn": 501,
          "accuracy": 0.79125
        }
      },
      "auroc": 0.9407815104166668
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3944,
          "fn": 856,
          "accuracy": 0.8216666666666667
        },
        "0.01": {
          "tp": 3833,
          "fn": 967,
          "accuracy": 0.7985416666666667
        }
      },
      "auroc": 0.9448210503472223
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1960,
          "fn": 440,
          "accuracy": 0.8166666666666667
        },
        "0.01": {
          "tp": 1893,
          "fn": 507,
          "accuracy": 0.78875
        }
      },
      "auroc": 0.9456093750000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1915,
          "fn": 485,
          "accuracy": 0.7979166666666667
        },
        "0.01": {
          "tp": 1756,
          "fn": 644,
          "accuracy": 0.7316666666666667
        }
      },
      "auroc": 0.9397064236111111
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3875,
          "fn": 925,
          "accuracy": 0.8072916666666666
        },
        "0.01": {
          "tp": 3649,
          "fn": 1151,
          "accuracy": 0.7602083333333334
        }
      },
      "auroc": 0.9426578993055555
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3937,
          "fn": 863,
          "accuracy": 0.8202083333333333
        },
        "0.01": {
          "tp": 3827,
          "fn": 973,
          "accuracy": 0.7972916666666666
        }
      },
      "auroc": 0.947234982638889
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3882,
          "fn": 918,
          "accuracy": 0.80875
        },
        "0.01": {
          "tp": 3655,
          "fn": 1145,
          "accuracy": 0.7614583333333333
        }
      },
      "auroc": 0.940243967013889
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7819,
          "fn": 1781,
          "accuracy": 0.8144791666666666
        },
        "0.01": {
          "tp": 7482,
          "fn": 2118,
          "accuracy": 0.779375
        }
      },
      "auroc": 0.9437394748263889
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1787,
          "fn": 613,
          "accuracy": 0.7445833333333334
        },
        "0.01": {
          "tp": 1333,
          "fn": 1067,
          "accuracy": 0.5554166666666667
        }
      },
      "auroc": 0.9352864583333335
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1787,
          "fn": 613,
          "accuracy": 0.7445833333333334
        },
        "0.01": {
          "tp": 1333,
          "fn": 1067,
          "accuracy": 0.5554166666666667
        }
      },
      "auroc": 0.9352864583333335
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1593,
          "fn": 807,
          "accuracy": 0.66375
        },
        "0.01": {
          "tp": 1104,
          "fn": 1296,
          "accuracy": 0.46
        }
      },
      "auroc": 0.9219621527777778
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1593,
          "fn": 807,
          "accuracy": 0.66375
        },
        "0.01": {
          "tp": 1104,
          "fn": 1296,
          "accuracy": 0.46
        }
      },
      "auroc": 0.9219621527777778
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3380,
          "fn": 1420,
          "accuracy": 0.7041666666666667
        },
        "0.01": {
          "tp": 2437,
          "fn": 2363,
          "accuracy": 0.5077083333333333
        }
      },
      "auroc": 0.9286243055555554
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3380,
          "fn": 1420,
          "accuracy": 0.7041666666666667
        },
        "0.01": {
          "tp": 2437,
          "fn": 2363,
          "accuracy": 0.5077083333333333
        }
      },
      "auroc": 0.9286243055555554
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 882,
          "fn": 1518,
          "accuracy": 0.3675
        },
        "0.01": {
          "tp": 515,
          "fn": 1885,
          "accuracy": 0.21458333333333332
        }
      },
      "auroc": 0.8518249131944444
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 882,
          "fn": 1518,
          "accuracy": 0.3675
        },
        "0.01": {
          "tp": 515,
          "fn": 1885,
          "accuracy": 0.21458333333333332
        }
      },
      "auroc": 0.8518249131944444
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 564,
          "fn": 1836,
          "accuracy": 0.235
        },
        "0.01": {
          "tp": 353,
          "fn": 2047,
          "accuracy": 0.14708333333333334
        }
      },
      "auroc": 0.7758189236111112
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 564,
          "fn": 1836,
          "accuracy": 0.235
        },
        "0.01": {
          "tp": 353,
          "fn": 2047,
          "accuracy": 0.14708333333333334
        }
      },
      "auroc": 0.7758189236111112
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1446,
          "fn": 3354,
          "accuracy": 0.30125
        },
        "0.01": {
          "tp": 868,
          "fn": 3932,
          "accuracy": 0.18083333333333335
        }
      },
      "auroc": 0.8138219184027777
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1446,
          "fn": 3354,
          "accuracy": 0.30125
        },
        "0.01": {
          "tp": 868,
          "fn": 3932,
          "accuracy": 0.18083333333333335
        }
      },
      "auroc": 0.8138219184027777
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1969,
          "fn": 431,
          "accuracy": 0.8204166666666667
        },
        "0.01": {
          "tp": 1917,
          "fn": 483,
          "accuracy": 0.79875
        }
      },
      "auroc": 0.9271827256944445
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1969,
          "fn": 431,
          "accuracy": 0.8204166666666667
        },
        "0.01": {
          "tp": 1917,
          "fn": 483,
          "accuracy": 0.79875
        }
      },
      "auroc": 0.9271827256944445
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1946,
          "fn": 454,
          "accuracy": 0.8108333333333333
        },
        "0.01": {
          "tp": 1849,
          "fn": 551,
          "accuracy": 0.7704166666666666
        }
      },
      "auroc": 0.9233854166666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1946,
          "fn": 454,
          "accuracy": 0.8108333333333333
        },
        "0.01": {
          "tp": 1849,
          "fn": 551,
          "accuracy": 0.7704166666666666
        }
      },
      "auroc": 0.9233854166666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3915,
          "fn": 885,
          "accuracy": 0.815625
        },
        "0.01": {
          "tp": 3766,
          "fn": 1034,
          "accuracy": 0.7845833333333333
        }
      },
      "auroc": 0.9252840711805557
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3915,
          "fn": 885,
          "accuracy": 0.815625
        },
        "0.01": {
          "tp": 3766,
          "fn": 1034,
          "accuracy": 0.7845833333333333
        }
      },
      "auroc": 0.9252840711805557
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1950,
          "fn": 450,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 1867,
          "fn": 533,
          "accuracy": 0.7779166666666667
        }
      },
      "auroc": 0.9409989583333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1950,
          "fn": 450,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 1867,
          "fn": 533,
          "accuracy": 0.7779166666666667
        }
      },
      "auroc": 0.9409989583333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1516,
          "fn": 884,
          "accuracy": 0.6316666666666667
        },
        "0.01": {
          "tp": 1117,
          "fn": 1283,
          "accuracy": 0.46541666666666665
        }
      },
      "auroc": 0.9045527777777778
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1516,
          "fn": 884,
          "accuracy": 0.6316666666666667
        },
        "0.01": {
          "tp": 1117,
          "fn": 1283,
          "accuracy": 0.46541666666666665
        }
      },
      "auroc": 0.9045527777777778
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3466,
          "fn": 1334,
          "accuracy": 0.7220833333333333
        },
        "0.01": {
          "tp": 2984,
          "fn": 1816,
          "accuracy": 0.6216666666666667
        }
      },
      "auroc": 0.9227758680555553
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3466,
          "fn": 1334,
          "accuracy": 0.7220833333333333
        },
        "0.01": {
          "tp": 2984,
          "fn": 1816,
          "accuracy": 0.6216666666666667
        }
      },
      "auroc": 0.9227758680555553
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1714,
          "fn": 686,
          "accuracy": 0.7141666666666666
        },
        "0.01": {
          "tp": 1516,
          "fn": 884,
          "accuracy": 0.6316666666666667
        }
      },
      "auroc": 0.9393505208333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1714,
          "fn": 686,
          "accuracy": 0.7141666666666666
        },
        "0.01": {
          "tp": 1516,
          "fn": 884,
          "accuracy": 0.6316666666666667
        }
      },
      "auroc": 0.9393505208333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1580,
          "fn": 820,
          "accuracy": 0.6583333333333333
        },
        "0.01": {
          "tp": 1292,
          "fn": 1108,
          "accuracy": 0.5383333333333333
        }
      },
      "auroc": 0.9233711805555556
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1580,
          "fn": 820,
          "accuracy": 0.6583333333333333
        },
        "0.01": {
          "tp": 1292,
          "fn": 1108,
          "accuracy": 0.5383333333333333
        }
      },
      "auroc": 0.9233711805555556
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3294,
          "fn": 1506,
          "accuracy": 0.68625
        },
        "0.01": {
          "tp": 2808,
          "fn": 1992,
          "accuracy": 0.585
        }
      },
      "auroc": 0.9313608506944444
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3294,
          "fn": 1506,
          "accuracy": 0.68625
        },
        "0.01": {
          "tp": 2808,
          "fn": 1992,
          "accuracy": 0.585
        }
      },
      "auroc": 0.9313608506944444
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 20509,
          "fn": 5891,
          "accuracy": 0.7768560606060606
        },
        "0.01": {
          "tp": 18890,
          "fn": 7510,
          "accuracy": 0.7155303030303031
        }
      },
      "auroc": 0.9400536537247475
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 11916,
          "fn": 2484,
          "accuracy": 0.8275
        },
        "0.01": {
          "tp": 11241,
          "fn": 3159,
          "accuracy": 0.780625
        }
      },
      "auroc": 0.9591914641203704
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 32425,
          "fn": 8375,
          "accuracy": 0.7947303921568627
        },
        "0.01": {
          "tp": 30131,
          "fn": 10669,
          "accuracy": 0.7385049019607843
        }
      },
      "auroc": 0.9468081750408497
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 14994,
          "fn": 11406,
          "accuracy": 0.5679545454545455
        },
        "0.01": {
          "tp": 12238,
          "fn": 14162,
          "accuracy": 0.46356060606060606
        }
      },
      "auroc": 0.8847303188131315
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 10258,
          "fn": 4142,
          "accuracy": 0.7123611111111111
        },
        "0.01": {
          "tp": 8894,
          "fn": 5506,
          "accuracy": 0.6176388888888888
        }
      },
      "auroc": 0.9364627748842593
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 25252,
          "fn": 15548,
          "accuracy": 0.618921568627451
        },
        "0.01": {
          "tp": 21132,
          "fn": 19668,
          "accuracy": 0.5179411764705882
        }
      },
      "auroc": 0.9029888327205882
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 35503,
          "fn": 17297,
          "accuracy": 0.672405303030303
        },
        "0.01": {
          "tp": 31128,
          "fn": 21672,
          "accuracy": 0.5895454545454546
        }
      },
      "auroc": 0.9123919862689395
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 22174,
          "fn": 6626,
          "accuracy": 0.7699305555555556
        },
        "0.01": {
          "tp": 20135,
          "fn": 8665,
          "accuracy": 0.6991319444444445
        }
      },
      "auroc": 0.9478271195023148
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 57677,
          "fn": 23923,
          "accuracy": 0.7068259803921568
        },
        "0.01": {
          "tp": 51263,
          "fn": 30337,
          "accuracy": 0.6282230392156862
        }
      },
      "auroc": 0.924898503880719
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9979968750000001
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9983734375000001
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9983734375000001
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.99856171875
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9987229166666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.986803125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        }
      },
      "auroc": 0.9927630208333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.7025635416666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9944614583333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        }
      },
      "auroc": 0.8485125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        },
        "0.01": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        }
      },
      "auroc": 0.8506432291666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9906322916666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 601,
          "fn": 199,
          "accuracy": 0.75125
        },
        "0.01": {
          "tp": 567,
          "fn": 233,
          "accuracy": 0.70875
        }
      },
      "auroc": 0.9206377604166666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9986239583333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9963781249999999
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9975010416666665
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.99835
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9864052083333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9923776041666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9984869791666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9913916666666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 775,
          "fn": 25,
          "accuracy": 0.96875
        },
        "0.01": {
          "tp": 749,
          "fn": 51,
          "accuracy": 0.93625
        }
      },
      "auroc": 0.9949393229166666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.993009375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9958796875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.669975
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.6883510416666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        }
      },
      "auroc": 0.6791630208333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8343625000000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        },
        "0.01": {
          "tp": 176,
          "fn": 224,
          "accuracy": 0.44
        }
      },
      "auroc": 0.8406802083333335
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 412,
          "fn": 388,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 380,
          "fn": 420,
          "accuracy": 0.475
        }
      },
      "auroc": 0.8375213541666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9987364583333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9785124999999999
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9886244791666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.7518916666666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.7822739583333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 299,
          "accuracy": 0.2525
        },
        "0.01": {
          "tp": 70,
          "fn": 330,
          "accuracy": 0.175
        }
      },
      "auroc": 0.7670828125
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        },
        "0.01": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        }
      },
      "auroc": 0.8753140625
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        },
        "0.01": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        }
      },
      "auroc": 0.8803932291666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 462,
          "fn": 338,
          "accuracy": 0.5775
        },
        "0.01": {
          "tp": 400,
          "fn": 400,
          "accuracy": 0.5
        }
      },
      "auroc": 0.8778536458333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9984604166666666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9929635416666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9957119791666668
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9986052083333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9958567708333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        },
        "0.01": {
          "tp": 785,
          "fn": 15,
          "accuracy": 0.98125
        }
      },
      "auroc": 0.9972309895833333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.995528125
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.995528125
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9767281250000001
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9767281250000001
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        }
      },
      "auroc": 0.986128125
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        }
      },
      "auroc": 0.986128125
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9254156250000001
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9254156250000001
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.8843833333333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.8843833333333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 115,
          "fn": 285,
          "accuracy": 0.2875
        }
      },
      "auroc": 0.9048994791666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 115,
          "fn": 285,
          "accuracy": 0.2875
        }
      },
      "auroc": 0.9048994791666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9971343750000001
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9971343750000001
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9979421875000001
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9979421875000001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9686322916666668
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9686322916666668
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.9501593749999999
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.9501593749999999
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.9593958333333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.9593958333333334
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2052,
          "fn": 148,
          "accuracy": 0.9327272727272727
        },
        "0.01": {
          "tp": 1969,
          "fn": 231,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9890372159090908
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1133,
          "fn": 67,
          "accuracy": 0.9441666666666667
        },
        "0.01": {
          "tp": 1045,
          "fn": 155,
          "accuracy": 0.8708333333333333
        }
      },
      "auroc": 0.9920338541666666
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3185,
          "fn": 215,
          "accuracy": 0.9367647058823529
        },
        "0.01": {
          "tp": 3014,
          "fn": 386,
          "accuracy": 0.8864705882352941
        }
      },
      "auroc": 0.9900948529411764
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1444,
          "fn": 756,
          "accuracy": 0.6563636363636364
        },
        "0.01": {
          "tp": 1338,
          "fn": 862,
          "accuracy": 0.6081818181818182
        }
      },
      "auroc": 0.902467803030303
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 837,
          "fn": 363,
          "accuracy": 0.6975
        },
        "0.01": {
          "tp": 786,
          "fn": 414,
          "accuracy": 0.655
        }
      },
      "auroc": 0.9070753472222222
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2281,
          "fn": 1119,
          "accuracy": 0.6708823529411765
        },
        "0.01": {
          "tp": 2124,
          "fn": 1276,
          "accuracy": 0.6247058823529412
        }
      },
      "auroc": 0.9040939950980393
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3496,
          "fn": 904,
          "accuracy": 0.7945454545454546
        },
        "0.01": {
          "tp": 3307,
          "fn": 1093,
          "accuracy": 0.7515909090909091
        }
      },
      "auroc": 0.9457525094696969
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1970,
          "fn": 430,
          "accuracy": 0.8208333333333333
        },
        "0.01": {
          "tp": 1831,
          "fn": 569,
          "accuracy": 0.7629166666666667
        }
      },
      "auroc": 0.9495546006944444
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 5466,
          "fn": 1334,
          "accuracy": 0.8038235294117647
        },
        "0.01": {
          "tp": 5138,
          "fn": 1662,
          "accuracy": 0.7555882352941177
        }
      },
      "auroc": 0.9470944240196079
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9979968750000001
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9983734375000001
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9983734375000001
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.99856171875
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9987229166666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.986803125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        }
      },
      "auroc": 0.9927630208333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.7025635416666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9944614583333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        }
      },
      "auroc": 0.8485125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        },
        "0.01": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        }
      },
      "auroc": 0.8506432291666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9906322916666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 601,
          "fn": 199,
          "accuracy": 0.75125
        },
        "0.01": {
          "tp": 567,
          "fn": 233,
          "accuracy": 0.70875
        }
      },
      "auroc": 0.9206377604166666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9986239583333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9963781249999999
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9975010416666665
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.99835
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9864052083333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9923776041666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9984869791666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9913916666666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 775,
          "fn": 25,
          "accuracy": 0.96875
        },
        "0.01": {
          "tp": 749,
          "fn": 51,
          "accuracy": 0.93625
        }
      },
      "auroc": 0.9949393229166666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.993009375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9958796875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.669975
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.6883510416666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        }
      },
      "auroc": 0.6791630208333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8343625000000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        },
        "0.01": {
          "tp": 176,
          "fn": 224,
          "accuracy": 0.44
        }
      },
      "auroc": 0.8406802083333335
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 412,
          "fn": 388,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 380,
          "fn": 420,
          "accuracy": 0.475
        }
      },
      "auroc": 0.8375213541666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9987364583333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9785124999999999
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9886244791666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.7518916666666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.7822739583333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 299,
          "accuracy": 0.2525
        },
        "0.01": {
          "tp": 70,
          "fn": 330,
          "accuracy": 0.175
        }
      },
      "auroc": 0.7670828125
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        },
        "0.01": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        }
      },
      "auroc": 0.8753140625
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        },
        "0.01": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        }
      },
      "auroc": 0.8803932291666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 462,
          "fn": 338,
          "accuracy": 0.5775
        },
        "0.01": {
          "tp": 400,
          "fn": 400,
          "accuracy": 0.5
        }
      },
      "auroc": 0.8778536458333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9984604166666666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9929635416666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9957119791666668
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9986052083333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9958567708333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        },
        "0.01": {
          "tp": 785,
          "fn": 15,
          "accuracy": 0.98125
        }
      },
      "auroc": 0.9972309895833333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.995528125
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.995528125
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9767281250000001
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9767281250000001
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        }
      },
      "auroc": 0.986128125
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        }
      },
      "auroc": 0.986128125
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9254156250000001
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9254156250000001
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.8843833333333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.8843833333333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 115,
          "fn": 285,
          "accuracy": 0.2875
        }
      },
      "auroc": 0.9048994791666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 115,
          "fn": 285,
          "accuracy": 0.2875
        }
      },
      "auroc": 0.9048994791666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9971343750000001
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9971343750000001
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9979421875000001
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9979421875000001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9686322916666668
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9686322916666668
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.9501593749999999
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.9501593749999999
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.9593958333333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.9593958333333334
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2052,
          "fn": 148,
          "accuracy": 0.9327272727272727
        },
        "0.01": {
          "tp": 1969,
          "fn": 231,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9890372159090908
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1133,
          "fn": 67,
          "accuracy": 0.9441666666666667
        },
        "0.01": {
          "tp": 1045,
          "fn": 155,
          "accuracy": 0.8708333333333333
        }
      },
      "auroc": 0.9920338541666666
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3185,
          "fn": 215,
          "accuracy": 0.9367647058823529
        },
        "0.01": {
          "tp": 3014,
          "fn": 386,
          "accuracy": 0.8864705882352941
        }
      },
      "auroc": 0.9900948529411764
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1444,
          "fn": 756,
          "accuracy": 0.6563636363636364
        },
        "0.01": {
          "tp": 1338,
          "fn": 862,
          "accuracy": 0.6081818181818182
        }
      },
      "auroc": 0.902467803030303
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 837,
          "fn": 363,
          "accuracy": 0.6975
        },
        "0.01": {
          "tp": 786,
          "fn": 414,
          "accuracy": 0.655
        }
      },
      "auroc": 0.9070753472222222
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2281,
          "fn": 1119,
          "accuracy": 0.6708823529411765
        },
        "0.01": {
          "tp": 2124,
          "fn": 1276,
          "accuracy": 0.6247058823529412
        }
      },
      "auroc": 0.9040939950980393
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3496,
          "fn": 904,
          "accuracy": 0.7945454545454546
        },
        "0.01": {
          "tp": 3307,
          "fn": 1093,
          "accuracy": 0.7515909090909091
        }
      },
      "auroc": 0.9457525094696969
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1970,
          "fn": 430,
          "accuracy": 0.8208333333333333
        },
        "0.01": {
          "tp": 1831,
          "fn": 569,
          "accuracy": 0.7629166666666667
        }
      },
      "auroc": 0.9495546006944444
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 5466,
          "fn": 1334,
          "accuracy": 0.8038235294117647
        },
        "0.01": {
          "tp": 5138,
          "fn": 1662,
          "accuracy": 0.7555882352941177
        }
      },
      "auroc": 0.9470944240196079
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.997309375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9980296875000001
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9980296875000001
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9983898437500001
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9981104166666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9878635416666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        }
      },
      "auroc": 0.9929869791666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.5885510416666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.994371875
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        }
      },
      "auroc": 0.7914614583333335
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        }
      },
      "auroc": 0.7933307291666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        }
      },
      "auroc": 0.9911177083333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 593,
          "fn": 207,
          "accuracy": 0.74125
        },
        "0.01": {
          "tp": 553,
          "fn": 247,
          "accuracy": 0.69125
        }
      },
      "auroc": 0.89222421875
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9979635416666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.992940625
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9954520833333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.997178125
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.97810625
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        }
      },
      "auroc": 0.9876421875000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9975708333333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9855234375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 753,
          "fn": 47,
          "accuracy": 0.94125
        },
        "0.01": {
          "tp": 729,
          "fn": 71,
          "accuracy": 0.91125
        }
      },
      "auroc": 0.9915471354166667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.998628125
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.9791875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        }
      },
      "auroc": 0.9889078125
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.537540625
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.6183895833333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 381,
          "accuracy": 0.0475
        },
        "0.01": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        }
      },
      "auroc": 0.5779651041666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.768084375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 224,
          "accuracy": 0.44
        },
        "0.01": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        }
      },
      "auroc": 0.7987885416666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 420,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 313,
          "fn": 487,
          "accuracy": 0.39125
        }
      },
      "auroc": 0.7834364583333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.997859375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.9399291666666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 259,
          "fn": 141,
          "accuracy": 0.6475
        }
      },
      "auroc": 0.9688942708333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.63099375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.7514114583333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 83,
          "fn": 317,
          "accuracy": 0.2075
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.6912026041666668
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        },
        "0.01": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        }
      },
      "auroc": 0.8144265625
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        },
        "0.01": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        }
      },
      "auroc": 0.8456703125
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 410,
          "accuracy": 0.4875
        },
        "0.01": {
          "tp": 315,
          "fn": 485,
          "accuracy": 0.39375
        }
      },
      "auroc": 0.8300484374999999
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9987364583333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9987432291666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9981635416666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9848239583333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        }
      },
      "auroc": 0.9914937500000001
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9984567708333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9917802083333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 782,
          "fn": 18,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 765,
          "fn": 35,
          "accuracy": 0.95625
        }
      },
      "auroc": 0.9951184895833334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.9824927083333335
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.9824927083333335
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        }
      },
      "auroc": 0.94270625
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        }
      },
      "auroc": 0.94270625
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        }
      },
      "auroc": 0.9625994791666668
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        }
      },
      "auroc": 0.9625994791666668
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.8347708333333332
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.8347708333333332
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.7897125
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.7897125
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 296,
          "accuracy": 0.26
        },
        "0.01": {
          "tp": 46,
          "fn": 354,
          "accuracy": 0.115
        }
      },
      "auroc": 0.8122416666666668
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 296,
          "accuracy": 0.26
        },
        "0.01": {
          "tp": 46,
          "fn": 354,
          "accuracy": 0.115
        }
      },
      "auroc": 0.8122416666666668
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9903260416666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9903260416666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9945380208333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9945380208333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        }
      },
      "auroc": 0.9296145833333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        }
      },
      "auroc": 0.9296145833333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.8917375
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.8917375
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        }
      },
      "auroc": 0.9106760416666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        }
      },
      "auroc": 0.9106760416666667
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1940,
          "fn": 260,
          "accuracy": 0.8818181818181818
        },
        "0.01": {
          "tp": 1825,
          "fn": 375,
          "accuracy": 0.8295454545454546
        }
      },
      "auroc": 0.9758581439393939
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1036,
          "fn": 164,
          "accuracy": 0.8633333333333333
        },
        "0.01": {
          "tp": 913,
          "fn": 287,
          "accuracy": 0.7608333333333334
        }
      },
      "auroc": 0.9829012152777779
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2976,
          "fn": 424,
          "accuracy": 0.8752941176470588
        },
        "0.01": {
          "tp": 2738,
          "fn": 662,
          "accuracy": 0.8052941176470588
        }
      },
      "auroc": 0.9783439338235294
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1308,
          "fn": 892,
          "accuracy": 0.5945454545454546
        },
        "0.01": {
          "tp": 1182,
          "fn": 1018,
          "accuracy": 0.5372727272727272
        }
      },
      "auroc": 0.8513099431818183
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 820,
          "fn": 380,
          "accuracy": 0.6833333333333333
        },
        "0.01": {
          "tp": 763,
          "fn": 437,
          "accuracy": 0.6358333333333334
        }
      },
      "auroc": 0.8874020833333333
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2128,
          "fn": 1272,
          "accuracy": 0.6258823529411764
        },
        "0.01": {
          "tp": 1945,
          "fn": 1455,
          "accuracy": 0.5720588235294117
        }
      },
      "auroc": 0.8640483455882353
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3248,
          "fn": 1152,
          "accuracy": 0.7381818181818182
        },
        "0.01": {
          "tp": 3007,
          "fn": 1393,
          "accuracy": 0.6834090909090909
        }
      },
      "auroc": 0.913584043560606
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1856,
          "fn": 544,
          "accuracy": 0.7733333333333333
        },
        "0.01": {
          "tp": 1676,
          "fn": 724,
          "accuracy": 0.6983333333333334
        }
      },
      "auroc": 0.9351516493055556
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 5104,
          "fn": 1696,
          "accuracy": 0.7505882352941177
        },
        "0.01": {
          "tp": 4683,
          "fn": 2117,
          "accuracy": 0.6886764705882353
        }
      },
      "auroc": 0.9211961397058823
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9978510416666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9966427083333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9972468749999999
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9979979166666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9919572916666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9949776041666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9979244791666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        }
      },
      "auroc": 0.9943
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 774,
          "fn": 26,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 747,
          "fn": 53,
          "accuracy": 0.93375
        }
      },
      "auroc": 0.9961122395833333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.99546875
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9925520833333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9940104166666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        },
        "0.01": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        }
      },
      "auroc": 0.48312187500000003
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9944645833333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.7387932291666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        }
      },
      "auroc": 0.7392953125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        }
      },
      "auroc": 0.9935083333333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 580,
          "fn": 220,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 552,
          "fn": 248,
          "accuracy": 0.69
        }
      },
      "auroc": 0.8664018229166667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9924395833333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9782437500000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        }
      },
      "auroc": 0.9853416666666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9890197916666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        }
      },
      "auroc": 0.95475625
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": {
          "tp": 301,
          "fn": 99,
          "accuracy": 0.7525
        }
      },
      "auroc": 0.9718880208333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9907296875
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        },
        "0.01": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9665
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 701,
          "fn": 99,
          "accuracy": 0.87625
        },
        "0.01": {
          "tp": 636,
          "fn": 164,
          "accuracy": 0.795
        }
      },
      "auroc": 0.97861484375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9984416666666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.8888114583333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        },
        "0.01": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        }
      },
      "auroc": 0.9436265625
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.44194687499999996
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.656696875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 377,
          "accuracy": 0.0575
        },
        "0.01": {
          "tp": 12,
          "fn": 388,
          "accuracy": 0.03
        }
      },
      "auroc": 0.549321875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        }
      },
      "auroc": 0.7201942708333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 86,
          "fn": 314,
          "accuracy": 0.215
        },
        "0.01": {
          "tp": 37,
          "fn": 363,
          "accuracy": 0.0925
        }
      },
      "auroc": 0.7727541666666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 287,
          "fn": 513,
          "accuracy": 0.35875
        },
        "0.01": {
          "tp": 234,
          "fn": 566,
          "accuracy": 0.2925
        }
      },
      "auroc": 0.74647421875
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9929677083333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.8193635416666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        },
        "0.01": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        }
      },
      "auroc": 0.906165625
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.534446875
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        },
        "0.01": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        }
      },
      "auroc": 0.8037020833333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 308,
          "accuracy": 0.23
        },
        "0.01": {
          "tp": 69,
          "fn": 331,
          "accuracy": 0.1725
        }
      },
      "auroc": 0.6690744791666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        }
      },
      "auroc": 0.7637072916666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 284,
          "accuracy": 0.29
        },
        "0.01": {
          "tp": 80,
          "fn": 320,
          "accuracy": 0.2
        }
      },
      "auroc": 0.8115328125000001
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 317,
          "fn": 483,
          "accuracy": 0.39625
        },
        "0.01": {
          "tp": 263,
          "fn": 537,
          "accuracy": 0.32875
        }
      },
      "auroc": 0.7876200520833334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9986895833333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9970489583333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9978692708333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9939239583333332
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.9697083333333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        },
        "0.01": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        }
      },
      "auroc": 0.9818161458333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9963067708333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9833786458333335
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 739,
          "fn": 61,
          "accuracy": 0.92375
        },
        "0.01": {
          "tp": 690,
          "fn": 110,
          "accuracy": 0.8625
        }
      },
      "auroc": 0.9898427083333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.906271875
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.906271875
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.8285395833333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.8285395833333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        },
        "0.01": {
          "tp": 89,
          "fn": 311,
          "accuracy": 0.2225
        }
      },
      "auroc": 0.8674057291666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        },
        "0.01": {
          "tp": 89,
          "fn": 311,
          "accuracy": 0.2225
        }
      },
      "auroc": 0.8674057291666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.6933979166666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.6933979166666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.6493208333333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.6493208333333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 21,
          "fn": 379,
          "accuracy": 0.0525
        },
        "0.01": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        }
      },
      "auroc": 0.671359375
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 21,
          "fn": 379,
          "accuracy": 0.0525
        },
        "0.01": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        }
      },
      "auroc": 0.671359375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9987229166666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9987229166666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9987364583333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9987364583333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9965145833333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9965145833333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9719510416666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9719510416666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9842328124999999
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9842328124999999
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        },
        "0.01": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        }
      },
      "auroc": 0.8427833333333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        },
        "0.01": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        }
      },
      "auroc": 0.8427833333333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.8056302083333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.8056302083333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 260,
          "accuracy": 0.35
        },
        "0.01": {
          "tp": 113,
          "fn": 287,
          "accuracy": 0.2825
        }
      },
      "auroc": 0.8242067708333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 260,
          "accuracy": 0.35
        },
        "0.01": {
          "tp": 113,
          "fn": 287,
          "accuracy": 0.2825
        }
      },
      "auroc": 0.8242067708333334
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1729,
          "fn": 471,
          "accuracy": 0.7859090909090909
        },
        "0.01": {
          "tp": 1618,
          "fn": 582,
          "accuracy": 0.7354545454545455
        }
      },
      "auroc": 0.946688731060606
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 849,
          "fn": 351,
          "accuracy": 0.7075
        },
        "0.01": {
          "tp": 754,
          "fn": 446,
          "accuracy": 0.6283333333333333
        }
      },
      "auroc": 0.94544375
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2578,
          "fn": 822,
          "accuracy": 0.758235294117647
        },
        "0.01": {
          "tp": 2372,
          "fn": 1028,
          "accuracy": 0.6976470588235294
        }
      },
      "auroc": 0.9462493259803922
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1104,
          "fn": 1096,
          "accuracy": 0.5018181818181818
        },
        "0.01": {
          "tp": 997,
          "fn": 1203,
          "accuracy": 0.4531818181818182
        }
      },
      "auroc": 0.7904201704545455
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 780,
          "fn": 420,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 691,
          "fn": 509,
          "accuracy": 0.5758333333333333
        }
      },
      "auroc": 0.8952142361111111
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1884,
          "fn": 1516,
          "accuracy": 0.5541176470588235
        },
        "0.01": {
          "tp": 1688,
          "fn": 1712,
          "accuracy": 0.4964705882352941
        }
      },
      "auroc": 0.8274063112745098
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2833,
          "fn": 1567,
          "accuracy": 0.6438636363636364
        },
        "0.01": {
          "tp": 2615,
          "fn": 1785,
          "accuracy": 0.5943181818181819
        }
      },
      "auroc": 0.8685544507575758
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1629,
          "fn": 771,
          "accuracy": 0.67875
        },
        "0.01": {
          "tp": 1445,
          "fn": 955,
          "accuracy": 0.6020833333333333
        }
      },
      "auroc": 0.9203289930555555
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 4462,
          "fn": 2338,
          "accuracy": 0.6561764705882352
        },
        "0.01": {
          "tp": 4060,
          "fn": 2740,
          "accuracy": 0.5970588235294118
        }
      },
      "auroc": 0.8868278186274511
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9970989583333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9979244791666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9979244791666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        }
      },
      "auroc": 0.9983372395833334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9986125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9825854166666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9905989583333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.5704645833333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9942270833333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.7823458333333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        }
      },
      "auroc": 0.7845385416666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        }
      },
      "auroc": 0.98840625
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 587,
          "fn": 213,
          "accuracy": 0.73375
        },
        "0.01": {
          "tp": 547,
          "fn": 253,
          "accuracy": 0.68375
        }
      },
      "auroc": 0.8864723958333335
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9984145833333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.99045
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9944322916666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9972333333333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9793270833333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9882802083333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9978239583333335
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9848885416666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 758,
          "fn": 42,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 735,
          "fn": 65,
          "accuracy": 0.91875
        }
      },
      "auroc": 0.9913562499999999
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9987364583333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9835395833333332
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9911380208333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.505271875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.5314375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 389,
          "accuracy": 0.0275
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.5183546875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        },
        "0.01": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        }
      },
      "auroc": 0.7520041666666668
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 222,
          "accuracy": 0.445
        },
        "0.01": {
          "tp": 132,
          "fn": 268,
          "accuracy": 0.33
        }
      },
      "auroc": 0.7574885416666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 419,
          "accuracy": 0.47625
        },
        "0.01": {
          "tp": 334,
          "fn": 466,
          "accuracy": 0.4175
        }
      },
      "auroc": 0.7547463541666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9986822916666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.9468031250000001
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": {
          "tp": 291,
          "fn": 109,
          "accuracy": 0.7275
        }
      },
      "auroc": 0.9727427083333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.6228739583333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.680065625
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 320,
          "accuracy": 0.2
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.6514697916666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        },
        "0.01": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        }
      },
      "auroc": 0.8107781249999999
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 214,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        }
      },
      "auroc": 0.8134343749999999
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 413,
          "fn": 387,
          "accuracy": 0.51625
        },
        "0.01": {
          "tp": 345,
          "fn": 455,
          "accuracy": 0.43125
        }
      },
      "auroc": 0.81210625
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9982947916666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9848177083333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.99155625
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9985223958333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9917838541666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 784,
          "fn": 16,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 771,
          "fn": 29,
          "accuracy": 0.96375
        }
      },
      "auroc": 0.9951531250000001
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9876624999999999
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9876624999999999
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        }
      },
      "auroc": 0.951953125
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        }
      },
      "auroc": 0.951953125
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9698078125
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        }
      },
      "auroc": 0.9698078125
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.8423885416666665
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.8423885416666665
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        }
      },
      "auroc": 0.7994239583333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        }
      },
      "auroc": 0.7994239583333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 266,
          "accuracy": 0.335
        },
        "0.01": {
          "tp": 66,
          "fn": 334,
          "accuracy": 0.165
        }
      },
      "auroc": 0.8209062500000001
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 266,
          "accuracy": 0.335
        },
        "0.01": {
          "tp": 66,
          "fn": 334,
          "accuracy": 0.165
        }
      },
      "auroc": 0.8209062500000001
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9987364583333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9987364583333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9936437499999999
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9936437499999999
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9961901041666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9961901041666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.9249052083333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.9249052083333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.8946406250000001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.8946406250000001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        },
        "0.01": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        }
      },
      "auroc": 0.9097729166666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        },
        "0.01": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        }
      },
      "auroc": 0.9097729166666667
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1988,
          "fn": 212,
          "accuracy": 0.9036363636363637
        },
        "0.01": {
          "tp": 1890,
          "fn": 310,
          "accuracy": 0.8590909090909091
        }
      },
      "auroc": 0.9767625946969696
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1063,
          "fn": 137,
          "accuracy": 0.8858333333333334
        },
        "0.01": {
          "tp": 952,
          "fn": 248,
          "accuracy": 0.7933333333333333
        }
      },
      "auroc": 0.9834796874999999
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3051,
          "fn": 349,
          "accuracy": 0.8973529411764706
        },
        "0.01": {
          "tp": 2842,
          "fn": 558,
          "accuracy": 0.8358823529411765
        }
      },
      "auroc": 0.9791333333333333
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1363,
          "fn": 837,
          "accuracy": 0.6195454545454545
        },
        "0.01": {
          "tp": 1236,
          "fn": 964,
          "accuracy": 0.5618181818181818
        }
      },
      "auroc": 0.8482999999999999
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 810,
          "fn": 390,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 759,
          "fn": 441,
          "accuracy": 0.6325
        }
      },
      "auroc": 0.861162326388889
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2173,
          "fn": 1227,
          "accuracy": 0.6391176470588236
        },
        "0.01": {
          "tp": 1995,
          "fn": 1405,
          "accuracy": 0.586764705882353
        }
      },
      "auroc": 0.8528396446078432
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3351,
          "fn": 1049,
          "accuracy": 0.7615909090909091
        },
        "0.01": {
          "tp": 3126,
          "fn": 1274,
          "accuracy": 0.7104545454545454
        }
      },
      "auroc": 0.9125312973484849
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1873,
          "fn": 527,
          "accuracy": 0.7804166666666666
        },
        "0.01": {
          "tp": 1711,
          "fn": 689,
          "accuracy": 0.7129166666666666
        }
      },
      "auroc": 0.9223210069444444
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 5224,
          "fn": 1576,
          "accuracy": 0.768235294117647
        },
        "0.01": {
          "tp": 4837,
          "fn": 1963,
          "accuracy": 0.7113235294117647
        }
      },
      "auroc": 0.9159864889705882
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9864135416666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9872364583333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9868250000000001
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.989025
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.9688833333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        }
      },
      "auroc": 0.9789541666666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9877192708333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        }
      },
      "auroc": 0.9780598958333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 704,
          "fn": 96,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 639,
          "fn": 161,
          "accuracy": 0.79875
        }
      },
      "auroc": 0.9828895833333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9929489583333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.7166479166666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        }
      },
      "auroc": 0.8547984375000001
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.714140625
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.6537989583333332
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 44,
          "fn": 356,
          "accuracy": 0.11
        },
        "0.01": {
          "tp": 28,
          "fn": 372,
          "accuracy": 0.07
        }
      },
      "auroc": 0.6839697916666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 210,
          "fn": 190,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        }
      },
      "auroc": 0.8535447916666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 41,
          "fn": 359,
          "accuracy": 0.1025
        },
        "0.01": {
          "tp": 23,
          "fn": 377,
          "accuracy": 0.0575
        }
      },
      "auroc": 0.6852234375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 251,
          "fn": 549,
          "accuracy": 0.31375
        },
        "0.01": {
          "tp": 211,
          "fn": 589,
          "accuracy": 0.26375
        }
      },
      "auroc": 0.7693841145833333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9862572916666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        }
      },
      "auroc": 0.94859375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 273,
          "fn": 127,
          "accuracy": 0.6825
        }
      },
      "auroc": 0.9674255208333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9672854166666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.879046875
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 267,
          "fn": 133,
          "accuracy": 0.6675
        },
        "0.01": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9231661458333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9767713541666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        },
        "0.01": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.9138203125000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 589,
          "fn": 211,
          "accuracy": 0.73625
        },
        "0.01": {
          "tp": 497,
          "fn": 303,
          "accuracy": 0.62125
        }
      },
      "auroc": 0.9452958333333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.981659375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.9062510416666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 273,
          "fn": 127,
          "accuracy": 0.6825
        },
        "0.01": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        }
      },
      "auroc": 0.9439552083333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.6896770833333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5579458333333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 381,
          "accuracy": 0.0475
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.6238114583333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        }
      },
      "auroc": 0.8356682291666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 307,
          "accuracy": 0.2325
        },
        "0.01": {
          "tp": 53,
          "fn": 347,
          "accuracy": 0.1325
        }
      },
      "auroc": 0.7320984374999999
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 292,
          "fn": 508,
          "accuracy": 0.365
        },
        "0.01": {
          "tp": 228,
          "fn": 572,
          "accuracy": 0.285
        }
      },
      "auroc": 0.7838833333333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9822291666666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.8695291666666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 236,
          "fn": 164,
          "accuracy": 0.59
        },
        "0.01": {
          "tp": 181,
          "fn": 219,
          "accuracy": 0.4525
        }
      },
      "auroc": 0.9258791666666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.7256572916666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.5517937500000001
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        },
        "0.01": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        }
      },
      "auroc": 0.6387255208333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 151,
          "fn": 249,
          "accuracy": 0.3775
        }
      },
      "auroc": 0.8539432291666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 69,
          "fn": 331,
          "accuracy": 0.1725
        },
        "0.01": {
          "tp": 38,
          "fn": 362,
          "accuracy": 0.095
        }
      },
      "auroc": 0.7106614583333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 261,
          "fn": 539,
          "accuracy": 0.32625
        },
        "0.01": {
          "tp": 189,
          "fn": 611,
          "accuracy": 0.23625
        }
      },
      "auroc": 0.78230234375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.996140625
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9780187499999999
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9870796875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9851635416666668
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.9298854166666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        }
      },
      "auroc": 0.9575244791666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9906520833333332
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 267,
          "fn": 133,
          "accuracy": 0.6675
        }
      },
      "auroc": 0.9539520833333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 682,
          "fn": 118,
          "accuracy": 0.8525
        },
        "0.01": {
          "tp": 611,
          "fn": 189,
          "accuracy": 0.76375
        }
      },
      "auroc": 0.9723020833333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.9373947916666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.9373947916666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.8882322916666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.8882322916666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        },
        "0.01": {
          "tp": 133,
          "fn": 267,
          "accuracy": 0.3325
        }
      },
      "auroc": 0.9128135416666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        },
        "0.01": {
          "tp": 133,
          "fn": 267,
          "accuracy": 0.3325
        }
      },
      "auroc": 0.9128135416666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.8290156249999999
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.8290156249999999
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.8092770833333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.8092770833333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 312,
          "accuracy": 0.22
        },
        "0.01": {
          "tp": 44,
          "fn": 356,
          "accuracy": 0.11
        }
      },
      "auroc": 0.8191463541666668
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 312,
          "accuracy": 0.22
        },
        "0.01": {
          "tp": 44,
          "fn": 356,
          "accuracy": 0.11
        }
      },
      "auroc": 0.8191463541666668
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9971895833333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9971895833333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9925489583333332
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9925489583333332
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9948692708333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9948692708333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9915479166666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9915479166666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9563291666666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9563291666666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9739385416666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9739385416666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.8920739583333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.8920739583333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.8616812500000001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.8616812500000001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 128,
          "fn": 272,
          "accuracy": 0.32
        }
      },
      "auroc": 0.8768776041666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 128,
          "fn": 272,
          "accuracy": 0.32
        }
      },
      "auroc": 0.8768776041666666
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1738,
          "fn": 462,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 1545,
          "fn": 655,
          "accuracy": 0.7022727272727273
        }
      },
      "auroc": 0.9611700757575757
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 665,
          "fn": 535,
          "accuracy": 0.5541666666666667
        },
        "0.01": {
          "tp": 535,
          "fn": 665,
          "accuracy": 0.44583333333333336
        }
      },
      "auroc": 0.9010461805555555
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2403,
          "fn": 997,
          "accuracy": 0.706764705882353
        },
        "0.01": {
          "tp": 2080,
          "fn": 1320,
          "accuracy": 0.611764705882353
        }
      },
      "auroc": 0.9399498774509804
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1141,
          "fn": 1059,
          "accuracy": 0.5186363636363637
        },
        "0.01": {
          "tp": 932,
          "fn": 1268,
          "accuracy": 0.42363636363636364
        }
      },
      "auroc": 0.8708197916666667
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 427,
          "fn": 773,
          "accuracy": 0.35583333333333333
        },
        "0.01": {
          "tp": 334,
          "fn": 866,
          "accuracy": 0.2783333333333333
        }
      },
      "auroc": 0.7568923611111111
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1568,
          "fn": 1832,
          "accuracy": 0.4611764705882353
        },
        "0.01": {
          "tp": 1266,
          "fn": 2134,
          "accuracy": 0.3723529411764706
        }
      },
      "auroc": 0.8306101102941175
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2879,
          "fn": 1521,
          "accuracy": 0.6543181818181818
        },
        "0.01": {
          "tp": 2477,
          "fn": 1923,
          "accuracy": 0.5629545454545455
        }
      },
      "auroc": 0.9159949337121213
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1092,
          "fn": 1308,
          "accuracy": 0.455
        },
        "0.01": {
          "tp": 869,
          "fn": 1531,
          "accuracy": 0.3620833333333333
        }
      },
      "auroc": 0.8289692708333333
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3971,
          "fn": 2829,
          "accuracy": 0.5839705882352941
        },
        "0.01": {
          "tp": 3346,
          "fn": 3454,
          "accuracy": 0.49205882352941177
        }
      },
      "auroc": 0.8852799938725491
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9979333333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9983416666666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9983416666666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9985458333333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.998534375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9870041666666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        }
      },
      "auroc": 0.9927692708333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.6931364583333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9942927083333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        }
      },
      "auroc": 0.8437145833333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        },
        "0.01": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        }
      },
      "auroc": 0.8458354166666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9906484375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 601,
          "fn": 199,
          "accuracy": 0.75125
        },
        "0.01": {
          "tp": 564,
          "fn": 236,
          "accuracy": 0.705
        }
      },
      "auroc": 0.9182419270833333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9986031249999999
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9961593750000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.99738125
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9980520833333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9863500000000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9922010416666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9983276041666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9912546875000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 773,
          "fn": 27,
          "accuracy": 0.96625
        },
        "0.01": {
          "tp": 747,
          "fn": 53,
          "accuracy": 0.93375
        }
      },
      "auroc": 0.9947911458333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9916989583333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9952244791666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.6579260416666668
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.686575
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 381,
          "accuracy": 0.0475
        },
        "0.01": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        }
      },
      "auroc": 0.6722505208333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8283380208333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8391369791666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 409,
          "fn": 391,
          "accuracy": 0.51125
        },
        "0.01": {
          "tp": 374,
          "fn": 426,
          "accuracy": 0.4675
        }
      },
      "auroc": 0.8337375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9986354166666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.9763656249999999
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9875005208333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.7414385416666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.7850979166666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 300,
          "accuracy": 0.25
        },
        "0.01": {
          "tp": 68,
          "fn": 332,
          "accuracy": 0.17
        }
      },
      "auroc": 0.7632682291666668
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 236,
          "fn": 164,
          "accuracy": 0.59
        },
        "0.01": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        }
      },
      "auroc": 0.8700369791666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        },
        "0.01": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        }
      },
      "auroc": 0.8807317708333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 459,
          "fn": 341,
          "accuracy": 0.57375
        },
        "0.01": {
          "tp": 390,
          "fn": 410,
          "accuracy": 0.4875
        }
      },
      "auroc": 0.875384375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.998446875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9925385416666666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9954927083333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9985984375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9956442708333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        },
        "0.01": {
          "tp": 785,
          "fn": 15,
          "accuracy": 0.98125
        }
      },
      "auroc": 0.9971213541666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9936666666666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9936666666666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.9715541666666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.9715541666666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9826104166666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9826104166666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        }
      },
      "auroc": 0.91825
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        }
      },
      "auroc": 0.91825
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.87551875
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.87551875
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        },
        "0.01": {
          "tp": 104,
          "fn": 296,
          "accuracy": 0.26
        }
      },
      "auroc": 0.8968843750000001
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        },
        "0.01": {
          "tp": 104,
          "fn": 296,
          "accuracy": 0.26
        }
      },
      "auroc": 0.8968843750000001
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9971010416666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9971010416666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9979255208333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9979255208333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.9631072916666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.9631072916666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.9450958333333332
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.9450958333333332
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 274,
          "fn": 126,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 228,
          "fn": 172,
          "accuracy": 0.57
        }
      },
      "auroc": 0.9541015625
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 274,
          "fn": 126,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 228,
          "fn": 172,
          "accuracy": 0.57
        }
      },
      "auroc": 0.9541015625
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2038,
          "fn": 162,
          "accuracy": 0.9263636363636364
        },
        "0.01": {
          "tp": 1940,
          "fn": 260,
          "accuracy": 0.8818181818181818
        }
      },
      "auroc": 0.9876860795454547
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1126,
          "fn": 74,
          "accuracy": 0.9383333333333334
        },
        "0.01": {
          "tp": 1033,
          "fn": 167,
          "accuracy": 0.8608333333333333
        }
      },
      "auroc": 0.9914546875000001
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3164,
          "fn": 236,
          "accuracy": 0.9305882352941176
        },
        "0.01": {
          "tp": 2973,
          "fn": 427,
          "accuracy": 0.8744117647058823
        }
      },
      "auroc": 0.9890161764705883
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1425,
          "fn": 775,
          "accuracy": 0.6477272727272727
        },
        "0.01": {
          "tp": 1316,
          "fn": 884,
          "accuracy": 0.5981818181818181
        }
      },
      "auroc": 0.8977972537878788
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 836,
          "fn": 364,
          "accuracy": 0.6966666666666667
        },
        "0.01": {
          "tp": 786,
          "fn": 414,
          "accuracy": 0.655
        }
      },
      "auroc": 0.9071312500000001
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2261,
          "fn": 1139,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 2102,
          "fn": 1298,
          "accuracy": 0.6182352941176471
        }
      },
      "auroc": 0.9010916053921569
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3463,
          "fn": 937,
          "accuracy": 0.7870454545454545
        },
        "0.01": {
          "tp": 3256,
          "fn": 1144,
          "accuracy": 0.74
        }
      },
      "auroc": 0.9427416666666667
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1962,
          "fn": 438,
          "accuracy": 0.8175
        },
        "0.01": {
          "tp": 1819,
          "fn": 581,
          "accuracy": 0.7579166666666667
        }
      },
      "auroc": 0.94929296875
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 5425,
          "fn": 1375,
          "accuracy": 0.7977941176470589
        },
        "0.01": {
          "tp": 5075,
          "fn": 1725,
          "accuracy": 0.7463235294117647
        }
      },
      "auroc": 0.9450538909313726
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9979968750000001
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9983734375000001
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9983734375000001
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.99856171875
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9987229166666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9866072916666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9926651041666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.7028572916666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9936125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        }
      },
      "auroc": 0.8482348958333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        },
        "0.01": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        }
      },
      "auroc": 0.8507901041666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9901098958333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 600,
          "fn": 200,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 565,
          "fn": 235,
          "accuracy": 0.70625
        }
      },
      "auroc": 0.9204499999999999
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9986239583333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9964177083333332
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9975208333333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.99835
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9864572916666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9924036458333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9984869791666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9914375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 774,
          "fn": 26,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 749,
          "fn": 51,
          "accuracy": 0.93625
        }
      },
      "auroc": 0.9949622395833334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.993009375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9958796875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.6702687500000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.6877645833333335
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        }
      },
      "auroc": 0.6790166666666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        }
      },
      "auroc": 0.834509375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        },
        "0.01": {
          "tp": 176,
          "fn": 224,
          "accuracy": 0.44
        }
      },
      "auroc": 0.8403869791666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 412,
          "fn": 388,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 380,
          "fn": 420,
          "accuracy": 0.475
        }
      },
      "auroc": 0.8374481770833334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9987364583333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9785333333333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9886348958333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.7518520833333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.7799322916666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 299,
          "accuracy": 0.2525
        },
        "0.01": {
          "tp": 70,
          "fn": 330,
          "accuracy": 0.175
        }
      },
      "auroc": 0.7658921875
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        },
        "0.01": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        }
      },
      "auroc": 0.8752942708333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        },
        "0.01": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        }
      },
      "auroc": 0.8792328125000001
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 462,
          "fn": 338,
          "accuracy": 0.5775
        },
        "0.01": {
          "tp": 400,
          "fn": 400,
          "accuracy": 0.5
        }
      },
      "auroc": 0.8772635416666666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9984604166666666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9929635416666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9957119791666668
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9986052083333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9958567708333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        },
        "0.01": {
          "tp": 785,
          "fn": 15,
          "accuracy": 0.98125
        }
      },
      "auroc": 0.9972309895833333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.995528125
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.995528125
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9767281250000001
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9767281250000001
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        }
      },
      "auroc": 0.986128125
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        }
      },
      "auroc": 0.986128125
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9254156250000001
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.9254156250000001
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.8843833333333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.8843833333333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 115,
          "fn": 285,
          "accuracy": 0.2875
        }
      },
      "auroc": 0.9048994791666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 115,
          "fn": 285,
          "accuracy": 0.2875
        }
      },
      "auroc": 0.9048994791666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9971343750000001
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9971343750000001
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9979421875000001
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9979421875000001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9686322916666668
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9686322916666668
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.950103125
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.950103125
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.9593677083333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.9593677083333334
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2052,
          "fn": 148,
          "accuracy": 0.9327272727272727
        },
        "0.01": {
          "tp": 1969,
          "fn": 231,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9890372159090908
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1131,
          "fn": 69,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 1044,
          "fn": 156,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9920112847222222
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3183,
          "fn": 217,
          "accuracy": 0.9361764705882353
        },
        "0.01": {
          "tp": 3013,
          "fn": 387,
          "accuracy": 0.8861764705882353
        }
      },
      "auroc": 0.990086887254902
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1444,
          "fn": 756,
          "accuracy": 0.6563636363636364
        },
        "0.01": {
          "tp": 1338,
          "fn": 862,
          "accuracy": 0.6081818181818182
        }
      },
      "auroc": 0.9025125
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 837,
          "fn": 363,
          "accuracy": 0.6975
        },
        "0.01": {
          "tp": 785,
          "fn": 415,
          "accuracy": 0.6541666666666667
        }
      },
      "auroc": 0.9064545138888889
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2281,
          "fn": 1119,
          "accuracy": 0.6708823529411765
        },
        "0.01": {
          "tp": 2123,
          "fn": 1277,
          "accuracy": 0.6244117647058823
        }
      },
      "auroc": 0.9039037990196079
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3496,
          "fn": 904,
          "accuracy": 0.7945454545454546
        },
        "0.01": {
          "tp": 3307,
          "fn": 1093,
          "accuracy": 0.7515909090909091
        }
      },
      "auroc": 0.9457748579545455
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1968,
          "fn": 432,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 1829,
          "fn": 571,
          "accuracy": 0.7620833333333333
        }
      },
      "auroc": 0.9492328993055555
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 5464,
          "fn": 1336,
          "accuracy": 0.8035294117647059
        },
        "0.01": {
          "tp": 5136,
          "fn": 1664,
          "accuracy": 0.7552941176470588
        }
      },
      "auroc": 0.9469953431372549
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.17419062500000004
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.139834375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.15701249999999997
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.16689791666666665
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.12750937499999998
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.1472036458333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.17054427083333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.13367187500000002
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.15210807291666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.4429833333333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.22016145833333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        },
        "0.01": {
          "tp": 16,
          "fn": 384,
          "accuracy": 0.04
        }
      },
      "auroc": 0.3315723958333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.14444062500000002
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.26324895833333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 393,
          "accuracy": 0.0175
        },
        "0.01": {
          "tp": 7,
          "fn": 393,
          "accuracy": 0.0175
        }
      },
      "auroc": 0.20384479166666664
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 373,
          "accuracy": 0.0675
        },
        "0.01": {
          "tp": 19,
          "fn": 381,
          "accuracy": 0.0475
        }
      },
      "auroc": 0.2937119791666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.24170520833333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 768,
          "accuracy": 0.04
        },
        "0.01": {
          "tp": 23,
          "fn": 777,
          "accuracy": 0.02875
        }
      },
      "auroc": 0.26770859375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.17004166666666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.13422708333333336
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.15213437500000002
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.1544875
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.11403854166666669
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.13426302083333336
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.16226458333333332
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.1241328125
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.14319869791666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.5889875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.19643854166666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 51,
          "fn": 349,
          "accuracy": 0.1275
        },
        "0.01": {
          "tp": 43,
          "fn": 357,
          "accuracy": 0.1075
        }
      },
      "auroc": 0.39271302083333337
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.13001041666666668
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.13260104166666664
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.1313057291666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 51,
          "fn": 349,
          "accuracy": 0.1275
        },
        "0.01": {
          "tp": 43,
          "fn": 357,
          "accuracy": 0.1075
        }
      },
      "auroc": 0.35949895833333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.16451979166666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 52,
          "fn": 748,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 44,
          "fn": 756,
          "accuracy": 0.055
        }
      },
      "auroc": 0.262009375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.34738854166666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.21961145833333331
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 369,
          "accuracy": 0.0775
        },
        "0.01": {
          "tp": 27,
          "fn": 373,
          "accuracy": 0.0675
        }
      },
      "auroc": 0.28350000000000003
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.16888854166666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.18873333333333336
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        },
        "0.01": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        }
      },
      "auroc": 0.1788109375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 374,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 22,
          "fn": 378,
          "accuracy": 0.055
        }
      },
      "auroc": 0.25813854166666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 30,
          "fn": 370,
          "accuracy": 0.075
        },
        "0.01": {
          "tp": 30,
          "fn": 370,
          "accuracy": 0.075
        }
      },
      "auroc": 0.20417239583333335
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 744,
          "accuracy": 0.07
        },
        "0.01": {
          "tp": 52,
          "fn": 748,
          "accuracy": 0.065
        }
      },
      "auroc": 0.23115546875000004
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.25676041666666666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.17890833333333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.21783437500000002
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.17003749999999998
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.13391979166666668
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.15197864583333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.21339895833333328
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.1564140625
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.18490651041666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.171171875
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.171171875
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.14107708333333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.14107708333333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.15612447916666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.15612447916666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.10040625
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.10040625
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.09924895833333335
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.09924895833333335
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.09982760416666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.09982760416666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.1517427083333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.1517427083333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.12297083333333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.12297083333333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.13735677083333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.13735677083333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.116021875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.116021875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.061298958333333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.061298958333333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08866041666666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08866041666666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.13989375
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.13989375
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.13985520833333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.13985520833333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.13987447916666668
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.13987447916666668
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 2112,
          "accuracy": 0.04
        },
        "0.01": {
          "tp": 68,
          "fn": 2132,
          "accuracy": 0.03090909090909091
        }
      },
      "auroc": 0.24178077651515148
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 1181,
          "accuracy": 0.015833333333333335
        },
        "0.01": {
          "tp": 18,
          "fn": 1182,
          "accuracy": 0.015
        }
      },
      "auroc": 0.18153020833333333
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 107,
          "fn": 3293,
          "accuracy": 0.03147058823529412
        },
        "0.01": {
          "tp": 86,
          "fn": 3314,
          "accuracy": 0.025294117647058825
        }
      },
      "auroc": 0.2205158700980392
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 2183,
          "accuracy": 0.007727272727272728
        },
        "0.01": {
          "tp": 17,
          "fn": 2183,
          "accuracy": 0.007727272727272728
        }
      },
      "auroc": 0.13629214015151514
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 1183,
          "accuracy": 0.014166666666666666
        },
        "0.01": {
          "tp": 17,
          "fn": 1183,
          "accuracy": 0.014166666666666666
        }
      },
      "auroc": 0.16000850694444446
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 3366,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 34,
          "fn": 3366,
          "accuracy": 0.01
        }
      },
      "auroc": 0.14466262254901963
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 4295,
          "accuracy": 0.023863636363636365
        },
        "0.01": {
          "tp": 85,
          "fn": 4315,
          "accuracy": 0.019318181818181818
        }
      },
      "auroc": 0.18903645833333335
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 2364,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 35,
          "fn": 2365,
          "accuracy": 0.014583333333333334
        }
      },
      "auroc": 0.1707693576388889
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 6659,
          "accuracy": 0.02073529411764706
        },
        "0.01": {
          "tp": 120,
          "fn": 6680,
          "accuracy": 0.01764705882352941
        }
      },
      "auroc": 0.18258924632352944
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9987364583333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9987432291666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9979239583333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9983369791666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9983302083333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        }
      },
      "auroc": 0.9985401041666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9981125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.986803125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9924578124999999
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.5868802083333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9944614583333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        }
      },
      "auroc": 0.7906708333333332
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        },
        "0.01": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        }
      },
      "auroc": 0.7924963541666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9906322916666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 589,
          "fn": 211,
          "accuracy": 0.73625
        },
        "0.01": {
          "tp": 548,
          "fn": 252,
          "accuracy": 0.685
        }
      },
      "auroc": 0.8915643229166667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9979041666666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.994875
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9963895833333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9971937500000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.980140625
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        }
      },
      "auroc": 0.9886671874999999
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9975489583333332
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        }
      },
      "auroc": 0.9875078125000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 764,
          "fn": 36,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 733,
          "fn": 67,
          "accuracy": 0.91625
        }
      },
      "auroc": 0.9925283854166667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9985208333333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        }
      },
      "auroc": 0.978759375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9886401041666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.5526812499999999
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.6960010416666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 21,
          "fn": 379,
          "accuracy": 0.0525
        },
        "0.01": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        }
      },
      "auroc": 0.6243411458333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.7756010416666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 127,
          "fn": 273,
          "accuracy": 0.3175
        }
      },
      "auroc": 0.8373802083333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 419,
          "accuracy": 0.47625
        },
        "0.01": {
          "tp": 326,
          "fn": 474,
          "accuracy": 0.4075
        }
      },
      "auroc": 0.8064906250000001
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9978104166666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.9474614583333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        }
      },
      "auroc": 0.9726359375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.6397750000000001
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.8088479166666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 90,
          "fn": 310,
          "accuracy": 0.225
        },
        "0.01": {
          "tp": 63,
          "fn": 337,
          "accuracy": 0.1575
        }
      },
      "auroc": 0.7243114583333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8187927083333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        },
        "0.01": {
          "tp": 134,
          "fn": 266,
          "accuracy": 0.335
        }
      },
      "auroc": 0.8781546875
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 411,
          "fn": 389,
          "accuracy": 0.51375
        },
        "0.01": {
          "tp": 338,
          "fn": 462,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.8484736979166667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9987364583333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9987432291666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9977302083333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9892354166666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.9934828124999999
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9982401041666666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9939859375000001
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 783,
          "fn": 17,
          "accuracy": 0.97875
        },
        "0.01": {
          "tp": 771,
          "fn": 29,
          "accuracy": 0.96375
        }
      },
      "auroc": 0.9961130208333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        }
      },
      "auroc": 0.966653125
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        }
      },
      "auroc": 0.966653125
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.9233583333333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.9233583333333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        }
      },
      "auroc": 0.9450057291666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        }
      },
      "auroc": 0.9450057291666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.8120010416666668
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.8120010416666668
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.7696604166666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.7696604166666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 322,
          "accuracy": 0.195
        },
        "0.01": {
          "tp": 39,
          "fn": 361,
          "accuracy": 0.0975
        }
      },
      "auroc": 0.7908307291666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 322,
          "accuracy": 0.195
        },
        "0.01": {
          "tp": 39,
          "fn": 361,
          "accuracy": 0.0975
        }
      },
      "auroc": 0.7908307291666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9910791666666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9910791666666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9949145833333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9949145833333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        }
      },
      "auroc": 0.9095958333333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        }
      },
      "auroc": 0.9095958333333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.8669208333333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.8669208333333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 153,
          "fn": 247,
          "accuracy": 0.3825
        }
      },
      "auroc": 0.8882583333333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 153,
          "fn": 247,
          "accuracy": 0.3825
        }
      },
      "auroc": 0.8882583333333333
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1895,
          "fn": 305,
          "accuracy": 0.8613636363636363
        },
        "0.01": {
          "tp": 1790,
          "fn": 410,
          "accuracy": 0.8136363636363636
        }
      },
      "auroc": 0.9705089015151516
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1056,
          "fn": 144,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 943,
          "fn": 257,
          "accuracy": 0.7858333333333334
        }
      },
      "auroc": 0.9842286458333334
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2951,
          "fn": 449,
          "accuracy": 0.8679411764705882
        },
        "0.01": {
          "tp": 2733,
          "fn": 667,
          "accuracy": 0.8038235294117647
        }
      },
      "auroc": 0.9753511642156862
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1244,
          "fn": 956,
          "accuracy": 0.5654545454545454
        },
        "0.01": {
          "tp": 1140,
          "fn": 1060,
          "accuracy": 0.5181818181818182
        }
      },
      "auroc": 0.8475253787878787
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 839,
          "fn": 361,
          "accuracy": 0.6991666666666667
        },
        "0.01": {
          "tp": 781,
          "fn": 419,
          "accuracy": 0.6508333333333334
        }
      },
      "auroc": 0.9111017361111111
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2083,
          "fn": 1317,
          "accuracy": 0.6126470588235294
        },
        "0.01": {
          "tp": 1921,
          "fn": 1479,
          "accuracy": 0.565
        }
      },
      "auroc": 0.8699640931372549
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3139,
          "fn": 1261,
          "accuracy": 0.7134090909090909
        },
        "0.01": {
          "tp": 2930,
          "fn": 1470,
          "accuracy": 0.6659090909090909
        }
      },
      "auroc": 0.9090171401515152
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1895,
          "fn": 505,
          "accuracy": 0.7895833333333333
        },
        "0.01": {
          "tp": 1724,
          "fn": 676,
          "accuracy": 0.7183333333333334
        }
      },
      "auroc": 0.9476651909722221
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 5034,
          "fn": 1766,
          "accuracy": 0.7402941176470588
        },
        "0.01": {
          "tp": 4654,
          "fn": 2146,
          "accuracy": 0.6844117647058824
        }
      },
      "auroc": 0.9226576286764707
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9979447916666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9983473958333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9983473958333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9985486979166667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9986833333333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9846270833333335
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9916552083333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.6680916666666668
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9943416666666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        }
      },
      "auroc": 0.8312166666666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        },
        "0.01": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        }
      },
      "auroc": 0.8333875
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        }
      },
      "auroc": 0.989484375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 596,
          "fn": 204,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 559,
          "fn": 241,
          "accuracy": 0.69875
        }
      },
      "auroc": 0.9114359375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9985135416666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.993775
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9961442708333335
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9983364583333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9837739583333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9910552083333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9984250000000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9887744791666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 767,
          "fn": 33,
          "accuracy": 0.95875
        },
        "0.01": {
          "tp": 739,
          "fn": 61,
          "accuracy": 0.92375
        }
      },
      "auroc": 0.9935997395833333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.990471875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9946109375000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.6206614583333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.6398885416666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 383,
          "accuracy": 0.0425
        },
        "0.01": {
          "tp": 7,
          "fn": 393,
          "accuracy": 0.0175
        }
      },
      "auroc": 0.630275
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        }
      },
      "auroc": 0.8097057291666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 164,
          "fn": 236,
          "accuracy": 0.41
        }
      },
      "auroc": 0.8151802083333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 406,
          "fn": 394,
          "accuracy": 0.5075
        },
        "0.01": {
          "tp": 367,
          "fn": 433,
          "accuracy": 0.45875
        }
      },
      "auroc": 0.8124429687499999
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9986822916666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        }
      },
      "auroc": 0.9660958333333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9823890625
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.7191197916666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.7535333333333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 308,
          "accuracy": 0.23
        },
        "0.01": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        }
      },
      "auroc": 0.7363265625000001
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 232,
          "fn": 168,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        }
      },
      "auroc": 0.8589010416666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 147,
          "fn": 253,
          "accuracy": 0.3675
        }
      },
      "auroc": 0.8598145833333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 438,
          "fn": 362,
          "accuracy": 0.5475
        },
        "0.01": {
          "tp": 370,
          "fn": 430,
          "accuracy": 0.4625
        }
      },
      "auroc": 0.8593578125
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9983541666666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9913010416666668
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9948276041666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9985520833333332
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9950255208333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 788,
          "fn": 12,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        }
      },
      "auroc": 0.9967888020833334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9946447916666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9946447916666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.9693197916666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.9693197916666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9819822916666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9819822916666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        }
      },
      "auroc": 0.8984322916666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        }
      },
      "auroc": 0.8984322916666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        },
        "0.01": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        }
      },
      "auroc": 0.8483979166666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        },
        "0.01": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        }
      },
      "auroc": 0.8483979166666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 234,
          "accuracy": 0.415
        },
        "0.01": {
          "tp": 98,
          "fn": 302,
          "accuracy": 0.245
        }
      },
      "auroc": 0.8734151041666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 234,
          "accuracy": 0.415
        },
        "0.01": {
          "tp": 98,
          "fn": 302,
          "accuracy": 0.245
        }
      },
      "auroc": 0.8734151041666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.99875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.993871875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.993871875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9963109375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9963109375
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9525104166666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.9525104166666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.9363927083333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.9363927083333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        },
        "0.01": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9444515625000001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        },
        "0.01": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        }
      },
      "auroc": 0.9444515625000001
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2027,
          "fn": 173,
          "accuracy": 0.9213636363636364
        },
        "0.01": {
          "tp": 1938,
          "fn": 262,
          "accuracy": 0.8809090909090909
        }
      },
      "auroc": 0.985019696969697
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1108,
          "fn": 92,
          "accuracy": 0.9233333333333333
        },
        "0.01": {
          "tp": 1007,
          "fn": 193,
          "accuracy": 0.8391666666666666
        }
      },
      "auroc": 0.9887449652777778
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3135,
          "fn": 265,
          "accuracy": 0.9220588235294118
        },
        "0.01": {
          "tp": 2945,
          "fn": 455,
          "accuracy": 0.8661764705882353
        }
      },
      "auroc": 0.9863344975490196
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1410,
          "fn": 790,
          "accuracy": 0.6409090909090909
        },
        "0.01": {
          "tp": 1296,
          "fn": 904,
          "accuracy": 0.5890909090909091
        }
      },
      "auroc": 0.886367803030303
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 826,
          "fn": 374,
          "accuracy": 0.6883333333333334
        },
        "0.01": {
          "tp": 770,
          "fn": 430,
          "accuracy": 0.6416666666666667
        }
      },
      "auroc": 0.8934638888888888
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2236,
          "fn": 1164,
          "accuracy": 0.6576470588235294
        },
        "0.01": {
          "tp": 2066,
          "fn": 1334,
          "accuracy": 0.6076470588235294
        }
      },
      "auroc": 0.8888723039215687
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3437,
          "fn": 963,
          "accuracy": 0.7811363636363636
        },
        "0.01": {
          "tp": 3234,
          "fn": 1166,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9356937500000001
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1934,
          "fn": 466,
          "accuracy": 0.8058333333333333
        },
        "0.01": {
          "tp": 1777,
          "fn": 623,
          "accuracy": 0.7404166666666666
        }
      },
      "auroc": 0.9411044270833334
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 5371,
          "fn": 1429,
          "accuracy": 0.7898529411764705
        },
        "0.01": {
          "tp": 5011,
          "fn": 1789,
          "accuracy": 0.7369117647058824
        }
      },
      "auroc": 0.9376034007352941
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.15149583333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.15148541666666665
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.15149062500000002
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.1499510416666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.15534583333333332
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.15264843750000004
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.1507234375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.153415625
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.15206953125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.2868052083333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.29151041666666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 386,
          "accuracy": 0.035
        },
        "0.01": {
          "tp": 12,
          "fn": 388,
          "accuracy": 0.03
        }
      },
      "auroc": 0.2891578125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.20651250000000002
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.332271875
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        },
        "0.01": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        }
      },
      "auroc": 0.2693921875
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 16,
          "fn": 384,
          "accuracy": 0.04
        }
      },
      "auroc": 0.2466588541666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.3118911458333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 781,
          "accuracy": 0.02375
        },
        "0.01": {
          "tp": 17,
          "fn": 783,
          "accuracy": 0.02125
        }
      },
      "auroc": 0.279275
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.1553125
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.1576125
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.1564625
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.1544229166666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.17100937500000002
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.16271614583333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.15486770833333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.1643109375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.15958932291666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.4201645833333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.19850833333333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 367,
          "accuracy": 0.0825
        },
        "0.01": {
          "tp": 27,
          "fn": 373,
          "accuracy": 0.0675
        }
      },
      "auroc": 0.30933645833333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.19831354166666668
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.23199062499999998
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.21515208333333335
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 367,
          "accuracy": 0.0825
        },
        "0.01": {
          "tp": 27,
          "fn": 373,
          "accuracy": 0.0675
        }
      },
      "auroc": 0.30923906250000005
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.21524947916666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 767,
          "accuracy": 0.04125
        },
        "0.01": {
          "tp": 27,
          "fn": 773,
          "accuracy": 0.03375
        }
      },
      "auroc": 0.26224427083333335
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.24556250000000002
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.23930104166666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 373,
          "accuracy": 0.0675
        },
        "0.01": {
          "tp": 24,
          "fn": 376,
          "accuracy": 0.06
        }
      },
      "auroc": 0.24243177083333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.22831041666666668
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.27863958333333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        },
        "0.01": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        }
      },
      "auroc": 0.253475
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 378,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 19,
          "fn": 381,
          "accuracy": 0.0475
        }
      },
      "auroc": 0.23693645833333332
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 30,
          "fn": 370,
          "accuracy": 0.075
        },
        "0.01": {
          "tp": 30,
          "fn": 370,
          "accuracy": 0.075
        }
      },
      "auroc": 0.25897031249999997
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 52,
          "fn": 748,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 49,
          "fn": 751,
          "accuracy": 0.06125
        }
      },
      "auroc": 0.24795338541666662
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.16531041666666665
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.15579895833333335
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.1605546875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.15950520833333331
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.17556562500000003
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.1675354166666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.1624078125
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.16568229166666665
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.16404505208333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.1564479166666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.1564479166666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.15329687499999997
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.15329687499999997
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.15487239583333332
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.15487239583333332
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.15776875
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.15776875
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.169990625
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.169990625
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.1638796875
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.1638796875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.1388916666666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.1388916666666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.136134375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.136134375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.13751302083333336
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.13751302083333336
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.130059375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.130059375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.13603541666666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.13603541666666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.13304739583333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.13304739583333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.15659166666666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.15659166666666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.16404791666666665
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.16404791666666665
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.16031979166666668
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.16031979166666668
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 2143,
          "accuracy": 0.02590909090909091
        },
        "0.01": {
          "tp": 46,
          "fn": 2154,
          "accuracy": 0.02090909090909091
        }
      },
      "auroc": 0.19676458333333335
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 1183,
          "accuracy": 0.014166666666666666
        },
        "0.01": {
          "tp": 17,
          "fn": 1183,
          "accuracy": 0.014166666666666666
        }
      },
      "auroc": 0.1990361111111111
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 74,
          "fn": 3326,
          "accuracy": 0.02176470588235294
        },
        "0.01": {
          "tp": 63,
          "fn": 3337,
          "accuracy": 0.01852941176470588
        }
      },
      "auroc": 0.19756629901960784
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 2184,
          "accuracy": 0.007272727272727273
        },
        "0.01": {
          "tp": 16,
          "fn": 2184,
          "accuracy": 0.007272727272727273
        }
      },
      "auroc": 0.16877462121212122
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 1186,
          "accuracy": 0.011666666666666667
        },
        "0.01": {
          "tp": 14,
          "fn": 1186,
          "accuracy": 0.011666666666666667
        }
      },
      "auroc": 0.22413715277777777
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 30,
          "fn": 3370,
          "accuracy": 0.008823529411764706
        },
        "0.01": {
          "tp": 30,
          "fn": 3370,
          "accuracy": 0.008823529411764706
        }
      },
      "auroc": 0.18831433823529414
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 73,
          "fn": 4327,
          "accuracy": 0.01659090909090909
        },
        "0.01": {
          "tp": 62,
          "fn": 4338,
          "accuracy": 0.014090909090909091
        }
      },
      "auroc": 0.1827696022727273
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 2369,
          "accuracy": 0.012916666666666667
        },
        "0.01": {
          "tp": 31,
          "fn": 2369,
          "accuracy": 0.012916666666666667
        }
      },
      "auroc": 0.21158663194444444
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 6696,
          "accuracy": 0.015294117647058824
        },
        "0.01": {
          "tp": 93,
          "fn": 6707,
          "accuracy": 0.013676470588235293
        }
      },
      "auroc": 0.192940318627451
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1976,
          "fn": 424,
          "accuracy": 0.8233333333333334
        },
        "0.01": {
          "tp": 1960,
          "fn": 440,
          "accuracy": 0.8166666666666667
        }
      },
      "auroc": 0.8583292534722222
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1972,
          "fn": 428,
          "accuracy": 0.8216666666666667
        },
        "0.01": {
          "tp": 1947,
          "fn": 453,
          "accuracy": 0.81125
        }
      },
      "auroc": 0.8554321180555555
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3948,
          "fn": 852,
          "accuracy": 0.8225
        },
        "0.01": {
          "tp": 3907,
          "fn": 893,
          "accuracy": 0.8139583333333333
        }
      },
      "auroc": 0.8568806857638889
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1984,
          "fn": 416,
          "accuracy": 0.8266666666666667
        },
        "0.01": {
          "tp": 1968,
          "fn": 432,
          "accuracy": 0.82
        }
      },
      "auroc": 0.85782265625
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1929,
          "fn": 471,
          "accuracy": 0.80375
        },
        "0.01": {
          "tp": 1893,
          "fn": 507,
          "accuracy": 0.78875
        }
      },
      "auroc": 0.8521580729166667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3913,
          "fn": 887,
          "accuracy": 0.8152083333333333
        },
        "0.01": {
          "tp": 3861,
          "fn": 939,
          "accuracy": 0.804375
        }
      },
      "auroc": 0.8549903645833333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3960,
          "fn": 840,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 3928,
          "fn": 872,
          "accuracy": 0.8183333333333334
        }
      },
      "auroc": 0.8580759548611112
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3901,
          "fn": 899,
          "accuracy": 0.8127083333333334
        },
        "0.01": {
          "tp": 3840,
          "fn": 960,
          "accuracy": 0.8
        }
      },
      "auroc": 0.8537950954861111
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7861,
          "fn": 1739,
          "accuracy": 0.8188541666666667
        },
        "0.01": {
          "tp": 7768,
          "fn": 1832,
          "accuracy": 0.8091666666666667
        }
      },
      "auroc": 0.8559355251736112
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2020,
          "fn": 380,
          "accuracy": 0.8416666666666667
        },
        "0.01": {
          "tp": 1959,
          "fn": 441,
          "accuracy": 0.81625
        }
      },
      "auroc": 0.89220234375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1641,
          "fn": 759,
          "accuracy": 0.68375
        },
        "0.01": {
          "tp": 1415,
          "fn": 985,
          "accuracy": 0.5895833333333333
        }
      },
      "auroc": 0.8424973958333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3661,
          "fn": 1139,
          "accuracy": 0.7627083333333333
        },
        "0.01": {
          "tp": 3374,
          "fn": 1426,
          "accuracy": 0.7029166666666666
        }
      },
      "auroc": 0.8673498697916666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 221,
          "fn": 2179,
          "accuracy": 0.09208333333333334
        },
        "0.01": {
          "tp": 161,
          "fn": 2239,
          "accuracy": 0.06708333333333333
        }
      },
      "auroc": 0.5636103298611111
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1768,
          "fn": 632,
          "accuracy": 0.7366666666666667
        },
        "0.01": {
          "tp": 1738,
          "fn": 662,
          "accuracy": 0.7241666666666666
        }
      },
      "auroc": 0.8498345486111111
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1989,
          "fn": 2811,
          "accuracy": 0.414375
        },
        "0.01": {
          "tp": 1899,
          "fn": 2901,
          "accuracy": 0.395625
        }
      },
      "auroc": 0.7067224392361111
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2241,
          "fn": 2559,
          "accuracy": 0.466875
        },
        "0.01": {
          "tp": 2120,
          "fn": 2680,
          "accuracy": 0.44166666666666665
        }
      },
      "auroc": 0.7279063368055556
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3409,
          "fn": 1391,
          "accuracy": 0.7102083333333333
        },
        "0.01": {
          "tp": 3153,
          "fn": 1647,
          "accuracy": 0.656875
        }
      },
      "auroc": 0.8461659722222221
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5650,
          "fn": 3950,
          "accuracy": 0.5885416666666666
        },
        "0.01": {
          "tp": 5273,
          "fn": 4327,
          "accuracy": 0.5492708333333334
        }
      },
      "auroc": 0.7870361545138889
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1965,
          "fn": 435,
          "accuracy": 0.81875
        },
        "0.01": {
          "tp": 1909,
          "fn": 491,
          "accuracy": 0.7954166666666667
        }
      },
      "auroc": 0.85761015625
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1859,
          "fn": 541,
          "accuracy": 0.7745833333333333
        },
        "0.01": {
          "tp": 1753,
          "fn": 647,
          "accuracy": 0.7304166666666667
        }
      },
      "auroc": 0.8480042534722223
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3824,
          "fn": 976,
          "accuracy": 0.7966666666666666
        },
        "0.01": {
          "tp": 3662,
          "fn": 1138,
          "accuracy": 0.7629166666666667
        }
      },
      "auroc": 0.8528072048611111
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1950,
          "fn": 450,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 1908,
          "fn": 492,
          "accuracy": 0.795
        }
      },
      "auroc": 0.8540216145833333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1655,
          "fn": 745,
          "accuracy": 0.6895833333333333
        },
        "0.01": {
          "tp": 1493,
          "fn": 907,
          "accuracy": 0.6220833333333333
        }
      },
      "auroc": 0.8321513888888888
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3605,
          "fn": 1195,
          "accuracy": 0.7510416666666667
        },
        "0.01": {
          "tp": 3401,
          "fn": 1399,
          "accuracy": 0.7085416666666666
        }
      },
      "auroc": 0.8430865017361111
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3915,
          "fn": 885,
          "accuracy": 0.815625
        },
        "0.01": {
          "tp": 3817,
          "fn": 983,
          "accuracy": 0.7952083333333333
        }
      },
      "auroc": 0.8558158854166666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3514,
          "fn": 1286,
          "accuracy": 0.7320833333333333
        },
        "0.01": {
          "tp": 3246,
          "fn": 1554,
          "accuracy": 0.67625
        }
      },
      "auroc": 0.8400778211805555
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7429,
          "fn": 2171,
          "accuracy": 0.7738541666666666
        },
        "0.01": {
          "tp": 7063,
          "fn": 2537,
          "accuracy": 0.7357291666666667
        }
      },
      "auroc": 0.8479468532986112
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2066,
          "fn": 334,
          "accuracy": 0.8608333333333333
        },
        "0.01": {
          "tp": 2034,
          "fn": 366,
          "accuracy": 0.8475
        }
      },
      "auroc": 0.9149073784722223
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1607,
          "fn": 793,
          "accuracy": 0.6695833333333333
        },
        "0.01": {
          "tp": 1279,
          "fn": 1121,
          "accuracy": 0.5329166666666667
        }
      },
      "auroc": 0.8410578993055555
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3673,
          "fn": 1127,
          "accuracy": 0.7652083333333334
        },
        "0.01": {
          "tp": 3313,
          "fn": 1487,
          "accuracy": 0.6902083333333333
        }
      },
      "auroc": 0.8779826388888888
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 2339,
          "accuracy": 0.025416666666666667
        },
        "0.01": {
          "tp": 27,
          "fn": 2373,
          "accuracy": 0.01125
        }
      },
      "auroc": 0.5286873263888889
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 2277,
          "accuracy": 0.05125
        },
        "0.01": {
          "tp": 47,
          "fn": 2353,
          "accuracy": 0.019583333333333335
        }
      },
      "auroc": 0.5679993923611111
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 4616,
          "accuracy": 0.03833333333333333
        },
        "0.01": {
          "tp": 74,
          "fn": 4726,
          "accuracy": 0.015416666666666667
        }
      },
      "auroc": 0.548343359375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2127,
          "fn": 2673,
          "accuracy": 0.443125
        },
        "0.01": {
          "tp": 2061,
          "fn": 2739,
          "accuracy": 0.429375
        }
      },
      "auroc": 0.7217973524305555
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1730,
          "fn": 3070,
          "accuracy": 0.36041666666666666
        },
        "0.01": {
          "tp": 1326,
          "fn": 3474,
          "accuracy": 0.27625
        }
      },
      "auroc": 0.7045286458333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3857,
          "fn": 5743,
          "accuracy": 0.40177083333333335
        },
        "0.01": {
          "tp": 3387,
          "fn": 6213,
          "accuracy": 0.3528125
        }
      },
      "auroc": 0.7131629991319445
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1980,
          "fn": 420,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 1908,
          "fn": 492,
          "accuracy": 0.795
        }
      },
      "auroc": 0.879668923611111
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1288,
          "fn": 1112,
          "accuracy": 0.5366666666666666
        },
        "0.01": {
          "tp": 963,
          "fn": 1437,
          "accuracy": 0.40125
        }
      },
      "auroc": 0.8216682291666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3268,
          "fn": 1532,
          "accuracy": 0.6808333333333333
        },
        "0.01": {
          "tp": 2871,
          "fn": 1929,
          "accuracy": 0.598125
        }
      },
      "auroc": 0.8506685763888888
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 2088,
          "accuracy": 0.13
        },
        "0.01": {
          "tp": 217,
          "fn": 2183,
          "accuracy": 0.09041666666666667
        }
      },
      "auroc": 0.6055949652777778
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 603,
          "fn": 1797,
          "accuracy": 0.25125
        },
        "0.01": {
          "tp": 423,
          "fn": 1977,
          "accuracy": 0.17625
        }
      },
      "auroc": 0.6621921006944445
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 915,
          "fn": 3885,
          "accuracy": 0.190625
        },
        "0.01": {
          "tp": 640,
          "fn": 4160,
          "accuracy": 0.13333333333333333
        }
      },
      "auroc": 0.6338935329861111
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2292,
          "fn": 2508,
          "accuracy": 0.4775
        },
        "0.01": {
          "tp": 2125,
          "fn": 2675,
          "accuracy": 0.4427083333333333
        }
      },
      "auroc": 0.7426319444444444
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1891,
          "fn": 2909,
          "accuracy": 0.39395833333333335
        },
        "0.01": {
          "tp": 1386,
          "fn": 3414,
          "accuracy": 0.28875
        }
      },
      "auroc": 0.7419301649305556
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4183,
          "fn": 5417,
          "accuracy": 0.43572916666666667
        },
        "0.01": {
          "tp": 3511,
          "fn": 6089,
          "accuracy": 0.36572916666666666
        }
      },
      "auroc": 0.7422810546874999
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1993,
          "fn": 407,
          "accuracy": 0.8304166666666667
        },
        "0.01": {
          "tp": 1988,
          "fn": 412,
          "accuracy": 0.8283333333333334
        }
      },
      "auroc": 0.8672417534722222
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1969,
          "fn": 431,
          "accuracy": 0.8204166666666667
        },
        "0.01": {
          "tp": 1949,
          "fn": 451,
          "accuracy": 0.8120833333333334
        }
      },
      "auroc": 0.858312326388889
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3962,
          "fn": 838,
          "accuracy": 0.8254166666666667
        },
        "0.01": {
          "tp": 3937,
          "fn": 863,
          "accuracy": 0.8202083333333333
        }
      },
      "auroc": 0.8627770399305554
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1958,
          "fn": 442,
          "accuracy": 0.8158333333333333
        },
        "0.01": {
          "tp": 1910,
          "fn": 490,
          "accuracy": 0.7958333333333333
        }
      },
      "auroc": 0.8579167534722222
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1802,
          "fn": 598,
          "accuracy": 0.7508333333333334
        },
        "0.01": {
          "tp": 1678,
          "fn": 722,
          "accuracy": 0.6991666666666667
        }
      },
      "auroc": 0.8442238715277777
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3760,
          "fn": 1040,
          "accuracy": 0.7833333333333333
        },
        "0.01": {
          "tp": 3588,
          "fn": 1212,
          "accuracy": 0.7475
        }
      },
      "auroc": 0.8510703125000001
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3951,
          "fn": 849,
          "accuracy": 0.823125
        },
        "0.01": {
          "tp": 3898,
          "fn": 902,
          "accuracy": 0.8120833333333334
        }
      },
      "auroc": 0.8625792534722222
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3771,
          "fn": 1029,
          "accuracy": 0.785625
        },
        "0.01": {
          "tp": 3627,
          "fn": 1173,
          "accuracy": 0.755625
        }
      },
      "auroc": 0.8512680989583333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7722,
          "fn": 1878,
          "accuracy": 0.804375
        },
        "0.01": {
          "tp": 7525,
          "fn": 2075,
          "accuracy": 0.7838541666666666
        }
      },
      "auroc": 0.8569236762152778
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1646,
          "fn": 754,
          "accuracy": 0.6858333333333333
        },
        "0.01": {
          "tp": 1338,
          "fn": 1062,
          "accuracy": 0.5575
        }
      },
      "auroc": 0.8402492187499999
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1646,
          "fn": 754,
          "accuracy": 0.6858333333333333
        },
        "0.01": {
          "tp": 1338,
          "fn": 1062,
          "accuracy": 0.5575
        }
      },
      "auroc": 0.8402492187499999
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1374,
          "fn": 1026,
          "accuracy": 0.5725
        },
        "0.01": {
          "tp": 1056,
          "fn": 1344,
          "accuracy": 0.44
        }
      },
      "auroc": 0.8083518229166666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1374,
          "fn": 1026,
          "accuracy": 0.5725
        },
        "0.01": {
          "tp": 1056,
          "fn": 1344,
          "accuracy": 0.44
        }
      },
      "auroc": 0.8083518229166666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3020,
          "fn": 1780,
          "accuracy": 0.6291666666666667
        },
        "0.01": {
          "tp": 2394,
          "fn": 2406,
          "accuracy": 0.49875
        }
      },
      "auroc": 0.8243005208333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3020,
          "fn": 1780,
          "accuracy": 0.6291666666666667
        },
        "0.01": {
          "tp": 2394,
          "fn": 2406,
          "accuracy": 0.49875
        }
      },
      "auroc": 0.8243005208333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 787,
          "fn": 1613,
          "accuracy": 0.3279166666666667
        },
        "0.01": {
          "tp": 477,
          "fn": 1923,
          "accuracy": 0.19875
        }
      },
      "auroc": 0.7385565104166667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 787,
          "fn": 1613,
          "accuracy": 0.3279166666666667
        },
        "0.01": {
          "tp": 477,
          "fn": 1923,
          "accuracy": 0.19875
        }
      },
      "auroc": 0.7385565104166667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 535,
          "fn": 1865,
          "accuracy": 0.22291666666666668
        },
        "0.01": {
          "tp": 275,
          "fn": 2125,
          "accuracy": 0.11458333333333333
        }
      },
      "auroc": 0.705308420138889
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 535,
          "fn": 1865,
          "accuracy": 0.22291666666666668
        },
        "0.01": {
          "tp": 275,
          "fn": 2125,
          "accuracy": 0.11458333333333333
        }
      },
      "auroc": 0.705308420138889
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1322,
          "fn": 3478,
          "accuracy": 0.27541666666666664
        },
        "0.01": {
          "tp": 752,
          "fn": 4048,
          "accuracy": 0.15666666666666668
        }
      },
      "auroc": 0.7219324652777777
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1322,
          "fn": 3478,
          "accuracy": 0.27541666666666664
        },
        "0.01": {
          "tp": 752,
          "fn": 4048,
          "accuracy": 0.15666666666666668
        }
      },
      "auroc": 0.7219324652777777
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1995,
          "fn": 405,
          "accuracy": 0.83125
        },
        "0.01": {
          "tp": 1990,
          "fn": 410,
          "accuracy": 0.8291666666666667
        }
      },
      "auroc": 0.8563811631944445
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1995,
          "fn": 405,
          "accuracy": 0.83125
        },
        "0.01": {
          "tp": 1990,
          "fn": 410,
          "accuracy": 0.8291666666666667
        }
      },
      "auroc": 0.8563811631944445
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1993,
          "fn": 407,
          "accuracy": 0.8304166666666667
        },
        "0.01": {
          "tp": 1984,
          "fn": 416,
          "accuracy": 0.8266666666666667
        }
      },
      "auroc": 0.8533647569444445
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1993,
          "fn": 407,
          "accuracy": 0.8304166666666667
        },
        "0.01": {
          "tp": 1984,
          "fn": 416,
          "accuracy": 0.8266666666666667
        }
      },
      "auroc": 0.8533647569444445
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3988,
          "fn": 812,
          "accuracy": 0.8308333333333333
        },
        "0.01": {
          "tp": 3974,
          "fn": 826,
          "accuracy": 0.8279166666666666
        }
      },
      "auroc": 0.8548729600694445
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3988,
          "fn": 812,
          "accuracy": 0.8308333333333333
        },
        "0.01": {
          "tp": 3974,
          "fn": 826,
          "accuracy": 0.8279166666666666
        }
      },
      "auroc": 0.8548729600694445
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1978,
          "fn": 422,
          "accuracy": 0.8241666666666667
        },
        "0.01": {
          "tp": 1954,
          "fn": 446,
          "accuracy": 0.8141666666666667
        }
      },
      "auroc": 0.8520108506944446
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1978,
          "fn": 422,
          "accuracy": 0.8241666666666667
        },
        "0.01": {
          "tp": 1954,
          "fn": 446,
          "accuracy": 0.8141666666666667
        }
      },
      "auroc": 0.8520108506944446
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1856,
          "fn": 544,
          "accuracy": 0.7733333333333333
        },
        "0.01": {
          "tp": 1759,
          "fn": 641,
          "accuracy": 0.7329166666666667
        }
      },
      "auroc": 0.8402532986111111
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1856,
          "fn": 544,
          "accuracy": 0.7733333333333333
        },
        "0.01": {
          "tp": 1759,
          "fn": 641,
          "accuracy": 0.7329166666666667
        }
      },
      "auroc": 0.8402532986111111
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3834,
          "fn": 966,
          "accuracy": 0.79875
        },
        "0.01": {
          "tp": 3713,
          "fn": 1087,
          "accuracy": 0.7735416666666667
        }
      },
      "auroc": 0.8461320746527778
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3834,
          "fn": 966,
          "accuracy": 0.79875
        },
        "0.01": {
          "tp": 3713,
          "fn": 1087,
          "accuracy": 0.7735416666666667
        }
      },
      "auroc": 0.8461320746527778
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1250,
          "fn": 1150,
          "accuracy": 0.5208333333333334
        },
        "0.01": {
          "tp": 1050,
          "fn": 1350,
          "accuracy": 0.4375
        }
      },
      "auroc": 0.8014144097222222
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1250,
          "fn": 1150,
          "accuracy": 0.5208333333333334
        },
        "0.01": {
          "tp": 1050,
          "fn": 1350,
          "accuracy": 0.4375
        }
      },
      "auroc": 0.8014144097222222
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1116,
          "fn": 1284,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 881,
          "fn": 1519,
          "accuracy": 0.3670833333333333
        }
      },
      "auroc": 0.7797019965277778
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1116,
          "fn": 1284,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 881,
          "fn": 1519,
          "accuracy": 0.3670833333333333
        }
      },
      "auroc": 0.7797019965277778
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2366,
          "fn": 2434,
          "accuracy": 0.49291666666666667
        },
        "0.01": {
          "tp": 1931,
          "fn": 2869,
          "accuracy": 0.40229166666666666
        }
      },
      "auroc": 0.790558203125
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2366,
          "fn": 2434,
          "accuracy": 0.49291666666666667
        },
        "0.01": {
          "tp": 1931,
          "fn": 2869,
          "accuracy": 0.40229166666666666
        }
      },
      "auroc": 0.790558203125
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 19656,
          "fn": 6744,
          "accuracy": 0.7445454545454545
        },
        "0.01": {
          "tp": 18567,
          "fn": 7833,
          "accuracy": 0.7032954545454545
        }
      },
      "auroc": 0.8507792692550504
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 10336,
          "fn": 4064,
          "accuracy": 0.7177777777777777
        },
        "0.01": {
          "tp": 9306,
          "fn": 5094,
          "accuracy": 0.64625
        }
      },
      "auroc": 0.8444953703703704
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 29992,
          "fn": 10808,
          "accuracy": 0.7350980392156863
        },
        "0.01": {
          "tp": 27873,
          "fn": 12927,
          "accuracy": 0.6831617647058823
        }
      },
      "auroc": 0.8485614225898692
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 13360,
          "fn": 13040,
          "accuracy": 0.5060606060606061
        },
        "0.01": {
          "tp": 12146,
          "fn": 14254,
          "accuracy": 0.4600757575757576
        }
      },
      "auroc": 0.7504212673611111
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7880,
          "fn": 6520,
          "accuracy": 0.5472222222222223
        },
        "0.01": {
          "tp": 7272,
          "fn": 7128,
          "accuracy": 0.505
        }
      },
      "auroc": 0.7680932291666667
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 21240,
          "fn": 19560,
          "accuracy": 0.5205882352941177
        },
        "0.01": {
          "tp": 19418,
          "fn": 21382,
          "accuracy": 0.4759313725490196
        }
      },
      "auroc": 0.7566584303513072
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 33016,
          "fn": 19784,
          "accuracy": 0.6253030303030302
        },
        "0.01": {
          "tp": 30713,
          "fn": 22087,
          "accuracy": 0.581685606060606
        }
      },
      "auroc": 0.8006002683080808
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18216,
          "fn": 10584,
          "accuracy": 0.6325
        },
        "0.01": {
          "tp": 16578,
          "fn": 12222,
          "accuracy": 0.575625
        }
      },
      "auroc": 0.8062942997685185
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 51232,
          "fn": 30368,
          "accuracy": 0.6278431372549019
        },
        "0.01": {
          "tp": 47291,
          "fn": 34309,
          "accuracy": 0.579546568627451
        }
      },
      "auroc": 0.8026099264705882
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9711947916666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.94510625
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": null
      },
      "auroc": 0.9581505208333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.961028125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        },
        "0.01": null
      },
      "auroc": 0.9130104166666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": null
      },
      "auroc": 0.9370192708333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9661114583333332
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        },
        "0.01": null
      },
      "auroc": 0.9290583333333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 547,
          "fn": 253,
          "accuracy": 0.68375
        },
        "0.01": null
      },
      "auroc": 0.9475848958333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9851614583333335
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.976540625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9808510416666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        },
        "0.01": null
      },
      "auroc": 0.8311864583333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9738552083333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": null
      },
      "auroc": 0.9025208333333332
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.9081739583333335
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9751979166666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 618,
          "fn": 182,
          "accuracy": 0.7725
        },
        "0.01": null
      },
      "auroc": 0.9416859375000002
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9703083333333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": null
      },
      "auroc": 0.9089385416666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.9396234375000001
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.94018125
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.9173322916666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": null
      },
      "auroc": 0.9287567708333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": null
      },
      "auroc": 0.9552447916666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.9131354166666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 519,
          "fn": 281,
          "accuracy": 0.64875
        },
        "0.01": null
      },
      "auroc": 0.9341901041666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9935624999999999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9914333333333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9924979166666665
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": null
      },
      "auroc": 0.8569822916666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": null
      },
      "auroc": 0.7519114583333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 336,
          "accuracy": 0.16
        },
        "0.01": null
      },
      "auroc": 0.804446875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        },
        "0.01": null
      },
      "auroc": 0.9252723958333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        },
        "0.01": null
      },
      "auroc": 0.8716723958333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 463,
          "fn": 337,
          "accuracy": 0.57875
        },
        "0.01": null
      },
      "auroc": 0.8984723958333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9791333333333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9628916666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9710125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        },
        "0.01": null
      },
      "auroc": 0.809496875
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        },
        "0.01": null
      },
      "auroc": 0.8021864583333332
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 259,
          "accuracy": 0.3525
        },
        "0.01": null
      },
      "auroc": 0.8058416666666668
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        },
        "0.01": null
      },
      "auroc": 0.8943151041666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": null
      },
      "auroc": 0.8825390625
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 517,
          "fn": 283,
          "accuracy": 0.64625
        },
        "0.01": null
      },
      "auroc": 0.8884270833333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9843625
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": null
      },
      "auroc": 0.95723125
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.970796875
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9616812499999999
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        },
        "0.01": null
      },
      "auroc": 0.9008552083333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        },
        "0.01": null
      },
      "auroc": 0.9312682291666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9730218749999999
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        },
        "0.01": null
      },
      "auroc": 0.9290432291666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 576,
          "fn": 224,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9510325520833333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.9318635416666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.9318635416666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": null
      },
      "auroc": 0.8964093749999998
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": null
      },
      "auroc": 0.8964093749999998
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 289,
          "fn": 111,
          "accuracy": 0.7225
        },
        "0.01": null
      },
      "auroc": 0.9141364583333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 289,
          "fn": 111,
          "accuracy": 0.7225
        },
        "0.01": null
      },
      "auroc": 0.9141364583333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": null
      },
      "auroc": 0.7492114583333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": null
      },
      "auroc": 0.7492114583333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": null
      },
      "auroc": 0.6593677083333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": null
      },
      "auroc": 0.6593677083333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 303,
          "accuracy": 0.2425
        },
        "0.01": null
      },
      "auroc": 0.7042895833333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 303,
          "accuracy": 0.2425
        },
        "0.01": null
      },
      "auroc": 0.7042895833333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.973553125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.973553125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9431270833333332
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9431270833333332
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9583401041666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9583401041666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9710395833333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9710395833333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": null
      },
      "auroc": 0.8182302083333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": null
      },
      "auroc": 0.8182302083333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": null
      },
      "auroc": 0.8946348958333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": null
      },
      "auroc": 0.8946348958333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.9215156250000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.9215156250000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.857478125
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.857478125
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": null
      },
      "auroc": 0.8894968750000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": null
      },
      "auroc": 0.8894968750000001
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1878,
          "fn": 322,
          "accuracy": 0.8536363636363636
        },
        "0.01": null
      },
      "auroc": 0.9482642045454546
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 924,
          "fn": 276,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9570236111111111
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2802,
          "fn": 598,
          "accuracy": 0.8241176470588235
        },
        "0.01": null
      },
      "auroc": 0.9513557598039215
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1050,
          "fn": 1150,
          "accuracy": 0.4772727272727273
        },
        "0.01": null
      },
      "auroc": 0.8668335227272727
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 557,
          "fn": 643,
          "accuracy": 0.46416666666666667
        },
        "0.01": null
      },
      "auroc": 0.8765251736111113
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1607,
          "fn": 1793,
          "accuracy": 0.4726470588235294
        },
        "0.01": null
      },
      "auroc": 0.8702541053921569
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2928,
          "fn": 1472,
          "accuracy": 0.6654545454545454
        },
        "0.01": null
      },
      "auroc": 0.9075488636363636
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1481,
          "fn": 919,
          "accuracy": 0.6170833333333333
        },
        "0.01": null
      },
      "auroc": 0.9167743923611111
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 4409,
          "fn": 2391,
          "accuracy": 0.6483823529411765
        },
        "0.01": null
      },
      "auroc": 0.9108049325980392
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9711947916666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.94510625
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": null
      },
      "auroc": 0.9581505208333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.961028125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        },
        "0.01": null
      },
      "auroc": 0.9130104166666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": null
      },
      "auroc": 0.9370192708333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9661114583333332
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        },
        "0.01": null
      },
      "auroc": 0.9290583333333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 547,
          "fn": 253,
          "accuracy": 0.68375
        },
        "0.01": null
      },
      "auroc": 0.9475848958333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9851614583333335
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.976540625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9808510416666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        },
        "0.01": null
      },
      "auroc": 0.8311864583333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9738552083333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": null
      },
      "auroc": 0.9025208333333332
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.9081739583333335
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9751979166666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 618,
          "fn": 182,
          "accuracy": 0.7725
        },
        "0.01": null
      },
      "auroc": 0.9416859375000002
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9703083333333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": null
      },
      "auroc": 0.9089385416666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.9396234375000001
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.94018125
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.9173322916666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": null
      },
      "auroc": 0.9287567708333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": null
      },
      "auroc": 0.9552447916666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.9131354166666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 519,
          "fn": 281,
          "accuracy": 0.64875
        },
        "0.01": null
      },
      "auroc": 0.9341901041666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9935624999999999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9914333333333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9924979166666665
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": null
      },
      "auroc": 0.8569822916666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": null
      },
      "auroc": 0.7519114583333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 336,
          "accuracy": 0.16
        },
        "0.01": null
      },
      "auroc": 0.804446875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        },
        "0.01": null
      },
      "auroc": 0.9252723958333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        },
        "0.01": null
      },
      "auroc": 0.8716723958333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 463,
          "fn": 337,
          "accuracy": 0.57875
        },
        "0.01": null
      },
      "auroc": 0.8984723958333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9791333333333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9628916666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9710125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        },
        "0.01": null
      },
      "auroc": 0.809496875
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        },
        "0.01": null
      },
      "auroc": 0.8021864583333332
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 259,
          "accuracy": 0.3525
        },
        "0.01": null
      },
      "auroc": 0.8058416666666668
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        },
        "0.01": null
      },
      "auroc": 0.8943151041666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": null
      },
      "auroc": 0.8825390625
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 517,
          "fn": 283,
          "accuracy": 0.64625
        },
        "0.01": null
      },
      "auroc": 0.8884270833333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9843625
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": null
      },
      "auroc": 0.95723125
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.970796875
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9616812499999999
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        },
        "0.01": null
      },
      "auroc": 0.9008552083333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        },
        "0.01": null
      },
      "auroc": 0.9312682291666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9730218749999999
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        },
        "0.01": null
      },
      "auroc": 0.9290432291666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 576,
          "fn": 224,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9510325520833333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.9318635416666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.9318635416666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": null
      },
      "auroc": 0.8964072916666668
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": null
      },
      "auroc": 0.8964072916666668
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 289,
          "fn": 111,
          "accuracy": 0.7225
        },
        "0.01": null
      },
      "auroc": 0.9141354166666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 289,
          "fn": 111,
          "accuracy": 0.7225
        },
        "0.01": null
      },
      "auroc": 0.9141354166666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": null
      },
      "auroc": 0.7492114583333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": null
      },
      "auroc": 0.7492114583333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": null
      },
      "auroc": 0.6593677083333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": null
      },
      "auroc": 0.6593677083333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 303,
          "accuracy": 0.2425
        },
        "0.01": null
      },
      "auroc": 0.7042895833333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 303,
          "accuracy": 0.2425
        },
        "0.01": null
      },
      "auroc": 0.7042895833333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.973553125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.973553125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9431270833333332
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9431270833333332
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9583401041666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9583401041666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9710395833333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9710395833333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": null
      },
      "auroc": 0.8182302083333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": null
      },
      "auroc": 0.8182302083333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": null
      },
      "auroc": 0.8946348958333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": null
      },
      "auroc": 0.8946348958333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.9215156250000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.9215156250000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.857478125
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.857478125
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": null
      },
      "auroc": 0.8894968750000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": null
      },
      "auroc": 0.8894968750000001
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1878,
          "fn": 322,
          "accuracy": 0.8536363636363636
        },
        "0.01": null
      },
      "auroc": 0.9482642045454546
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 924,
          "fn": 276,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9570236111111111
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2802,
          "fn": 598,
          "accuracy": 0.8241176470588235
        },
        "0.01": null
      },
      "auroc": 0.9513557598039215
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1050,
          "fn": 1150,
          "accuracy": 0.4772727272727273
        },
        "0.01": null
      },
      "auroc": 0.8668333333333333
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 557,
          "fn": 643,
          "accuracy": 0.46416666666666667
        },
        "0.01": null
      },
      "auroc": 0.8765251736111113
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1607,
          "fn": 1793,
          "accuracy": 0.4726470588235294
        },
        "0.01": null
      },
      "auroc": 0.8702539828431373
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2928,
          "fn": 1472,
          "accuracy": 0.6654545454545454
        },
        "0.01": null
      },
      "auroc": 0.9075487689393938
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1481,
          "fn": 919,
          "accuracy": 0.6170833333333333
        },
        "0.01": null
      },
      "auroc": 0.9167743923611111
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 4409,
          "fn": 2391,
          "accuracy": 0.6483823529411765
        },
        "0.01": null
      },
      "auroc": 0.9108048713235294
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.9601302083333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        },
        "0.01": null
      },
      "auroc": 0.9253218750000001
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": null
      },
      "auroc": 0.9427260416666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": null
      },
      "auroc": 0.9482406250000001
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": null
      },
      "auroc": 0.8908218750000001
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 214,
          "accuracy": 0.465
        },
        "0.01": null
      },
      "auroc": 0.91953125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.9541854166666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 264,
          "accuracy": 0.34
        },
        "0.01": null
      },
      "auroc": 0.9080718750000001
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 426,
          "fn": 374,
          "accuracy": 0.5325
        },
        "0.01": null
      },
      "auroc": 0.9311286458333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9845822916666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9746447916666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9796135416666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": null
      },
      "auroc": 0.7916958333333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9724947916666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        },
        "0.01": null
      },
      "auroc": 0.8820953125
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": null
      },
      "auroc": 0.8881390625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": null
      },
      "auroc": 0.9735697916666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 598,
          "fn": 202,
          "accuracy": 0.7475
        },
        "0.01": null
      },
      "auroc": 0.9308544270833333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.95960625
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        },
        "0.01": null
      },
      "auroc": 0.8913270833333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 232,
          "fn": 168,
          "accuracy": 0.58
        },
        "0.01": null
      },
      "auroc": 0.9254666666666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": null
      },
      "auroc": 0.9233177083333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.9070447916666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        },
        "0.01": null
      },
      "auroc": 0.91518125
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 274,
          "fn": 126,
          "accuracy": 0.685
        },
        "0.01": null
      },
      "auroc": 0.9414619791666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        },
        "0.01": null
      },
      "auroc": 0.8991859375000001
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 453,
          "fn": 347,
          "accuracy": 0.56625
        },
        "0.01": null
      },
      "auroc": 0.9203239583333335
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.99353125
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9863364583333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9899338541666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": null
      },
      "auroc": 0.8086739583333331
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": null
      },
      "auroc": 0.6958854166666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 40,
          "fn": 360,
          "accuracy": 0.1
        },
        "0.01": null
      },
      "auroc": 0.7522796875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        },
        "0.01": null
      },
      "auroc": 0.9011026041666668
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.8411109375
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 439,
          "fn": 361,
          "accuracy": 0.54875
        },
        "0.01": null
      },
      "auroc": 0.8711067708333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9773739583333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": null
      },
      "auroc": 0.9583260416666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9678500000000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        },
        "0.01": null
      },
      "auroc": 0.77275625
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        },
        "0.01": null
      },
      "auroc": 0.7845020833333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        },
        "0.01": null
      },
      "auroc": 0.7786291666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        },
        "0.01": null
      },
      "auroc": 0.8750651041666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        },
        "0.01": null
      },
      "auroc": 0.8714140625000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 489,
          "fn": 311,
          "accuracy": 0.61125
        },
        "0.01": null
      },
      "auroc": 0.8732395833333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9779156250000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": null
      },
      "auroc": 0.9426645833333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        },
        "0.01": null
      },
      "auroc": 0.9602901041666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9505437499999999
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        },
        "0.01": null
      },
      "auroc": 0.87773125
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        },
        "0.01": null
      },
      "auroc": 0.9141374999999999
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": null
      },
      "auroc": 0.9642296875
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 238,
          "accuracy": 0.405
        },
        "0.01": null
      },
      "auroc": 0.9101979166666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 499,
          "fn": 301,
          "accuracy": 0.62375
        },
        "0.01": null
      },
      "auroc": 0.9372138020833334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9201229166666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9201229166666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": null
      },
      "auroc": 0.8823583333333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": null
      },
      "auroc": 0.8823583333333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        },
        "0.01": null
      },
      "auroc": 0.9012406250000001
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        },
        "0.01": null
      },
      "auroc": 0.9012406250000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": null
      },
      "auroc": 0.7226927083333332
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": null
      },
      "auroc": 0.7226927083333332
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        },
        "0.01": null
      },
      "auroc": 0.6278322916666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        },
        "0.01": null
      },
      "auroc": 0.6278322916666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 322,
          "accuracy": 0.195
        },
        "0.01": null
      },
      "auroc": 0.6752625000000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 322,
          "accuracy": 0.195
        },
        "0.01": null
      },
      "auroc": 0.6752625000000001
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9654270833333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9654270833333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": null
      },
      "auroc": 0.9267052083333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": null
      },
      "auroc": 0.9267052083333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": null
      },
      "auroc": 0.9460661458333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": null
      },
      "auroc": 0.9460661458333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.95965
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.95965
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.7779375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.7779375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        },
        "0.01": null
      },
      "auroc": 0.86879375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        },
        "0.01": null
      },
      "auroc": 0.86879375
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.9065947916666666
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.9065947916666666
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        },
        "0.01": null
      },
      "auroc": 0.8356677083333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        },
        "0.01": null
      },
      "auroc": 0.8356677083333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.8711312500000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.8711312500000001
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1770,
          "fn": 430,
          "accuracy": 0.8045454545454546
        },
        "0.01": null
      },
      "auroc": 0.9388751893939394
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 826,
          "fn": 374,
          "accuracy": 0.6883333333333334
        },
        "0.01": null
      },
      "auroc": 0.9464368055555555
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2596,
          "fn": 804,
          "accuracy": 0.7635294117647059
        },
        "0.01": null
      },
      "auroc": 0.9415439950980392
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 866,
          "fn": 1334,
          "accuracy": 0.3936363636363636
        },
        "0.01": null
      },
      "auroc": 0.8405208333333334
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 486,
          "fn": 714,
          "accuracy": 0.405
        },
        "0.01": null
      },
      "auroc": 0.854746701388889
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1352,
          "fn": 2048,
          "accuracy": 0.3976470588235294
        },
        "0.01": null
      },
      "auroc": 0.8455417279411764
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2636,
          "fn": 1764,
          "accuracy": 0.5990909090909091
        },
        "0.01": null
      },
      "auroc": 0.8896980113636364
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1312,
          "fn": 1088,
          "accuracy": 0.5466666666666666
        },
        "0.01": null
      },
      "auroc": 0.9005917534722223
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3948,
          "fn": 2852,
          "accuracy": 0.5805882352941176
        },
        "0.01": null
      },
      "auroc": 0.8935428615196079
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": null
      },
      "auroc": 0.9529895833333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": null
      },
      "auroc": 0.9128135416666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": null
      },
      "auroc": 0.9329015625
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": null
      },
      "auroc": 0.9403583333333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        },
        "0.01": null
      },
      "auroc": 0.88023125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 244,
          "accuracy": 0.39
        },
        "0.01": null
      },
      "auroc": 0.9102947916666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.9466739583333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 307,
          "accuracy": 0.2325
        },
        "0.01": null
      },
      "auroc": 0.8965223958333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 437,
          "accuracy": 0.45375
        },
        "0.01": null
      },
      "auroc": 0.9215981770833332
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9837291666666665
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9748447916666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9792869791666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        },
        "0.01": null
      },
      "auroc": 0.7785385416666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9751510416666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": null
      },
      "auroc": 0.8768447916666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": null
      },
      "auroc": 0.8811338541666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": null
      },
      "auroc": 0.9749979166666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 592,
          "fn": 208,
          "accuracy": 0.74
        },
        "0.01": null
      },
      "auroc": 0.9280658854166668
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.9452427083333335
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": null
      },
      "auroc": 0.8677114583333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        },
        "0.01": null
      },
      "auroc": 0.9064770833333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": null
      },
      "auroc": 0.9066302083333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": null
      },
      "auroc": 0.8966916666666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        },
        "0.01": null
      },
      "auroc": 0.9016609375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        },
        "0.01": null
      },
      "auroc": 0.9259364583333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 251,
          "accuracy": 0.3725
        },
        "0.01": null
      },
      "auroc": 0.8822015624999999
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 434,
          "accuracy": 0.4575
        },
        "0.01": null
      },
      "auroc": 0.9040690104166667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.99375
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9748302083333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9842901041666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        },
        "0.01": null
      },
      "auroc": 0.7979093749999999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        },
        "0.01": null
      },
      "auroc": 0.7203999999999999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 363,
          "accuracy": 0.0925
        },
        "0.01": null
      },
      "auroc": 0.7591546874999999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        },
        "0.01": null
      },
      "auroc": 0.8958296875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        },
        "0.01": null
      },
      "auroc": 0.8476151041666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 416,
          "fn": 384,
          "accuracy": 0.52
        },
        "0.01": null
      },
      "auroc": 0.8717223958333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9761489583333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9449958333333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9605723958333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        },
        "0.01": null
      },
      "auroc": 0.7667416666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": null
      },
      "auroc": 0.7977291666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 277,
          "accuracy": 0.3075
        },
        "0.01": null
      },
      "auroc": 0.7822354166666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        },
        "0.01": null
      },
      "auroc": 0.8714453124999999
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        },
        "0.01": null
      },
      "auroc": 0.8713624999999999
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 473,
          "fn": 327,
          "accuracy": 0.59125
        },
        "0.01": null
      },
      "auroc": 0.87140390625
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.964946875
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        },
        "0.01": null
      },
      "auroc": 0.916534375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        },
        "0.01": null
      },
      "auroc": 0.9407406250000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": null
      },
      "auroc": 0.9345187500000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": null
      },
      "auroc": 0.85769375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        },
        "0.01": null
      },
      "auroc": 0.89610625
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.9497328125
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 290,
          "accuracy": 0.275
        },
        "0.01": null
      },
      "auroc": 0.8871140625
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 400,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.9184234375
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": null
      },
      "auroc": 0.9060343750000001
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": null
      },
      "auroc": 0.9060343750000001
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": null
      },
      "auroc": 0.8585614583333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": null
      },
      "auroc": 0.8585614583333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        },
        "0.01": null
      },
      "auroc": 0.8822979166666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        },
        "0.01": null
      },
      "auroc": 0.8822979166666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        },
        "0.01": null
      },
      "auroc": 0.7121770833333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        },
        "0.01": null
      },
      "auroc": 0.7121770833333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": null
      },
      "auroc": 0.6318416666666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": null
      },
      "auroc": 0.6318416666666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 336,
          "accuracy": 0.16
        },
        "0.01": null
      },
      "auroc": 0.672009375
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 336,
          "accuracy": 0.16
        },
        "0.01": null
      },
      "auroc": 0.672009375
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.9517677083333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.9517677083333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": null
      },
      "auroc": 0.91728125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": null
      },
      "auroc": 0.91728125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 267,
          "fn": 133,
          "accuracy": 0.6675
        },
        "0.01": null
      },
      "auroc": 0.9345244791666666
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 267,
          "fn": 133,
          "accuracy": 0.6675
        },
        "0.01": null
      },
      "auroc": 0.9345244791666666
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.95394375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.95394375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.7890083333333332
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.7890083333333332
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 242,
          "accuracy": 0.395
        },
        "0.01": null
      },
      "auroc": 0.8714760416666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 242,
          "accuracy": 0.395
        },
        "0.01": null
      },
      "auroc": 0.8714760416666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.8936510416666665
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.8936510416666665
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        },
        "0.01": null
      },
      "auroc": 0.8300239583333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        },
        "0.01": null
      },
      "auroc": 0.8300239583333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        },
        "0.01": null
      },
      "auroc": 0.8618375000000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        },
        "0.01": null
      },
      "auroc": 0.8618375000000001
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1625,
          "fn": 575,
          "accuracy": 0.7386363636363636
        },
        "0.01": null
      },
      "auroc": 0.9303982954545454
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 710,
          "fn": 490,
          "accuracy": 0.5916666666666667
        },
        "0.01": null
      },
      "auroc": 0.9319550347222223
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2335,
          "fn": 1065,
          "accuracy": 0.6867647058823529
        },
        "0.01": null
      },
      "auroc": 0.9309477328431373
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 705,
          "fn": 1495,
          "accuracy": 0.32045454545454544
        },
        "0.01": null
      },
      "auroc": 0.8319466856060607
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 447,
          "fn": 753,
          "accuracy": 0.3725
        },
        "0.01": null
      },
      "auroc": 0.8546494791666668
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1152,
          "fn": 2248,
          "accuracy": 0.3388235294117647
        },
        "0.01": null
      },
      "auroc": 0.83995943627451
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2330,
          "fn": 2070,
          "accuracy": 0.5295454545454545
        },
        "0.01": null
      },
      "auroc": 0.8811724905303031
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1157,
          "fn": 1243,
          "accuracy": 0.4820833333333333
        },
        "0.01": null
      },
      "auroc": 0.8933022569444444
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3487,
          "fn": 3313,
          "accuracy": 0.5127941176470588
        },
        "0.01": null
      },
      "auroc": 0.8854535845588234
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": null
      },
      "auroc": 0.9612114583333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        },
        "0.01": null
      },
      "auroc": 0.9227395833333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        },
        "0.01": null
      },
      "auroc": 0.9419755208333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.9426187500000001
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": null
      },
      "auroc": 0.8842166666666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": null
      },
      "auroc": 0.9134177083333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9519151041666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 250,
          "accuracy": 0.375
        },
        "0.01": null
      },
      "auroc": 0.903478125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 438,
          "fn": 362,
          "accuracy": 0.5475
        },
        "0.01": null
      },
      "auroc": 0.9276966145833334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9846958333333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9699447916666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9773203125000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        },
        "0.01": null
      },
      "auroc": 0.7721791666666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9666479166666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.8694135416666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        },
        "0.01": null
      },
      "auroc": 0.8784375
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": null
      },
      "auroc": 0.9682963541666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 589,
          "fn": 211,
          "accuracy": 0.73625
        },
        "0.01": null
      },
      "auroc": 0.9233669270833333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9566895833333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        },
        "0.01": null
      },
      "auroc": 0.8836927083333335
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.9201911458333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.9241458333333332
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.9062520833333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        },
        "0.01": null
      },
      "auroc": 0.9151989583333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.9404177083333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 224,
          "accuracy": 0.44
        },
        "0.01": null
      },
      "auroc": 0.8949723958333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 446,
          "fn": 354,
          "accuracy": 0.5575
        },
        "0.01": null
      },
      "auroc": 0.9176950520833334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9935624999999999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9858416666666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9897020833333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": null
      },
      "auroc": 0.7558802083333332
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": null
      },
      "auroc": 0.5727729166666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 28,
          "fn": 372,
          "accuracy": 0.07
        },
        "0.01": null
      },
      "auroc": 0.6643265625
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        },
        "0.01": null
      },
      "auroc": 0.8747213541666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.7793072916666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 424,
          "fn": 376,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.8270143229166667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9777260416666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9528364583333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.96528125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": null
      },
      "auroc": 0.7406635416666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        },
        "0.01": null
      },
      "auroc": 0.7480229166666668
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 290,
          "accuracy": 0.275
        },
        "0.01": null
      },
      "auroc": 0.7443432291666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        },
        "0.01": null
      },
      "auroc": 0.8591947916666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        },
        "0.01": null
      },
      "auroc": 0.8504296875
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 472,
          "fn": 328,
          "accuracy": 0.59
        },
        "0.01": null
      },
      "auroc": 0.8548122395833333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9776510416666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": null
      },
      "auroc": 0.9395479166666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9585994791666668
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.9456729166666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": null
      },
      "auroc": 0.8628708333333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        },
        "0.01": null
      },
      "auroc": 0.9042718750000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.9616619791666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 237,
          "accuracy": 0.4075
        },
        "0.01": null
      },
      "auroc": 0.901209375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 489,
          "fn": 311,
          "accuracy": 0.61125
        },
        "0.01": null
      },
      "auroc": 0.9314356770833334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.926428125
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.926428125
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": null
      },
      "auroc": 0.8825135416666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": null
      },
      "auroc": 0.8825135416666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        },
        "0.01": null
      },
      "auroc": 0.9044708333333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        },
        "0.01": null
      },
      "auroc": 0.9044708333333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        },
        "0.01": null
      },
      "auroc": 0.7131854166666668
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        },
        "0.01": null
      },
      "auroc": 0.7131854166666668
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        },
        "0.01": null
      },
      "auroc": 0.620371875
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        },
        "0.01": null
      },
      "auroc": 0.620371875
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 319,
          "accuracy": 0.2025
        },
        "0.01": null
      },
      "auroc": 0.6667786458333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 319,
          "accuracy": 0.2025
        },
        "0.01": null
      },
      "auroc": 0.6667786458333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9655031249999999
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9655031249999999
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.9221145833333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.9221145833333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.9438088541666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.9438088541666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9532125
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9532125
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": null
      },
      "auroc": 0.7662916666666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": null
      },
      "auroc": 0.7662916666666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 238,
          "accuracy": 0.405
        },
        "0.01": null
      },
      "auroc": 0.8597520833333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 238,
          "accuracy": 0.405
        },
        "0.01": null
      },
      "auroc": 0.8597520833333334
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.9063572916666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.9063572916666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        },
        "0.01": null
      },
      "auroc": 0.8312145833333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        },
        "0.01": null
      },
      "auroc": 0.8312145833333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        },
        "0.01": null
      },
      "auroc": 0.8687859375000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        },
        "0.01": null
      },
      "auroc": 0.8687859375000001
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1747,
          "fn": 453,
          "accuracy": 0.7940909090909091
        },
        "0.01": null
      },
      "auroc": 0.937838446969697
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 823,
          "fn": 377,
          "accuracy": 0.6858333333333333
        },
        "0.01": null
      },
      "auroc": 0.9424338541666667
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2570,
          "fn": 830,
          "accuracy": 0.7558823529411764
        },
        "0.01": null
      },
      "auroc": 0.9394603553921568
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 845,
          "fn": 1355,
          "accuracy": 0.3840909090909091
        },
        "0.01": null
      },
      "auroc": 0.8276060606060607
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 474,
          "fn": 726,
          "accuracy": 0.395
        },
        "0.01": null
      },
      "auroc": 0.8234638888888889
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1319,
          "fn": 2081,
          "accuracy": 0.38794117647058823
        },
        "0.01": null
      },
      "auroc": 0.8261441176470589
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2592,
          "fn": 1808,
          "accuracy": 0.5890909090909091
        },
        "0.01": null
      },
      "auroc": 0.8827222537878788
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1297,
          "fn": 1103,
          "accuracy": 0.5404166666666667
        },
        "0.01": null
      },
      "auroc": 0.8829488715277778
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3889,
          "fn": 2911,
          "accuracy": 0.5719117647058823
        },
        "0.01": null
      },
      "auroc": 0.8828022365196078
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": null
      },
      "auroc": 0.9556572916666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": null
      },
      "auroc": 0.9307885416666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": null
      },
      "auroc": 0.9432229166666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.9483572916666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.9153020833333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.9318296875000001
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        },
        "0.01": null
      },
      "auroc": 0.9520072916666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        },
        "0.01": null
      },
      "auroc": 0.9230453125000001
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 518,
          "fn": 282,
          "accuracy": 0.6475
        },
        "0.01": null
      },
      "auroc": 0.9375263020833333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9716020833333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.89346875
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        },
        "0.01": null
      },
      "auroc": 0.9325354166666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        },
        "0.01": null
      },
      "auroc": 0.8634000000000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        },
        "0.01": null
      },
      "auroc": 0.8030791666666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 280,
          "accuracy": 0.3
        },
        "0.01": null
      },
      "auroc": 0.8332395833333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        },
        "0.01": null
      },
      "auroc": 0.9175010416666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 255,
          "accuracy": 0.3625
        },
        "0.01": null
      },
      "auroc": 0.8482739583333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 403,
          "fn": 397,
          "accuracy": 0.50375
        },
        "0.01": null
      },
      "auroc": 0.8828875
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": null
      },
      "auroc": 0.9501614583333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": null
      },
      "auroc": 0.887984375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        },
        "0.01": null
      },
      "auroc": 0.9190729166666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": null
      },
      "auroc": 0.938471875
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        },
        "0.01": null
      },
      "auroc": 0.82159375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        },
        "0.01": null
      },
      "auroc": 0.8800328124999999
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        },
        "0.01": null
      },
      "auroc": 0.9443166666666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 282,
          "accuracy": 0.295
        },
        "0.01": null
      },
      "auroc": 0.8547890625
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 401,
          "accuracy": 0.49875
        },
        "0.01": null
      },
      "auroc": 0.8995528645833334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9604385416666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9829291666666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": null
      },
      "auroc": 0.9716838541666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        },
        "0.01": null
      },
      "auroc": 0.8854354166666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        },
        "0.01": null
      },
      "auroc": 0.8131239583333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 296,
          "accuracy": 0.26
        },
        "0.01": null
      },
      "auroc": 0.8492796874999999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        },
        "0.01": null
      },
      "auroc": 0.9229369791666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        },
        "0.01": null
      },
      "auroc": 0.8980265625
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 461,
          "fn": 339,
          "accuracy": 0.57625
        },
        "0.01": null
      },
      "auroc": 0.9104817708333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9696822916666668
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": null
      },
      "auroc": 0.9124166666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 301,
          "fn": 99,
          "accuracy": 0.7525
        },
        "0.01": null
      },
      "auroc": 0.9410494791666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        },
        "0.01": null
      },
      "auroc": 0.8824583333333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": null
      },
      "auroc": 0.7643291666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 285,
          "accuracy": 0.2875
        },
        "0.01": null
      },
      "auroc": 0.82339375
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        },
        "0.01": null
      },
      "auroc": 0.9260703124999999
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 247,
          "accuracy": 0.3825
        },
        "0.01": null
      },
      "auroc": 0.8383729166666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 416,
          "fn": 384,
          "accuracy": 0.52
        },
        "0.01": null
      },
      "auroc": 0.8822216145833333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9697010416666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": null
      },
      "auroc": 0.9529927083333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.961346875
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9502395833333332
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        },
        "0.01": null
      },
      "auroc": 0.9185020833333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        },
        "0.01": null
      },
      "auroc": 0.9343708333333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        },
        "0.01": null
      },
      "auroc": 0.9599703125
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        },
        "0.01": null
      },
      "auroc": 0.9357473958333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 551,
          "fn": 249,
          "accuracy": 0.68875
        },
        "0.01": null
      },
      "auroc": 0.9478588541666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.9435937499999999
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.9435937499999999
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.9133895833333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.9133895833333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9284916666666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9284916666666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        },
        "0.01": null
      },
      "auroc": 0.8241729166666668
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        },
        "0.01": null
      },
      "auroc": 0.8241729166666668
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": null
      },
      "auroc": 0.7558604166666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": null
      },
      "auroc": 0.7558604166666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 259,
          "accuracy": 0.3525
        },
        "0.01": null
      },
      "auroc": 0.7900166666666668
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 259,
          "accuracy": 0.3525
        },
        "0.01": null
      },
      "auroc": 0.7900166666666668
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9616635416666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9616635416666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": null
      },
      "auroc": 0.936121875
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": null
      },
      "auroc": 0.936121875
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 301,
          "fn": 99,
          "accuracy": 0.7525
        },
        "0.01": null
      },
      "auroc": 0.9488927083333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 301,
          "fn": 99,
          "accuracy": 0.7525
        },
        "0.01": null
      },
      "auroc": 0.9488927083333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.942596875
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.942596875
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        },
        "0.01": null
      },
      "auroc": 0.8315916666666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        },
        "0.01": null
      },
      "auroc": 0.8315916666666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        },
        "0.01": null
      },
      "auroc": 0.8870942708333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        },
        "0.01": null
      },
      "auroc": 0.8870942708333334
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.9234958333333334
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.9234958333333334
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.86404375
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.86404375
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        },
        "0.01": null
      },
      "auroc": 0.8937697916666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        },
        "0.01": null
      },
      "auroc": 0.8937697916666667
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1682,
          "fn": 518,
          "accuracy": 0.7645454545454545
        },
        "0.01": null
      },
      "auroc": 0.942978693181818
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 775,
          "fn": 425,
          "accuracy": 0.6458333333333334
        },
        "0.01": null
      },
      "auroc": 0.9267633680555556
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2457,
          "fn": 943,
          "accuracy": 0.7226470588235294
        },
        "0.01": null
      },
      "auroc": 0.9372556372549019
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1113,
          "fn": 1087,
          "accuracy": 0.5059090909090909
        },
        "0.01": null
      },
      "auroc": 0.8881245265151514
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 320,
          "fn": 880,
          "accuracy": 0.26666666666666666
        },
        "0.01": null
      },
      "auroc": 0.839321701388889
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1433,
          "fn": 1967,
          "accuracy": 0.4214705882352941
        },
        "0.01": null
      },
      "auroc": 0.8709
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2795,
          "fn": 1605,
          "accuracy": 0.6352272727272728
        },
        "0.01": null
      },
      "auroc": 0.9155516098484848
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1095,
          "fn": 1305,
          "accuracy": 0.45625
        },
        "0.01": null
      },
      "auroc": 0.8830425347222222
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3890,
          "fn": 2910,
          "accuracy": 0.5720588235294117
        },
        "0.01": null
      },
      "auroc": 0.9040778186274511
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9711770833333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.94510625
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": null
      },
      "auroc": 0.9581416666666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.961028125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        },
        "0.01": null
      },
      "auroc": 0.9130104166666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": null
      },
      "auroc": 0.9370192708333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9661026041666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        },
        "0.01": null
      },
      "auroc": 0.9290583333333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 547,
          "fn": 253,
          "accuracy": 0.68375
        },
        "0.01": null
      },
      "auroc": 0.94758046875
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9851614583333335
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9765208333333335
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9808411458333335
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        },
        "0.01": null
      },
      "auroc": 0.82960625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9739697916666668
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": null
      },
      "auroc": 0.9017880208333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.9073838541666668
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9752453125
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 618,
          "fn": 182,
          "accuracy": 0.7725
        },
        "0.01": null
      },
      "auroc": 0.9413145833333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9703083333333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": null
      },
      "auroc": 0.9089947916666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 271,
          "fn": 129,
          "accuracy": 0.6775
        },
        "0.01": null
      },
      "auroc": 0.9396515624999999
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.9401739583333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.9172239583333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": null
      },
      "auroc": 0.9286989583333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": null
      },
      "auroc": 0.9552411458333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": null
      },
      "auroc": 0.9131093749999999
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 520,
          "fn": 280,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.9341752604166667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9935624999999999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9913520833333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9924572916666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": null
      },
      "auroc": 0.8564197916666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": null
      },
      "auroc": 0.7513802083333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 336,
          "accuracy": 0.16
        },
        "0.01": null
      },
      "auroc": 0.8039000000000001
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        },
        "0.01": null
      },
      "auroc": 0.9249911458333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        },
        "0.01": null
      },
      "auroc": 0.8713661458333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 463,
          "fn": 337,
          "accuracy": 0.57875
        },
        "0.01": null
      },
      "auroc": 0.8981786458333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9790697916666669
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9628177083333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.97094375
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        },
        "0.01": null
      },
      "auroc": 0.8084927083333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        },
        "0.01": null
      },
      "auroc": 0.8020197916666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 259,
          "accuracy": 0.3525
        },
        "0.01": null
      },
      "auroc": 0.80525625
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        },
        "0.01": null
      },
      "auroc": 0.89378125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": null
      },
      "auroc": 0.88241875
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 517,
          "fn": 283,
          "accuracy": 0.64625
        },
        "0.01": null
      },
      "auroc": 0.8880999999999999
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9843541666666668
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": null
      },
      "auroc": 0.9572083333333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.97078125
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9616625
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        },
        "0.01": null
      },
      "auroc": 0.9008739583333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        },
        "0.01": null
      },
      "auroc": 0.9312682291666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9730083333333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        },
        "0.01": null
      },
      "auroc": 0.9290411458333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 576,
          "fn": 224,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9510247395833333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.9317645833333335
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.9317645833333335
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": null
      },
      "auroc": 0.8965072916666665
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": null
      },
      "auroc": 0.8965072916666665
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 289,
          "fn": 111,
          "accuracy": 0.7225
        },
        "0.01": null
      },
      "auroc": 0.9141359375
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 289,
          "fn": 111,
          "accuracy": 0.7225
        },
        "0.01": null
      },
      "auroc": 0.9141359375
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": null
      },
      "auroc": 0.7491708333333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": null
      },
      "auroc": 0.7491708333333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": null
      },
      "auroc": 0.6595843749999999
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": null
      },
      "auroc": 0.6595843749999999
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 303,
          "accuracy": 0.2425
        },
        "0.01": null
      },
      "auroc": 0.7043776041666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 303,
          "accuracy": 0.2425
        },
        "0.01": null
      },
      "auroc": 0.7043776041666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.973553125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.973553125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9431270833333332
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9431270833333332
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9583401041666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9583401041666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.970996875
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.970996875
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": null
      },
      "auroc": 0.8182083333333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": null
      },
      "auroc": 0.8182083333333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": null
      },
      "auroc": 0.8946026041666666
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": null
      },
      "auroc": 0.8946026041666666
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.9215156250000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.9215156250000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.8575572916666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.8575572916666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": null
      },
      "auroc": 0.8895364583333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": null
      },
      "auroc": 0.8895364583333333
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1878,
          "fn": 322,
          "accuracy": 0.8536363636363636
        },
        "0.01": null
      },
      "auroc": 0.9482394886363636
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 925,
          "fn": 275,
          "accuracy": 0.7708333333333334
        },
        "0.01": null
      },
      "auroc": 0.9570000000000001
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2803,
          "fn": 597,
          "accuracy": 0.8244117647058824
        },
        "0.01": null
      },
      "auroc": 0.9513314338235295
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1050,
          "fn": 1150,
          "accuracy": 0.4772727272727273
        },
        "0.01": null
      },
      "auroc": 0.8665788825757574
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 557,
          "fn": 643,
          "accuracy": 0.46416666666666667
        },
        "0.01": null
      },
      "auroc": 0.8764130208333333
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1607,
          "fn": 1793,
          "accuracy": 0.4726470588235294
        },
        "0.01": null
      },
      "auroc": 0.8700497549019608
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2928,
          "fn": 1472,
          "accuracy": 0.6654545454545454
        },
        "0.01": null
      },
      "auroc": 0.9074091856060607
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1482,
          "fn": 918,
          "accuracy": 0.6175
        },
        "0.01": null
      },
      "auroc": 0.9167065104166667
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 4410,
          "fn": 2390,
          "accuracy": 0.6485294117647059
        },
        "0.01": null
      },
      "auroc": 0.9106905943627451
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9711947916666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.94510625
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": null
      },
      "auroc": 0.9581505208333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.961028125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        },
        "0.01": null
      },
      "auroc": 0.9130104166666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": null
      },
      "auroc": 0.9370192708333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9661114583333332
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        },
        "0.01": null
      },
      "auroc": 0.9290583333333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 547,
          "fn": 253,
          "accuracy": 0.68375
        },
        "0.01": null
      },
      "auroc": 0.9475848958333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9851614583333335
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9762854166666668
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9807234375000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        },
        "0.01": null
      },
      "auroc": 0.8311333333333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.973865625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": null
      },
      "auroc": 0.9024994791666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.9081473958333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9750755208333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 618,
          "fn": 182,
          "accuracy": 0.7725
        },
        "0.01": null
      },
      "auroc": 0.9416114583333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9703083333333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": null
      },
      "auroc": 0.9089614583333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.9396348958333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.9401770833333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.9171406249999999
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": null
      },
      "auroc": 0.9286588541666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": null
      },
      "auroc": 0.9552427083333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.9130510416666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 519,
          "fn": 281,
          "accuracy": 0.64875
        },
        "0.01": null
      },
      "auroc": 0.9341468749999999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9935624999999999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9914333333333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9924979166666665
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": null
      },
      "auroc": 0.857053125
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": null
      },
      "auroc": 0.7518583333333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 336,
          "accuracy": 0.16
        },
        "0.01": null
      },
      "auroc": 0.8044557291666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        },
        "0.01": null
      },
      "auroc": 0.9253078125
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        },
        "0.01": null
      },
      "auroc": 0.8716458333333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 463,
          "fn": 337,
          "accuracy": 0.57875
        },
        "0.01": null
      },
      "auroc": 0.8984768229166666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9791333333333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9628916666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9710125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        },
        "0.01": null
      },
      "auroc": 0.8094541666666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        },
        "0.01": null
      },
      "auroc": 0.8019208333333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 259,
          "accuracy": 0.3525
        },
        "0.01": null
      },
      "auroc": 0.8056875
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        },
        "0.01": null
      },
      "auroc": 0.8942937500000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": null
      },
      "auroc": 0.8824062500000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 517,
          "fn": 283,
          "accuracy": 0.64625
        },
        "0.01": null
      },
      "auroc": 0.88835
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9843625
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": null
      },
      "auroc": 0.95723125
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.970796875
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9616812499999999
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        },
        "0.01": null
      },
      "auroc": 0.9008552083333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        },
        "0.01": null
      },
      "auroc": 0.9312682291666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9730218749999999
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        },
        "0.01": null
      },
      "auroc": 0.9290432291666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 576,
          "fn": 224,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9510325520833333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.9318635416666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.9318635416666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": null
      },
      "auroc": 0.8964093749999998
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": null
      },
      "auroc": 0.8964093749999998
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 289,
          "fn": 111,
          "accuracy": 0.7225
        },
        "0.01": null
      },
      "auroc": 0.9141364583333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 289,
          "fn": 111,
          "accuracy": 0.7225
        },
        "0.01": null
      },
      "auroc": 0.9141364583333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": null
      },
      "auroc": 0.7492114583333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": null
      },
      "auroc": 0.7492114583333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": null
      },
      "auroc": 0.6593677083333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": null
      },
      "auroc": 0.6593677083333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 303,
          "accuracy": 0.2425
        },
        "0.01": null
      },
      "auroc": 0.7042895833333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 303,
          "accuracy": 0.2425
        },
        "0.01": null
      },
      "auroc": 0.7042895833333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.973553125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.973553125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9431270833333332
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9431270833333332
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9583401041666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9583401041666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9710395833333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9710395833333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": null
      },
      "auroc": 0.8182302083333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": null
      },
      "auroc": 0.8182302083333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": null
      },
      "auroc": 0.8946348958333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": null
      },
      "auroc": 0.8946348958333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.9215156250000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.9215156250000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.857478125
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.857478125
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": null
      },
      "auroc": 0.8894968750000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": null
      },
      "auroc": 0.8894968750000001
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1878,
          "fn": 322,
          "accuracy": 0.8536363636363636
        },
        "0.01": null
      },
      "auroc": 0.9482642045454546
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 924,
          "fn": 276,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9569848958333333
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2802,
          "fn": 598,
          "accuracy": 0.8241176470588235
        },
        "0.01": null
      },
      "auroc": 0.9513420955882352
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1050,
          "fn": 1150,
          "accuracy": 0.4772727272727273
        },
        "0.01": null
      },
      "auroc": 0.8668308712121213
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 557,
          "fn": 643,
          "accuracy": 0.46416666666666667
        },
        "0.01": null
      },
      "auroc": 0.8764418402777778
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1607,
          "fn": 1793,
          "accuracy": 0.4726470588235294
        },
        "0.01": null
      },
      "auroc": 0.8702229779411764
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2928,
          "fn": 1472,
          "accuracy": 0.6654545454545454
        },
        "0.01": null
      },
      "auroc": 0.9075475378787878
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1481,
          "fn": 919,
          "accuracy": 0.6170833333333333
        },
        "0.01": null
      },
      "auroc": 0.9167133680555556
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 4409,
          "fn": 2391,
          "accuracy": 0.6483823529411765
        },
        "0.01": null
      },
      "auroc": 0.910782536764706
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.4010239583333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.35358125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.37730260416666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.37205937499999997
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.33837708333333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.3552182291666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.38654166666666673
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.34597916666666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.36626041666666664
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9529937500000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": null
      },
      "auroc": 0.6594177083333332
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        },
        "0.01": null
      },
      "auroc": 0.8062057291666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        },
        "0.01": null
      },
      "auroc": 0.5594718750000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": null
      },
      "auroc": 0.5061510416666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        },
        "0.01": null
      },
      "auroc": 0.5328114583333332
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 214,
          "accuracy": 0.465
        },
        "0.01": null
      },
      "auroc": 0.7562328125
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 384,
          "accuracy": 0.04
        },
        "0.01": null
      },
      "auroc": 0.5827843749999999
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 598,
          "accuracy": 0.2525
        },
        "0.01": null
      },
      "auroc": 0.66950859375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.41979479166666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.26716979166666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.3434822916666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.32630208333333327
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.2545114583333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.2904067708333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.37304843750000005
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.260840625
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 798,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.31694453124999994
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9885979166666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": null
      },
      "auroc": 0.9162458333333332
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": null
      },
      "auroc": 0.952421875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": null
      },
      "auroc": 0.5859552083333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.44637604166666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 387,
          "accuracy": 0.0325
        },
        "0.01": null
      },
      "auroc": 0.516165625
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        },
        "0.01": null
      },
      "auroc": 0.7872765625
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 274,
          "accuracy": 0.315
        },
        "0.01": null
      },
      "auroc": 0.6813109374999999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 334,
          "fn": 466,
          "accuracy": 0.4175
        },
        "0.01": null
      },
      "auroc": 0.73429375
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9430864583333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.890425
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        },
        "0.01": null
      },
      "auroc": 0.9167557291666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        },
        "0.01": null
      },
      "auroc": 0.552090625
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        },
        "0.01": null
      },
      "auroc": 0.5014833333333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 354,
          "accuracy": 0.115
        },
        "0.01": null
      },
      "auroc": 0.5267869791666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        },
        "0.01": null
      },
      "auroc": 0.7475885416666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        },
        "0.01": null
      },
      "auroc": 0.6959541666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 437,
          "accuracy": 0.45375
        },
        "0.01": null
      },
      "auroc": 0.7217713541666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": null
      },
      "auroc": 0.45095937499999994
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.36082708333333335
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 393,
          "accuracy": 0.0175
        },
        "0.01": null
      },
      "auroc": 0.4058932291666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.36290520833333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.29016250000000005
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.32653385416666664
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.40693229166666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.32549479166666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 792,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.36621354166666664
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        },
        "0.01": null
      },
      "auroc": 0.7681041666666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        },
        "0.01": null
      },
      "auroc": 0.7681041666666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        },
        "0.01": null
      },
      "auroc": 0.6809677083333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        },
        "0.01": null
      },
      "auroc": 0.6809677083333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 315,
          "accuracy": 0.2125
        },
        "0.01": null
      },
      "auroc": 0.7245359375
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 315,
          "accuracy": 0.2125
        },
        "0.01": null
      },
      "auroc": 0.7245359375
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.41193020833333327
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.41193020833333327
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.366659375
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.366659375
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": null
      },
      "auroc": 0.3892947916666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": null
      },
      "auroc": 0.3892947916666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.21508125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.21508125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.16668541666666664
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.16668541666666664
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.19088333333333332
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.19088333333333332
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.17761458333333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.17761458333333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.07393541666666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.07393541666666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.12577500000000003
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.12577500000000003
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.46080625
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.46080625
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.3769291666666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.3769291666666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.4188677083333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.4188677083333333
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 607,
          "fn": 1593,
          "accuracy": 0.2759090909090909
        },
        "0.01": null
      },
      "auroc": 0.5627266098484848
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 278,
          "fn": 922,
          "accuracy": 0.23166666666666666
        },
        "0.01": null
      },
      "auroc": 0.5746111111111111
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 885,
          "fn": 2515,
          "accuracy": 0.26029411764705884
        },
        "0.01": null
      },
      "auroc": 0.5669211397058824
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 77,
          "fn": 2123,
          "accuracy": 0.035
        },
        "0.01": null
      },
      "auroc": 0.40217831439393936
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 39,
          "fn": 1161,
          "accuracy": 0.0325
        },
        "0.01": null
      },
      "auroc": 0.3895102430555556
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 3284,
          "accuracy": 0.03411764705882353
        },
        "0.01": null
      },
      "auroc": 0.3977072303921569
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 684,
          "fn": 3716,
          "accuracy": 0.15545454545454546
        },
        "0.01": null
      },
      "auroc": 0.4824524621212121
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 317,
          "fn": 2083,
          "accuracy": 0.13208333333333333
        },
        "0.01": null
      },
      "auroc": 0.4820606770833334
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1001,
          "fn": 5799,
          "accuracy": 0.1472058823529412
        },
        "0.01": null
      },
      "auroc": 0.48231418504901963
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.95484375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        },
        "0.01": null
      },
      "auroc": 0.925296875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        },
        "0.01": null
      },
      "auroc": 0.9400703125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": null
      },
      "auroc": 0.9434947916666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        },
        "0.01": null
      },
      "auroc": 0.8985104166666668
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        },
        "0.01": null
      },
      "auroc": 0.9210026041666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.9491692708333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 259,
          "accuracy": 0.3525
        },
        "0.01": null
      },
      "auroc": 0.9119036458333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 425,
          "fn": 375,
          "accuracy": 0.53125
        },
        "0.01": null
      },
      "auroc": 0.9305364583333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.98375625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.976540625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9801484375
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        },
        "0.01": null
      },
      "auroc": 0.7959343750000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9738552083333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        },
        "0.01": null
      },
      "auroc": 0.8848947916666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        },
        "0.01": null
      },
      "auroc": 0.8898453124999999
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9751979166666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 587,
          "fn": 213,
          "accuracy": 0.73375
        },
        "0.01": null
      },
      "auroc": 0.9325216145833333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": null
      },
      "auroc": 0.9514666666666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        },
        "0.01": null
      },
      "auroc": 0.9086697916666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        },
        "0.01": null
      },
      "auroc": 0.9300682291666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        },
        "0.01": null
      },
      "auroc": 0.91598125
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": null
      },
      "auroc": 0.9186322916666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        },
        "0.01": null
      },
      "auroc": 0.9173067708333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": null
      },
      "auroc": 0.9337239583333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        },
        "0.01": null
      },
      "auroc": 0.9136510416666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 441,
          "fn": 359,
          "accuracy": 0.55125
        },
        "0.01": null
      },
      "auroc": 0.9236875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9935791666666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.98875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9911645833333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        },
        "0.01": null
      },
      "auroc": 0.8273572916666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        },
        "0.01": null
      },
      "auroc": 0.7522354166666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 355,
          "accuracy": 0.1125
        },
        "0.01": null
      },
      "auroc": 0.7897963541666668
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        },
        "0.01": null
      },
      "auroc": 0.9104682291666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        },
        "0.01": null
      },
      "auroc": 0.8704927083333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 442,
          "fn": 358,
          "accuracy": 0.5525
        },
        "0.01": null
      },
      "auroc": 0.8904804687499999
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9765062499999999
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.957634375
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9670703125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        },
        "0.01": null
      },
      "auroc": 0.7777052083333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.811378125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 268,
          "accuracy": 0.33
        },
        "0.01": null
      },
      "auroc": 0.7945416666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        },
        "0.01": null
      },
      "auroc": 0.8771057291666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": null
      },
      "auroc": 0.8845062499999999
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 500,
          "fn": 300,
          "accuracy": 0.625
        },
        "0.01": null
      },
      "auroc": 0.8808059895833333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9702947916666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": null
      },
      "auroc": 0.9428333333333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.9565640625
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": null
      },
      "auroc": 0.9429124999999999
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": null
      },
      "auroc": 0.8864791666666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 234,
          "accuracy": 0.415
        },
        "0.01": null
      },
      "auroc": 0.9146958333333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        },
        "0.01": null
      },
      "auroc": 0.9566036458333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 241,
          "accuracy": 0.3975
        },
        "0.01": null
      },
      "auroc": 0.9146562500000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 458,
          "fn": 342,
          "accuracy": 0.5725
        },
        "0.01": null
      },
      "auroc": 0.9356299479166666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": null
      },
      "auroc": 0.9171354166666665
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": null
      },
      "auroc": 0.9171354166666665
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": null
      },
      "auroc": 0.8767458333333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": null
      },
      "auroc": 0.8767458333333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        },
        "0.01": null
      },
      "auroc": 0.896940625
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        },
        "0.01": null
      },
      "auroc": 0.896940625
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        },
        "0.01": null
      },
      "auroc": 0.7120520833333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        },
        "0.01": null
      },
      "auroc": 0.7120520833333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        },
        "0.01": null
      },
      "auroc": 0.6268395833333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        },
        "0.01": null
      },
      "auroc": 0.6268395833333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 329,
          "accuracy": 0.1775
        },
        "0.01": null
      },
      "auroc": 0.6694458333333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 329,
          "accuracy": 0.1775
        },
        "0.01": null
      },
      "auroc": 0.6694458333333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9596666666666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9596666666666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": null
      },
      "auroc": 0.9257041666666668
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": null
      },
      "auroc": 0.9257041666666668
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        },
        "0.01": null
      },
      "auroc": 0.9426854166666666
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        },
        "0.01": null
      },
      "auroc": 0.9426854166666666
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": null
      },
      "auroc": 0.9358270833333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": null
      },
      "auroc": 0.9358270833333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.7831395833333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.7831395833333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        },
        "0.01": null
      },
      "auroc": 0.8594833333333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        },
        "0.01": null
      },
      "auroc": 0.8594833333333334
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": null
      },
      "auroc": 0.9007364583333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": null
      },
      "auroc": 0.9007364583333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        },
        "0.01": null
      },
      "auroc": 0.83275625
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        },
        "0.01": null
      },
      "auroc": 0.83275625
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        },
        "0.01": null
      },
      "auroc": 0.8667463541666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        },
        "0.01": null
      },
      "auroc": 0.8667463541666667
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1619,
          "fn": 581,
          "accuracy": 0.735909090909091
        },
        "0.01": null
      },
      "auroc": 0.9323513257575757
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 840,
          "fn": 360,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.9499541666666667
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2459,
          "fn": 941,
          "accuracy": 0.7232352941176471
        },
        "0.01": null
      },
      "auroc": 0.9385640931372549
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 793,
          "fn": 1407,
          "accuracy": 0.36045454545454547
        },
        "0.01": null
      },
      "auroc": 0.8407791666666667
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 519,
          "fn": 681,
          "accuracy": 0.4325
        },
        "0.01": null
      },
      "auroc": 0.8735151041666666
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1312,
          "fn": 2088,
          "accuracy": 0.38588235294117645
        },
        "0.01": null
      },
      "auroc": 0.8523330269607845
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2412,
          "fn": 1988,
          "accuracy": 0.5481818181818182
        },
        "0.01": null
      },
      "auroc": 0.8865652462121212
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1359,
          "fn": 1041,
          "accuracy": 0.56625
        },
        "0.01": null
      },
      "auroc": 0.9117346354166668
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3771,
          "fn": 3029,
          "accuracy": 0.5545588235294118
        },
        "0.01": null
      },
      "auroc": 0.8954485600490196
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.960053125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        },
        "0.01": null
      },
      "auroc": 0.9259614583333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        },
        "0.01": null
      },
      "auroc": 0.9430072916666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.9480802083333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        },
        "0.01": null
      },
      "auroc": 0.8969572916666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": null
      },
      "auroc": 0.92251875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9540666666666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 240,
          "accuracy": 0.4
        },
        "0.01": null
      },
      "auroc": 0.911459375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 448,
          "fn": 352,
          "accuracy": 0.56
        },
        "0.01": null
      },
      "auroc": 0.9327630208333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9846583333333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9740302083333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9793442708333335
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        },
        "0.01": null
      },
      "auroc": 0.8119031250000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9722270833333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        },
        "0.01": null
      },
      "auroc": 0.8920651041666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": null
      },
      "auroc": 0.8982807291666668
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": null
      },
      "auroc": 0.9731286458333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 609,
          "fn": 191,
          "accuracy": 0.76125
        },
        "0.01": null
      },
      "auroc": 0.9357046875000001
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9630531250000001
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        },
        "0.01": null
      },
      "auroc": 0.896003125
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.929528125
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": null
      },
      "auroc": 0.928134375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": null
      },
      "auroc": 0.9082291666666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        },
        "0.01": null
      },
      "auroc": 0.9181817708333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.94559375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        },
        "0.01": null
      },
      "auroc": 0.9021161458333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 472,
          "fn": 328,
          "accuracy": 0.59
        },
        "0.01": null
      },
      "auroc": 0.9238549479166667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9935624999999999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9892770833333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9914197916666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        },
        "0.01": null
      },
      "auroc": 0.846103125
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        },
        "0.01": null
      },
      "auroc": 0.6852666666666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 345,
          "accuracy": 0.1375
        },
        "0.01": null
      },
      "auroc": 0.7656848958333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        },
        "0.01": null
      },
      "auroc": 0.9198328124999999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        },
        "0.01": null
      },
      "auroc": 0.837271875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 452,
          "fn": 348,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.87855234375
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9774229166666668
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9569947916666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9672088541666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        },
        "0.01": null
      },
      "auroc": 0.7915208333333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        },
        "0.01": null
      },
      "auroc": 0.7773656250000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 270,
          "accuracy": 0.325
        },
        "0.01": null
      },
      "auroc": 0.7844432291666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": null
      },
      "auroc": 0.884471875
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        },
        "0.01": null
      },
      "auroc": 0.8671802083333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 497,
          "fn": 303,
          "accuracy": 0.62125
        },
        "0.01": null
      },
      "auroc": 0.8758260416666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9806958333333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": null
      },
      "auroc": 0.9486927083333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        },
        "0.01": null
      },
      "auroc": 0.9646942708333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": null
      },
      "auroc": 0.9538749999999999
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        },
        "0.01": null
      },
      "auroc": 0.8853583333333332
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        },
        "0.01": null
      },
      "auroc": 0.9196166666666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9672854166666668
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        },
        "0.01": null
      },
      "auroc": 0.9170255208333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 532,
          "fn": 268,
          "accuracy": 0.665
        },
        "0.01": null
      },
      "auroc": 0.94215546875
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.929453125
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.929453125
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.8928145833333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.8928145833333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        },
        "0.01": null
      },
      "auroc": 0.9111338541666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        },
        "0.01": null
      },
      "auroc": 0.9111338541666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        },
        "0.01": null
      },
      "auroc": 0.7323864583333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        },
        "0.01": null
      },
      "auroc": 0.7323864583333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        },
        "0.01": null
      },
      "auroc": 0.6428541666666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        },
        "0.01": null
      },
      "auroc": 0.6428541666666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 308,
          "accuracy": 0.23
        },
        "0.01": null
      },
      "auroc": 0.6876203125
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 308,
          "accuracy": 0.23
        },
        "0.01": null
      },
      "auroc": 0.6876203125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9660114583333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9660114583333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.9265958333333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.9265958333333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        },
        "0.01": null
      },
      "auroc": 0.9463036458333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        },
        "0.01": null
      },
      "auroc": 0.9463036458333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": null
      },
      "auroc": 0.9500083333333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": null
      },
      "auroc": 0.9500083333333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.7662739583333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.7662739583333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        },
        "0.01": null
      },
      "auroc": 0.8581411458333335
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        },
        "0.01": null
      },
      "auroc": 0.8581411458333335
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.9152958333333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.9152958333333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        },
        "0.01": null
      },
      "auroc": 0.843446875
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        },
        "0.01": null
      },
      "auroc": 0.843446875
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        },
        "0.01": null
      },
      "auroc": 0.8793713541666666
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        },
        "0.01": null
      },
      "auroc": 0.8793713541666666
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1767,
          "fn": 433,
          "accuracy": 0.8031818181818182
        },
        "0.01": null
      },
      "auroc": 0.9411455492424242
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 854,
          "fn": 346,
          "accuracy": 0.7116666666666667
        },
        "0.01": null
      },
      "auroc": 0.9484932291666667
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2621,
          "fn": 779,
          "accuracy": 0.7708823529411765
        },
        "0.01": null
      },
      "auroc": 0.9437388480392157
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 926,
          "fn": 1274,
          "accuracy": 0.4209090909090909
        },
        "0.01": null
      },
      "auroc": 0.8501456439393938
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 518,
          "fn": 682,
          "accuracy": 0.43166666666666664
        },
        "0.01": null
      },
      "auroc": 0.8542340277777778
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1444,
          "fn": 1956,
          "accuracy": 0.42470588235294116
        },
        "0.01": null
      },
      "auroc": 0.8515886029411766
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2693,
          "fn": 1707,
          "accuracy": 0.6120454545454546
        },
        "0.01": null
      },
      "auroc": 0.8956455965909089
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1372,
          "fn": 1028,
          "accuracy": 0.5716666666666667
        },
        "0.01": null
      },
      "auroc": 0.9013636284722222
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 4065,
          "fn": 2735,
          "accuracy": 0.5977941176470588
        },
        "0.01": null
      },
      "auroc": 0.8976637254901961
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.40290208333333327
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.40889791666666664
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.4059
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.38739895833333327
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.40331666666666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.3953578125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.39515052083333335
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.40610729166666676
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.40062890624999997
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": null
      },
      "auroc": 0.9374447916666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.6324041666666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 240,
          "accuracy": 0.4
        },
        "0.01": null
      },
      "auroc": 0.7849244791666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        },
        "0.01": null
      },
      "auroc": 0.5739770833333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.5656927083333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 386,
          "accuracy": 0.035
        },
        "0.01": null
      },
      "auroc": 0.5698348958333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        },
        "0.01": null
      },
      "auroc": 0.7557109375000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.5990484374999999
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 626,
          "accuracy": 0.2175
        },
        "0.01": null
      },
      "auroc": 0.6773796875
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.3977145833333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.39606875
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.3968916666666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.37494166666666673
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.39786041666666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.3864010416666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.3863281249999999
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.39696458333333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.39164635416666665
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9760677083333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": null
      },
      "auroc": 0.7210854166666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        },
        "0.01": null
      },
      "auroc": 0.8485765625000001
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        },
        "0.01": null
      },
      "auroc": 0.582253125
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.4654572916666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.5238552083333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.7791604166666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 373,
          "accuracy": 0.0675
        },
        "0.01": null
      },
      "auroc": 0.5932713541666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 573,
          "accuracy": 0.28375
        },
        "0.01": null
      },
      "auroc": 0.6862158854166667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": null
      },
      "auroc": 0.9446302083333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": null
      },
      "auroc": 0.8470041666666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": null
      },
      "auroc": 0.8958171875000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        },
        "0.01": null
      },
      "auroc": 0.5908447916666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        },
        "0.01": null
      },
      "auroc": 0.5643291666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 44,
          "fn": 356,
          "accuracy": 0.11
        },
        "0.01": null
      },
      "auroc": 0.5775869791666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        },
        "0.01": null
      },
      "auroc": 0.7677375
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 228,
          "accuracy": 0.43
        },
        "0.01": null
      },
      "auroc": 0.7056666666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 443,
          "accuracy": 0.44625
        },
        "0.01": null
      },
      "auroc": 0.7367020833333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.39857395833333326
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.3951645833333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.3968692708333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.3853864583333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.3889114583333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.3871489583333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.39198020833333336
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.3920380208333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 796,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.39200911458333326
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        },
        "0.01": null
      },
      "auroc": 0.7659729166666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        },
        "0.01": null
      },
      "auroc": 0.7659729166666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        },
        "0.01": null
      },
      "auroc": 0.6865229166666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        },
        "0.01": null
      },
      "auroc": 0.6865229166666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 316,
          "accuracy": 0.21
        },
        "0.01": null
      },
      "auroc": 0.7262479166666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 316,
          "accuracy": 0.21
        },
        "0.01": null
      },
      "auroc": 0.7262479166666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.4989843749999999
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.4989843749999999
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.46332604166666663
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.46332604166666663
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.48115520833333336
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.48115520833333336
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.25127604166666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.25127604166666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.2549125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.2549125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.25309427083333336
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.25309427083333336
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.21997395833333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.21997395833333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.2028375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.2028375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.21140572916666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.21140572916666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.474559375
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.474559375
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.45360729166666675
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.45360729166666675
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.46408333333333335
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.46408333333333335
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 579,
          "fn": 1621,
          "accuracy": 0.2631818181818182
        },
        "0.01": null
      },
      "auroc": 0.5698272727272727
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 1030,
          "accuracy": 0.14166666666666666
        },
        "0.01": null
      },
      "auroc": 0.5667708333333333
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 749,
          "fn": 2651,
          "accuracy": 0.22029411764705883
        },
        "0.01": null
      },
      "auroc": 0.5687485294117647
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 72,
          "fn": 2128,
          "accuracy": 0.03272727272727273
        },
        "0.01": null
      },
      "auroc": 0.45054621212121204
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 1167,
          "accuracy": 0.0275
        },
        "0.01": null
      },
      "auroc": 0.4642612847222222
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 3295,
          "accuracy": 0.030882352941176472
        },
        "0.01": null
      },
      "auroc": 0.4553868259803921
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 651,
          "fn": 3749,
          "accuracy": 0.14795454545454545
        },
        "0.01": null
      },
      "auroc": 0.5101867424242424
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 203,
          "fn": 2197,
          "accuracy": 0.08458333333333333
        },
        "0.01": null
      },
      "auroc": 0.5155160590277778
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 854,
          "fn": 5946,
          "accuracy": 0.12558823529411764
        },
        "0.01": null
      },
      "auroc": 0.5120676776960784
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1644,
          "fn": 756,
          "accuracy": 0.685
        },
        "0.01": null
      },
      "auroc": 0.8694644097222223
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1002,
          "fn": 1398,
          "accuracy": 0.4175
        },
        "0.01": null
      },
      "auroc": 0.8404855034722222
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2646,
          "fn": 2154,
          "accuracy": 0.55125
        },
        "0.01": null
      },
      "auroc": 0.8549749565972222
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1421,
          "fn": 979,
          "accuracy": 0.5920833333333333
        },
        "0.01": null
      },
      "auroc": 0.8562267361111111
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 739,
          "fn": 1661,
          "accuracy": 0.30791666666666667
        },
        "0.01": null
      },
      "auroc": 0.8133145833333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2160,
          "fn": 2640,
          "accuracy": 0.45
        },
        "0.01": null
      },
      "auroc": 0.8347706597222223
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3065,
          "fn": 1735,
          "accuracy": 0.6385416666666667
        },
        "0.01": null
      },
      "auroc": 0.8628455729166666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1741,
          "fn": 3059,
          "accuracy": 0.36270833333333335
        },
        "0.01": null
      },
      "auroc": 0.8269000434027777
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4806,
          "fn": 4794,
          "accuracy": 0.500625
        },
        "0.01": null
      },
      "auroc": 0.8448728081597222
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2294,
          "fn": 106,
          "accuracy": 0.9558333333333333
        },
        "0.01": null
      },
      "auroc": 0.9770090277777778
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1742,
          "fn": 658,
          "accuracy": 0.7258333333333333
        },
        "0.01": null
      },
      "auroc": 0.9134319444444445
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4036,
          "fn": 764,
          "accuracy": 0.8408333333333333
        },
        "0.01": null
      },
      "auroc": 0.9452204861111111
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 545,
          "fn": 1855,
          "accuracy": 0.22708333333333333
        },
        "0.01": null
      },
      "auroc": 0.7725177083333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1645,
          "fn": 755,
          "accuracy": 0.6854166666666667
        },
        "0.01": null
      },
      "auroc": 0.8859037326388889
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2190,
          "fn": 2610,
          "accuracy": 0.45625
        },
        "0.01": null
      },
      "auroc": 0.8292107204861111
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2839,
          "fn": 1961,
          "accuracy": 0.5914583333333333
        },
        "0.01": null
      },
      "auroc": 0.8747633680555555
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3387,
          "fn": 1413,
          "accuracy": 0.705625
        },
        "0.01": null
      },
      "auroc": 0.8996678385416667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 6226,
          "fn": 3374,
          "accuracy": 0.6485416666666667
        },
        "0.01": null
      },
      "auroc": 0.887215603298611
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1616,
          "fn": 784,
          "accuracy": 0.6733333333333333
        },
        "0.01": null
      },
      "auroc": 0.868746875
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 819,
          "fn": 1581,
          "accuracy": 0.34125
        },
        "0.01": null
      },
      "auroc": 0.8028717013888891
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2435,
          "fn": 2365,
          "accuracy": 0.5072916666666667
        },
        "0.01": null
      },
      "auroc": 0.8358092881944444
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1197,
          "fn": 1203,
          "accuracy": 0.49875
        },
        "0.01": null
      },
      "auroc": 0.8332198784722222
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1024,
          "fn": 1376,
          "accuracy": 0.4266666666666667
        },
        "0.01": null
      },
      "auroc": 0.806653732638889
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2221,
          "fn": 2579,
          "accuracy": 0.46270833333333333
        },
        "0.01": null
      },
      "auroc": 0.8199368055555556
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2813,
          "fn": 1987,
          "accuracy": 0.5860416666666667
        },
        "0.01": null
      },
      "auroc": 0.850983376736111
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1843,
          "fn": 2957,
          "accuracy": 0.38395833333333335
        },
        "0.01": null
      },
      "auroc": 0.8047627170138889
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4656,
          "fn": 4944,
          "accuracy": 0.485
        },
        "0.01": null
      },
      "auroc": 0.827873046875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2351,
          "fn": 49,
          "accuracy": 0.9795833333333334
        },
        "0.01": null
      },
      "auroc": 0.9889449652777778
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2108,
          "fn": 292,
          "accuracy": 0.8783333333333333
        },
        "0.01": null
      },
      "auroc": 0.9592456597222223
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4459,
          "fn": 341,
          "accuracy": 0.9289583333333333
        },
        "0.01": null
      },
      "auroc": 0.9740953125
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 416,
          "fn": 1984,
          "accuracy": 0.17333333333333334
        },
        "0.01": null
      },
      "auroc": 0.7930837673611111
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 2228,
          "accuracy": 0.07166666666666667
        },
        "0.01": null
      },
      "auroc": 0.6798815972222222
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 588,
          "fn": 4212,
          "accuracy": 0.1225
        },
        "0.01": null
      },
      "auroc": 0.7364826822916667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2767,
          "fn": 2033,
          "accuracy": 0.5764583333333333
        },
        "0.01": null
      },
      "auroc": 0.8910143663194444
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2280,
          "fn": 2520,
          "accuracy": 0.475
        },
        "0.01": null
      },
      "auroc": 0.8195636284722222
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5047,
          "fn": 4553,
          "accuracy": 0.5257291666666667
        },
        "0.01": null
      },
      "auroc": 0.8552889973958332
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2266,
          "fn": 134,
          "accuracy": 0.9441666666666667
        },
        "0.01": null
      },
      "auroc": 0.9715872395833334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1980,
          "fn": 420,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9393438368055556
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4246,
          "fn": 554,
          "accuracy": 0.8845833333333334
        },
        "0.01": null
      },
      "auroc": 0.9554655381944444
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 457,
          "fn": 1943,
          "accuracy": 0.19041666666666668
        },
        "0.01": null
      },
      "auroc": 0.75931015625
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 932,
          "fn": 1468,
          "accuracy": 0.3883333333333333
        },
        "0.01": null
      },
      "auroc": 0.7464544270833333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1389,
          "fn": 3411,
          "accuracy": 0.289375
        },
        "0.01": null
      },
      "auroc": 0.7528822916666665
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2723,
          "fn": 2077,
          "accuracy": 0.5672916666666666
        },
        "0.01": null
      },
      "auroc": 0.8654486979166668
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2912,
          "fn": 1888,
          "accuracy": 0.6066666666666667
        },
        "0.01": null
      },
      "auroc": 0.8428991319444444
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5635,
          "fn": 3965,
          "accuracy": 0.5869791666666667
        },
        "0.01": null
      },
      "auroc": 0.8541739149305556
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1893,
          "fn": 507,
          "accuracy": 0.78875
        },
        "0.01": null
      },
      "auroc": 0.8856816840277777
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1322,
          "fn": 1078,
          "accuracy": 0.5508333333333333
        },
        "0.01": null
      },
      "auroc": 0.8523466145833334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3215,
          "fn": 1585,
          "accuracy": 0.6697916666666667
        },
        "0.01": null
      },
      "auroc": 0.8690141493055555
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1478,
          "fn": 922,
          "accuracy": 0.6158333333333333
        },
        "0.01": null
      },
      "auroc": 0.8560633680555556
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 552,
          "fn": 1848,
          "accuracy": 0.23
        },
        "0.01": null
      },
      "auroc": 0.7975957465277779
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2030,
          "fn": 2770,
          "accuracy": 0.42291666666666666
        },
        "0.01": null
      },
      "auroc": 0.8268295572916666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3371,
          "fn": 1429,
          "accuracy": 0.7022916666666666
        },
        "0.01": null
      },
      "auroc": 0.8708725260416668
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1874,
          "fn": 2926,
          "accuracy": 0.3904166666666667
        },
        "0.01": null
      },
      "auroc": 0.8249711805555555
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5245,
          "fn": 4355,
          "accuracy": 0.5463541666666667
        },
        "0.01": null
      },
      "auroc": 0.8479218532986112
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1583,
          "fn": 817,
          "accuracy": 0.6595833333333333
        },
        "0.01": null
      },
      "auroc": 0.90035
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1583,
          "fn": 817,
          "accuracy": 0.6595833333333333
        },
        "0.01": null
      },
      "auroc": 0.90035
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1318,
          "fn": 1082,
          "accuracy": 0.5491666666666667
        },
        "0.01": null
      },
      "auroc": 0.8549672743055556
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1318,
          "fn": 1082,
          "accuracy": 0.5491666666666667
        },
        "0.01": null
      },
      "auroc": 0.8549672743055556
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2901,
          "fn": 1899,
          "accuracy": 0.604375
        },
        "0.01": null
      },
      "auroc": 0.8776586371527778
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2901,
          "fn": 1899,
          "accuracy": 0.604375
        },
        "0.01": null
      },
      "auroc": 0.8776586371527778
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 533,
          "fn": 1867,
          "accuracy": 0.22208333333333333
        },
        "0.01": null
      },
      "auroc": 0.6936988715277778
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 533,
          "fn": 1867,
          "accuracy": 0.22208333333333333
        },
        "0.01": null
      },
      "auroc": 0.6936988715277778
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 2007,
          "accuracy": 0.16375
        },
        "0.01": null
      },
      "auroc": 0.6144394097222222
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 2007,
          "accuracy": 0.16375
        },
        "0.01": null
      },
      "auroc": 0.6144394097222222
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 926,
          "fn": 3874,
          "accuracy": 0.19291666666666665
        },
        "0.01": null
      },
      "auroc": 0.654069140625
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 926,
          "fn": 3874,
          "accuracy": 0.19291666666666665
        },
        "0.01": null
      },
      "auroc": 0.654069140625
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1819,
          "fn": 581,
          "accuracy": 0.7579166666666667
        },
        "0.01": null
      },
      "auroc": 0.8442174479166666
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1819,
          "fn": 581,
          "accuracy": 0.7579166666666667
        },
        "0.01": null
      },
      "auroc": 0.8442174479166666
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1370,
          "fn": 1030,
          "accuracy": 0.5708333333333333
        },
        "0.01": null
      },
      "auroc": 0.812385763888889
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1370,
          "fn": 1030,
          "accuracy": 0.5708333333333333
        },
        "0.01": null
      },
      "auroc": 0.812385763888889
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3189,
          "fn": 1611,
          "accuracy": 0.664375
        },
        "0.01": null
      },
      "auroc": 0.8283016059027778
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3189,
          "fn": 1611,
          "accuracy": 0.664375
        },
        "0.01": null
      },
      "auroc": 0.8283016059027778
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1617,
          "fn": 783,
          "accuracy": 0.67375
        },
        "0.01": null
      },
      "auroc": 0.8314118923611111
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1617,
          "fn": 783,
          "accuracy": 0.67375
        },
        "0.01": null
      },
      "auroc": 0.8314118923611111
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 2297,
          "accuracy": 0.042916666666666665
        },
        "0.01": null
      },
      "auroc": 0.688659548611111
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 2297,
          "accuracy": 0.042916666666666665
        },
        "0.01": null
      },
      "auroc": 0.688659548611111
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1720,
          "fn": 3080,
          "accuracy": 0.35833333333333334
        },
        "0.01": null
      },
      "auroc": 0.7600357204861112
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1720,
          "fn": 3080,
          "accuracy": 0.35833333333333334
        },
        "0.01": null
      },
      "auroc": 0.7600357204861112
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1292,
          "fn": 1108,
          "accuracy": 0.5383333333333333
        },
        "0.01": null
      },
      "auroc": 0.83896328125
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1292,
          "fn": 1108,
          "accuracy": 0.5383333333333333
        },
        "0.01": null
      },
      "auroc": 0.83896328125
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 899,
          "fn": 1501,
          "accuracy": 0.3745833333333333
        },
        "0.01": null
      },
      "auroc": 0.7748067708333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 899,
          "fn": 1501,
          "accuracy": 0.3745833333333333
        },
        "0.01": null
      },
      "auroc": 0.7748067708333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2191,
          "fn": 2609,
          "accuracy": 0.45645833333333335
        },
        "0.01": null
      },
      "auroc": 0.8068850260416666
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2191,
          "fn": 2609,
          "accuracy": 0.45645833333333335
        },
        "0.01": null
      },
      "auroc": 0.8068850260416666
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18908,
          "fn": 7492,
          "accuracy": 0.7162121212121212
        },
        "0.01": null
      },
      "auroc": 0.8790977904040405
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8973,
          "fn": 5427,
          "accuracy": 0.623125
        },
        "0.01": null
      },
      "auroc": 0.884620876736111
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 27881,
          "fn": 12919,
          "accuracy": 0.6833578431372549
        },
        "0.01": null
      },
      "auroc": 0.88104711499183
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9597,
          "fn": 16803,
          "accuracy": 0.3635227272727273
        },
        "0.01": null
      },
      "auroc": 0.7832436710858587
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5064,
          "fn": 9336,
          "accuracy": 0.3516666666666667
        },
        "0.01": null
      },
      "auroc": 0.788300636574074
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 14661,
          "fn": 26139,
          "accuracy": 0.3593382352941176
        },
        "0.01": null
      },
      "auroc": 0.7850284824346405
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 28505,
          "fn": 24295,
          "accuracy": 0.5398674242424243
        },
        "0.01": null
      },
      "auroc": 0.8311707307449495
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 14037,
          "fn": 14763,
          "accuracy": 0.4873958333333333
        },
        "0.01": null
      },
      "auroc": 0.8364607566550927
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 42542,
          "fn": 39058,
          "accuracy": 0.5213480392156863
        },
        "0.01": null
      },
      "auroc": 0.8330377987132352
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.993878125
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9945432291666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9945432291666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.99487578125
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9944822916666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9948453125
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        },
        "0.01": null
      },
      "auroc": 0.7658739583333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9942239583333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        },
        "0.01": null
      },
      "auroc": 0.8800489583333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        },
        "0.01": null
      },
      "auroc": 0.880178125
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9947161458333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 635,
          "fn": 165,
          "accuracy": 0.79375
        },
        "0.01": null
      },
      "auroc": 0.9374471354166667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9786677083333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9869380208333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9950687500000001
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.9767927083333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9859307291666668
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9951385416666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9777302083333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 738,
          "fn": 62,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.986434375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9950687500000001
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9862833333333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9906760416666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": null
      },
      "auroc": 0.7230197916666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": null
      },
      "auroc": 0.744334375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 374,
          "accuracy": 0.065
        },
        "0.01": null
      },
      "auroc": 0.7336770833333335
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": null
      },
      "auroc": 0.8590442708333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.8653088541666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 403,
          "fn": 397,
          "accuracy": 0.50375
        },
        "0.01": null
      },
      "auroc": 0.8621765625
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9921458333333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": null
      },
      "auroc": 0.933121875
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        },
        "0.01": null
      },
      "auroc": 0.9626338541666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        },
        "0.01": null
      },
      "auroc": 0.7779291666666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": null
      },
      "auroc": 0.9061614583333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 234,
          "accuracy": 0.415
        },
        "0.01": null
      },
      "auroc": 0.8420453125
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        },
        "0.01": null
      },
      "auroc": 0.8850375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        },
        "0.01": null
      },
      "auroc": 0.9196416666666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 438,
          "fn": 362,
          "accuracy": 0.5475
        },
        "0.01": null
      },
      "auroc": 0.9023395833333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9567958333333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": null
      },
      "auroc": 0.9760020833333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": null
      },
      "auroc": 0.9760020833333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 747,
          "fn": 53,
          "accuracy": 0.93375
        },
        "0.01": null
      },
      "auroc": 0.9856052083333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9912416666666666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9912416666666666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9861958333333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9861958333333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.98871875
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.98871875
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9890208333333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9890208333333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.986784375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.986784375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9879026041666668
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9879026041666668
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9948322916666666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9948322916666666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9949989583333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9949989583333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.994915625
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.994915625
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9919458333333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9919458333333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.991696875
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.991696875
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9918213541666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9918213541666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2161,
          "fn": 39,
          "accuracy": 0.9822727272727273
        },
        "0.01": null
      },
      "auroc": 0.9935973484848484
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1028,
          "fn": 172,
          "accuracy": 0.8566666666666667
        },
        "0.01": null
      },
      "auroc": 0.9806163194444445
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3189,
          "fn": 211,
          "accuracy": 0.9379411764705883
        },
        "0.01": null
      },
      "auroc": 0.9890158088235295
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1646,
          "fn": 554,
          "accuracy": 0.7481818181818182
        },
        "0.01": null
      },
      "auroc": 0.9279266098484849
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 847,
          "fn": 353,
          "accuracy": 0.7058333333333333
        },
        "0.01": null
      },
      "auroc": 0.9286977430555556
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2493,
          "fn": 907,
          "accuracy": 0.7332352941176471
        },
        "0.01": null
      },
      "auroc": 0.9281987745098039
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3807,
          "fn": 593,
          "accuracy": 0.8652272727272727
        },
        "0.01": null
      },
      "auroc": 0.9607619791666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1875,
          "fn": 525,
          "accuracy": 0.78125
        },
        "0.01": null
      },
      "auroc": 0.9546570312500001
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 5682,
          "fn": 1118,
          "accuracy": 0.8355882352941176
        },
        "0.01": null
      },
      "auroc": 0.9586072916666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.993878125
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9945432291666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9945432291666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.99487578125
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9944822916666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9948453125
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        },
        "0.01": null
      },
      "auroc": 0.7658739583333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9942239583333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        },
        "0.01": null
      },
      "auroc": 0.8800489583333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        },
        "0.01": null
      },
      "auroc": 0.880178125
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9947161458333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 635,
          "fn": 165,
          "accuracy": 0.79375
        },
        "0.01": null
      },
      "auroc": 0.9374471354166667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9786677083333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9869380208333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9950687500000001
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.9767927083333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9859307291666668
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9951385416666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9777302083333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 738,
          "fn": 62,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.986434375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9950687500000001
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9862833333333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9906760416666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": null
      },
      "auroc": 0.7230197916666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": null
      },
      "auroc": 0.744334375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 374,
          "accuracy": 0.065
        },
        "0.01": null
      },
      "auroc": 0.7336770833333335
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": null
      },
      "auroc": 0.8590442708333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.8653088541666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 403,
          "fn": 397,
          "accuracy": 0.50375
        },
        "0.01": null
      },
      "auroc": 0.8621765625
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9921458333333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": null
      },
      "auroc": 0.933121875
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        },
        "0.01": null
      },
      "auroc": 0.9626338541666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        },
        "0.01": null
      },
      "auroc": 0.7779291666666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": null
      },
      "auroc": 0.9061614583333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 234,
          "accuracy": 0.415
        },
        "0.01": null
      },
      "auroc": 0.8420453125
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        },
        "0.01": null
      },
      "auroc": 0.8850375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        },
        "0.01": null
      },
      "auroc": 0.9196416666666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 438,
          "fn": 362,
          "accuracy": 0.5475
        },
        "0.01": null
      },
      "auroc": 0.9023395833333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9567958333333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": null
      },
      "auroc": 0.9760020833333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": null
      },
      "auroc": 0.9760020833333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 747,
          "fn": 53,
          "accuracy": 0.93375
        },
        "0.01": null
      },
      "auroc": 0.9856052083333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9912416666666666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9912416666666666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9861958333333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9861958333333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.98871875
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.98871875
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9890208333333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9890208333333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.986784375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.986784375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9879026041666668
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9879026041666668
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9948322916666666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9948322916666666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9949989583333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9949989583333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.994915625
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.994915625
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9919458333333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9919458333333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.991696875
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.991696875
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9918213541666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9918213541666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2161,
          "fn": 39,
          "accuracy": 0.9822727272727273
        },
        "0.01": null
      },
      "auroc": 0.9935973484848484
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1028,
          "fn": 172,
          "accuracy": 0.8566666666666667
        },
        "0.01": null
      },
      "auroc": 0.9806163194444445
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3189,
          "fn": 211,
          "accuracy": 0.9379411764705883
        },
        "0.01": null
      },
      "auroc": 0.9890158088235295
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1646,
          "fn": 554,
          "accuracy": 0.7481818181818182
        },
        "0.01": null
      },
      "auroc": 0.9279266098484849
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 847,
          "fn": 353,
          "accuracy": 0.7058333333333333
        },
        "0.01": null
      },
      "auroc": 0.9286977430555556
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2493,
          "fn": 907,
          "accuracy": 0.7332352941176471
        },
        "0.01": null
      },
      "auroc": 0.9281987745098039
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3807,
          "fn": 593,
          "accuracy": 0.8652272727272727
        },
        "0.01": null
      },
      "auroc": 0.9607619791666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1875,
          "fn": 525,
          "accuracy": 0.78125
        },
        "0.01": null
      },
      "auroc": 0.9546570312500001
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 5682,
          "fn": 1118,
          "accuracy": 0.8355882352941176
        },
        "0.01": null
      },
      "auroc": 0.9586072916666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9932583333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9942333333333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9942333333333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.9947208333333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9914260416666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9933171875
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        },
        "0.01": null
      },
      "auroc": 0.677809375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9942541666666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        },
        "0.01": null
      },
      "auroc": 0.8360317708333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        },
        "0.01": null
      },
      "auroc": 0.8346177083333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.99473125
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 614,
          "fn": 186,
          "accuracy": 0.7675
        },
        "0.01": null
      },
      "auroc": 0.9146744791666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9769854166666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.986096875
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9947395833333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9753333333333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": null
      },
      "auroc": 0.9850364583333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9949739583333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9761593749999999
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 734,
          "fn": 66,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9855666666666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9937312500000001
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": null
      },
      "auroc": 0.96666875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": null
      },
      "auroc": 0.9802
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.6294604166666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": null
      },
      "auroc": 0.7194145833333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 381,
          "accuracy": 0.0475
        },
        "0.01": null
      },
      "auroc": 0.6744375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        },
        "0.01": null
      },
      "auroc": 0.8115958333333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 245,
          "accuracy": 0.3875
        },
        "0.01": null
      },
      "auroc": 0.8430416666666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 448,
          "accuracy": 0.44
        },
        "0.01": null
      },
      "auroc": 0.8273187499999999
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.986140625
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        },
        "0.01": null
      },
      "auroc": 0.881325
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        },
        "0.01": null
      },
      "auroc": 0.9337328125000001
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        },
        "0.01": null
      },
      "auroc": 0.7003468749999999
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": null
      },
      "auroc": 0.9042447916666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 249,
          "accuracy": 0.3775
        },
        "0.01": null
      },
      "auroc": 0.8022958333333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        },
        "0.01": null
      },
      "auroc": 0.84324375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 243,
          "accuracy": 0.3925
        },
        "0.01": null
      },
      "auroc": 0.8927848958333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 429,
          "accuracy": 0.46375
        },
        "0.01": null
      },
      "auroc": 0.8680143229166667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9947197916666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9949640625
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9947437499999999
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.9361270833333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": null
      },
      "auroc": 0.9654354166666668
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9949760416666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": null
      },
      "auroc": 0.9654234375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 725,
          "fn": 75,
          "accuracy": 0.90625
        },
        "0.01": null
      },
      "auroc": 0.9801997395833333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9825427083333332
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9825427083333332
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9717760416666666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9717760416666666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9771593749999999
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9771593749999999
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": null
      },
      "auroc": 0.9805520833333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": null
      },
      "auroc": 0.9805520833333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": null
      },
      "auroc": 0.9708510416666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": null
      },
      "auroc": 0.9708510416666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": null
      },
      "auroc": 0.9757015625000001
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": null
      },
      "auroc": 0.9757015625000001
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9937645833333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9937645833333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9941583333333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9941583333333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9939614583333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9939614583333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9875979166666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9875979166666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9850302083333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9850302083333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": null
      },
      "auroc": 0.9863140625000001
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": null
      },
      "auroc": 0.9863140625000001
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2099,
          "fn": 101,
          "accuracy": 0.9540909090909091
        },
        "0.01": null
      },
      "auroc": 0.9905989583333333
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 945,
          "fn": 255,
          "accuracy": 0.7875
        },
        "0.01": null
      },
      "auroc": 0.9683526041666666
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3044,
          "fn": 356,
          "accuracy": 0.8952941176470588
        },
        "0.01": null
      },
      "auroc": 0.9827473039215686
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1523,
          "fn": 677,
          "accuracy": 0.6922727272727273
        },
        "0.01": null
      },
      "auroc": 0.9008483901515152
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 824,
          "fn": 376,
          "accuracy": 0.6866666666666666
        },
        "0.01": null
      },
      "auroc": 0.9204387152777778
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2347,
          "fn": 1053,
          "accuracy": 0.6902941176470588
        },
        "0.01": null
      },
      "auroc": 0.9077626225490196
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3622,
          "fn": 778,
          "accuracy": 0.8231818181818182
        },
        "0.01": null
      },
      "auroc": 0.9457236742424243
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1769,
          "fn": 631,
          "accuracy": 0.7370833333333333
        },
        "0.01": null
      },
      "auroc": 0.9443956597222223
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 5391,
          "fn": 1409,
          "accuracy": 0.7927941176470589
        },
        "0.01": null
      },
      "auroc": 0.945254963235294
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9939927083333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.993215625
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9936041666666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9939916666666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9832041666666668
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9885979166666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9939921875
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": null
      },
      "auroc": 0.9882098958333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 767,
          "fn": 33,
          "accuracy": 0.95875
        },
        "0.01": null
      },
      "auroc": 0.9911010416666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.976559375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9949020833333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": null
      },
      "auroc": 0.9857307291666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        },
        "0.01": null
      },
      "auroc": 0.5842270833333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.994240625
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        },
        "0.01": null
      },
      "auroc": 0.7892338541666668
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        },
        "0.01": null
      },
      "auroc": 0.7803932291666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9945713541666668
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 576,
          "fn": 224,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.8874822916666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9927104166666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9733520833333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.98303125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9872895833333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.9719447916666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": null
      },
      "auroc": 0.9796171875
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.99
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": null
      },
      "auroc": 0.9726484375000001
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 714,
          "fn": 86,
          "accuracy": 0.8925
        },
        "0.01": null
      },
      "auroc": 0.98132421875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9862906250000001
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": null
      },
      "auroc": 0.8901552083333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 228,
          "fn": 172,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.9382229166666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.5382333333333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": null
      },
      "auroc": 0.72368125
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        },
        "0.01": null
      },
      "auroc": 0.6309572916666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 214,
          "accuracy": 0.465
        },
        "0.01": null
      },
      "auroc": 0.7622619791666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 60,
          "fn": 340,
          "accuracy": 0.15
        },
        "0.01": null
      },
      "auroc": 0.8069182291666668
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 246,
          "fn": 554,
          "accuracy": 0.3075
        },
        "0.01": null
      },
      "auroc": 0.7845901041666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9634145833333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": null
      },
      "auroc": 0.7766145833333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 222,
          "accuracy": 0.445
        },
        "0.01": null
      },
      "auroc": 0.8700145833333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        },
        "0.01": null
      },
      "auroc": 0.6107979166666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": null
      },
      "auroc": 0.9081625
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 255,
          "accuracy": 0.3625
        },
        "0.01": null
      },
      "auroc": 0.7594802083333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 214,
          "accuracy": 0.465
        },
        "0.01": null
      },
      "auroc": 0.78710625
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 263,
          "accuracy": 0.3425
        },
        "0.01": null
      },
      "auroc": 0.8423885416666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 477,
          "accuracy": 0.40375
        },
        "0.01": null
      },
      "auroc": 0.8147473958333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9891479166666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": null
      },
      "auroc": 0.9770958333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.983121875
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9814395833333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": null
      },
      "auroc": 0.8625645833333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        },
        "0.01": null
      },
      "auroc": 0.9220020833333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9852937500000001
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        },
        "0.01": null
      },
      "auroc": 0.9198302083333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 559,
          "fn": 241,
          "accuracy": 0.69875
        },
        "0.01": null
      },
      "auroc": 0.9525619791666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        },
        "0.01": null
      },
      "auroc": 0.921553125
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        },
        "0.01": null
      },
      "auroc": 0.921553125
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": null
      },
      "auroc": 0.898728125
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": null
      },
      "auroc": 0.898728125
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 274,
          "accuracy": 0.315
        },
        "0.01": null
      },
      "auroc": 0.910140625
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 274,
          "accuracy": 0.315
        },
        "0.01": null
      },
      "auroc": 0.910140625
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": null
      },
      "auroc": 0.9249239583333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": null
      },
      "auroc": 0.9249239583333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": null
      },
      "auroc": 0.9105145833333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": null
      },
      "auroc": 0.9105145833333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 257,
          "accuracy": 0.3575
        },
        "0.01": null
      },
      "auroc": 0.9177192708333334
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 257,
          "accuracy": 0.3575
        },
        "0.01": null
      },
      "auroc": 0.9177192708333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9925010416666666
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9925010416666666
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9900322916666666
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9900322916666666
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9912666666666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9912666666666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9874541666666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9874541666666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.9653635416666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.9653635416666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9764088541666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9764088541666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": null
      },
      "auroc": 0.9649114583333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": null
      },
      "auroc": 0.9649114583333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.9558791666666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.9558791666666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        },
        "0.01": null
      },
      "auroc": 0.9603953125
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        },
        "0.01": null
      },
      "auroc": 0.9603953125
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1754,
          "fn": 446,
          "accuracy": 0.7972727272727272
        },
        "0.01": null
      },
      "auroc": 0.9721326704545455
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 772,
          "fn": 428,
          "accuracy": 0.6433333333333333
        },
        "0.01": null
      },
      "auroc": 0.9342225694444445
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2526,
          "fn": 874,
          "accuracy": 0.7429411764705882
        },
        "0.01": null
      },
      "auroc": 0.9587526348039216
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1140,
          "fn": 1060,
          "accuracy": 0.5181818181818182
        },
        "0.01": null
      },
      "auroc": 0.8560451704545455
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 743,
          "fn": 457,
          "accuracy": 0.6191666666666666
        },
        "0.01": null
      },
      "auroc": 0.9072996527777777
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1883,
          "fn": 1517,
          "accuracy": 0.5538235294117647
        },
        "0.01": null
      },
      "auroc": 0.874134987745098
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2894,
          "fn": 1506,
          "accuracy": 0.6577272727272727
        },
        "0.01": null
      },
      "auroc": 0.9140889204545455
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1515,
          "fn": 885,
          "accuracy": 0.63125
        },
        "0.01": null
      },
      "auroc": 0.9207611111111111
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 4409,
          "fn": 2391,
          "accuracy": 0.6483823529411765
        },
        "0.01": null
      },
      "auroc": 0.9164438112745098
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.99285
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9940291666666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9940291666666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9946187500000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9911479166666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.993178125
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        },
        "0.01": null
      },
      "auroc": 0.631009375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9942239583333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        },
        "0.01": null
      },
      "auroc": 0.8126166666666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        },
        "0.01": null
      },
      "auroc": 0.8110786458333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9947161458333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 617,
          "fn": 183,
          "accuracy": 0.77125
        },
        "0.01": null
      },
      "auroc": 0.9028973958333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.97725
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9862291666666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.994765625
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.97479375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9847796875
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9949869791666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9760218749999999
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 734,
          "fn": 66,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9855044270833334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9941802083333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": null
      },
      "auroc": 0.962684375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": null
      },
      "auroc": 0.9784322916666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.5581635416666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        },
        "0.01": null
      },
      "auroc": 0.6639395833333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 383,
          "accuracy": 0.0425
        },
        "0.01": null
      },
      "auroc": 0.6110515624999999
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": null
      },
      "auroc": 0.7761718750000001
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 259,
          "accuracy": 0.3525
        },
        "0.01": null
      },
      "auroc": 0.8133119791666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 460,
          "accuracy": 0.425
        },
        "0.01": null
      },
      "auroc": 0.7947419270833334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9841083333333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        },
        "0.01": null
      },
      "auroc": 0.8588010416666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        },
        "0.01": null
      },
      "auroc": 0.9214546875
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        },
        "0.01": null
      },
      "auroc": 0.6641708333333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.8790822916666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 257,
          "accuracy": 0.3575
        },
        "0.01": null
      },
      "auroc": 0.7716265625000001
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.8241395833333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 260,
          "accuracy": 0.35
        },
        "0.01": null
      },
      "auroc": 0.8689416666666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 454,
          "accuracy": 0.4325
        },
        "0.01": null
      },
      "auroc": 0.846540625
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9951385416666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.99490625
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9950223958333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.99490625
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": null
      },
      "auroc": 0.9377145833333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": null
      },
      "auroc": 0.9663104166666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9950223958333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": null
      },
      "auroc": 0.9663104166666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 730,
          "fn": 70,
          "accuracy": 0.9125
        },
        "0.01": null
      },
      "auroc": 0.9806664062500001
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": null
      },
      "auroc": 0.9801489583333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": null
      },
      "auroc": 0.9801489583333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.966021875
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.966021875
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": null
      },
      "auroc": 0.9730854166666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": null
      },
      "auroc": 0.9730854166666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.973790625
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.973790625
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.9659354166666666
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.9659354166666666
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        },
        "0.01": null
      },
      "auroc": 0.9698630208333334
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        },
        "0.01": null
      },
      "auroc": 0.9698630208333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9935052083333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9935052083333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9925770833333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9925770833333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9930411458333332
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9930411458333332
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9870947916666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9870947916666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": null
      },
      "auroc": 0.9825197916666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": null
      },
      "auroc": 0.9825197916666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9848072916666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9848072916666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2068,
          "fn": 132,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9895217803030303
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 923,
          "fn": 277,
          "accuracy": 0.7691666666666667
        },
        "0.01": null
      },
      "auroc": 0.9640097222222223
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2991,
          "fn": 409,
          "accuracy": 0.8797058823529412
        },
        "0.01": null
      },
      "auroc": 0.9805175245098039
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1507,
          "fn": 693,
          "accuracy": 0.685
        },
        "0.01": null
      },
      "auroc": 0.8854987689393939
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 819,
          "fn": 381,
          "accuracy": 0.6825
        },
        "0.01": null
      },
      "auroc": 0.9071006944444444
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2326,
          "fn": 1074,
          "accuracy": 0.6841176470588235
        },
        "0.01": null
      },
      "auroc": 0.8931229779411765
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3575,
          "fn": 825,
          "accuracy": 0.8125
        },
        "0.01": null
      },
      "auroc": 0.9375102746212121
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1742,
          "fn": 658,
          "accuracy": 0.7258333333333333
        },
        "0.01": null
      },
      "auroc": 0.9355552083333334
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 5317,
          "fn": 1483,
          "accuracy": 0.7819117647058823
        },
        "0.01": null
      },
      "auroc": 0.9368202512254903
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": null
      },
      "auroc": 0.9602885416666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.9547927083333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        },
        "0.01": null
      },
      "auroc": 0.957540625
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": null
      },
      "auroc": 0.9563250000000001
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": null
      },
      "auroc": 0.9345979166666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.9454614583333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        },
        "0.01": null
      },
      "auroc": 0.9583067708333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": null
      },
      "auroc": 0.9446953124999999
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 529,
          "fn": 271,
          "accuracy": 0.66125
        },
        "0.01": null
      },
      "auroc": 0.9515010416666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        },
        "0.01": null
      },
      "auroc": 0.8887510416666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        },
        "0.01": null
      },
      "auroc": 0.7106104166666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 296,
          "accuracy": 0.26
        },
        "0.01": null
      },
      "auroc": 0.7996807291666668
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": null
      },
      "auroc": 0.6725291666666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        },
        "0.01": null
      },
      "auroc": 0.6484677083333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 362,
          "accuracy": 0.095
        },
        "0.01": null
      },
      "auroc": 0.6604984375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 289,
          "accuracy": 0.2775
        },
        "0.01": null
      },
      "auroc": 0.7806401041666668
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 369,
          "accuracy": 0.0775
        },
        "0.01": null
      },
      "auroc": 0.6795390625000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 658,
          "accuracy": 0.1775
        },
        "0.01": null
      },
      "auroc": 0.7300895833333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": null
      },
      "auroc": 0.9601916666666668
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        },
        "0.01": null
      },
      "auroc": 0.7753708333333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        },
        "0.01": null
      },
      "auroc": 0.86778125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.944103125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        },
        "0.01": null
      },
      "auroc": 0.75380625
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 234,
          "accuracy": 0.415
        },
        "0.01": null
      },
      "auroc": 0.8489546875
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 273,
          "fn": 127,
          "accuracy": 0.6825
        },
        "0.01": null
      },
      "auroc": 0.9521473958333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 332,
          "accuracy": 0.17
        },
        "0.01": null
      },
      "auroc": 0.7645885416666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 459,
          "accuracy": 0.42625
        },
        "0.01": null
      },
      "auroc": 0.85836796875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        },
        "0.01": null
      },
      "auroc": 0.7511135416666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        },
        "0.01": null
      },
      "auroc": 0.8048427083333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 83,
          "fn": 317,
          "accuracy": 0.2075
        },
        "0.01": null
      },
      "auroc": 0.777978125
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": null
      },
      "auroc": 0.7499010416666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        },
        "0.01": null
      },
      "auroc": 0.6108010416666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 368,
          "accuracy": 0.08
        },
        "0.01": null
      },
      "auroc": 0.6803510416666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 341,
          "accuracy": 0.1475
        },
        "0.01": null
      },
      "auroc": 0.7505072916666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        },
        "0.01": null
      },
      "auroc": 0.707821875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 685,
          "accuracy": 0.14375
        },
        "0.01": null
      },
      "auroc": 0.7291645833333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        },
        "0.01": null
      },
      "auroc": 0.8906854166666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        },
        "0.01": null
      },
      "auroc": 0.73869375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 278,
          "accuracy": 0.305
        },
        "0.01": null
      },
      "auroc": 0.8146895833333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": null
      },
      "auroc": 0.7281187499999999
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": null
      },
      "auroc": 0.6211447916666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 28,
          "fn": 372,
          "accuracy": 0.07
        },
        "0.01": null
      },
      "auroc": 0.6746317708333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 290,
          "accuracy": 0.275
        },
        "0.01": null
      },
      "auroc": 0.8094020833333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 40,
          "fn": 360,
          "accuracy": 0.1
        },
        "0.01": null
      },
      "auroc": 0.6799192708333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 650,
          "accuracy": 0.1875
        },
        "0.01": null
      },
      "auroc": 0.7446606770833333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": null
      },
      "auroc": 0.923871875
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": null
      },
      "auroc": 0.9415312499999999
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        },
        "0.01": null
      },
      "auroc": 0.9327015624999999
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": null
      },
      "auroc": 0.9465083333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": null
      },
      "auroc": 0.8940208333333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        },
        "0.01": null
      },
      "auroc": 0.9202645833333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.9351901041666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        },
        "0.01": null
      },
      "auroc": 0.9177760416666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 471,
          "fn": 329,
          "accuracy": 0.58875
        },
        "0.01": null
      },
      "auroc": 0.9264830729166667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": null
      },
      "auroc": 0.8499854166666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": null
      },
      "auroc": 0.8499854166666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        },
        "0.01": null
      },
      "auroc": 0.8510614583333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        },
        "0.01": null
      },
      "auroc": 0.8510614583333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 278,
          "accuracy": 0.305
        },
        "0.01": null
      },
      "auroc": 0.8505234375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 278,
          "accuracy": 0.305
        },
        "0.01": null
      },
      "auroc": 0.8505234375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        },
        "0.01": null
      },
      "auroc": 0.8718999999999999
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        },
        "0.01": null
      },
      "auroc": 0.8718999999999999
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        },
        "0.01": null
      },
      "auroc": 0.8674854166666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        },
        "0.01": null
      },
      "auroc": 0.8674854166666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 263,
          "accuracy": 0.3425
        },
        "0.01": null
      },
      "auroc": 0.8696927083333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 263,
          "accuracy": 0.3425
        },
        "0.01": null
      },
      "auroc": 0.8696927083333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9574968749999999
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9574968749999999
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": null
      },
      "auroc": 0.9607625
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": null
      },
      "auroc": 0.9607625
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        },
        "0.01": null
      },
      "auroc": 0.9591296874999999
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        },
        "0.01": null
      },
      "auroc": 0.9591296874999999
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": null
      },
      "auroc": 0.9455052083333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": null
      },
      "auroc": 0.9455052083333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.958790625
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.958790625
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.9521479166666666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.9521479166666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": null
      },
      "auroc": 0.9144114583333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": null
      },
      "auroc": 0.9144114583333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        },
        "0.01": null
      },
      "auroc": 0.8918479166666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        },
        "0.01": null
      },
      "auroc": 0.8918479166666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        },
        "0.01": null
      },
      "auroc": 0.9031296875
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        },
        "0.01": null
      },
      "auroc": 0.9031296875
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1127,
          "fn": 1073,
          "accuracy": 0.5122727272727273
        },
        "0.01": null
      },
      "auroc": 0.9012910037878789
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 825,
          "accuracy": 0.3125
        },
        "0.01": null
      },
      "auroc": 0.820973611111111
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1502,
          "fn": 1898,
          "accuracy": 0.44176470588235295
        },
        "0.01": null
      },
      "auroc": 0.8729436887254902
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 959,
          "fn": 1241,
          "accuracy": 0.4359090909090909
        },
        "0.01": null
      },
      "auroc": 0.866130303030303
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 916,
          "accuracy": 0.23666666666666666
        },
        "0.01": null
      },
      "auroc": 0.743806423611111
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1243,
          "fn": 2157,
          "accuracy": 0.36558823529411766
        },
        "0.01": null
      },
      "auroc": 0.8229571691176469
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2086,
          "fn": 2314,
          "accuracy": 0.4740909090909091
        },
        "0.01": null
      },
      "auroc": 0.8837106534090908
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 659,
          "fn": 1741,
          "accuracy": 0.27458333333333335
        },
        "0.01": null
      },
      "auroc": 0.7823900173611111
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2745,
          "fn": 4055,
          "accuracy": 0.4036764705882353
        },
        "0.01": null
      },
      "auroc": 0.8479504289215687
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9927354166666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.993971875
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.993971875
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        },
        "0.01": null
      },
      "auroc": 0.9945901041666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.98276875
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9889885416666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        },
        "0.01": null
      },
      "auroc": 0.7242270833333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9944604166666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.8593437500000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        },
        "0.01": null
      },
      "auroc": 0.8534979166666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.994834375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 592,
          "fn": 208,
          "accuracy": 0.74
        },
        "0.01": null
      },
      "auroc": 0.9241661458333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9944833333333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9772927083333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9858880208333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9936875
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": null
      },
      "auroc": 0.975334375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9845109375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9940854166666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": null
      },
      "auroc": 0.9763135416666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 736,
          "fn": 64,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9851994791666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9883364583333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.9644645833333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9764005208333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.677578125
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        },
        "0.01": null
      },
      "auroc": 0.7400927083333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 24,
          "fn": 376,
          "accuracy": 0.06
        },
        "0.01": null
      },
      "auroc": 0.7088354166666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        },
        "0.01": null
      },
      "auroc": 0.8329572916666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 253,
          "accuracy": 0.3675
        },
        "0.01": null
      },
      "auroc": 0.8522786458333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 468,
          "accuracy": 0.415
        },
        "0.01": null
      },
      "auroc": 0.8426179687500001
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9808083333333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        },
        "0.01": null
      },
      "auroc": 0.8961479166666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.938478125
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        },
        "0.01": null
      },
      "auroc": 0.7222625
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": null
      },
      "auroc": 0.9075479166666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 251,
          "accuracy": 0.3725
        },
        "0.01": null
      },
      "auroc": 0.8149052083333335
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.8515354166666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 243,
          "accuracy": 0.3925
        },
        "0.01": null
      },
      "auroc": 0.9018479166666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 439,
          "accuracy": 0.45125
        },
        "0.01": null
      },
      "auroc": 0.8766916666666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.99471875
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.994325
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.994521875
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.993715625
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.9366541666666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.9651848958333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9942171875
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9654895833333332
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 723,
          "fn": 77,
          "accuracy": 0.90375
        },
        "0.01": null
      },
      "auroc": 0.9798533854166667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": null
      },
      "auroc": 0.9635312500000001
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": null
      },
      "auroc": 0.9635312500000001
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": null
      },
      "auroc": 0.9487041666666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": null
      },
      "auroc": 0.9487041666666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        },
        "0.01": null
      },
      "auroc": 0.9561177083333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        },
        "0.01": null
      },
      "auroc": 0.9561177083333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": null
      },
      "auroc": 0.97093125
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": null
      },
      "auroc": 0.97093125
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": null
      },
      "auroc": 0.9631270833333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": null
      },
      "auroc": 0.9631270833333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        },
        "0.01": null
      },
      "auroc": 0.9670291666666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        },
        "0.01": null
      },
      "auroc": 0.9670291666666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9951385416666666
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9951385416666666
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9951734375
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9951734375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.993290625
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.993290625
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9919239583333332
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9919239583333332
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9926072916666666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9926072916666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9836708333333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9836708333333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.9781635416666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.9781635416666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": null
      },
      "auroc": 0.9809171875
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": null
      },
      "auroc": 0.9809171875
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1945,
          "fn": 255,
          "accuracy": 0.884090909090909
        },
        "0.01": null
      },
      "auroc": 0.9857232954545454
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 939,
          "fn": 261,
          "accuracy": 0.7825
        },
        "0.01": null
      },
      "auroc": 0.9704411458333333
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2884,
          "fn": 516,
          "accuracy": 0.8482352941176471
        },
        "0.01": null
      },
      "auroc": 0.9803295955882353
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1442,
          "fn": 758,
          "accuracy": 0.6554545454545454
        },
        "0.01": null
      },
      "auroc": 0.9076124053030303
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 827,
          "fn": 373,
          "accuracy": 0.6891666666666667
        },
        "0.01": null
      },
      "auroc": 0.9244708333333334
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2269,
          "fn": 1131,
          "accuracy": 0.6673529411764706
        },
        "0.01": null
      },
      "auroc": 0.9135624387254901
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3387,
          "fn": 1013,
          "accuracy": 0.7697727272727273
        },
        "0.01": null
      },
      "auroc": 0.9466678503787879
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1766,
          "fn": 634,
          "accuracy": 0.7358333333333333
        },
        "0.01": null
      },
      "auroc": 0.9474559895833332
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 5153,
          "fn": 1647,
          "accuracy": 0.7577941176470588
        },
        "0.01": null
      },
      "auroc": 0.9469460171568627
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.993878125
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9945432291666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9945432291666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.99487578125
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9944822916666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9948453125
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        },
        "0.01": null
      },
      "auroc": 0.7660031249999999
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9942239583333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        },
        "0.01": null
      },
      "auroc": 0.8801135416666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        },
        "0.01": null
      },
      "auroc": 0.8802427083333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9947161458333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 635,
          "fn": 165,
          "accuracy": 0.79375
        },
        "0.01": null
      },
      "auroc": 0.9374794270833333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9786677083333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9869380208333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9950687500000001
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.9768041666666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9859364583333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9951385416666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9777359375000001
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 738,
          "fn": 62,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.9864372395833334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9950687500000001
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9862833333333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9906760416666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": null
      },
      "auroc": 0.7230197916666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": null
      },
      "auroc": 0.7443166666666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 374,
          "accuracy": 0.065
        },
        "0.01": null
      },
      "auroc": 0.7336682291666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": null
      },
      "auroc": 0.8590442708333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.8653000000000001
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 403,
          "fn": 397,
          "accuracy": 0.50375
        },
        "0.01": null
      },
      "auroc": 0.8621721354166666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9921458333333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": null
      },
      "auroc": 0.933121875
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        },
        "0.01": null
      },
      "auroc": 0.9626338541666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        },
        "0.01": null
      },
      "auroc": 0.7779677083333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": null
      },
      "auroc": 0.9059802083333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 235,
          "accuracy": 0.4125
        },
        "0.01": null
      },
      "auroc": 0.8419739583333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        },
        "0.01": null
      },
      "auroc": 0.8850567708333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.9195510416666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 437,
          "fn": 363,
          "accuracy": 0.54625
        },
        "0.01": null
      },
      "auroc": 0.90230390625
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9567958333333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": null
      },
      "auroc": 0.9760020833333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": null
      },
      "auroc": 0.9760020833333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 747,
          "fn": 53,
          "accuracy": 0.93375
        },
        "0.01": null
      },
      "auroc": 0.9856052083333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9912416666666666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9912416666666666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9861958333333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9861958333333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.98871875
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.98871875
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9890208333333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9890208333333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.986784375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.986784375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9879026041666668
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9879026041666668
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9948322916666666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9948322916666666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9949989583333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9949989583333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.994915625
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.994915625
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9919458333333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9919458333333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.991696875
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.991696875
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9918213541666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9918213541666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2161,
          "fn": 39,
          "accuracy": 0.9822727272727273
        },
        "0.01": null
      },
      "auroc": 0.9935973484848484
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1028,
          "fn": 172,
          "accuracy": 0.8566666666666667
        },
        "0.01": null
      },
      "auroc": 0.9806163194444445
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3189,
          "fn": 211,
          "accuracy": 0.9379411764705883
        },
        "0.01": null
      },
      "auroc": 0.9890158088235295
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1646,
          "fn": 554,
          "accuracy": 0.7481818181818182
        },
        "0.01": null
      },
      "auroc": 0.927941856060606
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 846,
          "fn": 354,
          "accuracy": 0.705
        },
        "0.01": null
      },
      "auroc": 0.9286664930555555
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2492,
          "fn": 908,
          "accuracy": 0.7329411764705882
        },
        "0.01": null
      },
      "auroc": 0.9281976102941175
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3807,
          "fn": 593,
          "accuracy": 0.8652272727272727
        },
        "0.01": null
      },
      "auroc": 0.9607696022727272
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1874,
          "fn": 526,
          "accuracy": 0.7808333333333334
        },
        "0.01": null
      },
      "auroc": 0.95464140625
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 5681,
          "fn": 1119,
          "accuracy": 0.8354411764705882
        },
        "0.01": null
      },
      "auroc": 0.9586067095588235
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.18243541666666668
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.153821875
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.16812864583333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.17993541666666665
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.15011354166666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.16502447916666668
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.18118541666666665
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1519677083333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1665765625
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        },
        "0.01": null
      },
      "auroc": 0.4409708333333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.08174166666666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 377,
          "accuracy": 0.0575
        },
        "0.01": null
      },
      "auroc": 0.26135625
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": null
      },
      "auroc": 0.15343229166666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": null
      },
      "auroc": 0.14671458333333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        },
        "0.01": null
      },
      "auroc": 0.1500734375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 30,
          "fn": 370,
          "accuracy": 0.075
        },
        "0.01": null
      },
      "auroc": 0.29720156249999996
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.11422812500000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 762,
          "accuracy": 0.0475
        },
        "0.01": null
      },
      "auroc": 0.20571484375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1877875
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.05067291666666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11923020833333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.15933854166666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.055469791666666664
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.10740416666666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.17356302083333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.05307135416666668
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1133171875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        },
        "0.01": null
      },
      "auroc": 0.7132822916666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.12033645833333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 322,
          "accuracy": 0.195
        },
        "0.01": null
      },
      "auroc": 0.416809375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.07431041666666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.06749999999999999
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.07090520833333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 322,
          "accuracy": 0.195
        },
        "0.01": null
      },
      "auroc": 0.3937963541666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.09391822916666669
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 722,
          "accuracy": 0.0975
        },
        "0.01": null
      },
      "auroc": 0.2438572916666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": null
      },
      "auroc": 0.5185395833333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.06906666666666668
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        },
        "0.01": null
      },
      "auroc": 0.293803125
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": null
      },
      "auroc": 0.22317708333333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": null
      },
      "auroc": 0.15914791666666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 378,
          "accuracy": 0.055
        },
        "0.01": null
      },
      "auroc": 0.19116249999999999
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 24,
          "fn": 376,
          "accuracy": 0.06
        },
        "0.01": null
      },
      "auroc": 0.3708583333333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 384,
          "accuracy": 0.04
        },
        "0.01": null
      },
      "auroc": 0.11410729166666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 40,
          "fn": 760,
          "accuracy": 0.05
        },
        "0.01": null
      },
      "auroc": 0.24248281250000003
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13503020833333335
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.09423020833333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11463020833333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1282802083333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.09326666666666668
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1107734375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13165520833333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.09374843750000002
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11270182291666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11316145833333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11316145833333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.125840625
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.125840625
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11950104166666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11950104166666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.08049270833333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.08049270833333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.08643645833333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.08643645833333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.08346458333333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.08346458333333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.14486145833333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.14486145833333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13086874999999998
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13086874999999998
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13786510416666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13786510416666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.17951145833333335
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.17951145833333335
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.15763020833333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.15763020833333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.16857083333333336
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.16857083333333336
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.15172812500000002
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.15172812500000002
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.14218645833333332
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.14218645833333332
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.14695729166666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.14695729166666666
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 2088,
          "accuracy": 0.05090909090909091
        },
        "0.01": null
      },
      "auroc": 0.25889100378787877
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 1193,
          "accuracy": 0.005833333333333334
        },
        "0.01": null
      },
      "auroc": 0.09497829861111111
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 3281,
          "accuracy": 0.035
        },
        "0.01": null
      },
      "auroc": 0.20103946078431373
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 21,
          "fn": 2179,
          "accuracy": 0.009545454545454546
        },
        "0.01": null
      },
      "auroc": 0.14194876893939393
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 1183,
          "accuracy": 0.014166666666666666
        },
        "0.01": null
      },
      "auroc": 0.11203541666666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 3362,
          "accuracy": 0.011176470588235295
        },
        "0.01": null
      },
      "auroc": 0.13139111519607843
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 4267,
          "accuracy": 0.030227272727272728
        },
        "0.01": null
      },
      "auroc": 0.20041988636363636
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 24,
          "fn": 2376,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.1035068576388889
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 6643,
          "accuracy": 0.023088235294117646
        },
        "0.01": null
      },
      "auroc": 0.16621528799019608
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9923322916666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9937703124999999
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9937703124999999
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9944893229166667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.987271875
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9912401041666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        },
        "0.01": null
      },
      "auroc": 0.6546197916666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9942239583333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        },
        "0.01": null
      },
      "auroc": 0.824421875
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        },
        "0.01": null
      },
      "auroc": 0.8209458333333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9947161458333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 606,
          "fn": 194,
          "accuracy": 0.7575
        },
        "0.01": null
      },
      "auroc": 0.9078309895833333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9949291666666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.981059375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9879942708333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9941083333333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9786177083333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9863630208333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.99451875
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9798385416666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 745,
          "fn": 55,
          "accuracy": 0.93125
        },
        "0.01": null
      },
      "auroc": 0.9871786458333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.994628125
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9796260416666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": null
      },
      "auroc": 0.9871270833333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.622740625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": null
      },
      "auroc": 0.7741
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 371,
          "accuracy": 0.0725
        },
        "0.01": null
      },
      "auroc": 0.6984203124999999
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": null
      },
      "auroc": 0.808684375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        },
        "0.01": null
      },
      "auroc": 0.8768630208333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 408,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.8427736979166667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9851604166666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        },
        "0.01": null
      },
      "auroc": 0.9161947916666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        },
        "0.01": null
      },
      "auroc": 0.9506776041666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        },
        "0.01": null
      },
      "auroc": 0.7044333333333332
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": null
      },
      "auroc": 0.9203677083333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 240,
          "accuracy": 0.4
        },
        "0.01": null
      },
      "auroc": 0.8124005208333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        },
        "0.01": null
      },
      "auroc": 0.844796875
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        },
        "0.01": null
      },
      "auroc": 0.9182812499999999
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 404,
          "fn": 396,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.8815390625
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9951385416666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9944177083333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.994778125
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9941583333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.9382781250000001
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": null
      },
      "auroc": 0.9662182291666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9946484375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        },
        "0.01": null
      },
      "auroc": 0.9663479166666668
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 726,
          "fn": 74,
          "accuracy": 0.9075
        },
        "0.01": null
      },
      "auroc": 0.9804981770833333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9851625
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9851625
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9771572916666666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9771572916666666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": null
      },
      "auroc": 0.9811598958333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": null
      },
      "auroc": 0.9811598958333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9805864583333332
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9805864583333332
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.970203125
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.970203125
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        },
        "0.01": null
      },
      "auroc": 0.9753947916666668
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        },
        "0.01": null
      },
      "auroc": 0.9753947916666668
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9950687500000001
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9950687500000001
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9951385416666666
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9951385416666666
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9951036458333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9951036458333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9925750000000001
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9925750000000001
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9903645833333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9903645833333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9914697916666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9914697916666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.986096875
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.986096875
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9836291666666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9836291666666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": null
      },
      "auroc": 0.9848630208333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": null
      },
      "auroc": 0.9848630208333333
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2088,
          "fn": 112,
          "accuracy": 0.9490909090909091
        },
        "0.01": null
      },
      "auroc": 0.9901660037878788
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1003,
          "fn": 197,
          "accuracy": 0.8358333333333333
        },
        "0.01": null
      },
      "auroc": 0.9769524305555556
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3091,
          "fn": 309,
          "accuracy": 0.9091176470588235
        },
        "0.01": null
      },
      "auroc": 0.9855023897058823
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1523,
          "fn": 677,
          "accuracy": 0.6922727272727273
        },
        "0.01": null
      },
      "auroc": 0.8983419507575757
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 849,
          "fn": 351,
          "accuracy": 0.7075
        },
        "0.01": null
      },
      "auroc": 0.9329866319444444
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2372,
          "fn": 1028,
          "accuracy": 0.6976470588235294
        },
        "0.01": null
      },
      "auroc": 0.9105694852941177
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3611,
          "fn": 789,
          "accuracy": 0.8206818181818182
        },
        "0.01": null
      },
      "auroc": 0.9442539772727273
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1852,
          "fn": 548,
          "accuracy": 0.7716666666666666
        },
        "0.01": null
      },
      "auroc": 0.95496953125
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 5463,
          "fn": 1337,
          "accuracy": 0.8033823529411764
        },
        "0.01": null
      },
      "auroc": 0.9480359374999999
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.993878125
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9945432291666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9945432291666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.99487578125
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9942270833333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9947177083333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        },
        "0.01": null
      },
      "auroc": 0.7404104166666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.99429375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 236,
          "fn": 164,
          "accuracy": 0.59
        },
        "0.01": null
      },
      "auroc": 0.8673520833333332
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        },
        "0.01": null
      },
      "auroc": 0.8673187499999999
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9947510416666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 632,
          "fn": 168,
          "accuracy": 0.79
        },
        "0.01": null
      },
      "auroc": 0.9310348958333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9785229166666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9868656250000001
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9949760416666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.9766083333333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": null
      },
      "auroc": 0.9857921875000001
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9950921875
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9775656250000001
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 737,
          "fn": 63,
          "accuracy": 0.92125
        },
        "0.01": null
      },
      "auroc": 0.98632890625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9947666666666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9846489583333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9897078125000001
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.7013489583333332
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        },
        "0.01": null
      },
      "auroc": 0.7210354166666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 378,
          "accuracy": 0.055
        },
        "0.01": null
      },
      "auroc": 0.7111921875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.8480578125
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        },
        "0.01": null
      },
      "auroc": 0.8528421875000001
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 406,
          "accuracy": 0.4925
        },
        "0.01": null
      },
      "auroc": 0.85045
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9912489583333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        },
        "0.01": null
      },
      "auroc": 0.9226947916666668
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        },
        "0.01": null
      },
      "auroc": 0.956971875
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": null
      },
      "auroc": 0.756278125
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": null
      },
      "auroc": 0.901790625
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 238,
          "accuracy": 0.405
        },
        "0.01": null
      },
      "auroc": 0.829034375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": null
      },
      "auroc": 0.8737635416666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        },
        "0.01": null
      },
      "auroc": 0.9122427083333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 427,
          "fn": 373,
          "accuracy": 0.53375
        },
        "0.01": null
      },
      "auroc": 0.893003125
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.9539645833333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": null
      },
      "auroc": 0.9745864583333335
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": null
      },
      "auroc": 0.9745864583333335
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 745,
          "fn": 55,
          "accuracy": 0.93125
        },
        "0.01": null
      },
      "auroc": 0.9848973958333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9904010416666666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9904010416666666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9844625
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9844625
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9874317708333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9874317708333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9880447916666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9880447916666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.985084375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.985084375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": null
      },
      "auroc": 0.9865645833333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": null
      },
      "auroc": 0.9865645833333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9952083333333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9947885416666666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9947885416666666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9949989583333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9949989583333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.99489375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.99489375
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9910145833333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9910145833333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.99065625
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.99065625
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9908354166666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9908354166666666
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2154,
          "fn": 46,
          "accuracy": 0.9790909090909091
        },
        "0.01": null
      },
      "auroc": 0.9932113636363636
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1017,
          "fn": 183,
          "accuracy": 0.8475
        },
        "0.01": null
      },
      "auroc": 0.9785819444444444
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3171,
          "fn": 229,
          "accuracy": 0.9326470588235294
        },
        "0.01": null
      },
      "auroc": 0.9880480392156863
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1629,
          "fn": 571,
          "accuracy": 0.7404545454545455
        },
        "0.01": null
      },
      "auroc": 0.9212582386363637
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 842,
          "fn": 358,
          "accuracy": 0.7016666666666667
        },
        "0.01": null
      },
      "auroc": 0.923595138888889
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2471,
          "fn": 929,
          "accuracy": 0.726764705882353
        },
        "0.01": null
      },
      "auroc": 0.9220830269607844
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3783,
          "fn": 617,
          "accuracy": 0.8597727272727272
        },
        "0.01": null
      },
      "auroc": 0.9572348011363636
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1859,
          "fn": 541,
          "accuracy": 0.7745833333333333
        },
        "0.01": null
      },
      "auroc": 0.9510885416666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 5642,
          "fn": 1158,
          "accuracy": 0.8297058823529412
        },
        "0.01": null
      },
      "auroc": 0.9550655330882353
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.12811145833333332
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.10488229166666665
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11649687499999999
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1301854166666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.105296875
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11774114583333332
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.12914843750000002
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.10508958333333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11711901041666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": null
      },
      "auroc": 0.42861875
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.07896562500000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 24,
          "fn": 376,
          "accuracy": 0.06
        },
        "0.01": null
      },
      "auroc": 0.2537921875
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": null
      },
      "auroc": 0.15491354166666665
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.11652083333333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 387,
          "accuracy": 0.0325
        },
        "0.01": null
      },
      "auroc": 0.1357171875
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 30,
          "fn": 370,
          "accuracy": 0.075
        },
        "0.01": null
      },
      "auroc": 0.29176614583333327
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 393,
          "accuracy": 0.0175
        },
        "0.01": null
      },
      "auroc": 0.09774322916666668
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 763,
          "accuracy": 0.04625
        },
        "0.01": null
      },
      "auroc": 0.19475468750000002
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.19315208333333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.0735375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13334479166666668
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1725375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.073684375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.1231109375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.18284479166666665
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.07361093749999999
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 799,
          "accuracy": 0.00125
        },
        "0.01": null
      },
      "auroc": 0.12822786458333335
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        },
        "0.01": null
      },
      "auroc": 0.74411875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.1705364583333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 312,
          "accuracy": 0.22
        },
        "0.01": null
      },
      "auroc": 0.4573276041666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.10084583333333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.06454270833333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.08269427083333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 86,
          "fn": 314,
          "accuracy": 0.215
        },
        "0.01": null
      },
      "auroc": 0.4224822916666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.11753958333333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 712,
          "accuracy": 0.11
        },
        "0.01": null
      },
      "auroc": 0.27001093750000005
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        },
        "0.01": null
      },
      "auroc": 0.5931958333333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.10937708333333335
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 43,
          "fn": 357,
          "accuracy": 0.1075
        },
        "0.01": null
      },
      "auroc": 0.35128645833333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": null
      },
      "auroc": 0.22545625
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        },
        "0.01": null
      },
      "auroc": 0.14219062500000001
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 378,
          "accuracy": 0.055
        },
        "0.01": null
      },
      "auroc": 0.1838234375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        },
        "0.01": null
      },
      "auroc": 0.40932604166666664
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        },
        "0.01": null
      },
      "auroc": 0.12578385416666665
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 65,
          "fn": 735,
          "accuracy": 0.08125
        },
        "0.01": null
      },
      "auroc": 0.2675549479166667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.22134687499999997
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11875520833333335
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.17005104166666665
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.16371875000000002
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.10786354166666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13579114583333332
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1925328125
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11330937499999999
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.15292109375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.20980729166666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.20980729166666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.18347604166666664
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.18347604166666664
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.1966416666666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.1966416666666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.18525104166666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.18525104166666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.20029895833333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.20029895833333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.19277499999999997
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.19277499999999997
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.22626770833333332
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.22626770833333332
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.18676041666666668
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.18676041666666668
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.20651406249999998
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.20651406249999998
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.21331250000000002
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.21331250000000002
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.18465
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.18465
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.19898125
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.19898125
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.19500625
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.19500625
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1619625
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1619625
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.178484375
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.178484375
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 2052,
          "accuracy": 0.06727272727272728
        },
        "0.01": null
      },
      "auroc": 0.3034716856060606
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 1191,
          "accuracy": 0.0075
        },
        "0.01": null
      },
      "auroc": 0.1093423611111111
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 3243,
          "accuracy": 0.04617647058823529
        },
        "0.01": null
      },
      "auroc": 0.23495545343137256
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 24,
          "fn": 2176,
          "accuracy": 0.01090909090909091
        },
        "0.01": null
      },
      "auroc": 0.16952774621212124
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 1184,
          "accuracy": 0.013333333333333334
        },
        "0.01": null
      },
      "auroc": 0.10168315972222224
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 40,
          "fn": 3360,
          "accuracy": 0.011764705882352941
        },
        "0.01": null
      },
      "auroc": 0.1455825980392157
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 4228,
          "accuracy": 0.03909090909090909
        },
        "0.01": null
      },
      "auroc": 0.23649971590909089
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 25,
          "fn": 2375,
          "accuracy": 0.010416666666666666
        },
        "0.01": null
      },
      "auroc": 0.10551276041666666
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 6603,
          "accuracy": 0.028970588235294116
        },
        "0.01": null
      },
      "auroc": 0.1902690257352941
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1938,
          "fn": 462,
          "accuracy": 0.8075
        },
        "0.01": null
      },
      "auroc": 0.8522078993055555
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1922,
          "fn": 478,
          "accuracy": 0.8008333333333333
        },
        "0.01": null
      },
      "auroc": 0.8473649305555555
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3860,
          "fn": 940,
          "accuracy": 0.8041666666666667
        },
        "0.01": null
      },
      "auroc": 0.8497864149305556
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1938,
          "fn": 462,
          "accuracy": 0.8075
        },
        "0.01": null
      },
      "auroc": 0.8518420138888889
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1878,
          "fn": 522,
          "accuracy": 0.7825
        },
        "0.01": null
      },
      "auroc": 0.8433250868055555
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3816,
          "fn": 984,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.8475835503472222
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3876,
          "fn": 924,
          "accuracy": 0.8075
        },
        "0.01": null
      },
      "auroc": 0.8520249565972222
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3800,
          "fn": 1000,
          "accuracy": 0.7916666666666666
        },
        "0.01": null
      },
      "auroc": 0.8453450086805556
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7676,
          "fn": 1924,
          "accuracy": 0.7995833333333333
        },
        "0.01": null
      },
      "auroc": 0.8486849826388889
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1817,
          "fn": 583,
          "accuracy": 0.7570833333333333
        },
        "0.01": null
      },
      "auroc": 0.8887657118055555
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1818,
          "fn": 582,
          "accuracy": 0.7575
        },
        "0.01": null
      },
      "auroc": 0.8189905381944445
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3635,
          "fn": 1165,
          "accuracy": 0.7572916666666667
        },
        "0.01": null
      },
      "auroc": 0.853878125
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 317,
          "fn": 2083,
          "accuracy": 0.13208333333333333
        },
        "0.01": null
      },
      "auroc": 0.6075774305555556
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1807,
          "fn": 593,
          "accuracy": 0.7529166666666667
        },
        "0.01": null
      },
      "auroc": 0.82167265625
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2124,
          "fn": 2676,
          "accuracy": 0.4425
        },
        "0.01": null
      },
      "auroc": 0.7146250434027779
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2134,
          "fn": 2666,
          "accuracy": 0.44458333333333333
        },
        "0.01": null
      },
      "auroc": 0.7481715711805556
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3625,
          "fn": 1175,
          "accuracy": 0.7552083333333334
        },
        "0.01": null
      },
      "auroc": 0.8203315972222223
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5759,
          "fn": 3841,
          "accuracy": 0.5998958333333333
        },
        "0.01": null
      },
      "auroc": 0.7842515842013889
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1929,
          "fn": 471,
          "accuracy": 0.80375
        },
        "0.01": null
      },
      "auroc": 0.8578753472222223
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 805,
          "accuracy": 0.6645833333333333
        },
        "0.01": null
      },
      "auroc": 0.8083372395833333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3524,
          "fn": 1276,
          "accuracy": 0.7341666666666666
        },
        "0.01": null
      },
      "auroc": 0.8331062934027779
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1905,
          "fn": 495,
          "accuracy": 0.79375
        },
        "0.01": null
      },
      "auroc": 0.8517293402777777
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1527,
          "fn": 873,
          "accuracy": 0.63625
        },
        "0.01": null
      },
      "auroc": 0.8054985243055556
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3432,
          "fn": 1368,
          "accuracy": 0.715
        },
        "0.01": null
      },
      "auroc": 0.8286139322916666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3834,
          "fn": 966,
          "accuracy": 0.79875
        },
        "0.01": null
      },
      "auroc": 0.8548023437500001
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3122,
          "fn": 1678,
          "accuracy": 0.6504166666666666
        },
        "0.01": null
      },
      "auroc": 0.8069178819444445
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 6956,
          "fn": 2644,
          "accuracy": 0.7245833333333334
        },
        "0.01": null
      },
      "auroc": 0.8308601128472222
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1959,
          "fn": 441,
          "accuracy": 0.81625
        },
        "0.01": null
      },
      "auroc": 0.928804513888889
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1348,
          "fn": 1052,
          "accuracy": 0.5616666666666666
        },
        "0.01": null
      },
      "auroc": 0.8169011284722222
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3307,
          "fn": 1493,
          "accuracy": 0.6889583333333333
        },
        "0.01": null
      },
      "auroc": 0.8728528211805555
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 2343,
          "accuracy": 0.02375
        },
        "0.01": null
      },
      "auroc": 0.5684701388888889
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 2218,
          "accuracy": 0.07583333333333334
        },
        "0.01": null
      },
      "auroc": 0.6098410590277777
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 4561,
          "accuracy": 0.049791666666666665
        },
        "0.01": null
      },
      "auroc": 0.5891555989583332
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2016,
          "fn": 2784,
          "accuracy": 0.42
        },
        "0.01": null
      },
      "auroc": 0.7486373263888889
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1530,
          "fn": 3270,
          "accuracy": 0.31875
        },
        "0.01": null
      },
      "auroc": 0.71337109375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3546,
          "fn": 6054,
          "accuracy": 0.369375
        },
        "0.01": null
      },
      "auroc": 0.7310042100694445
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1794,
          "fn": 606,
          "accuracy": 0.7475
        },
        "0.01": null
      },
      "auroc": 0.9058116319444445
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 527,
          "fn": 1873,
          "accuracy": 0.21958333333333332
        },
        "0.01": null
      },
      "auroc": 0.7473567708333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2321,
          "fn": 2479,
          "accuracy": 0.48354166666666665
        },
        "0.01": null
      },
      "auroc": 0.8265842013888889
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 2025,
          "accuracy": 0.15625
        },
        "0.01": null
      },
      "auroc": 0.6390723090277777
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1104,
          "fn": 1296,
          "accuracy": 0.46
        },
        "0.01": null
      },
      "auroc": 0.7551651909722221
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1479,
          "fn": 3321,
          "accuracy": 0.308125
        },
        "0.01": null
      },
      "auroc": 0.69711875
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2169,
          "fn": 2631,
          "accuracy": 0.451875
        },
        "0.01": null
      },
      "auroc": 0.7724419704861111
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1631,
          "fn": 3169,
          "accuracy": 0.33979166666666666
        },
        "0.01": null
      },
      "auroc": 0.7512609809027777
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3800,
          "fn": 5800,
          "accuracy": 0.3958333333333333
        },
        "0.01": null
      },
      "auroc": 0.7618514756944444
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1908,
          "fn": 492,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.8525361979166666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1864,
          "fn": 536,
          "accuracy": 0.7766666666666666
        },
        "0.01": null
      },
      "auroc": 0.8409012152777777
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3772,
          "fn": 1028,
          "accuracy": 0.7858333333333334
        },
        "0.01": null
      },
      "auroc": 0.8467187065972221
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1885,
          "fn": 515,
          "accuracy": 0.7854166666666667
        },
        "0.01": null
      },
      "auroc": 0.8481920138888889
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1263,
          "fn": 1137,
          "accuracy": 0.52625
        },
        "0.01": null
      },
      "auroc": 0.7942368055555555
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3148,
          "fn": 1652,
          "accuracy": 0.6558333333333334
        },
        "0.01": null
      },
      "auroc": 0.8212144097222223
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3793,
          "fn": 1007,
          "accuracy": 0.7902083333333333
        },
        "0.01": null
      },
      "auroc": 0.8503641059027778
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3127,
          "fn": 1673,
          "accuracy": 0.6514583333333334
        },
        "0.01": null
      },
      "auroc": 0.8175690104166666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 6920,
          "fn": 2680,
          "accuracy": 0.7208333333333333
        },
        "0.01": null
      },
      "auroc": 0.8339665581597222
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1529,
          "fn": 871,
          "accuracy": 0.6370833333333333
        },
        "0.01": null
      },
      "auroc": 0.8308348958333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1529,
          "fn": 871,
          "accuracy": 0.6370833333333333
        },
        "0.01": null
      },
      "auroc": 0.8308348958333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1409,
          "fn": 991,
          "accuracy": 0.5870833333333333
        },
        "0.01": null
      },
      "auroc": 0.8221513020833333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1409,
          "fn": 991,
          "accuracy": 0.5870833333333333
        },
        "0.01": null
      },
      "auroc": 0.8221513020833333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2938,
          "fn": 1862,
          "accuracy": 0.6120833333333333
        },
        "0.01": null
      },
      "auroc": 0.8264930989583333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2938,
          "fn": 1862,
          "accuracy": 0.6120833333333333
        },
        "0.01": null
      },
      "auroc": 0.8264930989583333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1533,
          "fn": 867,
          "accuracy": 0.63875
        },
        "0.01": null
      },
      "auroc": 0.8269612847222222
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1533,
          "fn": 867,
          "accuracy": 0.63875
        },
        "0.01": null
      },
      "auroc": 0.8269612847222222
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1406,
          "fn": 994,
          "accuracy": 0.5858333333333333
        },
        "0.01": null
      },
      "auroc": 0.8233574652777778
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1406,
          "fn": 994,
          "accuracy": 0.5858333333333333
        },
        "0.01": null
      },
      "auroc": 0.8233574652777778
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2939,
          "fn": 1861,
          "accuracy": 0.6122916666666667
        },
        "0.01": null
      },
      "auroc": 0.8251593749999999
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2939,
          "fn": 1861,
          "accuracy": 0.6122916666666667
        },
        "0.01": null
      },
      "auroc": 0.8251593749999999
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1939,
          "fn": 461,
          "accuracy": 0.8079166666666666
        },
        "0.01": null
      },
      "auroc": 0.8568878472222222
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1939,
          "fn": 461,
          "accuracy": 0.8079166666666666
        },
        "0.01": null
      },
      "auroc": 0.8568878472222222
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1925,
          "fn": 475,
          "accuracy": 0.8020833333333334
        },
        "0.01": null
      },
      "auroc": 0.8524959201388889
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1925,
          "fn": 475,
          "accuracy": 0.8020833333333334
        },
        "0.01": null
      },
      "auroc": 0.8524959201388889
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3864,
          "fn": 936,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.8546918836805555
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3864,
          "fn": 936,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.8546918836805555
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1900,
          "fn": 500,
          "accuracy": 0.7916666666666666
        },
        "0.01": null
      },
      "auroc": 0.8565170138888889
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1900,
          "fn": 500,
          "accuracy": 0.7916666666666666
        },
        "0.01": null
      },
      "auroc": 0.8565170138888889
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1829,
          "fn": 571,
          "accuracy": 0.7620833333333333
        },
        "0.01": null
      },
      "auroc": 0.8512878472222223
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1829,
          "fn": 571,
          "accuracy": 0.7620833333333333
        },
        "0.01": null
      },
      "auroc": 0.8512878472222223
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3729,
          "fn": 1071,
          "accuracy": 0.776875
        },
        "0.01": null
      },
      "auroc": 0.8539024305555556
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3729,
          "fn": 1071,
          "accuracy": 0.776875
        },
        "0.01": null
      },
      "auroc": 0.8539024305555556
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1732,
          "fn": 668,
          "accuracy": 0.7216666666666667
        },
        "0.01": null
      },
      "auroc": 0.8447808159722223
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1732,
          "fn": 668,
          "accuracy": 0.7216666666666667
        },
        "0.01": null
      },
      "auroc": 0.8447808159722223
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1660,
          "fn": 740,
          "accuracy": 0.6916666666666667
        },
        "0.01": null
      },
      "auroc": 0.8372471354166668
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1660,
          "fn": 740,
          "accuracy": 0.6916666666666667
        },
        "0.01": null
      },
      "auroc": 0.8372471354166668
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3392,
          "fn": 1408,
          "accuracy": 0.7066666666666667
        },
        "0.01": null
      },
      "auroc": 0.8410139756944446
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3392,
          "fn": 1408,
          "accuracy": 0.7066666666666667
        },
        "0.01": null
      },
      "auroc": 0.8410139756944446
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 19978,
          "fn": 6422,
          "accuracy": 0.7567424242424242
        },
        "0.01": null
      },
      "auroc": 0.8638166508838384
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9074,
          "fn": 5326,
          "accuracy": 0.6301388888888889
        },
        "0.01": null
      },
      "auroc": 0.8133086371527778
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 29052,
          "fn": 11748,
          "accuracy": 0.7120588235294117
        },
        "0.01": null
      },
      "auroc": 0.8459902930964053
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 14706,
          "fn": 11694,
          "accuracy": 0.5570454545454545
        },
        "0.01": null
      },
      "auroc": 0.7775839015151516
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7761,
          "fn": 6639,
          "accuracy": 0.5389583333333333
        },
        "0.01": null
      },
      "auroc": 0.7716232204861111
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 22467,
          "fn": 18333,
          "accuracy": 0.5506617647058824
        },
        "0.01": null
      },
      "auroc": 0.775480131740196
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 34684,
          "fn": 18116,
          "accuracy": 0.6568939393939394
        },
        "0.01": null
      },
      "auroc": 0.820700276199495
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 16835,
          "fn": 11965,
          "accuracy": 0.5845486111111111
        },
        "0.01": null
      },
      "auroc": 0.7924659288194444
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 51519,
          "fn": 30081,
          "accuracy": 0.631360294117647
        },
        "0.01": null
      },
      "auroc": 0.8107352124183007
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9952052083333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9958317708333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9958317708333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        }
      },
      "auroc": 0.9961450520833334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9963906250000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9962114583333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9963010416666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.7998822916666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9962249999999999
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        }
      },
      "auroc": 0.8980536458333332
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        }
      },
      "auroc": 0.8981364583333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9962182291666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 662,
          "fn": 138,
          "accuracy": 0.8275
        },
        "0.01": {
          "tp": 626,
          "fn": 174,
          "accuracy": 0.7825
        }
      },
      "auroc": 0.9471773437500001
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9941218749999999
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.99601875
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9950703125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9960020833333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9908052083333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9934036458333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9950619791666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9934119791666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 792,
          "fn": 8,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 782,
          "fn": 18,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9942369791666668
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9945010416666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9954796875
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.7444531249999999
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.8463020833333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 287,
          "accuracy": 0.2825
        },
        "0.01": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        }
      },
      "auroc": 0.7953776041666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8704557291666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        }
      },
      "auroc": 0.9204015624999999
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 511,
          "fn": 289,
          "accuracy": 0.63875
        },
        "0.01": {
          "tp": 441,
          "fn": 359,
          "accuracy": 0.55125
        }
      },
      "auroc": 0.8954286458333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9937479166666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.995103125
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.8338322916666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9743562499999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.9040942708333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 273,
          "fn": 127,
          "accuracy": 0.6825
        }
      },
      "auroc": 0.9151453125
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        }
      },
      "auroc": 0.9840520833333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 664,
          "fn": 136,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 634,
          "fn": 166,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9495986979166666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.99408125
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9921083333333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9930947916666668
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.99425625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.984034375
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9891453125
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.99416875
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9880713541666668
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 784,
          "fn": 16,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 770,
          "fn": 30,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9911200520833334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.990875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.990875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9881375
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9881375
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.98950625
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.98950625
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        }
      },
      "auroc": 0.8770614583333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        }
      },
      "auroc": 0.8770614583333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.8350562499999999
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.8350562499999999
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        }
      },
      "auroc": 0.8560588541666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        }
      },
      "auroc": 0.8560588541666666
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9931583333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9931583333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9926427083333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9926427083333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9929005208333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9929005208333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9939322916666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9939322916666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9852447916666668
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9852447916666668
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9895885416666668
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9895885416666668
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.9365895833333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.9365895833333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8913395833333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8913395833333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        }
      },
      "auroc": 0.9139645833333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        }
      },
      "auroc": 0.9139645833333333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2063,
          "fn": 137,
          "accuracy": 0.9377272727272727
        },
        "0.01": {
          "tp": 1988,
          "fn": 212,
          "accuracy": 0.9036363636363637
        }
      },
      "auroc": 0.9786895833333333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1195,
          "fn": 5,
          "accuracy": 0.9958333333333333
        },
        "0.01": {
          "tp": 1179,
          "fn": 21,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9948409722222222
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3258,
          "fn": 142,
          "accuracy": 0.9582352941176471
        },
        "0.01": {
          "tp": 3167,
          "fn": 233,
          "accuracy": 0.9314705882352942
        }
      },
      "auroc": 0.9843900735294118
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1581,
          "fn": 619,
          "accuracy": 0.7186363636363636
        },
        "0.01": {
          "tp": 1410,
          "fn": 790,
          "accuracy": 0.6409090909090909
        }
      },
      "auroc": 0.9143004734848486
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1039,
          "fn": 161,
          "accuracy": 0.8658333333333333
        },
        "0.01": {
          "tp": 974,
          "fn": 226,
          "accuracy": 0.8116666666666666
        }
      },
      "auroc": 0.9644880208333333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2620,
          "fn": 780,
          "accuracy": 0.7705882352941177
        },
        "0.01": {
          "tp": 2384,
          "fn": 1016,
          "accuracy": 0.7011764705882353
        }
      },
      "auroc": 0.932013725490196
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3644,
          "fn": 756,
          "accuracy": 0.8281818181818181
        },
        "0.01": {
          "tp": 3398,
          "fn": 1002,
          "accuracy": 0.7722727272727272
        }
      },
      "auroc": 0.9464950284090909
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2234,
          "fn": 166,
          "accuracy": 0.9308333333333333
        },
        "0.01": {
          "tp": 2153,
          "fn": 247,
          "accuracy": 0.8970833333333333
        }
      },
      "auroc": 0.9796644965277779
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 5878,
          "fn": 922,
          "accuracy": 0.8644117647058823
        },
        "0.01": {
          "tp": 5551,
          "fn": 1249,
          "accuracy": 0.8163235294117647
        }
      },
      "auroc": 0.958201899509804
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9952052083333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9958317708333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9958317708333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        }
      },
      "auroc": 0.9961450520833334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9963906250000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9962114583333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9963010416666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.7998822916666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9962249999999999
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        }
      },
      "auroc": 0.8980536458333332
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        }
      },
      "auroc": 0.8981364583333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9962182291666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 662,
          "fn": 138,
          "accuracy": 0.8275
        },
        "0.01": {
          "tp": 626,
          "fn": 174,
          "accuracy": 0.7825
        }
      },
      "auroc": 0.9471773437500001
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9941218749999999
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.99601875
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9950703125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9960020833333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9908052083333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9934036458333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9950619791666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9934119791666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 792,
          "fn": 8,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 782,
          "fn": 18,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9942369791666668
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9945010416666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9954796875
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.7444531249999999
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.8463020833333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 287,
          "accuracy": 0.2825
        },
        "0.01": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        }
      },
      "auroc": 0.7953776041666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8704557291666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        }
      },
      "auroc": 0.9204015624999999
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 511,
          "fn": 289,
          "accuracy": 0.63875
        },
        "0.01": {
          "tp": 441,
          "fn": 359,
          "accuracy": 0.55125
        }
      },
      "auroc": 0.8954286458333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9937479166666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.995103125
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.8338322916666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9743562499999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.9040942708333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 273,
          "fn": 127,
          "accuracy": 0.6825
        }
      },
      "auroc": 0.9151453125
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        }
      },
      "auroc": 0.9840520833333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 664,
          "fn": 136,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 634,
          "fn": 166,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9495986979166666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.99408125
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9921083333333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9930947916666668
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.99425625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.984034375
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9891453125
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.99416875
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9880713541666668
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 784,
          "fn": 16,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 770,
          "fn": 30,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9911200520833334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.990875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.990875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9881375
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9881375
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.98950625
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.98950625
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        }
      },
      "auroc": 0.8770614583333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        }
      },
      "auroc": 0.8770614583333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.8350562499999999
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.8350562499999999
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        }
      },
      "auroc": 0.8560588541666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        }
      },
      "auroc": 0.8560588541666666
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9931583333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9931583333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9926427083333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9926427083333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9929005208333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9929005208333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9939322916666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9939322916666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9852447916666668
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9852447916666668
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9895885416666668
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9895885416666668
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.9365895833333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.9365895833333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8913395833333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8913395833333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        }
      },
      "auroc": 0.9139645833333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        }
      },
      "auroc": 0.9139645833333333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2063,
          "fn": 137,
          "accuracy": 0.9377272727272727
        },
        "0.01": {
          "tp": 1988,
          "fn": 212,
          "accuracy": 0.9036363636363637
        }
      },
      "auroc": 0.9786895833333333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1195,
          "fn": 5,
          "accuracy": 0.9958333333333333
        },
        "0.01": {
          "tp": 1179,
          "fn": 21,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9948409722222222
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3258,
          "fn": 142,
          "accuracy": 0.9582352941176471
        },
        "0.01": {
          "tp": 3167,
          "fn": 233,
          "accuracy": 0.9314705882352942
        }
      },
      "auroc": 0.9843900735294118
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1581,
          "fn": 619,
          "accuracy": 0.7186363636363636
        },
        "0.01": {
          "tp": 1410,
          "fn": 790,
          "accuracy": 0.6409090909090909
        }
      },
      "auroc": 0.9143004734848486
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1039,
          "fn": 161,
          "accuracy": 0.8658333333333333
        },
        "0.01": {
          "tp": 974,
          "fn": 226,
          "accuracy": 0.8116666666666666
        }
      },
      "auroc": 0.9644880208333333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2620,
          "fn": 780,
          "accuracy": 0.7705882352941177
        },
        "0.01": {
          "tp": 2384,
          "fn": 1016,
          "accuracy": 0.7011764705882353
        }
      },
      "auroc": 0.932013725490196
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3644,
          "fn": 756,
          "accuracy": 0.8281818181818181
        },
        "0.01": {
          "tp": 3398,
          "fn": 1002,
          "accuracy": 0.7722727272727272
        }
      },
      "auroc": 0.9464950284090909
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2234,
          "fn": 166,
          "accuracy": 0.9308333333333333
        },
        "0.01": {
          "tp": 2153,
          "fn": 247,
          "accuracy": 0.8970833333333333
        }
      },
      "auroc": 0.9796644965277779
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 5878,
          "fn": 922,
          "accuracy": 0.8644117647058823
        },
        "0.01": {
          "tp": 5551,
          "fn": 1249,
          "accuracy": 0.8163235294117647
        }
      },
      "auroc": 0.958201899509804
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9961927083333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9963255208333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964333333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9934333333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9949333333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964458333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9948130208333335
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9956294270833334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9962197916666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9962229166666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9962213541666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.7203770833333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.996228125
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        }
      },
      "auroc": 0.8583026041666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        }
      },
      "auroc": 0.8582984375
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9962255208333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 634,
          "fn": 166,
          "accuracy": 0.7925
        },
        "0.01": {
          "tp": 618,
          "fn": 182,
          "accuracy": 0.7725
        }
      },
      "auroc": 0.9272619791666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9932135416666668
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9952364583333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.994225
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9952927083333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9871989583333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9912458333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9942531250000001
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9912177083333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 768,
          "fn": 32,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9927354166666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9882395833333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9923489583333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.6332864583333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.8100052083333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 90,
          "fn": 310,
          "accuracy": 0.225
        },
        "0.01": {
          "tp": 46,
          "fn": 354,
          "accuracy": 0.115
        }
      },
      "auroc": 0.7216458333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        }
      },
      "auroc": 0.8148723958333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        },
        "0.01": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        }
      },
      "auroc": 0.8991223958333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 482,
          "fn": 318,
          "accuracy": 0.6025
        },
        "0.01": {
          "tp": 416,
          "fn": 384,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8569973958333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.99053125
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9934947916666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        },
        "0.01": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        }
      },
      "auroc": 0.7659666666666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9707197916666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        },
        "0.01": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        }
      },
      "auroc": 0.8683432291666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        },
        "0.01": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        }
      },
      "auroc": 0.8812125000000001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9806255208333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 654,
          "fn": 146,
          "accuracy": 0.8175
        },
        "0.01": {
          "tp": 618,
          "fn": 182,
          "accuracy": 0.7725
        }
      },
      "auroc": 0.9309190104166666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.994178125
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9919489583333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9930635416666668
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9928114583333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9761510416666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9844812500000002
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9934947916666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9840500000000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 776,
          "fn": 24,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 746,
          "fn": 54,
          "accuracy": 0.9325
        }
      },
      "auroc": 0.9887723958333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9878375
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9878375
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9819416666666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9819416666666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9848895833333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9848895833333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.8193854166666668
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.8193854166666668
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.7645427083333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.7645427083333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 244,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 84,
          "fn": 316,
          "accuracy": 0.21
        }
      },
      "auroc": 0.7919640625
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 244,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 84,
          "fn": 316,
          "accuracy": 0.21
        }
      },
      "auroc": 0.7919640625
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9928395833333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9928395833333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9908625
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9908625
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9918510416666668
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9918510416666668
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9931125000000001
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9931125000000001
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9615854166666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.9615854166666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9773489583333332
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9773489583333332
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.901071875
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.901071875
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        }
      },
      "auroc": 0.8529697916666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        }
      },
      "auroc": 0.8529697916666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        }
      },
      "auroc": 0.8770208333333332
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        }
      },
      "auroc": 0.8770208333333332
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2005,
          "fn": 195,
          "accuracy": 0.9113636363636364
        },
        "0.01": {
          "tp": 1915,
          "fn": 285,
          "accuracy": 0.8704545454545455
        }
      },
      "auroc": 0.969748484848485
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1188,
          "fn": 12,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 1148,
          "fn": 52,
          "accuracy": 0.9566666666666667
        }
      },
      "auroc": 0.9930619791666668
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3193,
          "fn": 207,
          "accuracy": 0.9391176470588235
        },
        "0.01": {
          "tp": 3063,
          "fn": 337,
          "accuracy": 0.9008823529411765
        }
      },
      "auroc": 0.9779767769607842
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1441,
          "fn": 759,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 1280,
          "fn": 920,
          "accuracy": 0.5818181818181818
        }
      },
      "auroc": 0.8778245265151515
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1024,
          "fn": 176,
          "accuracy": 0.8533333333333334
        },
        "0.01": {
          "tp": 943,
          "fn": 257,
          "accuracy": 0.7858333333333334
        }
      },
      "auroc": 0.9556227430555555
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2465,
          "fn": 935,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 2223,
          "fn": 1177,
          "accuracy": 0.6538235294117647
        }
      },
      "auroc": 0.9052827205882353
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3446,
          "fn": 954,
          "accuracy": 0.7831818181818182
        },
        "0.01": {
          "tp": 3195,
          "fn": 1205,
          "accuracy": 0.7261363636363637
        }
      },
      "auroc": 0.9237865056818182
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2212,
          "fn": 188,
          "accuracy": 0.9216666666666666
        },
        "0.01": {
          "tp": 2091,
          "fn": 309,
          "accuracy": 0.87125
        }
      },
      "auroc": 0.974342361111111
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 5658,
          "fn": 1142,
          "accuracy": 0.8320588235294117
        },
        "0.01": {
          "tp": 5286,
          "fn": 1514,
          "accuracy": 0.7773529411764706
        }
      },
      "auroc": 0.9416297487745099
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9944479166666668
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9897229166666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.9920854166666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9922666666666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.9762604166666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        }
      },
      "auroc": 0.9842635416666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9933572916666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9829916666666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 772,
          "fn": 28,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 700,
          "fn": 100,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9881744791666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9961614583333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9961781249999999
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9961697916666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.6737968750000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.996234375
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        },
        "0.01": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        }
      },
      "auroc": 0.835015625
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        },
        "0.01": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        }
      },
      "auroc": 0.8349791666666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.99620625
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 631,
          "fn": 169,
          "accuracy": 0.78875
        },
        "0.01": {
          "tp": 619,
          "fn": 181,
          "accuracy": 0.77375
        }
      },
      "auroc": 0.9155927083333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9863677083333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9835041666666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9849359375
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9853145833333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.974715625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9800151041666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        }
      },
      "auroc": 0.9858411458333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        }
      },
      "auroc": 0.9791098958333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 751,
          "fn": 49,
          "accuracy": 0.93875
        },
        "0.01": {
          "tp": 644,
          "fn": 156,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9824755208333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.9392541666666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        }
      },
      "auroc": 0.9678562500000001
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.6008614583333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.8215822916666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 315,
          "accuracy": 0.2125
        },
        "0.01": {
          "tp": 47,
          "fn": 353,
          "accuracy": 0.1175
        }
      },
      "auroc": 0.711221875
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.7986598958333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        },
        "0.01": {
          "tp": 131,
          "fn": 269,
          "accuracy": 0.3275
        }
      },
      "auroc": 0.8804182291666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 426,
          "fn": 374,
          "accuracy": 0.5325
        },
        "0.01": {
          "tp": 332,
          "fn": 468,
          "accuracy": 0.415
        }
      },
      "auroc": 0.8395390625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964333333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9836083333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9900208333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        }
      },
      "auroc": 0.7361229166666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9764427083333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        }
      },
      "auroc": 0.8562828124999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 271,
          "fn": 129,
          "accuracy": 0.6775
        },
        "0.01": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        }
      },
      "auroc": 0.8662781250000001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        }
      },
      "auroc": 0.9800255208333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 643,
          "fn": 157,
          "accuracy": 0.80375
        },
        "0.01": {
          "tp": 610,
          "fn": 190,
          "accuracy": 0.7625
        }
      },
      "auroc": 0.9231518229166666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9903760416666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9874552083333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9889156250000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.97920625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        }
      },
      "auroc": 0.9570718750000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        }
      },
      "auroc": 0.9681390625000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        }
      },
      "auroc": 0.9847911458333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9722635416666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 727,
          "fn": 73,
          "accuracy": 0.90875
        },
        "0.01": {
          "tp": 651,
          "fn": 149,
          "accuracy": 0.81375
        }
      },
      "auroc": 0.97852734375
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.9682479166666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.9682479166666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.9564364583333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.9564364583333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        }
      },
      "auroc": 0.9623421875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        }
      },
      "auroc": 0.9623421875
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.7610083333333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.7610083333333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        },
        "0.01": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        }
      },
      "auroc": 0.6917583333333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        },
        "0.01": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        }
      },
      "auroc": 0.6917583333333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 305,
          "accuracy": 0.2375
        },
        "0.01": {
          "tp": 52,
          "fn": 348,
          "accuracy": 0.13
        }
      },
      "auroc": 0.7263833333333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 305,
          "accuracy": 0.2375
        },
        "0.01": {
          "tp": 52,
          "fn": 348,
          "accuracy": 0.13
        }
      },
      "auroc": 0.7263833333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9879791666666666
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9879791666666666
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9791197916666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9791197916666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9835494791666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9835494791666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9812666666666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9812666666666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.8913395833333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.8913395833333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 301,
          "fn": 99,
          "accuracy": 0.7525
        },
        "0.01": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        }
      },
      "auroc": 0.9363031250000001
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 301,
          "fn": 99,
          "accuracy": 0.7525
        },
        "0.01": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        }
      },
      "auroc": 0.9363031250000001
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.8566270833333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.8566270833333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        },
        "0.01": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        }
      },
      "auroc": 0.8099645833333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        },
        "0.01": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        }
      },
      "auroc": 0.8099645833333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 122,
          "fn": 278,
          "accuracy": 0.305
        }
      },
      "auroc": 0.8332958333333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 122,
          "fn": 278,
          "accuracy": 0.305
        }
      },
      "auroc": 0.8332958333333333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1898,
          "fn": 302,
          "accuracy": 0.8627272727272727
        },
        "0.01": {
          "tp": 1737,
          "fn": 463,
          "accuracy": 0.7895454545454546
        }
      },
      "auroc": 0.9559430871212121
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1107,
          "fn": 93,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 982,
          "fn": 218,
          "accuracy": 0.8183333333333334
        }
      },
      "auroc": 0.9799538194444444
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3005,
          "fn": 395,
          "accuracy": 0.8838235294117647
        },
        "0.01": {
          "tp": 2719,
          "fn": 681,
          "accuracy": 0.7997058823529412
        }
      },
      "auroc": 0.9644174632352941
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1262,
          "fn": 938,
          "accuracy": 0.5736363636363636
        },
        "0.01": {
          "tp": 1033,
          "fn": 1167,
          "accuracy": 0.46954545454545454
        }
      },
      "auroc": 0.8451079545454545
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 982,
          "fn": 218,
          "accuracy": 0.8183333333333334
        },
        "0.01": {
          "tp": 839,
          "fn": 361,
          "accuracy": 0.6991666666666667
        }
      },
      "auroc": 0.9503845486111111
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2244,
          "fn": 1156,
          "accuracy": 0.66
        },
        "0.01": {
          "tp": 1872,
          "fn": 1528,
          "accuracy": 0.5505882352941176
        }
      },
      "auroc": 0.8822643995098038
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3160,
          "fn": 1240,
          "accuracy": 0.7181818181818181
        },
        "0.01": {
          "tp": 2770,
          "fn": 1630,
          "accuracy": 0.6295454545454545
        }
      },
      "auroc": 0.9005255208333334
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2089,
          "fn": 311,
          "accuracy": 0.8704166666666666
        },
        "0.01": {
          "tp": 1821,
          "fn": 579,
          "accuracy": 0.75875
        }
      },
      "auroc": 0.9651691840277777
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 5249,
          "fn": 1551,
          "accuracy": 0.7719117647058824
        },
        "0.01": {
          "tp": 4591,
          "fn": 2209,
          "accuracy": 0.6751470588235294
        }
      },
      "auroc": 0.9233409313725489
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964083333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9960072916666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9962078125
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964333333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9899979166666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.993215625
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964208333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9930026041666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 792,
          "fn": 8,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9947117187500001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9962604166666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9960552083333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9961578124999999
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.64801875
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9962249999999999
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 232,
          "fn": 168,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        }
      },
      "auroc": 0.822121875
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 232,
          "fn": 168,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        }
      },
      "auroc": 0.8221395833333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9961401041666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 632,
          "fn": 168,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 616,
          "fn": 184,
          "accuracy": 0.77
        }
      },
      "auroc": 0.90913984375
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9925541666666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.993328125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9929411458333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9949489583333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9856374999999999
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9902932291666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9937515625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9894828125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        },
        "0.01": {
          "tp": 760,
          "fn": 40,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9916171874999999
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9866395833333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9915489583333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.5409854166666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.6583135416666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        },
        "0.01": {
          "tp": 31,
          "fn": 369,
          "accuracy": 0.0775
        }
      },
      "auroc": 0.5996494791666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        },
        "0.01": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.768721875
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        },
        "0.01": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        }
      },
      "auroc": 0.8224765625
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 446,
          "fn": 354,
          "accuracy": 0.5575
        },
        "0.01": {
          "tp": 394,
          "fn": 406,
          "accuracy": 0.4925
        }
      },
      "auroc": 0.79559921875
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9881614583333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9923098958333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.6979645833333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9489635416666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        },
        "0.01": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        }
      },
      "auroc": 0.8234640625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 271,
          "fn": 129,
          "accuracy": 0.6775
        },
        "0.01": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        }
      },
      "auroc": 0.8472114583333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9685625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 639,
          "fn": 161,
          "accuracy": 0.79875
        },
        "0.01": {
          "tp": 603,
          "fn": 197,
          "accuracy": 0.75375
        }
      },
      "auroc": 0.9078869791666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9940333333333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9919020833333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9929677083333335
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9899302083333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9604541666666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9751921874999999
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9919817708333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9761781250000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 773,
          "fn": 27,
          "accuracy": 0.96625
        },
        "0.01": {
          "tp": 742,
          "fn": 58,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.9840799479166666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.985603125
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.985603125
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9780500000000001
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9780500000000001
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9818265625
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9818265625
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.7816447916666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.7816447916666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.7233125
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.7233125
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 259,
          "accuracy": 0.3525
        },
        "0.01": {
          "tp": 91,
          "fn": 309,
          "accuracy": 0.2275
        }
      },
      "auroc": 0.7524786458333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 259,
          "accuracy": 0.3525
        },
        "0.01": {
          "tp": 91,
          "fn": 309,
          "accuracy": 0.2275
        }
      },
      "auroc": 0.7524786458333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.993115625
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.993115625
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9921510416666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9921510416666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9926333333333335
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9926333333333335
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.993446875
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.993446875
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.9514364583333332
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.9514364583333332
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9724416666666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9724416666666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.8766083333333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.8766083333333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.8267375
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.8267375
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 232,
          "fn": 168,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        }
      },
      "auroc": 0.8516729166666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 232,
          "fn": 168,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        }
      },
      "auroc": 0.8516729166666666
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1988,
          "fn": 212,
          "accuracy": 0.9036363636363637
        },
        "0.01": {
          "tp": 1920,
          "fn": 280,
          "accuracy": 0.8727272727272727
        }
      },
      "auroc": 0.9638719696969698
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1181,
          "fn": 19,
          "accuracy": 0.9841666666666666
        },
        "0.01": {
          "tp": 1135,
          "fn": 65,
          "accuracy": 0.9458333333333333
        }
      },
      "auroc": 0.992015625
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3169,
          "fn": 231,
          "accuracy": 0.9320588235294117
        },
        "0.01": {
          "tp": 3055,
          "fn": 345,
          "accuracy": 0.8985294117647059
        }
      },
      "auroc": 0.9738050245098039
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1407,
          "fn": 793,
          "accuracy": 0.6395454545454545
        },
        "0.01": {
          "tp": 1278,
          "fn": 922,
          "accuracy": 0.5809090909090909
        }
      },
      "auroc": 0.8490880681818181
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 988,
          "fn": 212,
          "accuracy": 0.8233333333333334
        },
        "0.01": {
          "tp": 918,
          "fn": 282,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9232652777777777
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2395,
          "fn": 1005,
          "accuracy": 0.7044117647058824
        },
        "0.01": {
          "tp": 2196,
          "fn": 1204,
          "accuracy": 0.6458823529411765
        }
      },
      "auroc": 0.8752682598039215
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3395,
          "fn": 1005,
          "accuracy": 0.7715909090909091
        },
        "0.01": {
          "tp": 3198,
          "fn": 1202,
          "accuracy": 0.7268181818181818
        }
      },
      "auroc": 0.9064800189393939
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2169,
          "fn": 231,
          "accuracy": 0.90375
        },
        "0.01": {
          "tp": 2053,
          "fn": 347,
          "accuracy": 0.8554166666666667
        }
      },
      "auroc": 0.9576404513888888
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 5564,
          "fn": 1236,
          "accuracy": 0.8182352941176471
        },
        "0.01": {
          "tp": 5251,
          "fn": 1549,
          "accuracy": 0.7722058823529412
        }
      },
      "auroc": 0.9245366421568627
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.99518125
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9913635416666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9932723958333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9942854166666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9879989583333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9911421875
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9947333333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        }
      },
      "auroc": 0.98968125
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 785,
          "fn": 15,
          "accuracy": 0.98125
        },
        "0.01": {
          "tp": 730,
          "fn": 70,
          "accuracy": 0.9125
        }
      },
      "auroc": 0.9922072916666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9941385416666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.921221875
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        }
      },
      "auroc": 0.9576802083333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.853478125
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.8630604166666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 236,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 73,
          "fn": 327,
          "accuracy": 0.1825
        }
      },
      "auroc": 0.8582692708333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        }
      },
      "auroc": 0.9238083333333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        },
        "0.01": {
          "tp": 78,
          "fn": 322,
          "accuracy": 0.195
        }
      },
      "auroc": 0.8921411458333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 477,
          "fn": 323,
          "accuracy": 0.59625
        },
        "0.01": {
          "tp": 312,
          "fn": 488,
          "accuracy": 0.39
        }
      },
      "auroc": 0.9079747395833333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9884208333333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9876031250000001
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9880119791666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.988025
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9686677083333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        }
      },
      "auroc": 0.9783463541666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9882229166666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        }
      },
      "auroc": 0.9781354166666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 750,
          "fn": 50,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 657,
          "fn": 143,
          "accuracy": 0.82125
        }
      },
      "auroc": 0.9831791666666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9938291666666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.978403125
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        }
      },
      "auroc": 0.9861161458333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.834465625
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.7521395833333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 322,
          "accuracy": 0.195
        },
        "0.01": {
          "tp": 28,
          "fn": 372,
          "accuracy": 0.07
        }
      },
      "auroc": 0.7933026041666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        },
        "0.01": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        }
      },
      "auroc": 0.9141473958333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        },
        "0.01": {
          "tp": 163,
          "fn": 237,
          "accuracy": 0.4075
        }
      },
      "auroc": 0.8652713541666668
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 456,
          "fn": 344,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 381,
          "fn": 419,
          "accuracy": 0.47625
        }
      },
      "auroc": 0.8897093750000001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9948739583333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.9347479166666668
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        }
      },
      "auroc": 0.9648109375
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.867515625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        },
        "0.01": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        }
      },
      "auroc": 0.7919760416666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        },
        "0.01": {
          "tp": 35,
          "fn": 365,
          "accuracy": 0.0875
        }
      },
      "auroc": 0.8297458333333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 273,
          "fn": 127,
          "accuracy": 0.6825
        },
        "0.01": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        }
      },
      "auroc": 0.9311947916666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        },
        "0.01": {
          "tp": 78,
          "fn": 322,
          "accuracy": 0.195
        }
      },
      "auroc": 0.8633619791666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 446,
          "fn": 354,
          "accuracy": 0.5575
        },
        "0.01": {
          "tp": 301,
          "fn": 499,
          "accuracy": 0.37625
        }
      },
      "auroc": 0.8972783854166667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9905510416666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9927458333333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9916484375000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9855437499999999
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9709687499999999
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        }
      },
      "auroc": 0.97825625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9880473958333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9818572916666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 755,
          "fn": 45,
          "accuracy": 0.94375
        },
        "0.01": {
          "tp": 685,
          "fn": 115,
          "accuracy": 0.85625
        }
      },
      "auroc": 0.9849523437500001
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9795916666666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9795916666666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9728708333333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9728708333333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        }
      },
      "auroc": 0.9762312500000001
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        }
      },
      "auroc": 0.9762312500000001
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.8968812499999999
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.8968812499999999
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.8757052083333332
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.8757052083333332
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        },
        "0.01": {
          "tp": 134,
          "fn": 266,
          "accuracy": 0.335
        }
      },
      "auroc": 0.8862932291666668
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        },
        "0.01": {
          "tp": 134,
          "fn": 266,
          "accuracy": 0.335
        }
      },
      "auroc": 0.8862932291666668
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9914114583333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9914114583333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9865291666666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9865291666666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9889703125000001
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9889703125000001
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9897625
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9897625
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.9661552083333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.9661552083333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9779588541666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9779588541666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.9279739583333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.9279739583333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.9168624999999999
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.9168624999999999
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        },
        "0.01": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        }
      },
      "auroc": 0.9224182291666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        },
        "0.01": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        }
      },
      "auroc": 0.9224182291666667
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2008,
          "fn": 192,
          "accuracy": 0.9127272727272727
        },
        "0.01": {
          "tp": 1825,
          "fn": 375,
          "accuracy": 0.8295454545454546
        }
      },
      "auroc": 0.9766014204545455
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1015,
          "fn": 185,
          "accuracy": 0.8458333333333333
        },
        "0.01": {
          "tp": 808,
          "fn": 392,
          "accuracy": 0.6733333333333333
        }
      },
      "auroc": 0.9676809027777777
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3023,
          "fn": 377,
          "accuracy": 0.8891176470588236
        },
        "0.01": {
          "tp": 2633,
          "fn": 767,
          "accuracy": 0.7744117647058824
        }
      },
      "auroc": 0.9734530024509803
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1564,
          "fn": 636,
          "accuracy": 0.7109090909090909
        },
        "0.01": {
          "tp": 1220,
          "fn": 980,
          "accuracy": 0.5545454545454546
        }
      },
      "auroc": 0.9310396780303032
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 677,
          "fn": 523,
          "accuracy": 0.5641666666666667
        },
        "0.01": {
          "tp": 503,
          "fn": 697,
          "accuracy": 0.4191666666666667
        }
      },
      "auroc": 0.8891352430555555
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2241,
          "fn": 1159,
          "accuracy": 0.6591176470588235
        },
        "0.01": {
          "tp": 1723,
          "fn": 1677,
          "accuracy": 0.5067647058823529
        }
      },
      "auroc": 0.9162498774509804
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3572,
          "fn": 828,
          "accuracy": 0.8118181818181818
        },
        "0.01": {
          "tp": 3045,
          "fn": 1355,
          "accuracy": 0.6920454545454545
        }
      },
      "auroc": 0.9538205492424242
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1692,
          "fn": 708,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 1311,
          "fn": 1089,
          "accuracy": 0.54625
        }
      },
      "auroc": 0.9284080729166666
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 5264,
          "fn": 1536,
          "accuracy": 0.7741176470588236
        },
        "0.01": {
          "tp": 4356,
          "fn": 2444,
          "accuracy": 0.6405882352941177
        }
      },
      "auroc": 0.9448514399509804
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9952052083333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9958317708333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9958317708333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        }
      },
      "auroc": 0.9961450520833334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9963906250000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9962114583333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9963010416666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.79568125
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9962249999999999
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 259,
          "fn": 141,
          "accuracy": 0.6475
        },
        "0.01": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        }
      },
      "auroc": 0.8959531250000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 259,
          "fn": 141,
          "accuracy": 0.6475
        },
        "0.01": {
          "tp": 228,
          "fn": 172,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8960359375000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9962182291666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 659,
          "fn": 141,
          "accuracy": 0.82375
        },
        "0.01": {
          "tp": 625,
          "fn": 175,
          "accuracy": 0.78125
        }
      },
      "auroc": 0.9461270833333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9941218749999999
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9960083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9950651041666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9960020833333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9905083333333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9932552083333335
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9950619791666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9932583333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 792,
          "fn": 8,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 782,
          "fn": 18,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.99416015625
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9942895833333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9953739583333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.7399729166666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.8461020833333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        },
        "0.01": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        }
      },
      "auroc": 0.7930374999999998
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.868215625
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        }
      },
      "auroc": 0.9201958333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 510,
          "fn": 290,
          "accuracy": 0.6375
        },
        "0.01": {
          "tp": 439,
          "fn": 361,
          "accuracy": 0.54875
        }
      },
      "auroc": 0.8942057291666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9937135416666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9950859375
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.8306333333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9745666666666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        },
        "0.01": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        }
      },
      "auroc": 0.9026
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        },
        "0.01": {
          "tp": 271,
          "fn": 129,
          "accuracy": 0.6775
        }
      },
      "auroc": 0.9135458333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        }
      },
      "auroc": 0.9841401041666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 663,
          "fn": 137,
          "accuracy": 0.82875
        },
        "0.01": {
          "tp": 632,
          "fn": 168,
          "accuracy": 0.79
        }
      },
      "auroc": 0.94884296875
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.99408125
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9921083333333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9930947916666668
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.99425625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9839739583333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9891151041666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.99416875
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9880411458333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 784,
          "fn": 16,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 770,
          "fn": 30,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9911049479166667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9907979166666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9907979166666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.988146875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.988146875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9894723958333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9894723958333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        }
      },
      "auroc": 0.8752479166666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        }
      },
      "auroc": 0.8752479166666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.8319572916666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.8319572916666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        }
      },
      "auroc": 0.8536026041666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        }
      },
      "auroc": 0.8536026041666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9931583333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9931583333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9926427083333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9926427083333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9929005208333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9929005208333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9939322916666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9939322916666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.984915625
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.984915625
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9894239583333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9894239583333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9352031249999999
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        }
      },
      "auroc": 0.9352031249999999
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.8892208333333332
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.8892208333333332
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        },
        "0.01": {
          "tp": 228,
          "fn": 172,
          "accuracy": 0.57
        }
      },
      "auroc": 0.9122119791666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        },
        "0.01": {
          "tp": 228,
          "fn": 172,
          "accuracy": 0.57
        }
      },
      "auroc": 0.9122119791666667
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2062,
          "fn": 138,
          "accuracy": 0.9372727272727273
        },
        "0.01": {
          "tp": 1985,
          "fn": 215,
          "accuracy": 0.9022727272727272
        }
      },
      "auroc": 0.9783916666666667
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1195,
          "fn": 5,
          "accuracy": 0.9958333333333333
        },
        "0.01": {
          "tp": 1177,
          "fn": 23,
          "accuracy": 0.9808333333333333
        }
      },
      "auroc": 0.9947982638888888
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3257,
          "fn": 143,
          "accuracy": 0.9579411764705882
        },
        "0.01": {
          "tp": 3162,
          "fn": 238,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9841822303921568
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1574,
          "fn": 626,
          "accuracy": 0.7154545454545455
        },
        "0.01": {
          "tp": 1406,
          "fn": 794,
          "accuracy": 0.639090909090909
        }
      },
      "auroc": 0.9127170454545455
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1039,
          "fn": 161,
          "accuracy": 0.8658333333333333
        },
        "0.01": {
          "tp": 974,
          "fn": 226,
          "accuracy": 0.8116666666666666
        }
      },
      "auroc": 0.9644302083333334
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2613,
          "fn": 787,
          "accuracy": 0.7685294117647059
        },
        "0.01": {
          "tp": 2380,
          "fn": 1020,
          "accuracy": 0.7
        }
      },
      "auroc": 0.93096875
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3636,
          "fn": 764,
          "accuracy": 0.8263636363636364
        },
        "0.01": {
          "tp": 3391,
          "fn": 1009,
          "accuracy": 0.7706818181818181
        }
      },
      "auroc": 0.9455543560606061
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2234,
          "fn": 166,
          "accuracy": 0.9308333333333333
        },
        "0.01": {
          "tp": 2151,
          "fn": 249,
          "accuracy": 0.89625
        }
      },
      "auroc": 0.9796142361111111
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 5870,
          "fn": 930,
          "accuracy": 0.8632352941176471
        },
        "0.01": {
          "tp": 5542,
          "fn": 1258,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9575754901960785
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9952052083333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9958317708333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9958317708333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        }
      },
      "auroc": 0.9961450520833334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9963906250000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9962114583333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9963010416666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.8000093749999999
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9962395833333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        }
      },
      "auroc": 0.8981244791666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        }
      },
      "auroc": 0.8981999999999999
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9962255208333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 662,
          "fn": 138,
          "accuracy": 0.8275
        },
        "0.01": {
          "tp": 626,
          "fn": 174,
          "accuracy": 0.7825
        }
      },
      "auroc": 0.9472127604166667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9941218749999999
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.99601875
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9950703125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9960020833333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.990803125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9934026041666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9950619791666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9934109375
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 792,
          "fn": 8,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 782,
          "fn": 18,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9942364583333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9945010416666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9954796875
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.7444010416666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.8461104166666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 287,
          "accuracy": 0.2825
        },
        "0.01": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        }
      },
      "auroc": 0.7952557291666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8704296874999999
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        }
      },
      "auroc": 0.9203057291666665
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 511,
          "fn": 289,
          "accuracy": 0.63875
        },
        "0.01": {
          "tp": 441,
          "fn": 359,
          "accuracy": 0.55125
        }
      },
      "auroc": 0.8953677083333332
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9937479166666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.995103125
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.8335677083333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9743572916666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        }
      },
      "auroc": 0.9039625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 273,
          "fn": 127,
          "accuracy": 0.6825
        }
      },
      "auroc": 0.9150130208333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9840526041666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 664,
          "fn": 136,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 633,
          "fn": 167,
          "accuracy": 0.79125
        }
      },
      "auroc": 0.9495328125
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.99408125
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9921083333333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9930947916666668
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.99425625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9839489583333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9891026041666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.99416875
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9880286458333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 784,
          "fn": 16,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 770,
          "fn": 30,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9910986979166667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.990875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.990875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9881375
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9881375
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.98950625
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.98950625
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        }
      },
      "auroc": 0.8770614583333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        }
      },
      "auroc": 0.8770614583333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.8350645833333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.8350645833333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        }
      },
      "auroc": 0.8560630208333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        }
      },
      "auroc": 0.8560630208333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9931583333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9931583333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9926427083333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9926427083333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9929005208333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9929005208333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9939322916666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9939322916666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9852447916666668
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9852447916666668
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9895885416666668
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9895885416666668
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.936628125
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.936628125
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8909729166666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8909729166666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        }
      },
      "auroc": 0.9138005208333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        }
      },
      "auroc": 0.9138005208333333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2063,
          "fn": 137,
          "accuracy": 0.9377272727272727
        },
        "0.01": {
          "tp": 1988,
          "fn": 212,
          "accuracy": 0.9036363636363637
        }
      },
      "auroc": 0.9786930871212121
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1195,
          "fn": 5,
          "accuracy": 0.9958333333333333
        },
        "0.01": {
          "tp": 1179,
          "fn": 21,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9948409722222222
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3258,
          "fn": 142,
          "accuracy": 0.9582352941176471
        },
        "0.01": {
          "tp": 3167,
          "fn": 233,
          "accuracy": 0.9314705882352942
        }
      },
      "auroc": 0.9843923406862746
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1581,
          "fn": 619,
          "accuracy": 0.7186363636363636
        },
        "0.01": {
          "tp": 1410,
          "fn": 790,
          "accuracy": 0.6409090909090909
        }
      },
      "auroc": 0.9142506628787879
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1039,
          "fn": 161,
          "accuracy": 0.8658333333333333
        },
        "0.01": {
          "tp": 973,
          "fn": 227,
          "accuracy": 0.8108333333333333
        }
      },
      "auroc": 0.9644440972222224
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2620,
          "fn": 780,
          "accuracy": 0.7705882352941177
        },
        "0.01": {
          "tp": 2383,
          "fn": 1017,
          "accuracy": 0.7008823529411765
        }
      },
      "auroc": 0.9319659926470588
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3644,
          "fn": 756,
          "accuracy": 0.8281818181818181
        },
        "0.01": {
          "tp": 3398,
          "fn": 1002,
          "accuracy": 0.7722727272727272
        }
      },
      "auroc": 0.9464718750000001
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2234,
          "fn": 166,
          "accuracy": 0.9308333333333333
        },
        "0.01": {
          "tp": 2152,
          "fn": 248,
          "accuracy": 0.8966666666666666
        }
      },
      "auroc": 0.9796425347222222
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 5878,
          "fn": 922,
          "accuracy": 0.8644117647058823
        },
        "0.01": {
          "tp": 5550,
          "fn": 1250,
          "accuracy": 0.8161764705882353
        }
      },
      "auroc": 0.9581791666666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.6458291666666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.55940625
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 43,
          "fn": 357,
          "accuracy": 0.1075
        },
        "0.01": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        }
      },
      "auroc": 0.6026177083333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        },
        "0.01": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        }
      },
      "auroc": 0.6396083333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.5053291666666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 364,
          "accuracy": 0.09
        },
        "0.01": {
          "tp": 11,
          "fn": 389,
          "accuracy": 0.0275
        }
      },
      "auroc": 0.5724687500000001
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 339,
          "accuracy": 0.1525
        },
        "0.01": {
          "tp": 23,
          "fn": 377,
          "accuracy": 0.0575
        }
      },
      "auroc": 0.64271875
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.5323677083333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 721,
          "accuracy": 0.09875
        },
        "0.01": {
          "tp": 26,
          "fn": 774,
          "accuracy": 0.0325
        }
      },
      "auroc": 0.5875432291666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9816729166666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.5141510416666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": {
          "tp": 181,
          "fn": 219,
          "accuracy": 0.4525
        }
      },
      "auroc": 0.7479119791666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.46095729166666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.45030937500000007
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 40,
          "fn": 360,
          "accuracy": 0.1
        },
        "0.01": {
          "tp": 33,
          "fn": 367,
          "accuracy": 0.0825
        }
      },
      "auroc": 0.45563333333333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        }
      },
      "auroc": 0.7213151041666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 373,
          "accuracy": 0.0675
        },
        "0.01": {
          "tp": 24,
          "fn": 376,
          "accuracy": 0.06
        }
      },
      "auroc": 0.4822302083333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 561,
          "accuracy": 0.29875
        },
        "0.01": {
          "tp": 214,
          "fn": 586,
          "accuracy": 0.2675
        }
      },
      "auroc": 0.6017726562500001
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.7173041666666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.5483979166666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 60,
          "fn": 340,
          "accuracy": 0.15
        },
        "0.01": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        }
      },
      "auroc": 0.6328510416666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.6409666666666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.4661791666666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 41,
          "fn": 359,
          "accuracy": 0.1025
        },
        "0.01": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        }
      },
      "auroc": 0.5535729166666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 83,
          "fn": 317,
          "accuracy": 0.2075
        },
        "0.01": {
          "tp": 24,
          "fn": 376,
          "accuracy": 0.06
        }
      },
      "auroc": 0.6791354166666668
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.5072885416666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 699,
          "accuracy": 0.12625
        },
        "0.01": {
          "tp": 26,
          "fn": 774,
          "accuracy": 0.0325
        }
      },
      "auroc": 0.5932119791666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9950135416666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.8212999999999999
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        }
      },
      "auroc": 0.9081567708333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.37184375000000003
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.2925510416666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.3321973958333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        }
      },
      "auroc": 0.6834286458333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 332,
          "accuracy": 0.17
        },
        "0.01": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        }
      },
      "auroc": 0.5569255208333332
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 530,
          "accuracy": 0.3375
        },
        "0.01": {
          "tp": 218,
          "fn": 582,
          "accuracy": 0.2725
        }
      },
      "auroc": 0.6201770833333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9951989583333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9421614583333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9686802083333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        },
        "0.01": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        }
      },
      "auroc": 0.5912697916666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        }
      },
      "auroc": 0.533215625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 280,
          "accuracy": 0.3
        },
        "0.01": {
          "tp": 115,
          "fn": 285,
          "accuracy": 0.2875
        }
      },
      "auroc": 0.5622427083333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        },
        "0.01": {
          "tp": 256,
          "fn": 144,
          "accuracy": 0.64
        }
      },
      "auroc": 0.793234375
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        },
        "0.01": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        }
      },
      "auroc": 0.7376885416666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 490,
          "fn": 310,
          "accuracy": 0.6125
        },
        "0.01": {
          "tp": 483,
          "fn": 317,
          "accuracy": 0.60375
        }
      },
      "auroc": 0.7654614583333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.7912177083333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.611228125
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 301,
          "accuracy": 0.2475
        },
        "0.01": {
          "tp": 47,
          "fn": 353,
          "accuracy": 0.1175
        }
      },
      "auroc": 0.7012229166666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        },
        "0.01": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        }
      },
      "auroc": 0.62879375
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.423546875
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 367,
          "accuracy": 0.0825
        },
        "0.01": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        }
      },
      "auroc": 0.5261703125
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 284,
          "accuracy": 0.29
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.7100057291666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 384,
          "accuracy": 0.04
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.5173875000000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 668,
          "accuracy": 0.165
        },
        "0.01": {
          "tp": 57,
          "fn": 743,
          "accuracy": 0.07125
        }
      },
      "auroc": 0.6136966145833334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.8142552083333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.8142552083333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.7512156249999999
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.7512156249999999
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 257,
          "accuracy": 0.3575
        },
        "0.01": {
          "tp": 68,
          "fn": 332,
          "accuracy": 0.17
        }
      },
      "auroc": 0.7827354166666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 257,
          "accuracy": 0.3575
        },
        "0.01": {
          "tp": 68,
          "fn": 332,
          "accuracy": 0.17
        }
      },
      "auroc": 0.7827354166666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.45057187500000007
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.45057187500000007
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.43547499999999995
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.43547499999999995
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 374,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        }
      },
      "auroc": 0.4430234375
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 374,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        }
      },
      "auroc": 0.4430234375
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.6720739583333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.6720739583333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.5833458333333332
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.5833458333333332
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 341,
          "accuracy": 0.1475
        },
        "0.01": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        }
      },
      "auroc": 0.6277098958333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 341,
          "accuracy": 0.1475
        },
        "0.01": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        }
      },
      "auroc": 0.6277098958333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.5976864583333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.5976864583333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.33218645833333327
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.33218645833333327
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 381,
          "accuracy": 0.0475
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.4649364583333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 381,
          "accuracy": 0.0475
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.4649364583333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.5184791666666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.5184791666666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.4142541666666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.4142541666666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 39,
          "fn": 361,
          "accuracy": 0.0975
        },
        "0.01": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        }
      },
      "auroc": 0.4663666666666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 39,
          "fn": 361,
          "accuracy": 0.0975
        },
        "0.01": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        }
      },
      "auroc": 0.4663666666666667
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 932,
          "fn": 1268,
          "accuracy": 0.42363636363636364
        },
        "0.01": {
          "tp": 711,
          "fn": 1489,
          "accuracy": 0.3231818181818182
        }
      },
      "auroc": 0.7435730113636363
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 282,
          "fn": 918,
          "accuracy": 0.235
        },
        "0.01": {
          "tp": 208,
          "fn": 992,
          "accuracy": 0.17333333333333334
        }
      },
      "auroc": 0.6661074652777778
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1214,
          "fn": 2186,
          "accuracy": 0.35705882352941176
        },
        "0.01": {
          "tp": 919,
          "fn": 2481,
          "accuracy": 0.2702941176470588
        }
      },
      "auroc": 0.7162322303921569
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 289,
          "fn": 1911,
          "accuracy": 0.13136363636363638
        },
        "0.01": {
          "tp": 139,
          "fn": 2061,
          "accuracy": 0.06318181818181819
        }
      },
      "auroc": 0.531810606060606
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 94,
          "fn": 1106,
          "accuracy": 0.07833333333333334
        },
        "0.01": {
          "tp": 76,
          "fn": 1124,
          "accuracy": 0.06333333333333334
        }
      },
      "auroc": 0.44518854166666666
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 3017,
          "accuracy": 0.1126470588235294
        },
        "0.01": {
          "tp": 215,
          "fn": 3185,
          "accuracy": 0.06323529411764706
        }
      },
      "auroc": 0.5012381127450981
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1221,
          "fn": 3179,
          "accuracy": 0.2775
        },
        "0.01": {
          "tp": 850,
          "fn": 3550,
          "accuracy": 0.19318181818181818
        }
      },
      "auroc": 0.6376918087121213
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 2024,
          "accuracy": 0.15666666666666668
        },
        "0.01": {
          "tp": 284,
          "fn": 2116,
          "accuracy": 0.11833333333333333
        }
      },
      "auroc": 0.5556480034722223
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 5203,
          "accuracy": 0.2348529411764706
        },
        "0.01": {
          "tp": 1134,
          "fn": 5666,
          "accuracy": 0.16676470588235295
        }
      },
      "auroc": 0.6087351715686273
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9939614583333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9952098958333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9952098958333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9958341145833333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9960697916666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9962114583333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.996140625
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        },
        "0.01": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        }
      },
      "auroc": 0.7520854166666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9962249999999999
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 236,
          "fn": 164,
          "accuracy": 0.59
        },
        "0.01": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        }
      },
      "auroc": 0.8741552083333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        },
        "0.01": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        }
      },
      "auroc": 0.8740776041666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9962182291666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 635,
          "fn": 165,
          "accuracy": 0.79375
        },
        "0.01": {
          "tp": 620,
          "fn": 180,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9351479166666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9938447916666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9954375
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9946411458333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.995303125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.989428125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.992365625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9945739583333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9924328125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 789,
          "fn": 11,
          "accuracy": 0.98625
        },
        "0.01": {
          "tp": 770,
          "fn": 30,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9935033854166667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9893270833333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.9928927083333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.6767104166666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.8604124999999999
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 290,
          "accuracy": 0.275
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.7685614583333332
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        }
      },
      "auroc": 0.836584375
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        }
      },
      "auroc": 0.9248697916666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 504,
          "fn": 296,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 425,
          "fn": 375,
          "accuracy": 0.53125
        }
      },
      "auroc": 0.8807270833333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9932562500000001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9948572916666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        },
        "0.01": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        }
      },
      "auroc": 0.7964541666666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9795083333333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 259,
          "fn": 141,
          "accuracy": 0.6475
        },
        "0.01": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        }
      },
      "auroc": 0.8879812499999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        },
        "0.01": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        }
      },
      "auroc": 0.89645625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        }
      },
      "auroc": 0.9863822916666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 656,
          "fn": 144,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 629,
          "fn": 171,
          "accuracy": 0.78625
        }
      },
      "auroc": 0.9414192708333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9940854166666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9921083333333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9930968750000002
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9929010416666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.983578125
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9882395833333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9934932291666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        }
      },
      "auroc": 0.9878432291666668
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 780,
          "fn": 20,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 759,
          "fn": 41,
          "accuracy": 0.94875
        }
      },
      "auroc": 0.9906682291666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9863479166666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9863479166666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9803999999999999
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9803999999999999
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9833739583333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9833739583333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.8285427083333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.8285427083333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.7823104166666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.7823104166666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 245,
          "accuracy": 0.3875
        },
        "0.01": {
          "tp": 101,
          "fn": 299,
          "accuracy": 0.2525
        }
      },
      "auroc": 0.8054265625000001
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 245,
          "accuracy": 0.3875
        },
        "0.01": {
          "tp": 101,
          "fn": 299,
          "accuracy": 0.2525
        }
      },
      "auroc": 0.8054265625000001
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9931333333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9931333333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9914000000000001
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9914000000000001
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9922666666666666
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9922666666666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9935145833333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9935145833333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9703802083333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9703802083333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9819473958333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9819473958333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.90856875
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.90856875
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.8618166666666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.8618166666666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        }
      },
      "auroc": 0.8851927083333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        }
      },
      "auroc": 0.8851927083333333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2007,
          "fn": 193,
          "accuracy": 0.9122727272727272
        },
        "0.01": {
          "tp": 1924,
          "fn": 276,
          "accuracy": 0.8745454545454545
        }
      },
      "auroc": 0.9712256628787879
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1189,
          "fn": 11,
          "accuracy": 0.9908333333333333
        },
        "0.01": {
          "tp": 1154,
          "fn": 46,
          "accuracy": 0.9616666666666667
        }
      },
      "auroc": 0.9937998263888889
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3196,
          "fn": 204,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 3078,
          "fn": 322,
          "accuracy": 0.9052941176470588
        }
      },
      "auroc": 0.9791930147058825
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1449,
          "fn": 751,
          "accuracy": 0.6586363636363637
        },
        "0.01": {
          "tp": 1304,
          "fn": 896,
          "accuracy": 0.5927272727272728
        }
      },
      "auroc": 0.8905654356060606
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1056,
          "fn": 144,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 978,
          "fn": 222,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9671855902777777
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2505,
          "fn": 895,
          "accuracy": 0.736764705882353
        },
        "0.01": {
          "tp": 2282,
          "fn": 1118,
          "accuracy": 0.6711764705882353
        }
      },
      "auroc": 0.9176078431372549
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3456,
          "fn": 944,
          "accuracy": 0.7854545454545454
        },
        "0.01": {
          "tp": 3228,
          "fn": 1172,
          "accuracy": 0.7336363636363636
        }
      },
      "auroc": 0.9308955492424242
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2245,
          "fn": 155,
          "accuracy": 0.9354166666666667
        },
        "0.01": {
          "tp": 2132,
          "fn": 268,
          "accuracy": 0.8883333333333333
        }
      },
      "auroc": 0.9804927083333335
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 5701,
          "fn": 1099,
          "accuracy": 0.8383823529411765
        },
        "0.01": {
          "tp": 5360,
          "fn": 1440,
          "accuracy": 0.788235294117647
        }
      },
      "auroc": 0.9484004289215686
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964333333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.996284375
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9963588541666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9930208333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9947395833333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964458333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9946526041666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.99554921875
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9963906250000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9960270833333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9962088541666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.7557895833333335
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9961833333333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        }
      },
      "auroc": 0.8759864583333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        }
      },
      "auroc": 0.8760901041666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9961052083333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 649,
          "fn": 151,
          "accuracy": 0.81125
        },
        "0.01": {
          "tp": 623,
          "fn": 177,
          "accuracy": 0.77875
        }
      },
      "auroc": 0.93609765625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.994078125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9954895833333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9947838541666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9956041666666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9886322916666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9921182291666668
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9948411458333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9920609375000001
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 789,
          "fn": 11,
          "accuracy": 0.98625
        },
        "0.01": {
          "tp": 773,
          "fn": 27,
          "accuracy": 0.96625
        }
      },
      "auroc": 0.9934510416666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9926833333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9945708333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.6770093749999999
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.7641968749999999
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 307,
          "accuracy": 0.2325
        },
        "0.01": {
          "tp": 41,
          "fn": 359,
          "accuracy": 0.1025
        }
      },
      "auroc": 0.720603125
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        },
        "0.01": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        }
      },
      "auroc": 0.8367338541666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        },
        "0.01": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        }
      },
      "auroc": 0.8784401041666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 490,
          "fn": 310,
          "accuracy": 0.6125
        },
        "0.01": {
          "tp": 423,
          "fn": 377,
          "accuracy": 0.52875
        }
      },
      "auroc": 0.8575869791666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9964583333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9909229166666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.993690625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.8008510416666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9601614583333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        }
      },
      "auroc": 0.8805062499999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        }
      },
      "auroc": 0.8986546875000001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9755421875000001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 653,
          "fn": 147,
          "accuracy": 0.81625
        },
        "0.01": {
          "tp": 621,
          "fn": 179,
          "accuracy": 0.77625
        }
      },
      "auroc": 0.9370984375
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.99408125
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9917958333333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9929385416666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9936635416666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9760854166666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        }
      },
      "auroc": 0.9848744791666668
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9938723958333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9839406249999999
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        },
        "0.01": {
          "tp": 755,
          "fn": 45,
          "accuracy": 0.94375
        }
      },
      "auroc": 0.9889065104166668
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9891958333333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9891958333333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9864625
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9864625
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9878291666666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9878291666666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.8391395833333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.8391395833333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.7911197916666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.7911197916666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 122,
          "fn": 278,
          "accuracy": 0.305
        }
      },
      "auroc": 0.8151296875
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 122,
          "fn": 278,
          "accuracy": 0.305
        }
      },
      "auroc": 0.8151296875
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9931583333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9931583333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9925572916666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9925572916666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9928578125
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9928578125
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9936770833333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9936770833333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.97305625
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.97305625
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9833666666666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9833666666666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.9159447916666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.9159447916666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        }
      },
      "auroc": 0.8565916666666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        }
      },
      "auroc": 0.8565916666666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        }
      },
      "auroc": 0.8862682291666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        }
      },
      "auroc": 0.8862682291666666
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2034,
          "fn": 166,
          "accuracy": 0.9245454545454546
        },
        "0.01": {
          "tp": 1957,
          "fn": 243,
          "accuracy": 0.8895454545454545
        }
      },
      "auroc": 0.9731832386363637
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1191,
          "fn": 9,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 1155,
          "fn": 45,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9938671875
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3225,
          "fn": 175,
          "accuracy": 0.9485294117647058
        },
        "0.01": {
          "tp": 3112,
          "fn": 288,
          "accuracy": 0.9152941176470588
        }
      },
      "auroc": 0.980483455882353
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1523,
          "fn": 677,
          "accuracy": 0.6922727272727273
        },
        "0.01": {
          "tp": 1366,
          "fn": 834,
          "accuracy": 0.6209090909090909
        }
      },
      "auroc": 0.8926512310606061
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1009,
          "fn": 191,
          "accuracy": 0.8408333333333333
        },
        "0.01": {
          "tp": 948,
          "fn": 252,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9463800347222223
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2532,
          "fn": 868,
          "accuracy": 0.7447058823529412
        },
        "0.01": {
          "tp": 2314,
          "fn": 1086,
          "accuracy": 0.6805882352941176
        }
      },
      "auroc": 0.9116143382352941
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3557,
          "fn": 843,
          "accuracy": 0.8084090909090909
        },
        "0.01": {
          "tp": 3323,
          "fn": 1077,
          "accuracy": 0.7552272727272727
        }
      },
      "auroc": 0.9329172348484849
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2200,
          "fn": 200,
          "accuracy": 0.9166666666666666
        },
        "0.01": {
          "tp": 2103,
          "fn": 297,
          "accuracy": 0.87625
        }
      },
      "auroc": 0.9701236111111111
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 5757,
          "fn": 1043,
          "accuracy": 0.8466176470588235
        },
        "0.01": {
          "tp": 5426,
          "fn": 1374,
          "accuracy": 0.7979411764705883
        }
      },
      "auroc": 0.9460488970588237
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.29963541666666665
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.2645020833333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.28206875000000003
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.3179885416666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.23863750000000003
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.2783130208333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.3088119791666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.2515697916666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 799,
          "accuracy": 0.00125
        },
        "0.01": {
          "tp": 1,
          "fn": 799,
          "accuracy": 0.00125
        }
      },
      "auroc": 0.2801908854166667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.9080114583333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.4483989583333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 263,
          "accuracy": 0.3425
        },
        "0.01": {
          "tp": 109,
          "fn": 291,
          "accuracy": 0.2725
        }
      },
      "auroc": 0.6782052083333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.36558854166666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.4431916666666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 28,
          "fn": 372,
          "accuracy": 0.07
        },
        "0.01": {
          "tp": 24,
          "fn": 376,
          "accuracy": 0.06
        }
      },
      "auroc": 0.40439010416666665
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        },
        "0.01": {
          "tp": 118,
          "fn": 282,
          "accuracy": 0.295
        }
      },
      "auroc": 0.6368
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 383,
          "accuracy": 0.0425
        },
        "0.01": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        }
      },
      "auroc": 0.44579531250000004
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 635,
          "accuracy": 0.20625
        },
        "0.01": {
          "tp": 133,
          "fn": 667,
          "accuracy": 0.16625
        }
      },
      "auroc": 0.54129765625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.32828854166666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.265334375
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.29681145833333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.3007666666666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.25744374999999997
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.2791052083333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.3145276041666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.2613890625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        }
      },
      "auroc": 0.2879583333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.976584375
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.4428020833333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        },
        "0.01": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        }
      },
      "auroc": 0.7096932291666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.278321875
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.2748885416666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.2766052083333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        },
        "0.01": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        }
      },
      "auroc": 0.627453125
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.35884531249999996
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 614,
          "accuracy": 0.2325
        },
        "0.01": {
          "tp": 168,
          "fn": 632,
          "accuracy": 0.21
        }
      },
      "auroc": 0.49314921875
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9829260416666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9076270833333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9452765625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.5156281249999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        }
      },
      "auroc": 0.5192072916666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 287,
          "accuracy": 0.2825
        },
        "0.01": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        }
      },
      "auroc": 0.5174177083333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 236,
          "fn": 164,
          "accuracy": 0.59
        }
      },
      "auroc": 0.7492770833333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        },
        "0.01": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        }
      },
      "auroc": 0.7134171875
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 469,
          "fn": 331,
          "accuracy": 0.58625
        },
        "0.01": {
          "tp": 462,
          "fn": 338,
          "accuracy": 0.5775
        }
      },
      "auroc": 0.7313471354166667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.4059135416666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.241778125
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.3238458333333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.29264375
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.22735625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.26
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.34927864583333335
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.2345671875
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 794,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 3,
          "fn": 797,
          "accuracy": 0.00375
        }
      },
      "auroc": 0.2919229166666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.4706937499999999
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.4706937499999999
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.4059864583333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.4059864583333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 387,
          "accuracy": 0.0325
        },
        "0.01": {
          "tp": 7,
          "fn": 393,
          "accuracy": 0.0175
        }
      },
      "auroc": 0.4383401041666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 387,
          "accuracy": 0.0325
        },
        "0.01": {
          "tp": 7,
          "fn": 393,
          "accuracy": 0.0175
        }
      },
      "auroc": 0.4383401041666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.296259375
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.296259375
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.27684791666666664
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.27684791666666664
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.2865536458333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.2865536458333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.319890625
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.319890625
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.28116875
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.28116875
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.30052968750000003
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.30052968750000003
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.28955000000000003
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.28955000000000003
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.18762916666666668
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.18762916666666668
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.23858958333333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.23858958333333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.29039895833333335
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.29039895833333335
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.2723375
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.2723375
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.2813682291666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.2813682291666667
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 515,
          "fn": 1685,
          "accuracy": 0.2340909090909091
        },
        "0.01": {
          "tp": 460,
          "fn": 1740,
          "accuracy": 0.20909090909090908
        }
      },
      "auroc": 0.506195643939394
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 1021,
          "accuracy": 0.14916666666666667
        },
        "0.01": {
          "tp": 175,
          "fn": 1025,
          "accuracy": 0.14583333333333334
        }
      },
      "auroc": 0.42840711805555565
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 694,
          "fn": 2706,
          "accuracy": 0.20411764705882354
        },
        "0.01": {
          "tp": 635,
          "fn": 2765,
          "accuracy": 0.18676470588235294
        }
      },
      "auroc": 0.4787408700980392
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 2118,
          "accuracy": 0.03727272727272727
        },
        "0.01": {
          "tp": 75,
          "fn": 2125,
          "accuracy": 0.03409090909090909
        }
      },
      "auroc": 0.3177188446969697
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 69,
          "fn": 1131,
          "accuracy": 0.0575
        },
        "0.01": {
          "tp": 67,
          "fn": 1133,
          "accuracy": 0.05583333333333333
        }
      },
      "auroc": 0.3267875
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 3249,
          "accuracy": 0.04441176470588235
        },
        "0.01": {
          "tp": 142,
          "fn": 3258,
          "accuracy": 0.04176470588235294
        }
      },
      "auroc": 0.3209195465686275
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 597,
          "fn": 3803,
          "accuracy": 0.13568181818181818
        },
        "0.01": {
          "tp": 535,
          "fn": 3865,
          "accuracy": 0.1215909090909091
        }
      },
      "auroc": 0.4119572443181818
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 2152,
          "accuracy": 0.10333333333333333
        },
        "0.01": {
          "tp": 242,
          "fn": 2158,
          "accuracy": 0.10083333333333333
        }
      },
      "auroc": 0.37759730902777777
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 845,
          "fn": 5955,
          "accuracy": 0.12426470588235294
        },
        "0.01": {
          "tp": 777,
          "fn": 6023,
          "accuracy": 0.11426470588235293
        }
      },
      "auroc": 0.39983020833333327
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2030,
          "fn": 370,
          "accuracy": 0.8458333333333333
        },
        "0.01": {
          "tp": 2002,
          "fn": 398,
          "accuracy": 0.8341666666666666
        }
      },
      "auroc": 0.9088904513888889
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1998,
          "fn": 402,
          "accuracy": 0.8325
        },
        "0.01": {
          "tp": 1951,
          "fn": 449,
          "accuracy": 0.8129166666666666
        }
      },
      "auroc": 0.8979809027777778
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4028,
          "fn": 772,
          "accuracy": 0.8391666666666666
        },
        "0.01": {
          "tp": 3953,
          "fn": 847,
          "accuracy": 0.8235416666666666
        }
      },
      "auroc": 0.9034356770833332
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2027,
          "fn": 373,
          "accuracy": 0.8445833333333334
        },
        "0.01": {
          "tp": 1981,
          "fn": 419,
          "accuracy": 0.8254166666666667
        }
      },
      "auroc": 0.9096471354166668
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1964,
          "fn": 436,
          "accuracy": 0.8183333333333334
        },
        "0.01": {
          "tp": 1881,
          "fn": 519,
          "accuracy": 0.78375
        }
      },
      "auroc": 0.8882883680555556
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3991,
          "fn": 809,
          "accuracy": 0.8314583333333333
        },
        "0.01": {
          "tp": 3862,
          "fn": 938,
          "accuracy": 0.8045833333333333
        }
      },
      "auroc": 0.898967751736111
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4057,
          "fn": 743,
          "accuracy": 0.8452083333333333
        },
        "0.01": {
          "tp": 3983,
          "fn": 817,
          "accuracy": 0.8297916666666667
        }
      },
      "auroc": 0.9092687934027778
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3962,
          "fn": 838,
          "accuracy": 0.8254166666666667
        },
        "0.01": {
          "tp": 3832,
          "fn": 968,
          "accuracy": 0.7983333333333333
        }
      },
      "auroc": 0.8931346354166667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8019,
          "fn": 1581,
          "accuracy": 0.8353125
        },
        "0.01": {
          "tp": 7815,
          "fn": 1785,
          "accuracy": 0.8140625
        }
      },
      "auroc": 0.9012017144097222
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2321,
          "fn": 79,
          "accuracy": 0.9670833333333333
        },
        "0.01": {
          "tp": 2269,
          "fn": 131,
          "accuracy": 0.9454166666666667
        }
      },
      "auroc": 0.987540625
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1927,
          "fn": 473,
          "accuracy": 0.8029166666666666
        },
        "0.01": {
          "tp": 1843,
          "fn": 557,
          "accuracy": 0.7679166666666667
        }
      },
      "auroc": 0.9041093750000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4248,
          "fn": 552,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 4112,
          "fn": 688,
          "accuracy": 0.8566666666666667
        }
      },
      "auroc": 0.9458249999999999
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 547,
          "fn": 1853,
          "accuracy": 0.22791666666666666
        },
        "0.01": {
          "tp": 302,
          "fn": 2098,
          "accuracy": 0.12583333333333332
        }
      },
      "auroc": 0.70212890625
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1912,
          "fn": 488,
          "accuracy": 0.7966666666666666
        },
        "0.01": {
          "tp": 1844,
          "fn": 556,
          "accuracy": 0.7683333333333333
        }
      },
      "auroc": 0.89354765625
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2459,
          "fn": 2341,
          "accuracy": 0.5122916666666667
        },
        "0.01": {
          "tp": 2146,
          "fn": 2654,
          "accuracy": 0.44708333333333333
        }
      },
      "auroc": 0.79783828125
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2868,
          "fn": 1932,
          "accuracy": 0.5975
        },
        "0.01": {
          "tp": 2571,
          "fn": 2229,
          "accuracy": 0.535625
        }
      },
      "auroc": 0.844834765625
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3839,
          "fn": 961,
          "accuracy": 0.7997916666666667
        },
        "0.01": {
          "tp": 3687,
          "fn": 1113,
          "accuracy": 0.768125
        }
      },
      "auroc": 0.898828515625
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 6707,
          "fn": 2893,
          "accuracy": 0.6986458333333333
        },
        "0.01": {
          "tp": 6258,
          "fn": 3342,
          "accuracy": 0.651875
        }
      },
      "auroc": 0.8718316406250001
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2024,
          "fn": 376,
          "accuracy": 0.8433333333333334
        },
        "0.01": {
          "tp": 1928,
          "fn": 472,
          "accuracy": 0.8033333333333333
        }
      },
      "auroc": 0.91421328125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1988,
          "fn": 412,
          "accuracy": 0.8283333333333334
        },
        "0.01": {
          "tp": 1891,
          "fn": 509,
          "accuracy": 0.7879166666666667
        }
      },
      "auroc": 0.8956996527777777
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4012,
          "fn": 788,
          "accuracy": 0.8358333333333333
        },
        "0.01": {
          "tp": 3819,
          "fn": 981,
          "accuracy": 0.795625
        }
      },
      "auroc": 0.9049564670138889
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2015,
          "fn": 385,
          "accuracy": 0.8395833333333333
        },
        "0.01": {
          "tp": 1921,
          "fn": 479,
          "accuracy": 0.8004166666666667
        }
      },
      "auroc": 0.9066858506944445
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1898,
          "fn": 502,
          "accuracy": 0.7908333333333334
        },
        "0.01": {
          "tp": 1786,
          "fn": 614,
          "accuracy": 0.7441666666666666
        }
      },
      "auroc": 0.8817354166666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3913,
          "fn": 887,
          "accuracy": 0.8152083333333333
        },
        "0.01": {
          "tp": 3707,
          "fn": 1093,
          "accuracy": 0.7722916666666667
        }
      },
      "auroc": 0.8942106336805555
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4039,
          "fn": 761,
          "accuracy": 0.8414583333333333
        },
        "0.01": {
          "tp": 3849,
          "fn": 951,
          "accuracy": 0.801875
        }
      },
      "auroc": 0.9104495659722223
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3886,
          "fn": 914,
          "accuracy": 0.8095833333333333
        },
        "0.01": {
          "tp": 3677,
          "fn": 1123,
          "accuracy": 0.7660416666666666
        }
      },
      "auroc": 0.8887175347222223
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7925,
          "fn": 1675,
          "accuracy": 0.8255208333333334
        },
        "0.01": {
          "tp": 7526,
          "fn": 2074,
          "accuracy": 0.7839583333333333
        }
      },
      "auroc": 0.8995835503472223
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2379,
          "fn": 21,
          "accuracy": 0.99125
        },
        "0.01": {
          "tp": 2355,
          "fn": 45,
          "accuracy": 0.98125
        }
      },
      "auroc": 0.9944626736111111
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1958,
          "fn": 442,
          "accuracy": 0.8158333333333333
        },
        "0.01": {
          "tp": 1716,
          "fn": 684,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9263701388888889
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4337,
          "fn": 463,
          "accuracy": 0.9035416666666667
        },
        "0.01": {
          "tp": 4071,
          "fn": 729,
          "accuracy": 0.848125
        }
      },
      "auroc": 0.9604164062499999
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 273,
          "fn": 2127,
          "accuracy": 0.11375
        },
        "0.01": {
          "tp": 69,
          "fn": 2331,
          "accuracy": 0.02875
        }
      },
      "auroc": 0.6322303819444444
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 693,
          "fn": 1707,
          "accuracy": 0.28875
        },
        "0.01": {
          "tp": 379,
          "fn": 2021,
          "accuracy": 0.15791666666666668
        }
      },
      "auroc": 0.7182421875
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 966,
          "fn": 3834,
          "accuracy": 0.20125
        },
        "0.01": {
          "tp": 448,
          "fn": 4352,
          "accuracy": 0.09333333333333334
        }
      },
      "auroc": 0.6752362847222223
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2652,
          "fn": 2148,
          "accuracy": 0.5525
        },
        "0.01": {
          "tp": 2424,
          "fn": 2376,
          "accuracy": 0.505
        }
      },
      "auroc": 0.8133465277777778
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2651,
          "fn": 2149,
          "accuracy": 0.5522916666666666
        },
        "0.01": {
          "tp": 2095,
          "fn": 2705,
          "accuracy": 0.43645833333333334
        }
      },
      "auroc": 0.8223061631944444
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5303,
          "fn": 4297,
          "accuracy": 0.5523958333333333
        },
        "0.01": {
          "tp": 4519,
          "fn": 5081,
          "accuracy": 0.47072916666666664
        }
      },
      "auroc": 0.8178263454861112
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2379,
          "fn": 21,
          "accuracy": 0.99125
        },
        "0.01": {
          "tp": 2372,
          "fn": 28,
          "accuracy": 0.9883333333333333
        }
      },
      "auroc": 0.9950915798611112
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2245,
          "fn": 155,
          "accuracy": 0.9354166666666667
        },
        "0.01": {
          "tp": 2114,
          "fn": 286,
          "accuracy": 0.8808333333333334
        }
      },
      "auroc": 0.9754978298611111
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4624,
          "fn": 176,
          "accuracy": 0.9633333333333334
        },
        "0.01": {
          "tp": 4486,
          "fn": 314,
          "accuracy": 0.9345833333333333
        }
      },
      "auroc": 0.9852947048611111
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 906,
          "fn": 1494,
          "accuracy": 0.3775
        },
        "0.01": {
          "tp": 754,
          "fn": 1646,
          "accuracy": 0.31416666666666665
        }
      },
      "auroc": 0.7586365451388888
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1775,
          "fn": 625,
          "accuracy": 0.7395833333333334
        },
        "0.01": {
          "tp": 1620,
          "fn": 780,
          "accuracy": 0.675
        }
      },
      "auroc": 0.8814859374999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2681,
          "fn": 2119,
          "accuracy": 0.5585416666666667
        },
        "0.01": {
          "tp": 2374,
          "fn": 2426,
          "accuracy": 0.4945833333333333
        }
      },
      "auroc": 0.8200612413194445
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3285,
          "fn": 1515,
          "accuracy": 0.684375
        },
        "0.01": {
          "tp": 3126,
          "fn": 1674,
          "accuracy": 0.65125
        }
      },
      "auroc": 0.8768640624999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4020,
          "fn": 780,
          "accuracy": 0.8375
        },
        "0.01": {
          "tp": 3734,
          "fn": 1066,
          "accuracy": 0.7779166666666667
        }
      },
      "auroc": 0.9284918836805556
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7305,
          "fn": 2295,
          "accuracy": 0.7609375
        },
        "0.01": {
          "tp": 6860,
          "fn": 2740,
          "accuracy": 0.7145833333333333
        }
      },
      "auroc": 0.9026779730902778
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2068,
          "fn": 332,
          "accuracy": 0.8616666666666667
        },
        "0.01": {
          "tp": 1997,
          "fn": 403,
          "accuracy": 0.8320833333333333
        }
      },
      "auroc": 0.9275634548611111
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1996,
          "fn": 404,
          "accuracy": 0.8316666666666667
        },
        "0.01": {
          "tp": 1964,
          "fn": 436,
          "accuracy": 0.8183333333333334
        }
      },
      "auroc": 0.8974496527777778
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4064,
          "fn": 736,
          "accuracy": 0.8466666666666667
        },
        "0.01": {
          "tp": 3961,
          "fn": 839,
          "accuracy": 0.8252083333333333
        }
      },
      "auroc": 0.9125065538194445
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1985,
          "fn": 415,
          "accuracy": 0.8270833333333333
        },
        "0.01": {
          "tp": 1860,
          "fn": 540,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9027098958333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1813,
          "fn": 587,
          "accuracy": 0.7554166666666666
        },
        "0.01": {
          "tp": 1657,
          "fn": 743,
          "accuracy": 0.6904166666666667
        }
      },
      "auroc": 0.8676003472222222
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3798,
          "fn": 1002,
          "accuracy": 0.79125
        },
        "0.01": {
          "tp": 3517,
          "fn": 1283,
          "accuracy": 0.7327083333333333
        }
      },
      "auroc": 0.8851551215277778
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4053,
          "fn": 747,
          "accuracy": 0.844375
        },
        "0.01": {
          "tp": 3857,
          "fn": 943,
          "accuracy": 0.8035416666666667
        }
      },
      "auroc": 0.9151366753472223
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3809,
          "fn": 991,
          "accuracy": 0.7935416666666667
        },
        "0.01": {
          "tp": 3621,
          "fn": 1179,
          "accuracy": 0.754375
        }
      },
      "auroc": 0.882525
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7862,
          "fn": 1738,
          "accuracy": 0.8189583333333333
        },
        "0.01": {
          "tp": 7478,
          "fn": 2122,
          "accuracy": 0.7789583333333333
        }
      },
      "auroc": 0.8988308376736112
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1986,
          "fn": 414,
          "accuracy": 0.8275
        },
        "0.01": {
          "tp": 1765,
          "fn": 635,
          "accuracy": 0.7354166666666667
        }
      },
      "auroc": 0.9287663194444444
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1986,
          "fn": 414,
          "accuracy": 0.8275
        },
        "0.01": {
          "tp": 1765,
          "fn": 635,
          "accuracy": 0.7354166666666667
        }
      },
      "auroc": 0.9287663194444444
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1889,
          "fn": 511,
          "accuracy": 0.7870833333333334
        },
        "0.01": {
          "tp": 1679,
          "fn": 721,
          "accuracy": 0.6995833333333333
        }
      },
      "auroc": 0.9138269097222222
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1889,
          "fn": 511,
          "accuracy": 0.7870833333333334
        },
        "0.01": {
          "tp": 1679,
          "fn": 721,
          "accuracy": 0.6995833333333333
        }
      },
      "auroc": 0.9138269097222222
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3875,
          "fn": 925,
          "accuracy": 0.8072916666666666
        },
        "0.01": {
          "tp": 3444,
          "fn": 1356,
          "accuracy": 0.7175
        }
      },
      "auroc": 0.9212966145833332
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3875,
          "fn": 925,
          "accuracy": 0.8072916666666666
        },
        "0.01": {
          "tp": 3444,
          "fn": 1356,
          "accuracy": 0.7175
        }
      },
      "auroc": 0.9212966145833332
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1049,
          "fn": 1351,
          "accuracy": 0.4370833333333333
        },
        "0.01": {
          "tp": 676,
          "fn": 1724,
          "accuracy": 0.2816666666666667
        }
      },
      "auroc": 0.7649888020833334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1049,
          "fn": 1351,
          "accuracy": 0.4370833333333333
        },
        "0.01": {
          "tp": 676,
          "fn": 1724,
          "accuracy": 0.2816666666666667
        }
      },
      "auroc": 0.7649888020833334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1601,
          "accuracy": 0.3329166666666667
        },
        "0.01": {
          "tp": 484,
          "fn": 1916,
          "accuracy": 0.20166666666666666
        }
      },
      "auroc": 0.7231838541666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1601,
          "accuracy": 0.3329166666666667
        },
        "0.01": {
          "tp": 484,
          "fn": 1916,
          "accuracy": 0.20166666666666666
        }
      },
      "auroc": 0.7231838541666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1848,
          "fn": 2952,
          "accuracy": 0.385
        },
        "0.01": {
          "tp": 1160,
          "fn": 3640,
          "accuracy": 0.24166666666666667
        }
      },
      "auroc": 0.7440863281250001
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1848,
          "fn": 2952,
          "accuracy": 0.385
        },
        "0.01": {
          "tp": 1160,
          "fn": 3640,
          "accuracy": 0.24166666666666667
        }
      },
      "auroc": 0.7440863281250001
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2017,
          "fn": 383,
          "accuracy": 0.8404166666666667
        },
        "0.01": {
          "tp": 1976,
          "fn": 424,
          "accuracy": 0.8233333333333334
        }
      },
      "auroc": 0.9096862847222222
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2017,
          "fn": 383,
          "accuracy": 0.8404166666666667
        },
        "0.01": {
          "tp": 1976,
          "fn": 424,
          "accuracy": 0.8233333333333334
        }
      },
      "auroc": 0.9096862847222222
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1984,
          "fn": 416,
          "accuracy": 0.8266666666666667
        },
        "0.01": {
          "tp": 1939,
          "fn": 461,
          "accuracy": 0.8079166666666666
        }
      },
      "auroc": 0.8973087673611111
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1984,
          "fn": 416,
          "accuracy": 0.8266666666666667
        },
        "0.01": {
          "tp": 1939,
          "fn": 461,
          "accuracy": 0.8079166666666666
        }
      },
      "auroc": 0.8973087673611111
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4001,
          "fn": 799,
          "accuracy": 0.8335416666666666
        },
        "0.01": {
          "tp": 3915,
          "fn": 885,
          "accuracy": 0.815625
        }
      },
      "auroc": 0.9034975260416667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4001,
          "fn": 799,
          "accuracy": 0.8335416666666666
        },
        "0.01": {
          "tp": 3915,
          "fn": 885,
          "accuracy": 0.815625
        }
      },
      "auroc": 0.9034975260416667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1989,
          "fn": 411,
          "accuracy": 0.82875
        },
        "0.01": {
          "tp": 1943,
          "fn": 457,
          "accuracy": 0.8095833333333333
        }
      },
      "auroc": 0.9006454861111111
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1989,
          "fn": 411,
          "accuracy": 0.82875
        },
        "0.01": {
          "tp": 1943,
          "fn": 457,
          "accuracy": 0.8095833333333333
        }
      },
      "auroc": 0.9006454861111111
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1721,
          "fn": 679,
          "accuracy": 0.7170833333333333
        },
        "0.01": {
          "tp": 1449,
          "fn": 951,
          "accuracy": 0.60375
        }
      },
      "auroc": 0.8478682291666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1721,
          "fn": 679,
          "accuracy": 0.7170833333333333
        },
        "0.01": {
          "tp": 1449,
          "fn": 951,
          "accuracy": 0.60375
        }
      },
      "auroc": 0.8478682291666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3710,
          "fn": 1090,
          "accuracy": 0.7729166666666667
        },
        "0.01": {
          "tp": 3392,
          "fn": 1408,
          "accuracy": 0.7066666666666667
        }
      },
      "auroc": 0.8742568576388889
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3710,
          "fn": 1090,
          "accuracy": 0.7729166666666667
        },
        "0.01": {
          "tp": 3392,
          "fn": 1408,
          "accuracy": 0.7066666666666667
        }
      },
      "auroc": 0.8742568576388889
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1396,
          "fn": 1004,
          "accuracy": 0.5816666666666667
        },
        "0.01": {
          "tp": 1115,
          "fn": 1285,
          "accuracy": 0.46458333333333335
        }
      },
      "auroc": 0.8283902777777779
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1396,
          "fn": 1004,
          "accuracy": 0.5816666666666667
        },
        "0.01": {
          "tp": 1115,
          "fn": 1285,
          "accuracy": 0.46458333333333335
        }
      },
      "auroc": 0.8283902777777779
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1188,
          "fn": 1212,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 893,
          "fn": 1507,
          "accuracy": 0.3720833333333333
        }
      },
      "auroc": 0.7812006076388889
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1188,
          "fn": 1212,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 893,
          "fn": 1507,
          "accuracy": 0.3720833333333333
        }
      },
      "auroc": 0.7812006076388889
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2584,
          "fn": 2216,
          "accuracy": 0.5383333333333333
        },
        "0.01": {
          "tp": 2008,
          "fn": 2792,
          "accuracy": 0.41833333333333333
        }
      },
      "auroc": 0.8047954427083335
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2584,
          "fn": 2216,
          "accuracy": 0.5383333333333333
        },
        "0.01": {
          "tp": 2008,
          "fn": 2792,
          "accuracy": 0.41833333333333333
        }
      },
      "auroc": 0.8047954427083335
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 21638,
          "fn": 4762,
          "accuracy": 0.8196212121212121
        },
        "0.01": {
          "tp": 20398,
          "fn": 6002,
          "accuracy": 0.7726515151515152
        }
      },
      "auroc": 0.9145672032828283
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 12112,
          "fn": 2288,
          "accuracy": 0.8411111111111111
        },
        "0.01": {
          "tp": 11479,
          "fn": 2921,
          "accuracy": 0.7971527777777778
        }
      },
      "auroc": 0.9161845920138889
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 33750,
          "fn": 7050,
          "accuracy": 0.8272058823529411
        },
        "0.01": {
          "tp": 31877,
          "fn": 8923,
          "accuracy": 0.7812990196078431
        }
      },
      "auroc": 0.9151380463643791
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 15334,
          "fn": 11066,
          "accuracy": 0.5808333333333333
        },
        "0.01": {
          "tp": 13331,
          "fn": 13069,
          "accuracy": 0.5049621212121213
        }
      },
      "auroc": 0.8159479166666668
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 10055,
          "fn": 4345,
          "accuracy": 0.6982638888888889
        },
        "0.01": {
          "tp": 9167,
          "fn": 5233,
          "accuracy": 0.6365972222222223
        }
      },
      "auroc": 0.8551499855324074
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 25389,
          "fn": 15411,
          "accuracy": 0.6222794117647059
        },
        "0.01": {
          "tp": 22498,
          "fn": 18302,
          "accuracy": 0.551421568627451
        }
      },
      "auroc": 0.8297839409722222
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 36972,
          "fn": 15828,
          "accuracy": 0.7002272727272727
        },
        "0.01": {
          "tp": 33729,
          "fn": 19071,
          "accuracy": 0.6388068181818182
        }
      },
      "auroc": 0.8652575599747475
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 22167,
          "fn": 6633,
          "accuracy": 0.7696875
        },
        "0.01": {
          "tp": 20646,
          "fn": 8154,
          "accuracy": 0.716875
        }
      },
      "auroc": 0.8856672887731483
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 59139,
          "fn": 22461,
          "accuracy": 0.7247426470588235
        },
        "0.01": {
          "tp": 54375,
          "fn": 27225,
          "accuracy": 0.6663602941176471
        }
      },
      "auroc": 0.8724609936683007
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9813875000000001
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9848604166666668
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9848604166666668
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        },
        "0.01": null
      },
      "auroc": 0.986596875
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9859572916666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9873635416666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9866604166666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        },
        "0.01": null
      },
      "auroc": 0.6690937499999999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9855968749999999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        },
        "0.01": null
      },
      "auroc": 0.8273453125
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.8275255208333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9864802083333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 589,
          "fn": 211,
          "accuracy": 0.73625
        },
        "0.01": null
      },
      "auroc": 0.9070028645833333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9867156250000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9875244791666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9850708333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9867020833333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9858932291666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 779,
          "fn": 21,
          "accuracy": 0.97375
        },
        "0.01": null
      },
      "auroc": 0.9871132812500001
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9830166666666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": null
      },
      "auroc": 0.985675
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": null
      },
      "auroc": 0.6878947916666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.6552739583333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 389,
          "accuracy": 0.0275
        },
        "0.01": null
      },
      "auroc": 0.671584375
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.8381140625000001
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        },
        "0.01": null
      },
      "auroc": 0.8191453125
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 414,
          "accuracy": 0.4825
        },
        "0.01": null
      },
      "auroc": 0.8286296874999999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9876229166666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": null
      },
      "auroc": 0.9619010416666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": null
      },
      "auroc": 0.9747619791666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        },
        "0.01": null
      },
      "auroc": 0.7042791666666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": null
      },
      "auroc": 0.8184374999999999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 319,
          "accuracy": 0.2025
        },
        "0.01": null
      },
      "auroc": 0.7613583333333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        },
        "0.01": null
      },
      "auroc": 0.8459510416666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.8901692708333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 414,
          "fn": 386,
          "accuracy": 0.5175
        },
        "0.01": null
      },
      "auroc": 0.8680601562499999
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.987165625
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9877494791666668
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9877494791666668
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.98804140625
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9860041666666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9860041666666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9852458333333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9852458333333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.985625
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.985625
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": null
      },
      "auroc": 0.9169864583333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": null
      },
      "auroc": 0.9169864583333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": null
      },
      "auroc": 0.8393645833333332
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": null
      },
      "auroc": 0.8393645833333332
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        },
        "0.01": null
      },
      "auroc": 0.8781755208333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        },
        "0.01": null
      },
      "auroc": 0.8781755208333334
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.988225
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.988225
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9882791666666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9882791666666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": null
      },
      "auroc": 0.9424145833333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": null
      },
      "auroc": 0.9424145833333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.8962343749999999
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.8962343749999999
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9193244791666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9193244791666666
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2044,
          "fn": 156,
          "accuracy": 0.9290909090909091
        },
        "0.01": null
      },
      "auroc": 0.9771804924242424
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1101,
          "fn": 99,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9826105902777778
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3145,
          "fn": 255,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9790969975490197
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1444,
          "fn": 756,
          "accuracy": 0.6563636363636364
        },
        "0.01": null
      },
      "auroc": 0.8839700757575757
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 826,
          "fn": 374,
          "accuracy": 0.6883333333333334
        },
        "0.01": null
      },
      "auroc": 0.9021553819444444
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2270,
          "fn": 1130,
          "accuracy": 0.6676470588235294
        },
        "0.01": null
      },
      "auroc": 0.890388419117647
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3488,
          "fn": 912,
          "accuracy": 0.7927272727272727
        },
        "0.01": null
      },
      "auroc": 0.9305752840909092
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1927,
          "fn": 473,
          "accuracy": 0.8029166666666666
        },
        "0.01": null
      },
      "auroc": 0.942382986111111
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 5415,
          "fn": 1385,
          "accuracy": 0.7963235294117647
        },
        "0.01": null
      },
      "auroc": 0.9347427083333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9813875000000001
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9848604166666668
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9848604166666668
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        },
        "0.01": null
      },
      "auroc": 0.986596875
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9859572916666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9873635416666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9866604166666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        },
        "0.01": null
      },
      "auroc": 0.6690937499999999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9855968749999999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        },
        "0.01": null
      },
      "auroc": 0.8273453125
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.8275255208333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9864802083333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 589,
          "fn": 211,
          "accuracy": 0.73625
        },
        "0.01": null
      },
      "auroc": 0.9070028645833333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9867156250000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9875244791666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9850708333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9867020833333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9858932291666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 779,
          "fn": 21,
          "accuracy": 0.97375
        },
        "0.01": null
      },
      "auroc": 0.9871132812500001
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9830166666666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": null
      },
      "auroc": 0.985675
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": null
      },
      "auroc": 0.6878947916666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.6552739583333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 389,
          "accuracy": 0.0275
        },
        "0.01": null
      },
      "auroc": 0.671584375
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.8381140625000001
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        },
        "0.01": null
      },
      "auroc": 0.8191453125
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 414,
          "accuracy": 0.4825
        },
        "0.01": null
      },
      "auroc": 0.8286296874999999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9876229166666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": null
      },
      "auroc": 0.9619010416666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": null
      },
      "auroc": 0.9747619791666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        },
        "0.01": null
      },
      "auroc": 0.7042791666666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": null
      },
      "auroc": 0.8184374999999999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 319,
          "accuracy": 0.2025
        },
        "0.01": null
      },
      "auroc": 0.7613583333333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        },
        "0.01": null
      },
      "auroc": 0.8459510416666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.8901692708333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 414,
          "fn": 386,
          "accuracy": 0.5175
        },
        "0.01": null
      },
      "auroc": 0.8680601562499999
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.987165625
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9877494791666668
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9877494791666668
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.98804140625
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9860041666666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9860041666666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9852458333333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9852458333333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.985625
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.985625
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": null
      },
      "auroc": 0.9169864583333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": null
      },
      "auroc": 0.9169864583333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": null
      },
      "auroc": 0.8393645833333332
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": null
      },
      "auroc": 0.8393645833333332
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        },
        "0.01": null
      },
      "auroc": 0.8781755208333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        },
        "0.01": null
      },
      "auroc": 0.8781755208333334
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.988225
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.988225
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9882791666666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9882791666666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": null
      },
      "auroc": 0.9424145833333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": null
      },
      "auroc": 0.9424145833333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.8962343749999999
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.8962343749999999
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9193244791666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9193244791666666
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2044,
          "fn": 156,
          "accuracy": 0.9290909090909091
        },
        "0.01": null
      },
      "auroc": 0.9771804924242424
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1101,
          "fn": 99,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9826105902777778
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3145,
          "fn": 255,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9790969975490197
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1444,
          "fn": 756,
          "accuracy": 0.6563636363636364
        },
        "0.01": null
      },
      "auroc": 0.8839700757575757
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 826,
          "fn": 374,
          "accuracy": 0.6883333333333334
        },
        "0.01": null
      },
      "auroc": 0.9021553819444444
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2270,
          "fn": 1130,
          "accuracy": 0.6676470588235294
        },
        "0.01": null
      },
      "auroc": 0.890388419117647
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3488,
          "fn": 912,
          "accuracy": 0.7927272727272727
        },
        "0.01": null
      },
      "auroc": 0.9305752840909092
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1927,
          "fn": 473,
          "accuracy": 0.8029166666666666
        },
        "0.01": null
      },
      "auroc": 0.942382986111111
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 5415,
          "fn": 1385,
          "accuracy": 0.7963235294117647
        },
        "0.01": null
      },
      "auroc": 0.9347427083333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.988225
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9882791666666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9789145833333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9836239583333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9835697916666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 786,
          "fn": 14,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9859515625
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9795187500000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9871416666666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9833302083333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        },
        "0.01": null
      },
      "auroc": 0.5753833333333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9854395833333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": null
      },
      "auroc": 0.7804114583333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        },
        "0.01": null
      },
      "auroc": 0.7774510416666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.986290625
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 573,
          "fn": 227,
          "accuracy": 0.71625
        },
        "0.01": null
      },
      "auroc": 0.8818708333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9861083333333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9872208333333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9822
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9852666666666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": null
      },
      "auroc": 0.9841541666666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 765,
          "fn": 35,
          "accuracy": 0.95625
        },
        "0.01": null
      },
      "auroc": 0.98624375
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9881166666666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": null
      },
      "auroc": 0.9725114583333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": null
      },
      "auroc": 0.9803140625
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.5837458333333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.5911197916666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        },
        "0.01": null
      },
      "auroc": 0.5874328125
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.78593125
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        },
        "0.01": null
      },
      "auroc": 0.7818156249999999
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 450,
          "accuracy": 0.4375
        },
        "0.01": null
      },
      "auroc": 0.7838734374999999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9854385416666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        },
        "0.01": null
      },
      "auroc": 0.940046875
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        },
        "0.01": null
      },
      "auroc": 0.9627427083333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": null
      },
      "auroc": 0.6092947916666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        },
        "0.01": null
      },
      "auroc": 0.7953416666666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 73,
          "fn": 327,
          "accuracy": 0.1825
        },
        "0.01": null
      },
      "auroc": 0.7023182291666668
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        },
        "0.01": null
      },
      "auroc": 0.7973666666666668
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        },
        "0.01": null
      },
      "auroc": 0.8676942708333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 434,
          "accuracy": 0.4575
        },
        "0.01": null
      },
      "auroc": 0.83253046875
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9854208333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9868770833333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9868770833333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 789,
          "fn": 11,
          "accuracy": 0.98625
        },
        "0.01": null
      },
      "auroc": 0.9876052083333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.98163125
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.98163125
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9786895833333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9786895833333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": null
      },
      "auroc": 0.9801604166666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": null
      },
      "auroc": 0.9801604166666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        },
        "0.01": null
      },
      "auroc": 0.8695104166666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        },
        "0.01": null
      },
      "auroc": 0.8695104166666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        },
        "0.01": null
      },
      "auroc": 0.77126875
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        },
        "0.01": null
      },
      "auroc": 0.77126875
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        },
        "0.01": null
      },
      "auroc": 0.8203895833333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        },
        "0.01": null
      },
      "auroc": 0.8203895833333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9862135416666666
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9862135416666666
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9872734375000001
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9872734375000001
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9193916666666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9193916666666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": null
      },
      "auroc": 0.8603166666666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": null
      },
      "auroc": 0.8603166666666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.8898541666666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.8898541666666666
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1965,
          "fn": 235,
          "accuracy": 0.8931818181818182
        },
        "0.01": null
      },
      "auroc": 0.9695703598484848
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1036,
          "fn": 164,
          "accuracy": 0.8633333333333333
        },
        "0.01": null
      },
      "auroc": 0.9770611111111112
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3001,
          "fn": 399,
          "accuracy": 0.8826470588235295
        },
        "0.01": null
      },
      "auroc": 0.9722141544117647
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1371,
          "fn": 829,
          "accuracy": 0.6231818181818182
        },
        "0.01": null
      },
      "auroc": 0.8471132575757576
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 402,
          "accuracy": 0.665
        },
        "0.01": null
      },
      "auroc": 0.8864060763888888
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2169,
          "fn": 1231,
          "accuracy": 0.6379411764705882
        },
        "0.01": null
      },
      "auroc": 0.8609813112745097
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3336,
          "fn": 1064,
          "accuracy": 0.7581818181818182
        },
        "0.01": null
      },
      "auroc": 0.9083418087121213
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1834,
          "fn": 566,
          "accuracy": 0.7641666666666667
        },
        "0.01": null
      },
      "auroc": 0.93173359375
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 5170,
          "fn": 1630,
          "accuracy": 0.7602941176470588
        },
        "0.01": null
      },
      "auroc": 0.9165977328431372
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9875145833333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9869489583333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9872317708333332
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9873770833333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9663687500000001
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9768729166666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9874458333333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9766588541666668
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 755,
          "fn": 45,
          "accuracy": 0.94375
        },
        "0.01": null
      },
      "auroc": 0.9820523437500001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": null
      },
      "auroc": 0.965115625
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.987165625
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": null
      },
      "auroc": 0.9761406250000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": null
      },
      "auroc": 0.49127291666666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9851624999999999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": null
      },
      "auroc": 0.7382177083333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        },
        "0.01": null
      },
      "auroc": 0.7281942708333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9861640625000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 554,
          "fn": 246,
          "accuracy": 0.6925
        },
        "0.01": null
      },
      "auroc": 0.8571791666666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9775604166666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.982946875
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9714510416666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9798921875000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": null
      },
      "auroc": 0.9745057291666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 740,
          "fn": 60,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.98141953125
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9879364583333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        },
        "0.01": null
      },
      "auroc": 0.893578125
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": null
      },
      "auroc": 0.9407572916666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.5240583333333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": null
      },
      "auroc": 0.5777416666666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.5509000000000001
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": null
      },
      "auroc": 0.7559973958333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 341,
          "accuracy": 0.1475
        },
        "0.01": null
      },
      "auroc": 0.7356598958333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 542,
          "accuracy": 0.3225
        },
        "0.01": null
      },
      "auroc": 0.7458286458333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9763635416666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        },
        "0.01": null
      },
      "auroc": 0.8723927083333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        },
        "0.01": null
      },
      "auroc": 0.9243781249999999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": null
      },
      "auroc": 0.5335447916666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": null
      },
      "auroc": 0.8071229166666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 67,
          "fn": 333,
          "accuracy": 0.1675
        },
        "0.01": null
      },
      "auroc": 0.6703338541666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        },
        "0.01": null
      },
      "auroc": 0.7549541666666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 272,
          "accuracy": 0.32
        },
        "0.01": null
      },
      "auroc": 0.8397578125
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 310,
          "fn": 490,
          "accuracy": 0.3875
        },
        "0.01": null
      },
      "auroc": 0.7973559895833333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9743375
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.9813354166666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.9813354166666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 771,
          "fn": 29,
          "accuracy": 0.96375
        },
        "0.01": null
      },
      "auroc": 0.984834375
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.96980625
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.96980625
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.9569364583333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.9569364583333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        },
        "0.01": null
      },
      "auroc": 0.9633713541666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        },
        "0.01": null
      },
      "auroc": 0.9633713541666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        },
        "0.01": null
      },
      "auroc": 0.7601479166666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        },
        "0.01": null
      },
      "auroc": 0.7601479166666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        },
        "0.01": null
      },
      "auroc": 0.6567479166666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        },
        "0.01": null
      },
      "auroc": 0.6567479166666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 65,
          "fn": 335,
          "accuracy": 0.1625
        },
        "0.01": null
      },
      "auroc": 0.7084479166666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 65,
          "fn": 335,
          "accuracy": 0.1625
        },
        "0.01": null
      },
      "auroc": 0.7084479166666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.988225
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.988225
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.988225
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.988225
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.988225
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.988225
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9881645833333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9881645833333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9706458333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9706458333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": null
      },
      "auroc": 0.9794052083333334
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": null
      },
      "auroc": 0.9794052083333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": null
      },
      "auroc": 0.8722
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": null
      },
      "auroc": 0.8722
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": null
      },
      "auroc": 0.80185625
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": null
      },
      "auroc": 0.80185625
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": null
      },
      "auroc": 0.8370281249999999
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": null
      },
      "auroc": 0.8370281249999999
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1836,
          "fn": 364,
          "accuracy": 0.8345454545454546
        },
        "0.01": null
      },
      "auroc": 0.9520127840909091
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 896,
          "fn": 304,
          "accuracy": 0.7466666666666667
        },
        "0.01": null
      },
      "auroc": 0.9509965277777777
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2732,
          "fn": 668,
          "accuracy": 0.8035294117647059
        },
        "0.01": null
      },
      "auroc": 0.951654105392157
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1235,
          "fn": 965,
          "accuracy": 0.5613636363636364
        },
        "0.01": null
      },
      "auroc": 0.8079392045454545
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 753,
          "fn": 447,
          "accuracy": 0.6275
        },
        "0.01": null
      },
      "auroc": 0.8803640625
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1988,
          "fn": 1412,
          "accuracy": 0.5847058823529412
        },
        "0.01": null
      },
      "auroc": 0.833500919117647
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3071,
          "fn": 1329,
          "accuracy": 0.6979545454545455
        },
        "0.01": null
      },
      "auroc": 0.879975994318182
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1649,
          "fn": 751,
          "accuracy": 0.6870833333333334
        },
        "0.01": null
      },
      "auroc": 0.9156802951388889
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 4720,
          "fn": 2080,
          "accuracy": 0.6941176470588235
        },
        "0.01": null
      },
      "auroc": 0.892577512254902
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9787354166666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9835343750000001
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9835343750000001
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9859338541666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9842270833333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9866104166666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.98541875
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": null
      },
      "auroc": 0.5277645833333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9851593750000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        },
        "0.01": null
      },
      "auroc": 0.7564619791666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        },
        "0.01": null
      },
      "auroc": 0.7559958333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9858848958333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 574,
          "fn": 226,
          "accuracy": 0.7175
        },
        "0.01": null
      },
      "auroc": 0.8709403645833333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9849697916666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9866515625000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9821385416666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": null
      },
      "auroc": 0.9852359374999999
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9835541666666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 767,
          "fn": 33,
          "accuracy": 0.95875
        },
        "0.01": null
      },
      "auroc": 0.98594375
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": null
      },
      "auroc": 0.963746875
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": null
      },
      "auroc": 0.9760401041666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.5318979166666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.45986770833333335
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.4958828125
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        },
        "0.01": null
      },
      "auroc": 0.7601156249999999
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 264,
          "accuracy": 0.34
        },
        "0.01": null
      },
      "auroc": 0.7118072916666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 461,
          "accuracy": 0.42375
        },
        "0.01": null
      },
      "auroc": 0.7359614583333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9869583333333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.92415625
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.9555572916666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        },
        "0.01": null
      },
      "auroc": 0.5732791666666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        },
        "0.01": null
      },
      "auroc": 0.7115083333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 58,
          "fn": 342,
          "accuracy": 0.145
        },
        "0.01": null
      },
      "auroc": 0.64239375
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": null
      },
      "auroc": 0.78011875
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 257,
          "accuracy": 0.3575
        },
        "0.01": null
      },
      "auroc": 0.8178322916666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 450,
          "accuracy": 0.4375
        },
        "0.01": null
      },
      "auroc": 0.7989755208333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9861166666666666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.987225
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.987225
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9877791666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9825312500000001
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9825312500000001
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9772041666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9772041666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9798677083333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9798677083333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": null
      },
      "auroc": 0.8562427083333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": null
      },
      "auroc": 0.8562427083333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        },
        "0.01": null
      },
      "auroc": 0.7646729166666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        },
        "0.01": null
      },
      "auroc": 0.7646729166666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 270,
          "accuracy": 0.325
        },
        "0.01": null
      },
      "auroc": 0.8104578124999999
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 270,
          "accuracy": 0.325
        },
        "0.01": null
      },
      "auroc": 0.8104578124999999
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9865302083333334
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9865302083333334
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9874317708333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9874317708333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": null
      },
      "auroc": 0.9176958333333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": null
      },
      "auroc": 0.9176958333333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": null
      },
      "auroc": 0.871309375
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": null
      },
      "auroc": 0.871309375
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.8945026041666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.8945026041666667
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1987,
          "fn": 213,
          "accuracy": 0.9031818181818182
        },
        "0.01": null
      },
      "auroc": 0.9688777462121212
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1013,
          "fn": 187,
          "accuracy": 0.8441666666666666
        },
        "0.01": null
      },
      "auroc": 0.9726916666666667
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3000,
          "fn": 400,
          "accuracy": 0.8823529411764706
        },
        "0.01": null
      },
      "auroc": 0.9702238357843136
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1365,
          "fn": 835,
          "accuracy": 0.6204545454545455
        },
        "0.01": null
      },
      "auroc": 0.8350901515151514
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 792,
          "fn": 408,
          "accuracy": 0.66
        },
        "0.01": null
      },
      "auroc": 0.8505876736111111
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2157,
          "fn": 1243,
          "accuracy": 0.6344117647058823
        },
        "0.01": null
      },
      "auroc": 0.8405598651960784
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3352,
          "fn": 1048,
          "accuracy": 0.7618181818181818
        },
        "0.01": null
      },
      "auroc": 0.9019839488636364
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1805,
          "fn": 595,
          "accuracy": 0.7520833333333333
        },
        "0.01": null
      },
      "auroc": 0.9116396701388889
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 5157,
          "fn": 1643,
          "accuracy": 0.7583823529411765
        },
        "0.01": null
      },
      "auroc": 0.9053918504901961
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9813072916666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9803999999999999
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": null
      },
      "auroc": 0.9808536458333335
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9778270833333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.95510625
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": null
      },
      "auroc": 0.9664666666666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": null
      },
      "auroc": 0.9795671875
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": null
      },
      "auroc": 0.9677531250000001
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 686,
          "fn": 114,
          "accuracy": 0.8575
        },
        "0.01": null
      },
      "auroc": 0.97366015625
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": null
      },
      "auroc": 0.970221875
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.7071479166666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        },
        "0.01": null
      },
      "auroc": 0.8386848958333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        },
        "0.01": null
      },
      "auroc": 0.713725
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": null
      },
      "auroc": 0.638440625
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        },
        "0.01": null
      },
      "auroc": 0.6760828125
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 219,
          "accuracy": 0.4525
        },
        "0.01": null
      },
      "auroc": 0.8419734374999999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 386,
          "accuracy": 0.035
        },
        "0.01": null
      },
      "auroc": 0.6727942708333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 605,
          "accuracy": 0.24375
        },
        "0.01": null
      },
      "auroc": 0.7573838541666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9713770833333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": null
      },
      "auroc": 0.938740625
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.9550588541666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9721604166666665
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        },
        "0.01": null
      },
      "auroc": 0.8955260416666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        },
        "0.01": null
      },
      "auroc": 0.9338432291666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": null
      },
      "auroc": 0.9717687500000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.9171333333333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 553,
          "fn": 247,
          "accuracy": 0.69125
        },
        "0.01": null
      },
      "auroc": 0.9444510416666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9715260416666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        },
        "0.01": null
      },
      "auroc": 0.9214166666666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        },
        "0.01": null
      },
      "auroc": 0.9464713541666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": null
      },
      "auroc": 0.6762166666666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.567640625
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 388,
          "accuracy": 0.03
        },
        "0.01": null
      },
      "auroc": 0.6219286458333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        },
        "0.01": null
      },
      "auroc": 0.8238713541666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 305,
          "accuracy": 0.2375
        },
        "0.01": null
      },
      "auroc": 0.7445286458333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 289,
          "fn": 511,
          "accuracy": 0.36125
        },
        "0.01": null
      },
      "auroc": 0.7842
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9802760416666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": null
      },
      "auroc": 0.8896062499999999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 232,
          "fn": 168,
          "accuracy": 0.58
        },
        "0.01": null
      },
      "auroc": 0.9349411458333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        },
        "0.01": null
      },
      "auroc": 0.7384593749999999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.578859375
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 20,
          "fn": 380,
          "accuracy": 0.05
        },
        "0.01": null
      },
      "auroc": 0.658659375
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        },
        "0.01": null
      },
      "auroc": 0.8593677083333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 63,
          "fn": 337,
          "accuracy": 0.1575
        },
        "0.01": null
      },
      "auroc": 0.7342328125
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 252,
          "fn": 548,
          "accuracy": 0.315
        },
        "0.01": null
      },
      "auroc": 0.7968002604166666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9850083333333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": null
      },
      "auroc": 0.9754625
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.9802354166666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9792666666666666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.9319760416666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9556213541666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.9821375000000001
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9537192708333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 657,
          "fn": 143,
          "accuracy": 0.82125
        },
        "0.01": null
      },
      "auroc": 0.9679283854166666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": null
      },
      "auroc": 0.952978125
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": null
      },
      "auroc": 0.952978125
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": null
      },
      "auroc": 0.9448395833333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": null
      },
      "auroc": 0.9448395833333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        },
        "0.01": null
      },
      "auroc": 0.9489088541666668
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        },
        "0.01": null
      },
      "auroc": 0.9489088541666668
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": null
      },
      "auroc": 0.8527427083333332
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": null
      },
      "auroc": 0.8527427083333332
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        },
        "0.01": null
      },
      "auroc": 0.784
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        },
        "0.01": null
      },
      "auroc": 0.784
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 298,
          "accuracy": 0.255
        },
        "0.01": null
      },
      "auroc": 0.8183713541666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 298,
          "accuracy": 0.255
        },
        "0.01": null
      },
      "auroc": 0.8183713541666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.97265625
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.97265625
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9752916666666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9752916666666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": null
      },
      "auroc": 0.9739739583333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": null
      },
      "auroc": 0.9739739583333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9832875
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9832875
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.9467635416666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.9467635416666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9650255208333334
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9650255208333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": null
      },
      "auroc": 0.8802354166666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": null
      },
      "auroc": 0.8802354166666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.8262666666666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.8262666666666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.8532510416666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.8532510416666667
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1707,
          "fn": 493,
          "accuracy": 0.7759090909090909
        },
        "0.01": null
      },
      "auroc": 0.9546924242424242
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 612,
          "fn": 588,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.9021289930555556
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2319,
          "fn": 1081,
          "accuracy": 0.6820588235294117
        },
        "0.01": null
      },
      "auroc": 0.936140625
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1145,
          "fn": 1055,
          "accuracy": 0.5204545454545455
        },
        "0.01": null
      },
      "auroc": 0.8668015151515152
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 823,
          "accuracy": 0.31416666666666665
        },
        "0.01": null
      },
      "auroc": 0.7612581597222222
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1522,
          "fn": 1878,
          "accuracy": 0.4476470588235294
        },
        "0.01": null
      },
      "auroc": 0.8295509191176471
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2852,
          "fn": 1548,
          "accuracy": 0.6481818181818182
        },
        "0.01": null
      },
      "auroc": 0.9107469696969697
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 989,
          "fn": 1411,
          "accuracy": 0.41208333333333336
        },
        "0.01": null
      },
      "auroc": 0.831693576388889
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3841,
          "fn": 2959,
          "accuracy": 0.5648529411764706
        },
        "0.01": null
      },
      "auroc": 0.8828457720588235
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9813875000000001
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9848604166666668
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9848604166666668
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        },
        "0.01": null
      },
      "auroc": 0.986596875
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9850208333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9873635416666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9861921874999999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": null
      },
      "auroc": 0.6637239583333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9857104166666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.8247171875000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        },
        "0.01": null
      },
      "auroc": 0.8243723958333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9865369791666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 583,
          "fn": 217,
          "accuracy": 0.72875
        },
        "0.01": null
      },
      "auroc": 0.9054546875
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9867156250000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9875244791666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9850708333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9867020833333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9858932291666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 779,
          "fn": 21,
          "accuracy": 0.97375
        },
        "0.01": null
      },
      "auroc": 0.9871132812500001
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.982796875
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9855651041666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": null
      },
      "auroc": 0.681921875
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.6540177083333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 389,
          "accuracy": 0.0275
        },
        "0.01": null
      },
      "auroc": 0.6679697916666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.8351276041666668
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        },
        "0.01": null
      },
      "auroc": 0.8184072916666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 415,
          "accuracy": 0.48125
        },
        "0.01": null
      },
      "auroc": 0.8267674479166667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9873729166666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": null
      },
      "auroc": 0.9612802083333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": null
      },
      "auroc": 0.9743265624999999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        },
        "0.01": null
      },
      "auroc": 0.6992208333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": null
      },
      "auroc": 0.8199208333333332
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 320,
          "accuracy": 0.2
        },
        "0.01": null
      },
      "auroc": 0.7595708333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": null
      },
      "auroc": 0.843296875
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        },
        "0.01": null
      },
      "auroc": 0.8906005208333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 411,
          "fn": 389,
          "accuracy": 0.51375
        },
        "0.01": null
      },
      "auroc": 0.8669486979166666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.987165625
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9877494791666668
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9877494791666668
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.98804140625
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9844239583333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9844239583333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9846041666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9846041666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9845140625
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9845140625
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.9158145833333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.9158145833333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": null
      },
      "auroc": 0.8378187499999998
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": null
      },
      "auroc": 0.8378187499999998
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        },
        "0.01": null
      },
      "auroc": 0.8768166666666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        },
        "0.01": null
      },
      "auroc": 0.8768166666666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.988225
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.988225
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9882791666666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9882791666666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": null
      },
      "auroc": 0.9421333333333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": null
      },
      "auroc": 0.9421333333333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.8956427083333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.8956427083333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9188880208333332
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9188880208333332
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2029,
          "fn": 171,
          "accuracy": 0.9222727272727272
        },
        "0.01": null
      },
      "auroc": 0.976796875
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1099,
          "fn": 101,
          "accuracy": 0.9158333333333334
        },
        "0.01": null
      },
      "auroc": 0.9824704861111111
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3128,
          "fn": 272,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9787993259803921
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1438,
          "fn": 762,
          "accuracy": 0.6536363636363637
        },
        "0.01": null
      },
      "auroc": 0.8822264204545454
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 826,
          "fn": 374,
          "accuracy": 0.6883333333333334
        },
        "0.01": null
      },
      "auroc": 0.9022121527777778
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2264,
          "fn": 1136,
          "accuracy": 0.6658823529411765
        },
        "0.01": null
      },
      "auroc": 0.8892802083333332
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3467,
          "fn": 933,
          "accuracy": 0.7879545454545455
        },
        "0.01": null
      },
      "auroc": 0.9295116477272727
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1925,
          "fn": 475,
          "accuracy": 0.8020833333333334
        },
        "0.01": null
      },
      "auroc": 0.9423413194444444
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 5392,
          "fn": 1408,
          "accuracy": 0.7929411764705883
        },
        "0.01": null
      },
      "auroc": 0.9340397671568628
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9813875000000001
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9848604166666668
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9848604166666668
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        },
        "0.01": null
      },
      "auroc": 0.986596875
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9859572916666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9873635416666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9866604166666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        },
        "0.01": null
      },
      "auroc": 0.6690552083333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9855968749999999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        },
        "0.01": null
      },
      "auroc": 0.8273260416666668
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.82750625
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9864802083333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 589,
          "fn": 211,
          "accuracy": 0.73625
        },
        "0.01": null
      },
      "auroc": 0.9069932291666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9867156250000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9875244791666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9850104166666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.986671875
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9858630208333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 779,
          "fn": 21,
          "accuracy": 0.97375
        },
        "0.01": null
      },
      "auroc": 0.9870981770833334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9830166666666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": null
      },
      "auroc": 0.985675
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": null
      },
      "auroc": 0.6878947916666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.6546343749999999
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 389,
          "accuracy": 0.0275
        },
        "0.01": null
      },
      "auroc": 0.6712645833333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.8381140625000001
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        },
        "0.01": null
      },
      "auroc": 0.8188255208333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 414,
          "accuracy": 0.4825
        },
        "0.01": null
      },
      "auroc": 0.8284697916666668
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9876229166666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": null
      },
      "auroc": 0.9619010416666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": null
      },
      "auroc": 0.9747619791666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        },
        "0.01": null
      },
      "auroc": 0.7042791666666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": null
      },
      "auroc": 0.8182625
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 319,
          "accuracy": 0.2025
        },
        "0.01": null
      },
      "auroc": 0.7612708333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        },
        "0.01": null
      },
      "auroc": 0.8459510416666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.8900817708333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 414,
          "fn": 386,
          "accuracy": 0.5175
        },
        "0.01": null
      },
      "auroc": 0.86801640625
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.987165625
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9877494791666668
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9877494791666668
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.98804140625
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9860041666666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9860041666666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9852458333333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9852458333333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.985625
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.985625
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": null
      },
      "auroc": 0.9169864583333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": null
      },
      "auroc": 0.9169864583333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": null
      },
      "auroc": 0.8393927083333332
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": null
      },
      "auroc": 0.8393927083333332
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        },
        "0.01": null
      },
      "auroc": 0.8781895833333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        },
        "0.01": null
      },
      "auroc": 0.8781895833333334
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.988225
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.988225
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9882791666666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9882791666666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": null
      },
      "auroc": 0.9424145833333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": null
      },
      "auroc": 0.9424145833333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.8960447916666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.8960447916666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9192296875
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9192296875
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2044,
          "fn": 156,
          "accuracy": 0.9290909090909091
        },
        "0.01": null
      },
      "auroc": 0.9771804924242424
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1101,
          "fn": 99,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9826105902777778
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3145,
          "fn": 255,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9790969975490197
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1444,
          "fn": 756,
          "accuracy": 0.6563636363636364
        },
        "0.01": null
      },
      "auroc": 0.883951893939394
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 826,
          "fn": 374,
          "accuracy": 0.6883333333333334
        },
        "0.01": null
      },
      "auroc": 0.902009548611111
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2270,
          "fn": 1130,
          "accuracy": 0.6676470588235294
        },
        "0.01": null
      },
      "auroc": 0.8903251838235295
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3488,
          "fn": 912,
          "accuracy": 0.7927272727272727
        },
        "0.01": null
      },
      "auroc": 0.9305661931818183
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1927,
          "fn": 473,
          "accuracy": 0.8029166666666666
        },
        "0.01": null
      },
      "auroc": 0.9423100694444444
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 5415,
          "fn": 1385,
          "accuracy": 0.7963235294117647
        },
        "0.01": null
      },
      "auroc": 0.9347110906862746
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1907104166666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.19330104166666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1920057291666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.15924895833333336
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.16227395833333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.16076145833333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1749796875
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.17778750000000001
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.17638359374999998
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": null
      },
      "auroc": 0.52023125
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.28590625000000003
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 341,
          "accuracy": 0.1475
        },
        "0.01": null
      },
      "auroc": 0.40306875
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.2161875
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.25458749999999997
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.2353875
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 339,
          "accuracy": 0.1525
        },
        "0.01": null
      },
      "auroc": 0.36820937500000006
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.270246875
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 65,
          "fn": 735,
          "accuracy": 0.08125
        },
        "0.01": null
      },
      "auroc": 0.319228125
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1268760416666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13408958333333335
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13048281250000002
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.10179687500000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11794062500000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.10986875
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11433645833333335
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1260151041666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.12017578124999999
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": null
      },
      "auroc": 0.8018010416666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.3496989583333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        },
        "0.01": null
      },
      "auroc": 0.57575
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1845885416666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.17056666666666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1775776041666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        },
        "0.01": null
      },
      "auroc": 0.49319479166666663
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.2601328125
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 671,
          "accuracy": 0.16125
        },
        "0.01": null
      },
      "auroc": 0.37666380208333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        },
        "0.01": null
      },
      "auroc": 0.6481354166666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.43540833333333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 94,
          "fn": 306,
          "accuracy": 0.235
        },
        "0.01": null
      },
      "auroc": 0.541771875
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        },
        "0.01": null
      },
      "auroc": 0.2511552083333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": null
      },
      "auroc": 0.21991041666666664
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 383,
          "accuracy": 0.0425
        },
        "0.01": null
      },
      "auroc": 0.23553281250000002
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 299,
          "accuracy": 0.2525
        },
        "0.01": null
      },
      "auroc": 0.4496453125
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.327659375
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 689,
          "accuracy": 0.13875
        },
        "0.01": null
      },
      "auroc": 0.38865234374999996
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.07739791666666666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.03469687500000001
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.05604739583333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.07092500000000002
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.039506250000000014
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.055215625000000004
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.07416145833333335
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.037101562500000004
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.05563151041666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.35124791666666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.35124791666666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.34715729166666665
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.34715729166666665
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        },
        "0.01": null
      },
      "auroc": 0.34920260416666665
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        },
        "0.01": null
      },
      "auroc": 0.34920260416666665
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.21641562500000003
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.21641562500000003
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.17490833333333336
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.17490833333333336
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1956619791666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1956619791666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.031908333333333344
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.031908333333333344
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.031119791666666678
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.031119791666666678
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.0315140625
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.0315140625
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.06171666666666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.06171666666666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.02224687500000001
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.02224687500000001
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.04198177083333334
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.04198177083333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.14404583333333335
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.14404583333333335
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13050520833333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13050520833333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13727552083333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13727552083333333
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 281,
          "fn": 1919,
          "accuracy": 0.12772727272727272
        },
        "0.01": null
      },
      "auroc": 0.28822604166666665
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 1197,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.23885017361111113
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 3116,
          "accuracy": 0.08352941176470588
        },
        "0.01": null
      },
      "auroc": 0.27079926470588234
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 2185,
          "accuracy": 0.006818181818181818
        },
        "0.01": null
      },
      "auroc": 0.1536217803030303
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 1189,
          "accuracy": 0.009166666666666667
        },
        "0.01": null
      },
      "auroc": 0.16079756944444443
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 3374,
          "accuracy": 0.007647058823529412
        },
        "0.01": null
      },
      "auroc": 0.1561544117647059
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 4104,
          "accuracy": 0.06727272727272728
        },
        "0.01": null
      },
      "auroc": 0.2209239109848485
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 2386,
          "accuracy": 0.005833333333333334
        },
        "0.01": null
      },
      "auroc": 0.19982387152777778
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 310,
          "fn": 6490,
          "accuracy": 0.045588235294117645
        },
        "0.01": null
      },
      "auroc": 0.21347683823529412
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9799229166666668
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.984128125
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.984128125
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9862307291666668
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": null
      },
      "auroc": 0.97379375
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9873635416666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9805786458333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": null
      },
      "auroc": 0.5689197916666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9855968749999999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": null
      },
      "auroc": 0.7772583333333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 222,
          "accuracy": 0.445
        },
        "0.01": null
      },
      "auroc": 0.7713567708333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9864802083333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 563,
          "fn": 237,
          "accuracy": 0.70375
        },
        "0.01": null
      },
      "auroc": 0.8789184895833333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9862395833333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9872864583333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9841583333333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9862458333333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9851989583333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 776,
          "fn": 24,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9867661458333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9874604166666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": null
      },
      "auroc": 0.966553125
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": null
      },
      "auroc": 0.9770067708333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.5839302083333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": null
      },
      "auroc": 0.6556239583333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.6197770833333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": null
      },
      "auroc": 0.7856953125
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 256,
          "accuracy": 0.36
        },
        "0.01": null
      },
      "auroc": 0.8110885416666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 343,
          "fn": 457,
          "accuracy": 0.42875
        },
        "0.01": null
      },
      "auroc": 0.7983919270833334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9814447916666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": null
      },
      "auroc": 0.9471437500000001
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        },
        "0.01": null
      },
      "auroc": 0.9642942708333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        },
        "0.01": null
      },
      "auroc": 0.6064197916666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": null
      },
      "auroc": 0.8507583333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 315,
          "accuracy": 0.2125
        },
        "0.01": null
      },
      "auroc": 0.7285890625
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        },
        "0.01": null
      },
      "auroc": 0.7939322916666668
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        },
        "0.01": null
      },
      "auroc": 0.8989510416666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 416,
          "accuracy": 0.48
        },
        "0.01": null
      },
      "auroc": 0.8464416666666665
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9861645833333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9872489583333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9872489583333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9877911458333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9767854166666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.9767854166666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": null
      },
      "auroc": 0.9724604166666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": null
      },
      "auroc": 0.9724604166666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": null
      },
      "auroc": 0.9746229166666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": null
      },
      "auroc": 0.9746229166666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        },
        "0.01": null
      },
      "auroc": 0.8402812499999999
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        },
        "0.01": null
      },
      "auroc": 0.8402812499999999
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": null
      },
      "auroc": 0.7412197916666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": null
      },
      "auroc": 0.7412197916666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 291,
          "accuracy": 0.2725
        },
        "0.01": null
      },
      "auroc": 0.7907505208333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 291,
          "accuracy": 0.2725
        },
        "0.01": null
      },
      "auroc": 0.7907505208333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9851260416666666
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9851260416666666
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9867296875
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9867296875
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9009104166666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9009104166666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": null
      },
      "auroc": 0.8436375
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": null
      },
      "auroc": 0.8436375
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 271,
          "fn": 129,
          "accuracy": 0.6775
        },
        "0.01": null
      },
      "auroc": 0.8722739583333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 271,
          "fn": 129,
          "accuracy": 0.6775
        },
        "0.01": null
      },
      "auroc": 0.8722739583333333
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1927,
          "fn": 273,
          "accuracy": 0.8759090909090909
        },
        "0.01": null
      },
      "auroc": 0.9638493371212121
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1043,
          "fn": 157,
          "accuracy": 0.8691666666666666
        },
        "0.01": null
      },
      "auroc": 0.9773277777777778
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2970,
          "fn": 430,
          "accuracy": 0.8735294117647059
        },
        "0.01": null
      },
      "auroc": 0.9686064338235294
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1336,
          "fn": 864,
          "accuracy": 0.6072727272727273
        },
        "0.01": null
      },
      "auroc": 0.8413678977272727
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 831,
          "fn": 369,
          "accuracy": 0.6925
        },
        "0.01": null
      },
      "auroc": 0.9070374999999999
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2167,
          "fn": 1233,
          "accuracy": 0.6373529411764706
        },
        "0.01": null
      },
      "auroc": 0.8645454044117646
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3263,
          "fn": 1137,
          "accuracy": 0.7415909090909091
        },
        "0.01": null
      },
      "auroc": 0.9026086174242424
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1874,
          "fn": 526,
          "accuracy": 0.7808333333333334
        },
        "0.01": null
      },
      "auroc": 0.9421826388888888
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 5137,
          "fn": 1663,
          "accuracy": 0.7554411764705883
        },
        "0.01": null
      },
      "auroc": 0.916575919117647
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.979884375
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9841088541666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9841088541666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 789,
          "fn": 11,
          "accuracy": 0.98625
        },
        "0.01": null
      },
      "auroc": 0.9862210937500001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9844062499999999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9869166666666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9856614583333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": null
      },
      "auroc": 0.621521875
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9855906249999999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.80355625
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": null
      },
      "auroc": 0.8029640625
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9862536458333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 583,
          "fn": 217,
          "accuracy": 0.72875
        },
        "0.01": null
      },
      "auroc": 0.8946088541666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9865833333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9874583333333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9840125
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9861729166666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9852979166666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        },
        "0.01": null
      },
      "auroc": 0.986815625
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9810322916666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9846828125
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.6323916666666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.5651677083333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        },
        "0.01": null
      },
      "auroc": 0.5987796875
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.8103625
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        },
        "0.01": null
      },
      "auroc": 0.7731
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 427,
          "accuracy": 0.46625
        },
        "0.01": null
      },
      "auroc": 0.79173125
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9869364583333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": null
      },
      "auroc": 0.9501770833333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        },
        "0.01": null
      },
      "auroc": 0.9685567708333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        },
        "0.01": null
      },
      "auroc": 0.6617447916666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        },
        "0.01": null
      },
      "auroc": 0.785815625
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 332,
          "accuracy": 0.17
        },
        "0.01": null
      },
      "auroc": 0.7237802083333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        },
        "0.01": null
      },
      "auroc": 0.824340625
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        },
        "0.01": null
      },
      "auroc": 0.8679963541666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 421,
          "accuracy": 0.47375
        },
        "0.01": null
      },
      "auroc": 0.8461684895833333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9869895833333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9876614583333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9876614583333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9879973958333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9852927083333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9852927083333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9831624999999999
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9831624999999999
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9842276041666668
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9842276041666668
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        },
        "0.01": null
      },
      "auroc": 0.8971374999999999
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        },
        "0.01": null
      },
      "auroc": 0.8971374999999999
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        },
        "0.01": null
      },
      "auroc": 0.8046552083333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        },
        "0.01": null
      },
      "auroc": 0.8046552083333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 247,
          "accuracy": 0.3825
        },
        "0.01": null
      },
      "auroc": 0.8508963541666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 247,
          "accuracy": 0.3825
        },
        "0.01": null
      },
      "auroc": 0.8508963541666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9883333333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9855145833333334
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9855145833333334
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9869239583333334
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9869239583333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.9309822916666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.9309822916666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.8824375
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.8824375
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 291,
          "fn": 109,
          "accuracy": 0.7275
        },
        "0.01": null
      },
      "auroc": 0.9067098958333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 291,
          "fn": 109,
          "accuracy": 0.7275
        },
        "0.01": null
      },
      "auroc": 0.9067098958333333
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2016,
          "fn": 184,
          "accuracy": 0.9163636363636364
        },
        "0.01": null
      },
      "auroc": 0.9740686553030302
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1070,
          "fn": 130,
          "accuracy": 0.8916666666666667
        },
        "0.01": null
      },
      "auroc": 0.9802293402777777
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3086,
          "fn": 314,
          "accuracy": 0.9076470588235294
        },
        "0.01": null
      },
      "auroc": 0.9762430147058824
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1402,
          "fn": 798,
          "accuracy": 0.6372727272727273
        },
        "0.01": null
      },
      "auroc": 0.8658874053030303
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 811,
          "fn": 389,
          "accuracy": 0.6758333333333333
        },
        "0.01": null
      },
      "auroc": 0.8812434027777778
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2213,
          "fn": 1187,
          "accuracy": 0.6508823529411765
        },
        "0.01": null
      },
      "auroc": 0.871307169117647
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3418,
          "fn": 982,
          "accuracy": 0.7768181818181819
        },
        "0.01": null
      },
      "auroc": 0.9199780303030303
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1881,
          "fn": 519,
          "accuracy": 0.78375
        },
        "0.01": null
      },
      "auroc": 0.9307363715277779
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 5299,
          "fn": 1501,
          "accuracy": 0.779264705882353
        },
        "0.01": null
      },
      "auroc": 0.9237750919117648
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.09650104166666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.10470312500000001
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.10060208333333336
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.09512291666666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.10359375000000001
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.09935833333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.09581197916666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.10414843750000001
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.09998020833333332
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        },
        "0.01": null
      },
      "auroc": 0.4020083333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.28885208333333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 363,
          "accuracy": 0.0925
        },
        "0.01": null
      },
      "auroc": 0.3454302083333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.17515833333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.29071979166666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        },
        "0.01": null
      },
      "auroc": 0.23293906249999996
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 39,
          "fn": 361,
          "accuracy": 0.0975
        },
        "0.01": null
      },
      "auroc": 0.2885833333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": null
      },
      "auroc": 0.2897859375
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 758,
          "accuracy": 0.0525
        },
        "0.01": null
      },
      "auroc": 0.28918463541666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.0638625
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.10511875000000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.084490625
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.07553437499999999
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13760312500000002
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.10656874999999999
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.0696984375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1213609375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.09552968750000002
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.7354114583333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.17650312499999998
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 279,
          "accuracy": 0.3025
        },
        "0.01": null
      },
      "auroc": 0.4559572916666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1665385416666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.18253020833333336
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.17453437500000002
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 279,
          "accuracy": 0.3025
        },
        "0.01": null
      },
      "auroc": 0.45097500000000007
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.17951666666666669
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 679,
          "accuracy": 0.15125
        },
        "0.01": null
      },
      "auroc": 0.31524583333333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        },
        "0.01": null
      },
      "auroc": 0.5725166666666668
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.24148645833333332
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 75,
          "fn": 325,
          "accuracy": 0.1875
        },
        "0.01": null
      },
      "auroc": 0.40700156249999997
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        },
        "0.01": null
      },
      "auroc": 0.21289583333333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": null
      },
      "auroc": 0.25207395833333335
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 383,
          "accuracy": 0.0425
        },
        "0.01": null
      },
      "auroc": 0.23248489583333332
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 318,
          "accuracy": 0.205
        },
        "0.01": null
      },
      "auroc": 0.3927062499999999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.24678020833333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 708,
          "accuracy": 0.115
        },
        "0.01": null
      },
      "auroc": 0.3197432291666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.08082916666666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.07451875000000001
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.07767395833333335
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.079753125
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.08560833333333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.08268072916666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.08029114583333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.08006354166666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.08017734374999999
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.12180104166666668
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.12180104166666668
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.13926666666666665
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.13926666666666665
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.13053385416666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.13053385416666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13588020833333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13588020833333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13347083333333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13347083333333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13467552083333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13467552083333334
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.081315625
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.081315625
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.07814791666666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.07814791666666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.07973177083333334
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.07973177083333334
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.07338958333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.07338958333333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.081584375
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.081584375
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.07748697916666665
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.07748697916666665
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.09828125
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.09828125
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11078125
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11078125
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.10453124999999999
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.10453124999999999
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 230,
          "fn": 1970,
          "accuracy": 0.10454545454545454
        },
        "0.01": null
      },
      "auroc": 0.2237997159090909
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 1197,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.16519704861111112
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 3167,
          "accuracy": 0.06852941176470588
        },
        "0.01": null
      },
      "auroc": 0.20311642156862744
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 2186,
          "accuracy": 0.006363636363636364
        },
        "0.01": null
      },
      "auroc": 0.1225685606060606
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 1190,
          "accuracy": 0.008333333333333333
        },
        "0.01": null
      },
      "auroc": 0.17535486111111112
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 24,
          "fn": 3376,
          "accuracy": 0.007058823529411765
        },
        "0.01": null
      },
      "auroc": 0.14119901960784315
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 4156,
          "accuracy": 0.05545454545454546
        },
        "0.01": null
      },
      "auroc": 0.17318413825757575
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 2387,
          "accuracy": 0.005416666666666667
        },
        "0.01": null
      },
      "auroc": 0.1702759548611111
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 257,
          "fn": 6543,
          "accuracy": 0.037794117647058825
        },
        "0.01": null
      },
      "auroc": 0.17215772058823528
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1977,
          "fn": 423,
          "accuracy": 0.82375
        },
        "0.01": null
      },
      "auroc": 0.8468916666666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1971,
          "fn": 429,
          "accuracy": 0.82125
        },
        "0.01": null
      },
      "auroc": 0.8476592881944445
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3948,
          "fn": 852,
          "accuracy": 0.8225
        },
        "0.01": null
      },
      "auroc": 0.8472754774305556
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1974,
          "fn": 426,
          "accuracy": 0.8225
        },
        "0.01": null
      },
      "auroc": 0.8438535590277778
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1846,
          "fn": 554,
          "accuracy": 0.7691666666666667
        },
        "0.01": null
      },
      "auroc": 0.8358625
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3820,
          "fn": 980,
          "accuracy": 0.7958333333333333
        },
        "0.01": null
      },
      "auroc": 0.8398580295138889
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3951,
          "fn": 849,
          "accuracy": 0.823125
        },
        "0.01": null
      },
      "auroc": 0.8453726128472223
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3817,
          "fn": 983,
          "accuracy": 0.7952083333333333
        },
        "0.01": null
      },
      "auroc": 0.8417608940972222
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7768,
          "fn": 1832,
          "accuracy": 0.8091666666666667
        },
        "0.01": null
      },
      "auroc": 0.8435667534722222
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1900,
          "fn": 500,
          "accuracy": 0.7916666666666666
        },
        "0.01": null
      },
      "auroc": 0.8935346354166667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1743,
          "fn": 657,
          "accuracy": 0.72625
        },
        "0.01": null
      },
      "auroc": 0.8472131944444444
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3643,
          "fn": 1157,
          "accuracy": 0.7589583333333333
        },
        "0.01": null
      },
      "auroc": 0.8703739149305556
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 2279,
          "accuracy": 0.050416666666666665
        },
        "0.01": null
      },
      "auroc": 0.5467416666666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1735,
          "fn": 665,
          "accuracy": 0.7229166666666667
        },
        "0.01": null
      },
      "auroc": 0.8377664930555556
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1856,
          "fn": 2944,
          "accuracy": 0.38666666666666666
        },
        "0.01": null
      },
      "auroc": 0.6922540798611111
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2021,
          "fn": 2779,
          "accuracy": 0.42104166666666665
        },
        "0.01": null
      },
      "auroc": 0.7201381510416667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3478,
          "fn": 1322,
          "accuracy": 0.7245833333333334
        },
        "0.01": null
      },
      "auroc": 0.84248984375
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5499,
          "fn": 4101,
          "accuracy": 0.5728125
        },
        "0.01": null
      },
      "auroc": 0.7813139973958333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1969,
          "fn": 431,
          "accuracy": 0.8204166666666667
        },
        "0.01": null
      },
      "auroc": 0.8380929687500001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1842,
          "fn": 558,
          "accuracy": 0.7675
        },
        "0.01": null
      },
      "auroc": 0.8371894097222223
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3811,
          "fn": 989,
          "accuracy": 0.7939583333333333
        },
        "0.01": null
      },
      "auroc": 0.8376411892361111
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1978,
          "fn": 422,
          "accuracy": 0.8241666666666667
        },
        "0.01": null
      },
      "auroc": 0.8370409722222222
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1705,
          "fn": 695,
          "accuracy": 0.7104166666666667
        },
        "0.01": null
      },
      "auroc": 0.8329377604166667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3683,
          "fn": 1117,
          "accuracy": 0.7672916666666667
        },
        "0.01": null
      },
      "auroc": 0.8349893663194444
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3947,
          "fn": 853,
          "accuracy": 0.8222916666666666
        },
        "0.01": null
      },
      "auroc": 0.8375669704861111
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3547,
          "fn": 1253,
          "accuracy": 0.7389583333333334
        },
        "0.01": null
      },
      "auroc": 0.8350635850694446
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7494,
          "fn": 2106,
          "accuracy": 0.780625
        },
        "0.01": null
      },
      "auroc": 0.8363152777777777
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2229,
          "fn": 171,
          "accuracy": 0.92875
        },
        "0.01": null
      },
      "auroc": 0.9501876736111111
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1418,
          "fn": 982,
          "accuracy": 0.5908333333333333
        },
        "0.01": null
      },
      "auroc": 0.8464072916666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3647,
          "fn": 1153,
          "accuracy": 0.7597916666666666
        },
        "0.01": null
      },
      "auroc": 0.8982974826388888
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 2354,
          "accuracy": 0.019166666666666665
        },
        "0.01": null
      },
      "auroc": 0.5524144965277779
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 52,
          "fn": 2348,
          "accuracy": 0.021666666666666667
        },
        "0.01": null
      },
      "auroc": 0.5324548611111111
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 4702,
          "accuracy": 0.020416666666666666
        },
        "0.01": null
      },
      "auroc": 0.5424346788194444
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2275,
          "fn": 2525,
          "accuracy": 0.4739583333333333
        },
        "0.01": null
      },
      "auroc": 0.7513010850694445
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1470,
          "fn": 3330,
          "accuracy": 0.30625
        },
        "0.01": null
      },
      "auroc": 0.6894310763888889
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3745,
          "fn": 5855,
          "accuracy": 0.39010416666666664
        },
        "0.01": null
      },
      "auroc": 0.7203660807291667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2032,
          "fn": 368,
          "accuracy": 0.8466666666666667
        },
        "0.01": null
      },
      "auroc": 0.9223592881944445
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1137,
          "fn": 1263,
          "accuracy": 0.47375
        },
        "0.01": null
      },
      "auroc": 0.8372834201388888
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3169,
          "fn": 1631,
          "accuracy": 0.6602083333333333
        },
        "0.01": null
      },
      "auroc": 0.8798213541666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 210,
          "fn": 2190,
          "accuracy": 0.0875
        },
        "0.01": null
      },
      "auroc": 0.5832376736111111
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 518,
          "fn": 1882,
          "accuracy": 0.21583333333333332
        },
        "0.01": null
      },
      "auroc": 0.6897040798611112
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 728,
          "fn": 4072,
          "accuracy": 0.15166666666666667
        },
        "0.01": null
      },
      "auroc": 0.6364708767361111
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2242,
          "fn": 2558,
          "accuracy": 0.46708333333333335
        },
        "0.01": null
      },
      "auroc": 0.7527984809027778
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1655,
          "fn": 3145,
          "accuracy": 0.34479166666666666
        },
        "0.01": null
      },
      "auroc": 0.7634937500000001
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3897,
          "fn": 5703,
          "accuracy": 0.4059375
        },
        "0.01": null
      },
      "auroc": 0.7581461154513889
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1986,
          "fn": 414,
          "accuracy": 0.8275
        },
        "0.01": null
      },
      "auroc": 0.8365196180555556
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1967,
          "fn": 433,
          "accuracy": 0.8195833333333333
        },
        "0.01": null
      },
      "auroc": 0.83163984375
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3953,
          "fn": 847,
          "accuracy": 0.8235416666666666
        },
        "0.01": null
      },
      "auroc": 0.8340797309027779
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1983,
          "fn": 417,
          "accuracy": 0.82625
        },
        "0.01": null
      },
      "auroc": 0.8354120659722222
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1831,
          "fn": 569,
          "accuracy": 0.7629166666666667
        },
        "0.01": null
      },
      "auroc": 0.8270651909722222
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3814,
          "fn": 986,
          "accuracy": 0.7945833333333333
        },
        "0.01": null
      },
      "auroc": 0.8312386284722222
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3969,
          "fn": 831,
          "accuracy": 0.826875
        },
        "0.01": null
      },
      "auroc": 0.8359658420138888
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3798,
          "fn": 1002,
          "accuracy": 0.79125
        },
        "0.01": null
      },
      "auroc": 0.829352517361111
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7767,
          "fn": 1833,
          "accuracy": 0.8090625
        },
        "0.01": null
      },
      "auroc": 0.8326591796875
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1721,
          "fn": 679,
          "accuracy": 0.7170833333333333
        },
        "0.01": null
      },
      "auroc": 0.8553758680555557
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1721,
          "fn": 679,
          "accuracy": 0.7170833333333333
        },
        "0.01": null
      },
      "auroc": 0.8553758680555557
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1689,
          "fn": 711,
          "accuracy": 0.70375
        },
        "0.01": null
      },
      "auroc": 0.8533381944444445
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1689,
          "fn": 711,
          "accuracy": 0.70375
        },
        "0.01": null
      },
      "auroc": 0.8533381944444445
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3410,
          "fn": 1390,
          "accuracy": 0.7104166666666667
        },
        "0.01": null
      },
      "auroc": 0.85435703125
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3410,
          "fn": 1390,
          "accuracy": 0.7104166666666667
        },
        "0.01": null
      },
      "auroc": 0.85435703125
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 850,
          "fn": 1550,
          "accuracy": 0.3541666666666667
        },
        "0.01": null
      },
      "auroc": 0.7579276909722221
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 850,
          "fn": 1550,
          "accuracy": 0.3541666666666667
        },
        "0.01": null
      },
      "auroc": 0.7579276909722221
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 551,
          "fn": 1849,
          "accuracy": 0.22958333333333333
        },
        "0.01": null
      },
      "auroc": 0.6822403645833334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 551,
          "fn": 1849,
          "accuracy": 0.22958333333333333
        },
        "0.01": null
      },
      "auroc": 0.6822403645833334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1401,
          "fn": 3399,
          "accuracy": 0.291875
        },
        "0.01": null
      },
      "auroc": 0.7200840277777778
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1401,
          "fn": 3399,
          "accuracy": 0.291875
        },
        "0.01": null
      },
      "auroc": 0.7200840277777778
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1963,
          "fn": 437,
          "accuracy": 0.8179166666666666
        },
        "0.01": null
      },
      "auroc": 0.8317309895833334
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1963,
          "fn": 437,
          "accuracy": 0.8179166666666666
        },
        "0.01": null
      },
      "auroc": 0.8317309895833334
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1970,
          "fn": 430,
          "accuracy": 0.8208333333333333
        },
        "0.01": null
      },
      "auroc": 0.8316209201388889
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1970,
          "fn": 430,
          "accuracy": 0.8208333333333333
        },
        "0.01": null
      },
      "auroc": 0.8316209201388889
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3933,
          "fn": 867,
          "accuracy": 0.819375
        },
        "0.01": null
      },
      "auroc": 0.8316759548611111
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3933,
          "fn": 867,
          "accuracy": 0.819375
        },
        "0.01": null
      },
      "auroc": 0.8316759548611111
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1981,
          "fn": 419,
          "accuracy": 0.8254166666666667
        },
        "0.01": null
      },
      "auroc": 0.8344354166666668
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1981,
          "fn": 419,
          "accuracy": 0.8254166666666667
        },
        "0.01": null
      },
      "auroc": 0.8344354166666668
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1830,
          "fn": 570,
          "accuracy": 0.7625
        },
        "0.01": null
      },
      "auroc": 0.8264604166666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1830,
          "fn": 570,
          "accuracy": 0.7625
        },
        "0.01": null
      },
      "auroc": 0.8264604166666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3811,
          "fn": 989,
          "accuracy": 0.7939583333333333
        },
        "0.01": null
      },
      "auroc": 0.8304479166666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3811,
          "fn": 989,
          "accuracy": 0.7939583333333333
        },
        "0.01": null
      },
      "auroc": 0.8304479166666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1502,
          "fn": 898,
          "accuracy": 0.6258333333333334
        },
        "0.01": null
      },
      "auroc": 0.7860933159722222
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1502,
          "fn": 898,
          "accuracy": 0.6258333333333334
        },
        "0.01": null
      },
      "auroc": 0.7860933159722222
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1301,
          "fn": 1099,
          "accuracy": 0.5420833333333334
        },
        "0.01": null
      },
      "auroc": 0.7426055555555555
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1301,
          "fn": 1099,
          "accuracy": 0.5420833333333334
        },
        "0.01": null
      },
      "auroc": 0.7426055555555555
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2803,
          "fn": 1997,
          "accuracy": 0.5839583333333334
        },
        "0.01": null
      },
      "auroc": 0.7643494357638889
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2803,
          "fn": 1997,
          "accuracy": 0.5839583333333334
        },
        "0.01": null
      },
      "auroc": 0.7643494357638889
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 20110,
          "fn": 6290,
          "accuracy": 0.7617424242424242
        },
        "0.01": null
      },
      "auroc": 0.8502862847222221
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 10078,
          "fn": 4322,
          "accuracy": 0.6998611111111112
        },
        "0.01": null
      },
      "auroc": 0.8412320746527778
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 30188,
          "fn": 10612,
          "accuracy": 0.7399019607843137
        },
        "0.01": null
      },
      "auroc": 0.8470906811683007
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 13653,
          "fn": 12747,
          "accuracy": 0.517159090909091
        },
        "0.01": null
      },
      "auroc": 0.739542353219697
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7687,
          "fn": 6713,
          "accuracy": 0.5338194444444444
        },
        "0.01": null
      },
      "auroc": 0.7592984809027779
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 21340,
          "fn": 19460,
          "accuracy": 0.5230392156862745
        },
        "0.01": null
      },
      "auroc": 0.7465151041666667
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 33763,
          "fn": 19037,
          "accuracy": 0.6394507575757575
        },
        "0.01": null
      },
      "auroc": 0.7949143189709595
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 17765,
          "fn": 11035,
          "accuracy": 0.6168402777777777
        },
        "0.01": null
      },
      "auroc": 0.8002652777777777
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 51528,
          "fn": 30072,
          "accuracy": 0.6314705882352941
        },
        "0.01": null
      },
      "auroc": 0.7968028926674837
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9957822916666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958078125
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9957239583333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.99309375
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9944088541666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.995753125
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9944635416666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9951083333333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.993621875
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9944552083333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9940385416666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        },
        "0.01": null
      },
      "auroc": 0.7412552083333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9913052083333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.8662802083333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.8674385416666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9928802083333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 596,
          "fn": 204,
          "accuracy": 0.745
        },
        "0.01": null
      },
      "auroc": 0.930159375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9957822916666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9880135416666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9918979166666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9956354166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9889604166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9922979166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9957088541666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9884869791666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 757,
          "fn": 43,
          "accuracy": 0.94625
        },
        "0.01": null
      },
      "auroc": 0.9920979166666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9938489583333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9948411458333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": null
      },
      "auroc": 0.6462552083333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": null
      },
      "auroc": 0.7300947916666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 30,
          "fn": 370,
          "accuracy": 0.075
        },
        "0.01": null
      },
      "auroc": 0.688175
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        },
        "0.01": null
      },
      "auroc": 0.8210442708333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": null
      },
      "auroc": 0.861971875
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 424,
          "fn": 376,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.8415080729166666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9941302083333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9920958333333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9931130208333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": null
      },
      "auroc": 0.7974781249999999
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": null
      },
      "auroc": 0.9256302083333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        },
        "0.01": null
      },
      "auroc": 0.8615541666666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        },
        "0.01": null
      },
      "auroc": 0.8958041666666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": null
      },
      "auroc": 0.9588630208333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 568,
          "fn": 232,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.92733359375
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9953708333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9956020833333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9908375
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.9377010416666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9642692708333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9931041666666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": null
      },
      "auroc": 0.9667671875
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 706,
          "fn": 94,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.9799356770833333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9866489583333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9866489583333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9622354166666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9622354166666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.9744421875
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.9744421875
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": null
      },
      "auroc": 0.957015625
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": null
      },
      "auroc": 0.957015625
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": null
      },
      "auroc": 0.9253458333333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": null
      },
      "auroc": 0.9253458333333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.9411807291666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.9411807291666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9937677083333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9937677083333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9846125
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9846125
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9891901041666668
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9891901041666668
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9771510416666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9771510416666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": null
      },
      "auroc": 0.9686614583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": null
      },
      "auroc": 0.9686614583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9729062500000001
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9729062500000001
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2071,
          "fn": 129,
          "accuracy": 0.9413636363636364
        },
        "0.01": null
      },
      "auroc": 0.9891761363636364
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1160,
          "fn": 40,
          "accuracy": 0.9666666666666667
        },
        "0.01": null
      },
      "auroc": 0.9933467013888889
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3231,
          "fn": 169,
          "accuracy": 0.9502941176470588
        },
        "0.01": null
      },
      "auroc": 0.9906481004901961
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1487,
          "fn": 713,
          "accuracy": 0.6759090909090909
        },
        "0.01": null
      },
      "auroc": 0.9094430871212122
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 823,
          "fn": 377,
          "accuracy": 0.6858333333333333
        },
        "0.01": null
      },
      "auroc": 0.9277975694444444
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2310,
          "fn": 1090,
          "accuracy": 0.6794117647058824
        },
        "0.01": null
      },
      "auroc": 0.9159211397058824
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3558,
          "fn": 842,
          "accuracy": 0.8086363636363636
        },
        "0.01": null
      },
      "auroc": 0.9493096117424242
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1983,
          "fn": 417,
          "accuracy": 0.82625
        },
        "0.01": null
      },
      "auroc": 0.9605721354166666
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 5541,
          "fn": 1259,
          "accuracy": 0.8148529411764706
        },
        "0.01": null
      },
      "auroc": 0.9532846200980392
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9957822916666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958078125
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9957239583333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.99309375
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9944088541666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.995753125
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9944635416666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9951083333333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.993621875
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9944552083333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9940385416666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        },
        "0.01": null
      },
      "auroc": 0.7412552083333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9913052083333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.8662802083333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.8674385416666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9928802083333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 596,
          "fn": 204,
          "accuracy": 0.745
        },
        "0.01": null
      },
      "auroc": 0.930159375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9957822916666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9880135416666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9918979166666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9956354166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9889604166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9922979166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9957088541666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9884869791666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 757,
          "fn": 43,
          "accuracy": 0.94625
        },
        "0.01": null
      },
      "auroc": 0.9920979166666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9938489583333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9948411458333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": null
      },
      "auroc": 0.6462552083333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": null
      },
      "auroc": 0.7300947916666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 30,
          "fn": 370,
          "accuracy": 0.075
        },
        "0.01": null
      },
      "auroc": 0.688175
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        },
        "0.01": null
      },
      "auroc": 0.8210442708333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": null
      },
      "auroc": 0.861971875
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 424,
          "fn": 376,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.8415080729166666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9941302083333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9920958333333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9931130208333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": null
      },
      "auroc": 0.7974781249999999
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": null
      },
      "auroc": 0.9256302083333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        },
        "0.01": null
      },
      "auroc": 0.8615541666666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        },
        "0.01": null
      },
      "auroc": 0.8958041666666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": null
      },
      "auroc": 0.9588630208333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 568,
          "fn": 232,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.92733359375
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9953708333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9956020833333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9908375
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.9377010416666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9642692708333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9931041666666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": null
      },
      "auroc": 0.9667671875
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 706,
          "fn": 94,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.9799356770833333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9866489583333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9866489583333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9622354166666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9622354166666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.9744421875
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.9744421875
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": null
      },
      "auroc": 0.957015625
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": null
      },
      "auroc": 0.957015625
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": null
      },
      "auroc": 0.9253458333333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": null
      },
      "auroc": 0.9253458333333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.9411807291666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.9411807291666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9937677083333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9937677083333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9846125
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9846125
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9891901041666668
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9891901041666668
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9771510416666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9771510416666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": null
      },
      "auroc": 0.9686614583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": null
      },
      "auroc": 0.9686614583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9729062500000001
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9729062500000001
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2071,
          "fn": 129,
          "accuracy": 0.9413636363636364
        },
        "0.01": null
      },
      "auroc": 0.9891761363636364
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1160,
          "fn": 40,
          "accuracy": 0.9666666666666667
        },
        "0.01": null
      },
      "auroc": 0.9933467013888889
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3231,
          "fn": 169,
          "accuracy": 0.9502941176470588
        },
        "0.01": null
      },
      "auroc": 0.9906481004901961
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1487,
          "fn": 713,
          "accuracy": 0.6759090909090909
        },
        "0.01": null
      },
      "auroc": 0.9094430871212122
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 823,
          "fn": 377,
          "accuracy": 0.6858333333333333
        },
        "0.01": null
      },
      "auroc": 0.9277975694444444
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2310,
          "fn": 1090,
          "accuracy": 0.6794117647058824
        },
        "0.01": null
      },
      "auroc": 0.9159211397058824
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3558,
          "fn": 842,
          "accuracy": 0.8086363636363636
        },
        "0.01": null
      },
      "auroc": 0.9493096117424242
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1983,
          "fn": 417,
          "accuracy": 0.82625
        },
        "0.01": null
      },
      "auroc": 0.9605721354166666
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 5541,
          "fn": 1259,
          "accuracy": 0.8148529411764706
        },
        "0.01": null
      },
      "auroc": 0.9532846200980392
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.995584375
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9956802083333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9956322916666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9953395833333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9920385416666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9936890625000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9954619791666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.993859375
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 792,
          "fn": 8,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9946606770833334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9896875
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9945625
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.992125
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        },
        "0.01": null
      },
      "auroc": 0.6517593749999999
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9915322916666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": null
      },
      "auroc": 0.8216458333333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        },
        "0.01": null
      },
      "auroc": 0.8207234375000001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9930473958333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 581,
          "fn": 219,
          "accuracy": 0.72625
        },
        "0.01": null
      },
      "auroc": 0.9068854166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9954458333333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9838364583333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9896411458333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9951000000000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9870322916666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": null
      },
      "auroc": 0.9910661458333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9952729166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9854343750000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 741,
          "fn": 59,
          "accuracy": 0.92625
        },
        "0.01": null
      },
      "auroc": 0.9903536458333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.9847239583333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": null
      },
      "auroc": 0.9902786458333332
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.541209375
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        },
        "0.01": null
      },
      "auroc": 0.7018645833333335
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        },
        "0.01": null
      },
      "auroc": 0.6215369791666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": null
      },
      "auroc": 0.7685213541666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        },
        "0.01": null
      },
      "auroc": 0.8432942708333332
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 414,
          "accuracy": 0.4825
        },
        "0.01": null
      },
      "auroc": 0.8059078125000001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.99250625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9886072916666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9905567708333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": null
      },
      "auroc": 0.728671875
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": null
      },
      "auroc": 0.9229739583333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        },
        "0.01": null
      },
      "auroc": 0.8258229166666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        },
        "0.01": null
      },
      "auroc": 0.8605890625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        },
        "0.01": null
      },
      "auroc": 0.955790625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 553,
          "fn": 247,
          "accuracy": 0.69125
        },
        "0.01": null
      },
      "auroc": 0.90818984375
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9952822916666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9952343749999999
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9952583333333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9851072916666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        },
        "0.01": null
      },
      "auroc": 0.9058552083333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        },
        "0.01": null
      },
      "auroc": 0.9454812499999999
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9901947916666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": null
      },
      "auroc": 0.9505447916666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 679,
          "fn": 121,
          "accuracy": 0.84875
        },
        "0.01": null
      },
      "auroc": 0.9703697916666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": null
      },
      "auroc": 0.9719520833333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": null
      },
      "auroc": 0.9719520833333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": null
      },
      "auroc": 0.9322833333333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": null
      },
      "auroc": 0.9322833333333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 259,
          "fn": 141,
          "accuracy": 0.6475
        },
        "0.01": null
      },
      "auroc": 0.9521177083333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 259,
          "fn": 141,
          "accuracy": 0.6475
        },
        "0.01": null
      },
      "auroc": 0.9521177083333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        },
        "0.01": null
      },
      "auroc": 0.9161364583333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        },
        "0.01": null
      },
      "auroc": 0.9161364583333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        },
        "0.01": null
      },
      "auroc": 0.873659375
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        },
        "0.01": null
      },
      "auroc": 0.873659375
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        },
        "0.01": null
      },
      "auroc": 0.8948979166666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        },
        "0.01": null
      },
      "auroc": 0.8948979166666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9957822916666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9957822916666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958078125
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958078125
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9899489583333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9899489583333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9705479166666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9705479166666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": null
      },
      "auroc": 0.9802484375
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": null
      },
      "auroc": 0.9802484375
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.9587145833333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.9587145833333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9450479166666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9450479166666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.95188125
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.95188125
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1972,
          "fn": 228,
          "accuracy": 0.8963636363636364
        },
        "0.01": null
      },
      "auroc": 0.9815339962121212
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1110,
          "fn": 90,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.990440798611111
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3082,
          "fn": 318,
          "accuracy": 0.9064705882352941
        },
        "0.01": null
      },
      "auroc": 0.9846775735294118
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1344,
          "fn": 856,
          "accuracy": 0.610909090909091
        },
        "0.01": null
      },
      "auroc": 0.8740508522727273
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 805,
          "fn": 395,
          "accuracy": 0.6708333333333333
        },
        "0.01": null
      },
      "auroc": 0.9168828124999999
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2149,
          "fn": 1251,
          "accuracy": 0.6320588235294118
        },
        "0.01": null
      },
      "auroc": 0.8891680147058825
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3316,
          "fn": 1084,
          "accuracy": 0.7536363636363637
        },
        "0.01": null
      },
      "auroc": 0.9277924242424243
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1915,
          "fn": 485,
          "accuracy": 0.7979166666666667
        },
        "0.01": null
      },
      "auroc": 0.9536618055555556
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 5231,
          "fn": 1569,
          "accuracy": 0.769264705882353
        },
        "0.01": null
      },
      "auroc": 0.9369227941176472
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9903791666666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.986921875
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9886505208333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9897927083333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9797822916666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9847875
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9900859375000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.9833520833333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 752,
          "fn": 48,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9867190104166667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9838125
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.994340625
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": null
      },
      "auroc": 0.9890765625
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": null
      },
      "auroc": 0.5877229166666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9908822916666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.7893026041666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        },
        "0.01": null
      },
      "auroc": 0.7857677083333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9926114583333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 569,
          "fn": 231,
          "accuracy": 0.71125
        },
        "0.01": null
      },
      "auroc": 0.8891895833333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9920677083333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.96544375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        },
        "0.01": null
      },
      "auroc": 0.9787557291666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9904593749999999
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.9810802083333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9857697916666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": null
      },
      "auroc": 0.9912635416666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": null
      },
      "auroc": 0.9732619791666668
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 678,
          "fn": 122,
          "accuracy": 0.8475
        },
        "0.01": null
      },
      "auroc": 0.9822627604166667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.9426833333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        },
        "0.01": null
      },
      "auroc": 0.9692583333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.49880312499999996
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        },
        "0.01": null
      },
      "auroc": 0.7116958333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 371,
          "accuracy": 0.0725
        },
        "0.01": null
      },
      "auroc": 0.6052494791666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": null
      },
      "auroc": 0.7473182291666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 274,
          "accuracy": 0.315
        },
        "0.01": null
      },
      "auroc": 0.8271895833333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 473,
          "accuracy": 0.40875
        },
        "0.01": null
      },
      "auroc": 0.78725390625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9906708333333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9776354166666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.984153125
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        },
        "0.01": null
      },
      "auroc": 0.6901770833333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": null
      },
      "auroc": 0.9301510416666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 228,
          "accuracy": 0.43
        },
        "0.01": null
      },
      "auroc": 0.8101640625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        },
        "0.01": null
      },
      "auroc": 0.8404239583333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 295,
          "fn": 105,
          "accuracy": 0.7375
        },
        "0.01": null
      },
      "auroc": 0.9538932291666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 538,
          "fn": 262,
          "accuracy": 0.6725
        },
        "0.01": null
      },
      "auroc": 0.89715859375
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9919125
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.9863552083333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9891338541666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.9682375000000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        },
        "0.01": null
      },
      "auroc": 0.8644145833333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        },
        "0.01": null
      },
      "auroc": 0.9163260416666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": null
      },
      "auroc": 0.9800749999999999
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        },
        "0.01": null
      },
      "auroc": 0.9253848958333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 592,
          "fn": 208,
          "accuracy": 0.74
        },
        "0.01": null
      },
      "auroc": 0.9527299479166665
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.9368375
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.9368375
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": null
      },
      "auroc": 0.8852177083333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": null
      },
      "auroc": 0.8852177083333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 236,
          "accuracy": 0.41
        },
        "0.01": null
      },
      "auroc": 0.9110276041666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 236,
          "accuracy": 0.41
        },
        "0.01": null
      },
      "auroc": 0.9110276041666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        },
        "0.01": null
      },
      "auroc": 0.8625989583333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        },
        "0.01": null
      },
      "auroc": 0.8625989583333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": null
      },
      "auroc": 0.8211041666666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": null
      },
      "auroc": 0.8211041666666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 282,
          "accuracy": 0.295
        },
        "0.01": null
      },
      "auroc": 0.8418515625
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 282,
          "accuracy": 0.295
        },
        "0.01": null
      },
      "auroc": 0.8418515625
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9932114583333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9932114583333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9911635416666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9911635416666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9921875
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9921875
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9740291666666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.9740291666666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": null
      },
      "auroc": 0.9463458333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": null
      },
      "auroc": 0.9463458333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        },
        "0.01": null
      },
      "auroc": 0.9601875
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        },
        "0.01": null
      },
      "auroc": 0.9601875
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": null
      },
      "auroc": 0.9272989583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": null
      },
      "auroc": 0.9272989583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": null
      },
      "auroc": 0.9151114583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": null
      },
      "auroc": 0.9151114583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        },
        "0.01": null
      },
      "auroc": 0.9212052083333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        },
        "0.01": null
      },
      "auroc": 0.9212052083333333
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1804,
          "fn": 396,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9671501893939394
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 977,
          "fn": 223,
          "accuracy": 0.8141666666666667
        },
        "0.01": null
      },
      "auroc": 0.9755633680555555
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2781,
          "fn": 619,
          "accuracy": 0.8179411764705883
        },
        "0.01": null
      },
      "auroc": 0.9701195465686275
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1135,
          "fn": 1065,
          "accuracy": 0.5159090909090909
        },
        "0.01": null
      },
      "auroc": 0.8440123106060606
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 751,
          "fn": 449,
          "accuracy": 0.6258333333333334
        },
        "0.01": null
      },
      "auroc": 0.9096677083333333
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1886,
          "fn": 1514,
          "accuracy": 0.5547058823529412
        },
        "0.01": null
      },
      "auroc": 0.8671848039215686
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2939,
          "fn": 1461,
          "accuracy": 0.6679545454545455
        },
        "0.01": null
      },
      "auroc": 0.90558125
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1728,
          "fn": 672,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9426155381944445
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 4667,
          "fn": 2133,
          "accuracy": 0.6863235294117647
        },
        "0.01": null
      },
      "auroc": 0.918652175245098
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9955562499999999
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9956729166666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9956145833333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9956145833333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9925499999999999
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9940822916666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9955854166666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9941114583333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 792,
          "fn": 8,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9948484375000001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9912572916666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.994075
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9926661458333335
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        },
        "0.01": null
      },
      "auroc": 0.643446875
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.99114375
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.8172953124999999
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": null
      },
      "auroc": 0.8173520833333332
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.992609375
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 591,
          "fn": 209,
          "accuracy": 0.73875
        },
        "0.01": null
      },
      "auroc": 0.9049807291666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9955239583333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9849354166666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9902296875000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.995584375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9872104166666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9913973958333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9955541666666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.9860729166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 746,
          "fn": 54,
          "accuracy": 0.9325
        },
        "0.01": null
      },
      "auroc": 0.9908135416666668
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9866916666666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": null
      },
      "auroc": 0.9912625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": null
      },
      "auroc": 0.5047364583333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": null
      },
      "auroc": 0.6196729166666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        },
        "0.01": null
      },
      "auroc": 0.5622046875
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.7502848958333332
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        },
        "0.01": null
      },
      "auroc": 0.8031822916666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 400,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.7767335937500001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9930427083333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9848614583333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9889520833333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": null
      },
      "auroc": 0.727434375
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.8953447916666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        },
        "0.01": null
      },
      "auroc": 0.8113895833333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        },
        "0.01": null
      },
      "auroc": 0.8602385416666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        },
        "0.01": null
      },
      "auroc": 0.940103125
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 544,
          "fn": 256,
          "accuracy": 0.68
        },
        "0.01": null
      },
      "auroc": 0.9001708333333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9954291666666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9955270833333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.995478125
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9872854166666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": null
      },
      "auroc": 0.8966864583333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.9419859374999999
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9913572916666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9461067708333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 689,
          "fn": 111,
          "accuracy": 0.86125
        },
        "0.01": null
      },
      "auroc": 0.9687320312500001
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.97673125
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.97673125
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.9446343749999999
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.9446343749999999
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        },
        "0.01": null
      },
      "auroc": 0.9606828125
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        },
        "0.01": null
      },
      "auroc": 0.9606828125
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.9244541666666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.9244541666666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.8823197916666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.8823197916666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        },
        "0.01": null
      },
      "auroc": 0.9033869791666668
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        },
        "0.01": null
      },
      "auroc": 0.9033869791666668
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9956770833333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9956770833333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9957822916666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9957822916666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9957296875
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9957296875
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9901489583333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9901489583333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9750708333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9750708333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9826098958333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9826098958333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.9617395833333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.9617395833333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.95341875
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.95341875
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        },
        "0.01": null
      },
      "auroc": 0.9575791666666666
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        },
        "0.01": null
      },
      "auroc": 0.9575791666666666
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2019,
          "fn": 181,
          "accuracy": 0.9177272727272727
        },
        "0.01": null
      },
      "auroc": 0.9832176136363636
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1128,
          "fn": 72,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9902939236111111
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3147,
          "fn": 253,
          "accuracy": 0.9255882352941176
        },
        "0.01": null
      },
      "auroc": 0.9857151348039215
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1414,
          "fn": 786,
          "accuracy": 0.6427272727272727
        },
        "0.01": null
      },
      "auroc": 0.8732116477272727
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 406,
          "accuracy": 0.6616666666666666
        },
        "0.01": null
      },
      "auroc": 0.897101388888889
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2208,
          "fn": 1192,
          "accuracy": 0.6494117647058824
        },
        "0.01": null
      },
      "auroc": 0.8816433210784314
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3433,
          "fn": 967,
          "accuracy": 0.7802272727272728
        },
        "0.01": null
      },
      "auroc": 0.9282146306818182
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1922,
          "fn": 478,
          "accuracy": 0.8008333333333333
        },
        "0.01": null
      },
      "auroc": 0.94369765625
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 5355,
          "fn": 1445,
          "accuracy": 0.7875
        },
        "0.01": null
      },
      "auroc": 0.9336792279411765
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": null
      },
      "auroc": 0.970871875
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.964028125
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": null
      },
      "auroc": 0.96745
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9758291666666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.9448312499999999
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.9603302083333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.9733505208333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.9544296875
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 584,
          "fn": 216,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.9638901041666668
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": null
      },
      "auroc": 0.9764989583333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": null
      },
      "auroc": 0.6713416666666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 228,
          "accuracy": 0.43
        },
        "0.01": null
      },
      "auroc": 0.8239203124999999
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": null
      },
      "auroc": 0.7052302083333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        },
        "0.01": null
      },
      "auroc": 0.6389666666666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 371,
          "accuracy": 0.0725
        },
        "0.01": null
      },
      "auroc": 0.6720984375000001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 228,
          "accuracy": 0.43
        },
        "0.01": null
      },
      "auroc": 0.8408645833333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 371,
          "accuracy": 0.0725
        },
        "0.01": null
      },
      "auroc": 0.6551541666666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 599,
          "accuracy": 0.25125
        },
        "0.01": null
      },
      "auroc": 0.7480093750000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.9827010416666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        },
        "0.01": null
      },
      "auroc": 0.9044947916666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.9435979166666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9654281250000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        },
        "0.01": null
      },
      "auroc": 0.76654375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        },
        "0.01": null
      },
      "auroc": 0.8659859375000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.9740645833333332
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        },
        "0.01": null
      },
      "auroc": 0.8355192708333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 451,
          "fn": 349,
          "accuracy": 0.56375
        },
        "0.01": null
      },
      "auroc": 0.9047919270833333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9879729166666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        },
        "0.01": null
      },
      "auroc": 0.9226958333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.9553343750000001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.6382749999999999
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.48687083333333336
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.5625729166666668
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        },
        "0.01": null
      },
      "auroc": 0.8131239583333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 91,
          "fn": 309,
          "accuracy": 0.2275
        },
        "0.01": null
      },
      "auroc": 0.7047833333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 282,
          "fn": 518,
          "accuracy": 0.3525
        },
        "0.01": null
      },
      "auroc": 0.7589536458333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.9799291666666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        },
        "0.01": null
      },
      "auroc": 0.7979
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.8889145833333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": null
      },
      "auroc": 0.700896875
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.5540677083333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 386,
          "accuracy": 0.035
        },
        "0.01": null
      },
      "auroc": 0.6274822916666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        },
        "0.01": null
      },
      "auroc": 0.8404130208333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 367,
          "accuracy": 0.0825
        },
        "0.01": null
      },
      "auroc": 0.6759838541666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 210,
          "fn": 590,
          "accuracy": 0.2625
        },
        "0.01": null
      },
      "auroc": 0.7581984375
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9814572916666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.9639322916666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.9726947916666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.9552635416666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        },
        "0.01": null
      },
      "auroc": 0.8367770833333332
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        },
        "0.01": null
      },
      "auroc": 0.8960203125
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": null
      },
      "auroc": 0.9683604166666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        },
        "0.01": null
      },
      "auroc": 0.9003546875000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 500,
          "fn": 300,
          "accuracy": 0.625
        },
        "0.01": null
      },
      "auroc": 0.9343575520833334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": null
      },
      "auroc": 0.9281166666666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": null
      },
      "auroc": 0.9281166666666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        },
        "0.01": null
      },
      "auroc": 0.8966093749999999
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        },
        "0.01": null
      },
      "auroc": 0.8966093749999999
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        },
        "0.01": null
      },
      "auroc": 0.9123630208333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        },
        "0.01": null
      },
      "auroc": 0.9123630208333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": null
      },
      "auroc": 0.8785489583333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": null
      },
      "auroc": 0.8785489583333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": null
      },
      "auroc": 0.8529614583333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": null
      },
      "auroc": 0.8529614583333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 285,
          "accuracy": 0.2875
        },
        "0.01": null
      },
      "auroc": 0.8657552083333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 285,
          "accuracy": 0.2875
        },
        "0.01": null
      },
      "auroc": 0.8657552083333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.98195
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.98195
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.9625802083333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.9625802083333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": null
      },
      "auroc": 0.9722651041666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": null
      },
      "auroc": 0.9722651041666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.9644052083333332
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.9644052083333332
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.9247052083333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.9247052083333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 259,
          "fn": 141,
          "accuracy": 0.6475
        },
        "0.01": null
      },
      "auroc": 0.9445552083333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 259,
          "fn": 141,
          "accuracy": 0.6475
        },
        "0.01": null
      },
      "auroc": 0.9445552083333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.9325041666666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.9325041666666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.9128958333333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.9128958333333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        },
        "0.01": null
      },
      "auroc": 0.9227000000000001
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        },
        "0.01": null
      },
      "auroc": 0.9227000000000001
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1624,
          "fn": 576,
          "accuracy": 0.7381818181818182
        },
        "0.01": null
      },
      "auroc": 0.9604505681818182
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 495,
          "fn": 705,
          "accuracy": 0.4125
        },
        "0.01": null
      },
      "auroc": 0.8707321180555555
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2119,
          "fn": 1281,
          "accuracy": 0.6232352941176471
        },
        "0.01": null
      },
      "auroc": 0.9287852328431372
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 963,
          "fn": 1237,
          "accuracy": 0.43772727272727274
        },
        "0.01": null
      },
      "auroc": 0.8627886363636363
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 252,
          "fn": 948,
          "accuracy": 0.21
        },
        "0.01": null
      },
      "auroc": 0.7046762152777778
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1215,
          "fn": 2185,
          "accuracy": 0.3573529411764706
        },
        "0.01": null
      },
      "auroc": 0.8069842524509804
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2587,
          "fn": 1813,
          "accuracy": 0.5879545454545455
        },
        "0.01": null
      },
      "auroc": 0.9116196022727272
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 747,
          "fn": 1653,
          "accuracy": 0.31125
        },
        "0.01": null
      },
      "auroc": 0.7877041666666668
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3334,
          "fn": 3466,
          "accuracy": 0.4902941176470588
        },
        "0.01": null
      },
      "auroc": 0.8678847426470588
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9957822916666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958078125
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9956729166666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9925635416666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9941182291666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9957276041666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9941984375
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9949630208333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9929322916666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9945895833333332
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9937609374999999
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        },
        "0.01": null
      },
      "auroc": 0.7202625
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9917302083333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        },
        "0.01": null
      },
      "auroc": 0.8559963541666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        },
        "0.01": null
      },
      "auroc": 0.8565973958333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9931598958333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 594,
          "fn": 206,
          "accuracy": 0.7425
        },
        "0.01": null
      },
      "auroc": 0.9248786458333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.995575
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.98713125
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": null
      },
      "auroc": 0.9913531250000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9954927083333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.988540625
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9920166666666668
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9955338541666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": null
      },
      "auroc": 0.9878359375000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 753,
          "fn": 47,
          "accuracy": 0.94125
        },
        "0.01": null
      },
      "auroc": 0.9916848958333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9925041666666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.99416875
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": null
      },
      "auroc": 0.6325624999999999
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        },
        "0.01": null
      },
      "auroc": 0.7318010416666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 30,
          "fn": 370,
          "accuracy": 0.075
        },
        "0.01": null
      },
      "auroc": 0.6821817708333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": null
      },
      "auroc": 0.8141979166666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        },
        "0.01": null
      },
      "auroc": 0.8621526041666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 418,
          "fn": 382,
          "accuracy": 0.5225
        },
        "0.01": null
      },
      "auroc": 0.8381752604166667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9935385416666668
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9915822916666668
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9925604166666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        },
        "0.01": null
      },
      "auroc": 0.7850447916666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": null
      },
      "auroc": 0.9288916666666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        },
        "0.01": null
      },
      "auroc": 0.8569682291666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 256,
          "fn": 144,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.8892916666666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.9602369791666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 562,
          "fn": 238,
          "accuracy": 0.7025
        },
        "0.01": null
      },
      "auroc": 0.9247643229166667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9953708333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9957312500000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9955510416666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9887593750000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": null
      },
      "auroc": 0.931909375
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        },
        "0.01": null
      },
      "auroc": 0.960334375
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9920651041666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.9638203125
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 701,
          "fn": 99,
          "accuracy": 0.87625
        },
        "0.01": null
      },
      "auroc": 0.9779427083333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9816739583333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9816739583333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.9542614583333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.9542614583333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9679677083333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.9679677083333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.94729375
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.94729375
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.9119031249999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.9119031249999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        },
        "0.01": null
      },
      "auroc": 0.9295984374999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        },
        "0.01": null
      },
      "auroc": 0.9295984374999999
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9957822916666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9957822916666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958078125
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958078125
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.992915625
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.992915625
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.983325
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.983325
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9881203125
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9881203125
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.9718947916666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.9718947916666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.964015625
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.964015625
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.9679552083333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.9679552083333334
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2027,
          "fn": 173,
          "accuracy": 0.9213636363636364
        },
        "0.01": null
      },
      "auroc": 0.9871447916666667
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1149,
          "fn": 51,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9928953125000001
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3176,
          "fn": 224,
          "accuracy": 0.9341176470588235
        },
        "0.01": null
      },
      "auroc": 0.9891743872549019
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1438,
          "fn": 762,
          "accuracy": 0.6536363636363637
        },
        "0.01": null
      },
      "auroc": 0.9024666666666668
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 823,
          "fn": 377,
          "accuracy": 0.6858333333333333
        },
        "0.01": null
      },
      "auroc": 0.9275727430555555
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2261,
          "fn": 1139,
          "accuracy": 0.665
        },
        "0.01": null
      },
      "auroc": 0.9113276348039214
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3465,
          "fn": 935,
          "accuracy": 0.7875
        },
        "0.01": null
      },
      "auroc": 0.9448057291666666
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1972,
          "fn": 428,
          "accuracy": 0.8216666666666667
        },
        "0.01": null
      },
      "auroc": 0.9602340277777779
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 5437,
          "fn": 1363,
          "accuracy": 0.7995588235294118
        },
        "0.01": null
      },
      "auroc": 0.9502510110294118
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9957822916666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958078125
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9957239583333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.99309375
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9944088541666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.995753125
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9944635416666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9951083333333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.993621875
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9944010416666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9940114583333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        },
        "0.01": null
      },
      "auroc": 0.7378479166666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9912447916666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.8645463541666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.8657348958333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9928229166666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 596,
          "fn": 204,
          "accuracy": 0.745
        },
        "0.01": null
      },
      "auroc": 0.92927890625
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9957822916666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9875979166666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9916901041666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9956354166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9888645833333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9922500000000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9957088541666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9882312500000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 755,
          "fn": 45,
          "accuracy": 0.94375
        },
        "0.01": null
      },
      "auroc": 0.9919700520833333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9938489583333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9948411458333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": null
      },
      "auroc": 0.6448802083333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": null
      },
      "auroc": 0.727146875
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 30,
          "fn": 370,
          "accuracy": 0.075
        },
        "0.01": null
      },
      "auroc": 0.6860135416666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        },
        "0.01": null
      },
      "auroc": 0.8203567708333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": null
      },
      "auroc": 0.8604979166666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 424,
          "fn": 376,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.8404273437499999
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9941302083333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9920958333333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9931130208333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": null
      },
      "auroc": 0.7966333333333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": null
      },
      "auroc": 0.9246479166666668
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        },
        "0.01": null
      },
      "auroc": 0.860640625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        },
        "0.01": null
      },
      "auroc": 0.8953817708333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": null
      },
      "auroc": 0.958371875
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 568,
          "fn": 232,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.9268768229166666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9953708333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9956020833333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9908375
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.9376718749999999
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.9642546875000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9931041666666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": null
      },
      "auroc": 0.9667526041666668
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 706,
          "fn": 94,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.9799283854166667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9866489583333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9866489583333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9621843750000001
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.9621843750000001
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.9744166666666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.9744166666666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": null
      },
      "auroc": 0.9569916666666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": null
      },
      "auroc": 0.9569916666666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": null
      },
      "auroc": 0.92500625
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": null
      },
      "auroc": 0.92500625
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.9409989583333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.9409989583333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9937677083333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9937677083333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9846125
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9846125
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9891901041666668
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9891901041666668
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9771510416666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9771510416666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": null
      },
      "auroc": 0.9686614583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": null
      },
      "auroc": 0.9686614583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9729062500000001
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.9729062500000001
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2071,
          "fn": 129,
          "accuracy": 0.9413636363636364
        },
        "0.01": null
      },
      "auroc": 0.9891739583333333
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1159,
          "fn": 41,
          "accuracy": 0.9658333333333333
        },
        "0.01": null
      },
      "auroc": 0.9932684027777778
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3230,
          "fn": 170,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.990619056372549
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1487,
          "fn": 713,
          "accuracy": 0.6759090909090909
        },
        "0.01": null
      },
      "auroc": 0.9088960227272727
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 822,
          "fn": 378,
          "accuracy": 0.685
        },
        "0.01": null
      },
      "auroc": 0.9271116319444443
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2309,
          "fn": 1091,
          "accuracy": 0.6791176470588235
        },
        "0.01": null
      },
      "auroc": 0.9153250612745099
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3558,
          "fn": 842,
          "accuracy": 0.8086363636363636
        },
        "0.01": null
      },
      "auroc": 0.949034990530303
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1981,
          "fn": 419,
          "accuracy": 0.8254166666666667
        },
        "0.01": null
      },
      "auroc": 0.9601900173611111
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 5539,
          "fn": 1261,
          "accuracy": 0.8145588235294118
        },
        "0.01": null
      },
      "auroc": 0.9529720588235294
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.18613020833333335
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.17068229166666668
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.17840625000000002
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.18690104166666668
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.15717812499999997
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.17203958333333336
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.18651562499999996
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.16393020833333335
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.17522291666666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        },
        "0.01": null
      },
      "auroc": 0.3442354166666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.23089583333333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 386,
          "accuracy": 0.035
        },
        "0.01": null
      },
      "auroc": 0.287565625
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.15266770833333332
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": null
      },
      "auroc": 0.2649645833333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 389,
          "accuracy": 0.0275
        },
        "0.01": null
      },
      "auroc": 0.20881614583333336
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        },
        "0.01": null
      },
      "auroc": 0.24845156249999995
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 393,
          "accuracy": 0.0175
        },
        "0.01": null
      },
      "auroc": 0.24793020833333335
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 25,
          "fn": 775,
          "accuracy": 0.03125
        },
        "0.01": null
      },
      "auroc": 0.24819088541666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.20768020833333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.16076041666666668
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1842203125
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.18547708333333332
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.14684479166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1661609375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.19657864583333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.15380260416666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.17519062500000002
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": null
      },
      "auroc": 0.6992645833333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.24110937500000001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 343,
          "accuracy": 0.1425
        },
        "0.01": null
      },
      "auroc": 0.47018697916666663
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.12213229166666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.12105520833333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.12159375
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        },
        "0.01": null
      },
      "auroc": 0.4106984375
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.18108229166666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 743,
          "accuracy": 0.07125
        },
        "0.01": null
      },
      "auroc": 0.2958903645833333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": null
      },
      "auroc": 0.505296875
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": null
      },
      "auroc": 0.7892166666666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 244,
          "accuracy": 0.39
        },
        "0.01": null
      },
      "auroc": 0.6472567708333332
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        },
        "0.01": null
      },
      "auroc": 0.3242302083333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        },
        "0.01": null
      },
      "auroc": 0.3263041666666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 341,
          "accuracy": 0.1475
        },
        "0.01": null
      },
      "auroc": 0.3252671875
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 354,
          "accuracy": 0.115
        },
        "0.01": null
      },
      "auroc": 0.4147635416666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        },
        "0.01": null
      },
      "auroc": 0.5577604166666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 215,
          "fn": 585,
          "accuracy": 0.26875
        },
        "0.01": null
      },
      "auroc": 0.4862619791666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.23111979166666669
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.17342916666666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.20227447916666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.19175833333333336
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1413229166666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.166540625
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.21143906250000002
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.15737604166666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.18440755208333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.21503645833333335
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.21503645833333335
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.17750416666666666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.17750416666666666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.1962703125
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.1962703125
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.15751041666666665
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.15751041666666665
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13607083333333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13607083333333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.14679062499999998
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.14679062499999998
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.16511041666666665
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.16511041666666665
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13110937500000003
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13110937500000003
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1481098958333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1481098958333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.15707916666666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.15707916666666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1046
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1046
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13083958333333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13083958333333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.19233854166666664
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.19233854166666664
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.199196875
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.199196875
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.19576770833333335
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.19576770833333335
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 89,
          "fn": 2111,
          "accuracy": 0.04045454545454545
        },
        "0.01": null
      },
      "auroc": 0.2782547348484849
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 1061,
          "accuracy": 0.11583333333333333
        },
        "0.01": null
      },
      "auroc": 0.2943489583333333
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 228,
          "fn": 3172,
          "accuracy": 0.06705882352941177
        },
        "0.01": null
      },
      "auroc": 0.28393504901960787
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 2167,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.17378617424242424
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 1162,
          "accuracy": 0.03166666666666667
        },
        "0.01": null
      },
      "auroc": 0.19294496527777777
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 3329,
          "accuracy": 0.02088235294117647
        },
        "0.01": null
      },
      "auroc": 0.18054810049019607
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 4278,
          "accuracy": 0.02772727272727273
        },
        "0.01": null
      },
      "auroc": 0.22602045454545452
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 2223,
          "accuracy": 0.07375
        },
        "0.01": null
      },
      "auroc": 0.24364696180555556
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 299,
          "fn": 6501,
          "accuracy": 0.043970588235294115
        },
        "0.01": null
      },
      "auroc": 0.23224157475490198
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9947260416666668
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9954010416666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9950635416666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9932114583333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9908739583333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9920427083333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9939687500000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9931375
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 786,
          "fn": 14,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.993553125
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9833552083333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9944552083333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9889052083333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": null
      },
      "auroc": 0.6181520833333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9913052083333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        },
        "0.01": null
      },
      "auroc": 0.8047286458333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        },
        "0.01": null
      },
      "auroc": 0.8007536458333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9928802083333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 569,
          "fn": 231,
          "accuracy": 0.71125
        },
        "0.01": null
      },
      "auroc": 0.8968169270833333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.99351875
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9838427083333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9886807291666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9938968750000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9874739583333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": null
      },
      "auroc": 0.9906854166666668
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9937078125000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": null
      },
      "auroc": 0.9856583333333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 740,
          "fn": 60,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9896830729166667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": null
      },
      "auroc": 0.9755697916666668
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9857015625000001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.5367854166666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        },
        "0.01": null
      },
      "auroc": 0.7552854166666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 364,
          "accuracy": 0.09
        },
        "0.01": null
      },
      "auroc": 0.6460354166666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.7663093750000001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        },
        "0.01": null
      },
      "auroc": 0.8654276041666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 414,
          "accuracy": 0.4825
        },
        "0.01": null
      },
      "auroc": 0.8158684895833332
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9915791666666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9859312499999999
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9887552083333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": null
      },
      "auroc": 0.7132833333333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": null
      },
      "auroc": 0.9387718749999999
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 219,
          "accuracy": 0.4525
        },
        "0.01": null
      },
      "auroc": 0.8260276041666668
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": null
      },
      "auroc": 0.85243125
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": null
      },
      "auroc": 0.9623515625000001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 558,
          "fn": 242,
          "accuracy": 0.6975
        },
        "0.01": null
      },
      "auroc": 0.90739140625
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9948760416666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9948395833333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9948578125
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9755302083333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": null
      },
      "auroc": 0.9102281249999999
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        },
        "0.01": null
      },
      "auroc": 0.9428791666666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.985203125
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.9525338541666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 673,
          "fn": 127,
          "accuracy": 0.84125
        },
        "0.01": null
      },
      "auroc": 0.9688684895833334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": null
      },
      "auroc": 0.9479708333333332
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": null
      },
      "auroc": 0.9479708333333332
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": null
      },
      "auroc": 0.8978718750000001
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": null
      },
      "auroc": 0.8978718750000001
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.9229213541666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.9229213541666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        },
        "0.01": null
      },
      "auroc": 0.878771875
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        },
        "0.01": null
      },
      "auroc": 0.878771875
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": null
      },
      "auroc": 0.8399145833333335
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": null
      },
      "auroc": 0.8399145833333335
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 269,
          "accuracy": 0.3275
        },
        "0.01": null
      },
      "auroc": 0.8593432291666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 269,
          "accuracy": 0.3275
        },
        "0.01": null
      },
      "auroc": 0.8593432291666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.99565
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.99565
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9954135416666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9954135416666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9955317708333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9955317708333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.982134375
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.982134375
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.9680718749999999
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.9680718749999999
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        },
        "0.01": null
      },
      "auroc": 0.975103125
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        },
        "0.01": null
      },
      "auroc": 0.975103125
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.9428239583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.9428239583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.9280895833333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.9280895833333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.9354567708333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.9354567708333333
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1877,
          "fn": 323,
          "accuracy": 0.8531818181818182
        },
        "0.01": null
      },
      "auroc": 0.9728399621212122
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1094,
          "fn": 106,
          "accuracy": 0.9116666666666666
        },
        "0.01": null
      },
      "auroc": 0.9883399305555555
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2971,
          "fn": 429,
          "accuracy": 0.8738235294117647
        },
        "0.01": null
      },
      "auroc": 0.9783105392156861
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1273,
          "fn": 927,
          "accuracy": 0.5786363636363636
        },
        "0.01": null
      },
      "auroc": 0.8600200757575758
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 824,
          "fn": 376,
          "accuracy": 0.6866666666666666
        },
        "0.01": null
      },
      "auroc": 0.9289897569444444
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2097,
          "fn": 1303,
          "accuracy": 0.616764705882353
        },
        "0.01": null
      },
      "auroc": 0.8843623161764705
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3150,
          "fn": 1250,
          "accuracy": 0.7159090909090909
        },
        "0.01": null
      },
      "auroc": 0.9164300189393939
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1918,
          "fn": 482,
          "accuracy": 0.7991666666666667
        },
        "0.01": null
      },
      "auroc": 0.9586648437499999
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 5068,
          "fn": 1732,
          "accuracy": 0.7452941176470588
        },
        "0.01": null
      },
      "auroc": 0.9313364276960784
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9950625
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9957822916666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9954223958333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9948791666666668
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9921375
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9935083333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9949708333333332
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9939598958333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9944653645833333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9927875
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9945020833333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9936447916666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": null
      },
      "auroc": 0.6869989583333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9913989583333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.8391989583333332
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": null
      },
      "auroc": 0.8398932291666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9929505208333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 591,
          "fn": 209,
          "accuracy": 0.73875
        },
        "0.01": null
      },
      "auroc": 0.916421875
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9953687499999999
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9862208333333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9907947916666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.994759375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.9867145833333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9907369791666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9950640625
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.9864677083333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 751,
          "fn": 49,
          "accuracy": 0.93875
        },
        "0.01": null
      },
      "auroc": 0.9907658854166668
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9916750000000001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9937541666666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.5951739583333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": null
      },
      "auroc": 0.6786197916666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 377,
          "accuracy": 0.0575
        },
        "0.01": null
      },
      "auroc": 0.6368968749999999
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.7955036458333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.8351473958333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 408,
          "fn": 392,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.8153255208333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9919374999999999
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9907354166666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9913364583333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        },
        "0.01": null
      },
      "auroc": 0.7628447916666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": null
      },
      "auroc": 0.9125791666666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 229,
          "accuracy": 0.4275
        },
        "0.01": null
      },
      "auroc": 0.8377119791666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        },
        "0.01": null
      },
      "auroc": 0.8773911458333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": null
      },
      "auroc": 0.9516572916666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 557,
          "fn": 243,
          "accuracy": 0.69625
        },
        "0.01": null
      },
      "auroc": 0.9145242187500001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9953708333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9957822916666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9955765625
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9877
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": null
      },
      "auroc": 0.9232135416666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 301,
          "fn": 99,
          "accuracy": 0.7525
        },
        "0.01": null
      },
      "auroc": 0.9554567708333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9915354166666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        },
        "0.01": null
      },
      "auroc": 0.9594979166666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 699,
          "fn": 101,
          "accuracy": 0.87375
        },
        "0.01": null
      },
      "auroc": 0.9755166666666666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9785083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.9785083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.949815625
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.949815625
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": null
      },
      "auroc": 0.9641619791666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": null
      },
      "auroc": 0.9641619791666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.9416270833333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.9416270833333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": null
      },
      "auroc": 0.899790625
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": null
      },
      "auroc": 0.899790625
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.9207088541666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.9207088541666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9957822916666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9957822916666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958333333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958078125
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9958078125
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9934791666666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9934791666666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9797447916666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9797447916666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": null
      },
      "auroc": 0.9866119791666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": null
      },
      "auroc": 0.9866119791666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9718572916666666
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9718572916666666
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.9570583333333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.9570583333333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": null
      },
      "auroc": 0.9644578125000001
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": null
      },
      "auroc": 0.9644578125000001
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2038,
          "fn": 162,
          "accuracy": 0.9263636363636364
        },
        "0.01": null
      },
      "auroc": 0.9861467803030304
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1147,
          "fn": 53,
          "accuracy": 0.9558333333333333
        },
        "0.01": null
      },
      "auroc": 0.9924496527777777
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3185,
          "fn": 215,
          "accuracy": 0.9367647058823529
        },
        "0.01": null
      },
      "auroc": 0.9883713235294118
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1437,
          "fn": 763,
          "accuracy": 0.6531818181818182
        },
        "0.01": null
      },
      "auroc": 0.8913271780303029
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 805,
          "fn": 395,
          "accuracy": 0.6708333333333333
        },
        "0.01": null
      },
      "auroc": 0.9141105902777777
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2242,
          "fn": 1158,
          "accuracy": 0.6594117647058824
        },
        "0.01": null
      },
      "auroc": 0.8993683823529413
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3475,
          "fn": 925,
          "accuracy": 0.7897727272727273
        },
        "0.01": null
      },
      "auroc": 0.9387369791666667
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1952,
          "fn": 448,
          "accuracy": 0.8133333333333334
        },
        "0.01": null
      },
      "auroc": 0.9532801215277777
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 5427,
          "fn": 1373,
          "accuracy": 0.7980882352941177
        },
        "0.01": null
      },
      "auroc": 0.9438698529411764
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.12783124999999998
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13120937500000002
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.12952031249999998
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13518333333333335
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.12898645833333336
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13208489583333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1315072916666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13009791666666665
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13080260416666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.21121666666666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.22416354166666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        },
        "0.01": null
      },
      "auroc": 0.21769010416666668
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.16580833333333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": null
      },
      "auroc": 0.273678125
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.21974322916666664
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        },
        "0.01": null
      },
      "auroc": 0.18851250000000003
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.24892083333333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 785,
          "accuracy": 0.01875
        },
        "0.01": null
      },
      "auroc": 0.21871666666666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11874479166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11616354166666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11745416666666665
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11830520833333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13318541666666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1257453125
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11852500000000002
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.12467447916666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.12159973958333335
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": null
      },
      "auroc": 0.4253041666666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.14666770833333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        },
        "0.01": null
      },
      "auroc": 0.2859859375
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1405239583333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.15265
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1465869791666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        },
        "0.01": null
      },
      "auroc": 0.2829140625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.14965885416666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 782,
          "accuracy": 0.0225
        },
        "0.01": null
      },
      "auroc": 0.21628645833333332
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.24829583333333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": null
      },
      "auroc": 0.7474833333333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 265,
          "accuracy": 0.3375
        },
        "0.01": null
      },
      "auroc": 0.49788958333333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        },
        "0.01": null
      },
      "auroc": 0.325653125
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        },
        "0.01": null
      },
      "auroc": 0.3572927083333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 51,
          "fn": 349,
          "accuracy": 0.1275
        },
        "0.01": null
      },
      "auroc": 0.3414729166666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 28,
          "fn": 372,
          "accuracy": 0.07
        },
        "0.01": null
      },
      "auroc": 0.2869744791666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 242,
          "accuracy": 0.395
        },
        "0.01": null
      },
      "auroc": 0.5523880208333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 614,
          "accuracy": 0.2325
        },
        "0.01": null
      },
      "auroc": 0.41968125
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13234479166666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.113484375
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.12291458333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13421458333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.12097916666666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.127596875
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1332796875
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11723177083333332
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 800,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.12525572916666666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.14163229166666666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.14163229166666666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.14549166666666666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.14549166666666666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1435619791666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.1435619791666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13660416666666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13660416666666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13734895833333335
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13734895833333335
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13697656249999998
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.13697656249999998
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.108328125
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.108328125
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11055416666666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11055416666666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.10944114583333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.10944114583333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11269583333333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11269583333333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.12167083333333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.12167083333333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11718333333333332
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.11718333333333332
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.126134375
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.126134375
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.12637291666666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.12637291666666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.12625364583333332
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.12625364583333332
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 2173,
          "accuracy": 0.012272727272727272
        },
        "0.01": null
      },
      "auroc": 0.17173929924242426
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 1068,
          "accuracy": 0.11
        },
        "0.01": null
      },
      "auroc": 0.24652864583333334
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 3241,
          "accuracy": 0.046764705882352944
        },
        "0.01": null
      },
      "auroc": 0.19813553921568625
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 2171,
          "accuracy": 0.013181818181818182
        },
        "0.01": null
      },
      "auroc": 0.15101155303030303
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 1168,
          "accuracy": 0.02666666666666667
        },
        "0.01": null
      },
      "auroc": 0.19446197916666663
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 3339,
          "accuracy": 0.017941176470588235
        },
        "0.01": null
      },
      "auroc": 0.1663469975490196
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 4344,
          "accuracy": 0.012727272727272728
        },
        "0.01": null
      },
      "auroc": 0.16137542613636363
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 2236,
          "accuracy": 0.06833333333333333
        },
        "0.01": null
      },
      "auroc": 0.22049531249999998
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 220,
          "fn": 6580,
          "accuracy": 0.03235294117647059
        },
        "0.01": null
      },
      "auroc": 0.18224126838235294
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1948,
          "fn": 452,
          "accuracy": 0.8116666666666666
        },
        "0.01": null
      },
      "auroc": 0.8532725694444444
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1926,
          "fn": 474,
          "accuracy": 0.8025
        },
        "0.01": null
      },
      "auroc": 0.8515592881944445
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3874,
          "fn": 926,
          "accuracy": 0.8070833333333334
        },
        "0.01": null
      },
      "auroc": 0.8524159288194444
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1933,
          "fn": 467,
          "accuracy": 0.8054166666666667
        },
        "0.01": null
      },
      "auroc": 0.8541329861111111
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1877,
          "fn": 523,
          "accuracy": 0.7820833333333334
        },
        "0.01": null
      },
      "auroc": 0.8458519097222222
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3810,
          "fn": 990,
          "accuracy": 0.79375
        },
        "0.01": null
      },
      "auroc": 0.8499924479166667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3881,
          "fn": 919,
          "accuracy": 0.8085416666666667
        },
        "0.01": null
      },
      "auroc": 0.8537027777777777
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3803,
          "fn": 997,
          "accuracy": 0.7922916666666666
        },
        "0.01": null
      },
      "auroc": 0.8487055989583332
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7684,
          "fn": 1916,
          "accuracy": 0.8004166666666667
        },
        "0.01": null
      },
      "auroc": 0.8512041883680556
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1888,
          "fn": 512,
          "accuracy": 0.7866666666666666
        },
        "0.01": null
      },
      "auroc": 0.8705540798611111
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1765,
          "fn": 635,
          "accuracy": 0.7354166666666667
        },
        "0.01": null
      },
      "auroc": 0.8396864583333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3653,
          "fn": 1147,
          "accuracy": 0.7610416666666666
        },
        "0.01": null
      },
      "auroc": 0.8551202690972222
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 2260,
          "accuracy": 0.058333333333333334
        },
        "0.01": null
      },
      "auroc": 0.5960339409722222
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1731,
          "fn": 669,
          "accuracy": 0.72125
        },
        "0.01": null
      },
      "auroc": 0.8416214409722222
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1871,
          "fn": 2929,
          "accuracy": 0.38979166666666665
        },
        "0.01": null
      },
      "auroc": 0.7188276909722222
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2028,
          "fn": 2772,
          "accuracy": 0.4225
        },
        "0.01": null
      },
      "auroc": 0.7332940104166668
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3496,
          "fn": 1304,
          "accuracy": 0.7283333333333334
        },
        "0.01": null
      },
      "auroc": 0.8406539496527778
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5524,
          "fn": 4076,
          "accuracy": 0.5754166666666667
        },
        "0.01": null
      },
      "auroc": 0.7869739800347222
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1947,
          "fn": 453,
          "accuracy": 0.81125
        },
        "0.01": null
      },
      "auroc": 0.8553310763888888
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1636,
          "fn": 764,
          "accuracy": 0.6816666666666666
        },
        "0.01": null
      },
      "auroc": 0.8363711805555556
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3583,
          "fn": 1217,
          "accuracy": 0.7464583333333333
        },
        "0.01": null
      },
      "auroc": 0.8458511284722222
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1929,
          "fn": 471,
          "accuracy": 0.80375
        },
        "0.01": null
      },
      "auroc": 0.8517841145833334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1617,
          "fn": 783,
          "accuracy": 0.67375
        },
        "0.01": null
      },
      "auroc": 0.8276176215277777
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3546,
          "fn": 1254,
          "accuracy": 0.73875
        },
        "0.01": null
      },
      "auroc": 0.8397008680555555
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3876,
          "fn": 924,
          "accuracy": 0.8075
        },
        "0.01": null
      },
      "auroc": 0.853557595486111
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3253,
          "fn": 1547,
          "accuracy": 0.6777083333333334
        },
        "0.01": null
      },
      "auroc": 0.8319944010416667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7129,
          "fn": 2471,
          "accuracy": 0.7426041666666666
        },
        "0.01": null
      },
      "auroc": 0.8427759982638889
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2064,
          "fn": 336,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.9229201388888889
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1630,
          "fn": 770,
          "accuracy": 0.6791666666666667
        },
        "0.01": null
      },
      "auroc": 0.8471556423611111
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3694,
          "fn": 1106,
          "accuracy": 0.7695833333333333
        },
        "0.01": null
      },
      "auroc": 0.885037890625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 2354,
          "accuracy": 0.019166666666666665
        },
        "0.01": null
      },
      "auroc": 0.5122993923611111
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 214,
          "fn": 2186,
          "accuracy": 0.08916666666666667
        },
        "0.01": null
      },
      "auroc": 0.5955710069444444
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 260,
          "fn": 4540,
          "accuracy": 0.05416666666666667
        },
        "0.01": null
      },
      "auroc": 0.5539351996527778
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2110,
          "fn": 2690,
          "accuracy": 0.4395833333333333
        },
        "0.01": null
      },
      "auroc": 0.717609765625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1844,
          "fn": 2956,
          "accuracy": 0.38416666666666666
        },
        "0.01": null
      },
      "auroc": 0.7213633246527777
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3954,
          "fn": 5646,
          "accuracy": 0.411875
        },
        "0.01": null
      },
      "auroc": 0.7194865451388889
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1946,
          "fn": 454,
          "accuracy": 0.8108333333333333
        },
        "0.01": null
      },
      "auroc": 0.8890989583333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1988,
          "fn": 412,
          "accuracy": 0.8283333333333334
        },
        "0.01": null
      },
      "auroc": 0.9358533854166666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3934,
          "fn": 866,
          "accuracy": 0.8195833333333333
        },
        "0.01": null
      },
      "auroc": 0.9124761718750001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 595,
          "fn": 1805,
          "accuracy": 0.24791666666666667
        },
        "0.01": null
      },
      "auroc": 0.679152170138889
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1098,
          "fn": 1302,
          "accuracy": 0.4575
        },
        "0.01": null
      },
      "auroc": 0.7951904513888889
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1693,
          "fn": 3107,
          "accuracy": 0.35270833333333335
        },
        "0.01": null
      },
      "auroc": 0.7371713107638889
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2541,
          "fn": 2259,
          "accuracy": 0.529375
        },
        "0.01": null
      },
      "auroc": 0.7841255642361111
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3086,
          "fn": 1714,
          "accuracy": 0.6429166666666667
        },
        "0.01": null
      },
      "auroc": 0.8655219184027777
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5627,
          "fn": 3973,
          "accuracy": 0.5861458333333334
        },
        "0.01": null
      },
      "auroc": 0.8248237413194445
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1950,
          "fn": 450,
          "accuracy": 0.8125
        },
        "0.01": null
      },
      "auroc": 0.8582730034722224
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1905,
          "fn": 495,
          "accuracy": 0.79375
        },
        "0.01": null
      },
      "auroc": 0.8501513020833333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3855,
          "fn": 945,
          "accuracy": 0.803125
        },
        "0.01": null
      },
      "auroc": 0.8542121527777777
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1741,
          "fn": 659,
          "accuracy": 0.7254166666666667
        },
        "0.01": null
      },
      "auroc": 0.8455307291666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1055,
          "fn": 1345,
          "accuracy": 0.4395833333333333
        },
        "0.01": null
      },
      "auroc": 0.7787050347222222
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2796,
          "fn": 2004,
          "accuracy": 0.5825
        },
        "0.01": null
      },
      "auroc": 0.8121178819444443
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3691,
          "fn": 1109,
          "accuracy": 0.7689583333333333
        },
        "0.01": null
      },
      "auroc": 0.8519018663194444
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2960,
          "fn": 1840,
          "accuracy": 0.6166666666666667
        },
        "0.01": null
      },
      "auroc": 0.8144281684027779
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 6651,
          "fn": 2949,
          "accuracy": 0.6928125
        },
        "0.01": null
      },
      "auroc": 0.8331650173611111
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1513,
          "fn": 887,
          "accuracy": 0.6304166666666666
        },
        "0.01": null
      },
      "auroc": 0.8365338541666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1513,
          "fn": 887,
          "accuracy": 0.6304166666666666
        },
        "0.01": null
      },
      "auroc": 0.8365338541666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1151,
          "fn": 1249,
          "accuracy": 0.4795833333333333
        },
        "0.01": null
      },
      "auroc": 0.8058620659722222
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1151,
          "fn": 1249,
          "accuracy": 0.4795833333333333
        },
        "0.01": null
      },
      "auroc": 0.8058620659722222
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2664,
          "fn": 2136,
          "accuracy": 0.555
        },
        "0.01": null
      },
      "auroc": 0.8211979600694446
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2664,
          "fn": 2136,
          "accuracy": 0.555
        },
        "0.01": null
      },
      "auroc": 0.8211979600694446
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1046,
          "fn": 1354,
          "accuracy": 0.43583333333333335
        },
        "0.01": null
      },
      "auroc": 0.7928807291666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1046,
          "fn": 1354,
          "accuracy": 0.43583333333333335
        },
        "0.01": null
      },
      "auroc": 0.7928807291666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 918,
          "fn": 1482,
          "accuracy": 0.3825
        },
        "0.01": null
      },
      "auroc": 0.7608975694444444
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 918,
          "fn": 1482,
          "accuracy": 0.3825
        },
        "0.01": null
      },
      "auroc": 0.7608975694444444
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1964,
          "fn": 2836,
          "accuracy": 0.4091666666666667
        },
        "0.01": null
      },
      "auroc": 0.7768891493055556
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1964,
          "fn": 2836,
          "accuracy": 0.4091666666666667
        },
        "0.01": null
      },
      "auroc": 0.7768891493055556
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1966,
          "fn": 434,
          "accuracy": 0.8191666666666667
        },
        "0.01": null
      },
      "auroc": 0.8512311631944445
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1966,
          "fn": 434,
          "accuracy": 0.8191666666666667
        },
        "0.01": null
      },
      "auroc": 0.8512311631944445
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1942,
          "fn": 458,
          "accuracy": 0.8091666666666667
        },
        "0.01": null
      },
      "auroc": 0.8468002604166667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1942,
          "fn": 458,
          "accuracy": 0.8091666666666667
        },
        "0.01": null
      },
      "auroc": 0.8468002604166667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3908,
          "fn": 892,
          "accuracy": 0.8141666666666667
        },
        "0.01": null
      },
      "auroc": 0.8490157118055555
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3908,
          "fn": 892,
          "accuracy": 0.8141666666666667
        },
        "0.01": null
      },
      "auroc": 0.8490157118055555
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1852,
          "fn": 548,
          "accuracy": 0.7716666666666666
        },
        "0.01": null
      },
      "auroc": 0.8448449652777779
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1852,
          "fn": 548,
          "accuracy": 0.7716666666666666
        },
        "0.01": null
      },
      "auroc": 0.8448449652777779
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1657,
          "fn": 743,
          "accuracy": 0.6904166666666667
        },
        "0.01": null
      },
      "auroc": 0.8273266493055556
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1657,
          "fn": 743,
          "accuracy": 0.6904166666666667
        },
        "0.01": null
      },
      "auroc": 0.8273266493055556
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3509,
          "fn": 1291,
          "accuracy": 0.7310416666666667
        },
        "0.01": null
      },
      "auroc": 0.8360858072916667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3509,
          "fn": 1291,
          "accuracy": 0.7310416666666667
        },
        "0.01": null
      },
      "auroc": 0.8360858072916667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1570,
          "fn": 830,
          "accuracy": 0.6541666666666667
        },
        "0.01": null
      },
      "auroc": 0.8263966145833332
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1570,
          "fn": 830,
          "accuracy": 0.6541666666666667
        },
        "0.01": null
      },
      "auroc": 0.8263966145833332
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1475,
          "fn": 925,
          "accuracy": 0.6145833333333334
        },
        "0.01": null
      },
      "auroc": 0.8172659722222222
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1475,
          "fn": 925,
          "accuracy": 0.6145833333333334
        },
        "0.01": null
      },
      "auroc": 0.8172659722222222
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3045,
          "fn": 1755,
          "accuracy": 0.634375
        },
        "0.01": null
      },
      "auroc": 0.8218312934027778
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3045,
          "fn": 1755,
          "accuracy": 0.634375
        },
        "0.01": null
      },
      "auroc": 0.8218312934027778
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 19690,
          "fn": 6710,
          "accuracy": 0.7458333333333333
        },
        "0.01": null
      },
      "auroc": 0.8546670138888888
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 10850,
          "fn": 3550,
          "accuracy": 0.7534722222222222
        },
        "0.01": null
      },
      "auroc": 0.8601295428240741
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 30540,
          "fn": 10260,
          "accuracy": 0.7485294117647059
        },
        "0.01": null
      },
      "auroc": 0.8565949652777778
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 13527,
          "fn": 12873,
          "accuracy": 0.5123863636363636
        },
        "0.01": null
      },
      "auroc": 0.7633714409722222
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7592,
          "fn": 6808,
          "accuracy": 0.5272222222222223
        },
        "0.01": null
      },
      "auroc": 0.7807595775462963
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 21119,
          "fn": 19681,
          "accuracy": 0.5176225490196078
        },
        "0.01": null
      },
      "auroc": 0.7695084303513072
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 33217,
          "fn": 19583,
          "accuracy": 0.6291098484848485
        },
        "0.01": null
      },
      "auroc": 0.8090192274305555
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18442,
          "fn": 10358,
          "accuracy": 0.6403472222222222
        },
        "0.01": null
      },
      "auroc": 0.8204445601851851
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 51659,
          "fn": 29941,
          "accuracy": 0.6330759803921568
        },
        "0.01": null
      },
      "auroc": 0.8130516978145425
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1579,
          "fn": 21,
          "accuracy": 0.986875
        },
        "0.01": null
      },
      "auroc": 0.9865582682291667
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1516,
          "fn": 84,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9745014973958332
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3095,
          "fn": 105,
          "accuracy": 0.9671875
        },
        "0.01": null
      },
      "auroc": 0.9805298828125
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1556,
          "fn": 44,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9816395345052082
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1455,
          "fn": 145,
          "accuracy": 0.909375
        },
        "0.01": null
      },
      "auroc": 0.9566344075520833
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3011,
          "fn": 189,
          "accuracy": 0.9409375
        },
        "0.01": null
      },
      "auroc": 0.9691369710286457
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3135,
          "fn": 65,
          "accuracy": 0.9796875
        },
        "0.01": null
      },
      "auroc": 0.9840989013671875
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2971,
          "fn": 229,
          "accuracy": 0.9284375
        },
        "0.01": null
      },
      "auroc": 0.9655679524739582
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6106,
          "fn": 294,
          "accuracy": 0.9540625
        },
        "0.01": null
      },
      "auroc": 0.974833426920573
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1581,
          "fn": 19,
          "accuracy": 0.988125
        },
        "0.01": null
      },
      "auroc": 0.9942276041666667
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1552,
          "fn": 48,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9912793131510417
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3133,
          "fn": 67,
          "accuracy": 0.9790625
        },
        "0.01": null
      },
      "auroc": 0.9927534586588542
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 414,
          "fn": 1186,
          "accuracy": 0.25875
        },
        "0.01": null
      },
      "auroc": 0.738189013671875
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1547,
          "fn": 53,
          "accuracy": 0.966875
        },
        "0.01": null
      },
      "auroc": 0.9914383626302083
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1961,
          "fn": 1239,
          "accuracy": 0.6128125
        },
        "0.01": null
      },
      "auroc": 0.8648136881510416
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1995,
          "fn": 1205,
          "accuracy": 0.6234375
        },
        "0.01": null
      },
      "auroc": 0.8662083089192708
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3099,
          "fn": 101,
          "accuracy": 0.9684375
        },
        "0.01": null
      },
      "auroc": 0.9913588378906251
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 5094,
          "fn": 1306,
          "accuracy": 0.7959375
        },
        "0.01": null
      },
      "auroc": 0.9287835734049481
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1565,
          "fn": 35,
          "accuracy": 0.978125
        },
        "0.01": null
      },
      "auroc": 0.9741643554687501
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1432,
          "fn": 168,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.97241875
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2997,
          "fn": 203,
          "accuracy": 0.9365625
        },
        "0.01": null
      },
      "auroc": 0.9732915527343751
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1524,
          "fn": 76,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.96953681640625
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1416,
          "fn": 184,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9745471516927083
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2940,
          "fn": 260,
          "accuracy": 0.91875
        },
        "0.01": null
      },
      "auroc": 0.9720419840494792
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3089,
          "fn": 111,
          "accuracy": 0.9653125
        },
        "0.01": null
      },
      "auroc": 0.9718505859375
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2848,
          "fn": 352,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9734829508463542
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 5937,
          "fn": 463,
          "accuracy": 0.92765625
        },
        "0.01": null
      },
      "auroc": 0.9726667683919271
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9959988118489582
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1535,
          "fn": 65,
          "accuracy": 0.959375
        },
        "0.01": null
      },
      "auroc": 0.9826604166666666
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3135,
          "fn": 65,
          "accuracy": 0.9796875
        },
        "0.01": null
      },
      "auroc": 0.9893296142578125
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 1300,
          "accuracy": 0.1875
        },
        "0.01": null
      },
      "auroc": 0.7189167154947916
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 458,
          "fn": 1142,
          "accuracy": 0.28625
        },
        "0.01": null
      },
      "auroc": 0.7889130696614582
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 758,
          "fn": 2442,
          "accuracy": 0.236875
        },
        "0.01": null
      },
      "auroc": 0.753914892578125
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1900,
          "fn": 1300,
          "accuracy": 0.59375
        },
        "0.01": null
      },
      "auroc": 0.857457763671875
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1993,
          "fn": 1207,
          "accuracy": 0.6228125
        },
        "0.01": null
      },
      "auroc": 0.8857867431640625
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3893,
          "fn": 2507,
          "accuracy": 0.60828125
        },
        "0.01": null
      },
      "auroc": 0.8716222534179686
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1579,
          "fn": 21,
          "accuracy": 0.986875
        },
        "0.01": null
      },
      "auroc": 0.9937651204427084
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1349,
          "fn": 251,
          "accuracy": 0.843125
        },
        "0.01": null
      },
      "auroc": 0.9818794596354168
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2928,
          "fn": 272,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9878222900390625
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 536,
          "fn": 1064,
          "accuracy": 0.335
        },
        "0.01": null
      },
      "auroc": 0.779526220703125
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 998,
          "fn": 602,
          "accuracy": 0.62375
        },
        "0.01": null
      },
      "auroc": 0.9012251139322915
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1534,
          "fn": 1666,
          "accuracy": 0.479375
        },
        "0.01": null
      },
      "auroc": 0.8403756673177084
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2115,
          "fn": 1085,
          "accuracy": 0.6609375
        },
        "0.01": null
      },
      "auroc": 0.8866456705729167
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2347,
          "fn": 853,
          "accuracy": 0.7334375
        },
        "0.01": null
      },
      "auroc": 0.941552286783854
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 4462,
          "fn": 1938,
          "accuracy": 0.6971875
        },
        "0.01": null
      },
      "auroc": 0.9140989786783853
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1588,
          "fn": 12,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9820826497395834
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1535,
          "fn": 65,
          "accuracy": 0.959375
        },
        "0.01": null
      },
      "auroc": 0.969137158203125
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3123,
          "fn": 77,
          "accuracy": 0.9759375
        },
        "0.01": null
      },
      "auroc": 0.9756099039713539
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1529,
          "fn": 71,
          "accuracy": 0.955625
        },
        "0.01": null
      },
      "auroc": 0.9620621256510417
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1284,
          "fn": 316,
          "accuracy": 0.8025
        },
        "0.01": null
      },
      "auroc": 0.9417417805989583
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2813,
          "fn": 387,
          "accuracy": 0.8790625
        },
        "0.01": null
      },
      "auroc": 0.951901953125
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3117,
          "fn": 83,
          "accuracy": 0.9740625
        },
        "0.01": null
      },
      "auroc": 0.9720723876953125
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2819,
          "fn": 381,
          "accuracy": 0.8809375
        },
        "0.01": null
      },
      "auroc": 0.9554394694010416
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 5936,
          "fn": 464,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.9637559285481772
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1431,
          "fn": 169,
          "accuracy": 0.894375
        },
        "0.01": null
      },
      "auroc": 0.9231198079427083
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1431,
          "fn": 169,
          "accuracy": 0.894375
        },
        "0.01": null
      },
      "auroc": 0.9231198079427083
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1308,
          "fn": 292,
          "accuracy": 0.8175
        },
        "0.01": null
      },
      "auroc": 0.8986472493489582
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1308,
          "fn": 292,
          "accuracy": 0.8175
        },
        "0.01": null
      },
      "auroc": 0.8986472493489582
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2739,
          "fn": 461,
          "accuracy": 0.8559375
        },
        "0.01": null
      },
      "auroc": 0.9108835286458334
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2739,
          "fn": 461,
          "accuracy": 0.8559375
        },
        "0.01": null
      },
      "auroc": 0.9108835286458334
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 889,
          "fn": 711,
          "accuracy": 0.555625
        },
        "0.01": null
      },
      "auroc": 0.8026984374999999
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 889,
          "fn": 711,
          "accuracy": 0.555625
        },
        "0.01": null
      },
      "auroc": 0.8026984374999999
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 705,
          "fn": 895,
          "accuracy": 0.440625
        },
        "0.01": null
      },
      "auroc": 0.7509896809895833
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 705,
          "fn": 895,
          "accuracy": 0.440625
        },
        "0.01": null
      },
      "auroc": 0.7509896809895833
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1594,
          "fn": 1606,
          "accuracy": 0.498125
        },
        "0.01": null
      },
      "auroc": 0.7768440592447916
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1594,
          "fn": 1606,
          "accuracy": 0.498125
        },
        "0.01": null
      },
      "auroc": 0.7768440592447916
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1584,
          "fn": 16,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.974285009765625
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1584,
          "fn": 16,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.974285009765625
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1535,
          "fn": 65,
          "accuracy": 0.959375
        },
        "0.01": null
      },
      "auroc": 0.9590642740885416
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1535,
          "fn": 65,
          "accuracy": 0.959375
        },
        "0.01": null
      },
      "auroc": 0.9590642740885416
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3119,
          "fn": 81,
          "accuracy": 0.9746875
        },
        "0.01": null
      },
      "auroc": 0.9666746419270833
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3119,
          "fn": 81,
          "accuracy": 0.9746875
        },
        "0.01": null
      },
      "auroc": 0.9666746419270833
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1575,
          "fn": 25,
          "accuracy": 0.984375
        },
        "0.01": null
      },
      "auroc": 0.9780439290364583
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1575,
          "fn": 25,
          "accuracy": 0.984375
        },
        "0.01": null
      },
      "auroc": 0.9780439290364583
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1274,
          "fn": 326,
          "accuracy": 0.79625
        },
        "0.01": null
      },
      "auroc": 0.8852994466145834
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1274,
          "fn": 326,
          "accuracy": 0.79625
        },
        "0.01": null
      },
      "auroc": 0.8852994466145834
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2849,
          "fn": 351,
          "accuracy": 0.8903125
        },
        "0.01": null
      },
      "auroc": 0.9316716878255209
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2849,
          "fn": 351,
          "accuracy": 0.8903125
        },
        "0.01": null
      },
      "auroc": 0.9316716878255209
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1298,
          "fn": 302,
          "accuracy": 0.81125
        },
        "0.01": null
      },
      "auroc": 0.9235113606770833
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1298,
          "fn": 302,
          "accuracy": 0.81125
        },
        "0.01": null
      },
      "auroc": 0.9235113606770833
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1172,
          "fn": 428,
          "accuracy": 0.7325
        },
        "0.01": null
      },
      "auroc": 0.8852080403645833
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1172,
          "fn": 428,
          "accuracy": 0.7325
        },
        "0.01": null
      },
      "auroc": 0.8852080403645833
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2470,
          "fn": 730,
          "accuracy": 0.771875
        },
        "0.01": null
      },
      "auroc": 0.9043597005208335
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2470,
          "fn": 730,
          "accuracy": 0.771875
        },
        "0.01": null
      },
      "auroc": 0.9043597005208335
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 16269,
          "fn": 1331,
          "accuracy": 0.924375
        },
        "0.01": null
      },
      "auroc": 0.9571323049834282
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 8919,
          "fn": 681,
          "accuracy": 0.9290625
        },
        "0.01": null
      },
      "auroc": 0.9786460991753472
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 25188,
          "fn": 2012,
          "accuracy": 0.9260294117647059
        },
        "0.01": null
      },
      "auroc": 0.9647254088158702
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 11853,
          "fn": 5747,
          "accuracy": 0.6734659090909091
        },
        "0.01": null
      },
      "auroc": 0.8662799198035037
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 7158,
          "fn": 2442,
          "accuracy": 0.745625
        },
        "0.01": null
      },
      "auroc": 0.9257499810112846
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 19011,
          "fn": 8189,
          "accuracy": 0.6989338235294118
        },
        "0.01": null
      },
      "auroc": 0.8872693531709559
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 28122,
          "fn": 7078,
          "accuracy": 0.7989204545454546
        },
        "0.01": null
      },
      "auroc": 0.9117061123934659
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 16077,
          "fn": 3123,
          "accuracy": 0.83734375
        },
        "0.01": null
      },
      "auroc": 0.9521980400933159
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 44199,
          "fn": 10201,
          "accuracy": 0.8124816176470588
        },
        "0.01": null
      },
      "auroc": 0.925997380993413
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1579,
          "fn": 21,
          "accuracy": 0.986875
        },
        "0.01": null
      },
      "auroc": 0.9865582682291667
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1516,
          "fn": 84,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.97450029296875
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3095,
          "fn": 105,
          "accuracy": 0.9671875
        },
        "0.01": null
      },
      "auroc": 0.9805292805989583
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1556,
          "fn": 44,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9816396158854166
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1455,
          "fn": 145,
          "accuracy": 0.909375
        },
        "0.01": null
      },
      "auroc": 0.9566344075520833
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3011,
          "fn": 189,
          "accuracy": 0.9409375
        },
        "0.01": null
      },
      "auroc": 0.96913701171875
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3135,
          "fn": 65,
          "accuracy": 0.9796875
        },
        "0.01": null
      },
      "auroc": 0.9840989420572916
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2971,
          "fn": 229,
          "accuracy": 0.9284375
        },
        "0.01": null
      },
      "auroc": 0.9655673502604167
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 6106,
          "fn": 294,
          "accuracy": 0.9540625
        },
        "0.01": null
      },
      "auroc": 0.9748331461588542
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1581,
          "fn": 19,
          "accuracy": 0.988125
        },
        "0.01": null
      },
      "auroc": 0.9942276041666667
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1552,
          "fn": 48,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9912793131510417
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3133,
          "fn": 67,
          "accuracy": 0.9790625
        },
        "0.01": null
      },
      "auroc": 0.9927534586588542
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 414,
          "fn": 1186,
          "accuracy": 0.25875
        },
        "0.01": null
      },
      "auroc": 0.738189013671875
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1547,
          "fn": 53,
          "accuracy": 0.966875
        },
        "0.01": null
      },
      "auroc": 0.9914383626302083
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1961,
          "fn": 1239,
          "accuracy": 0.6128125
        },
        "0.01": null
      },
      "auroc": 0.8648136881510416
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1995,
          "fn": 1205,
          "accuracy": 0.6234375
        },
        "0.01": null
      },
      "auroc": 0.8662083089192708
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3099,
          "fn": 101,
          "accuracy": 0.9684375
        },
        "0.01": null
      },
      "auroc": 0.9913588378906251
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 5094,
          "fn": 1306,
          "accuracy": 0.7959375
        },
        "0.01": null
      },
      "auroc": 0.9287835734049481
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1565,
          "fn": 35,
          "accuracy": 0.978125
        },
        "0.01": null
      },
      "auroc": 0.9741643554687501
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1432,
          "fn": 168,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9724193522135416
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2997,
          "fn": 203,
          "accuracy": 0.9365625
        },
        "0.01": null
      },
      "auroc": 0.9732918538411459
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1524,
          "fn": 76,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9695348632812499
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1416,
          "fn": 184,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9745471516927083
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2940,
          "fn": 260,
          "accuracy": 0.91875
        },
        "0.01": null
      },
      "auroc": 0.9720410074869792
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3089,
          "fn": 111,
          "accuracy": 0.9653125
        },
        "0.01": null
      },
      "auroc": 0.971849609375
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2848,
          "fn": 352,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9734832519531251
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 5937,
          "fn": 463,
          "accuracy": 0.92765625
        },
        "0.01": null
      },
      "auroc": 0.9726664306640624
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9959988118489582
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1535,
          "fn": 65,
          "accuracy": 0.959375
        },
        "0.01": null
      },
      "auroc": 0.982658203125
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3135,
          "fn": 65,
          "accuracy": 0.9796875
        },
        "0.01": null
      },
      "auroc": 0.9893285074869792
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 1300,
          "accuracy": 0.1875
        },
        "0.01": null
      },
      "auroc": 0.7189307454427084
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 458,
          "fn": 1142,
          "accuracy": 0.28625
        },
        "0.01": null
      },
      "auroc": 0.7889130696614582
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 758,
          "fn": 2442,
          "accuracy": 0.236875
        },
        "0.01": null
      },
      "auroc": 0.7539219075520833
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1900,
          "fn": 1300,
          "accuracy": 0.59375
        },
        "0.01": null
      },
      "auroc": 0.8574647786458334
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1993,
          "fn": 1207,
          "accuracy": 0.6228125
        },
        "0.01": null
      },
      "auroc": 0.8857856363932293
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3893,
          "fn": 2507,
          "accuracy": 0.60828125
        },
        "0.01": null
      },
      "auroc": 0.8716252075195312
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1579,
          "fn": 21,
          "accuracy": 0.986875
        },
        "0.01": null
      },
      "auroc": 0.9937651204427084
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1349,
          "fn": 251,
          "accuracy": 0.843125
        },
        "0.01": null
      },
      "auroc": 0.9818794596354168
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2928,
          "fn": 272,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9878222900390625
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 536,
          "fn": 1064,
          "accuracy": 0.335
        },
        "0.01": null
      },
      "auroc": 0.7795302571614583
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 998,
          "fn": 602,
          "accuracy": 0.62375
        },
        "0.01": null
      },
      "auroc": 0.9012251139322915
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1534,
          "fn": 1666,
          "accuracy": 0.479375
        },
        "0.01": null
      },
      "auroc": 0.840377685546875
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2115,
          "fn": 1085,
          "accuracy": 0.6609375
        },
        "0.01": null
      },
      "auroc": 0.8866476888020832
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2347,
          "fn": 853,
          "accuracy": 0.7334375
        },
        "0.01": null
      },
      "auroc": 0.941552286783854
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 4462,
          "fn": 1938,
          "accuracy": 0.6971875
        },
        "0.01": null
      },
      "auroc": 0.9140999877929687
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1588,
          "fn": 12,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9820826497395834
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1535,
          "fn": 65,
          "accuracy": 0.959375
        },
        "0.01": null
      },
      "auroc": 0.9691384440104167
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3123,
          "fn": 77,
          "accuracy": 0.9759375
        },
        "0.01": null
      },
      "auroc": 0.9756105468749999
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1529,
          "fn": 71,
          "accuracy": 0.955625
        },
        "0.01": null
      },
      "auroc": 0.9620583333333333
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1284,
          "fn": 316,
          "accuracy": 0.8025
        },
        "0.01": null
      },
      "auroc": 0.9417417805989583
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2813,
          "fn": 387,
          "accuracy": 0.8790625
        },
        "0.01": null
      },
      "auroc": 0.9519000569661458
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3117,
          "fn": 83,
          "accuracy": 0.9740625
        },
        "0.01": null
      },
      "auroc": 0.9720704915364583
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2819,
          "fn": 381,
          "accuracy": 0.8809375
        },
        "0.01": null
      },
      "auroc": 0.9554401123046876
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 5936,
          "fn": 464,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.963755301920573
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1431,
          "fn": 169,
          "accuracy": 0.894375
        },
        "0.01": null
      },
      "auroc": 0.9231219889322917
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1431,
          "fn": 169,
          "accuracy": 0.894375
        },
        "0.01": null
      },
      "auroc": 0.9231219889322917
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1308,
          "fn": 292,
          "accuracy": 0.8175
        },
        "0.01": null
      },
      "auroc": 0.89864609375
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1308,
          "fn": 292,
          "accuracy": 0.8175
        },
        "0.01": null
      },
      "auroc": 0.89864609375
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2739,
          "fn": 461,
          "accuracy": 0.8559375
        },
        "0.01": null
      },
      "auroc": 0.9108840413411459
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2739,
          "fn": 461,
          "accuracy": 0.8559375
        },
        "0.01": null
      },
      "auroc": 0.9108840413411459
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 889,
          "fn": 711,
          "accuracy": 0.555625
        },
        "0.01": null
      },
      "auroc": 0.8026984374999999
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 889,
          "fn": 711,
          "accuracy": 0.555625
        },
        "0.01": null
      },
      "auroc": 0.8026984374999999
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 705,
          "fn": 895,
          "accuracy": 0.440625
        },
        "0.01": null
      },
      "auroc": 0.7509896809895833
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 705,
          "fn": 895,
          "accuracy": 0.440625
        },
        "0.01": null
      },
      "auroc": 0.7509896809895833
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1594,
          "fn": 1606,
          "accuracy": 0.498125
        },
        "0.01": null
      },
      "auroc": 0.7768440592447916
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1594,
          "fn": 1606,
          "accuracy": 0.498125
        },
        "0.01": null
      },
      "auroc": 0.7768440592447916
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1584,
          "fn": 16,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9742833658854169
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1584,
          "fn": 16,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9742833658854169
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1535,
          "fn": 65,
          "accuracy": 0.959375
        },
        "0.01": null
      },
      "auroc": 0.9590660481770834
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1535,
          "fn": 65,
          "accuracy": 0.959375
        },
        "0.01": null
      },
      "auroc": 0.9590660481770834
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3119,
          "fn": 81,
          "accuracy": 0.9746875
        },
        "0.01": null
      },
      "auroc": 0.96667470703125
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3119,
          "fn": 81,
          "accuracy": 0.9746875
        },
        "0.01": null
      },
      "auroc": 0.96667470703125
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1575,
          "fn": 25,
          "accuracy": 0.984375
        },
        "0.01": null
      },
      "auroc": 0.9780424316406249
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1575,
          "fn": 25,
          "accuracy": 0.984375
        },
        "0.01": null
      },
      "auroc": 0.9780424316406249
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1274,
          "fn": 326,
          "accuracy": 0.79625
        },
        "0.01": null
      },
      "auroc": 0.8853190104166668
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1274,
          "fn": 326,
          "accuracy": 0.79625
        },
        "0.01": null
      },
      "auroc": 0.8853190104166668
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2849,
          "fn": 351,
          "accuracy": 0.8903125
        },
        "0.01": null
      },
      "auroc": 0.9316807210286459
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2849,
          "fn": 351,
          "accuracy": 0.8903125
        },
        "0.01": null
      },
      "auroc": 0.9316807210286459
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1298,
          "fn": 302,
          "accuracy": 0.81125
        },
        "0.01": null
      },
      "auroc": 0.9235113606770833
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1298,
          "fn": 302,
          "accuracy": 0.81125
        },
        "0.01": null
      },
      "auroc": 0.9235113606770833
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1172,
          "fn": 428,
          "accuracy": 0.7325
        },
        "0.01": null
      },
      "auroc": 0.8852080403645833
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1172,
          "fn": 428,
          "accuracy": 0.7325
        },
        "0.01": null
      },
      "auroc": 0.8852080403645833
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2470,
          "fn": 730,
          "accuracy": 0.771875
        },
        "0.01": null
      },
      "auroc": 0.9043597005208335
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2470,
          "fn": 730,
          "accuracy": 0.771875
        },
        "0.01": null
      },
      "auroc": 0.9043597005208335
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 16269,
          "fn": 1331,
          "accuracy": 0.924375
        },
        "0.01": null
      },
      "auroc": 0.9571322176846591
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 8919,
          "fn": 681,
          "accuracy": 0.9290625
        },
        "0.01": null
      },
      "auroc": 0.9786458441840278
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 25188,
          "fn": 2012,
          "accuracy": 0.9260294117647059
        },
        "0.01": null
      },
      "auroc": 0.9647252623314952
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 11853,
          "fn": 5747,
          "accuracy": 0.6734659090909091
        },
        "0.01": null
      },
      "auroc": 0.8662828820430871
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 7158,
          "fn": 2442,
          "accuracy": 0.745625
        },
        "0.01": null
      },
      "auroc": 0.9257499810112846
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 19011,
          "fn": 8189,
          "accuracy": 0.6989338235294118
        },
        "0.01": null
      },
      "auroc": 0.8872712699142158
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 28122,
          "fn": 7078,
          "accuracy": 0.7989204545454546
        },
        "0.01": null
      },
      "auroc": 0.911707549863873
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 16077,
          "fn": 3123,
          "accuracy": 0.83734375
        },
        "0.01": null
      },
      "auroc": 0.9521979125976563
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 44199,
          "fn": 10201,
          "accuracy": 0.8124816176470588
        },
        "0.01": null
      },
      "auroc": 0.9259982661228553
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1557,
          "fn": 43,
          "accuracy": 0.973125
        },
        "0.01": null
      },
      "auroc": 0.9795265787760417
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1469,
          "fn": 131,
          "accuracy": 0.918125
        },
        "0.01": null
      },
      "auroc": 0.9641151204427084
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3026,
          "fn": 174,
          "accuracy": 0.945625
        },
        "0.01": null
      },
      "auroc": 0.971820849609375
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1523,
          "fn": 77,
          "accuracy": 0.951875
        },
        "0.01": null
      },
      "auroc": 0.9732080729166666
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1403,
          "fn": 197,
          "accuracy": 0.876875
        },
        "0.01": null
      },
      "auroc": 0.9447814778645834
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2926,
          "fn": 274,
          "accuracy": 0.914375
        },
        "0.01": null
      },
      "auroc": 0.9589947753906249
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3080,
          "fn": 120,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9763673258463542
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2872,
          "fn": 328,
          "accuracy": 0.8975
        },
        "0.01": null
      },
      "auroc": 0.9544482991536458
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 5952,
          "fn": 448,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9654078125000001
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1552,
          "fn": 48,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9920672363281251
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1552,
          "fn": 48,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9911098470052082
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3104,
          "fn": 96,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9915885416666665
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 1291,
          "accuracy": 0.193125
        },
        "0.01": null
      },
      "auroc": 0.67964892578125
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1548,
          "fn": 52,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9912957519531251
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1857,
          "fn": 1343,
          "accuracy": 0.5803125
        },
        "0.01": null
      },
      "auroc": 0.8354723388671874
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1861,
          "fn": 1339,
          "accuracy": 0.5815625
        },
        "0.01": null
      },
      "auroc": 0.8358580810546875
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3100,
          "fn": 100,
          "accuracy": 0.96875
        },
        "0.01": null
      },
      "auroc": 0.9912027994791667
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 4961,
          "fn": 1439,
          "accuracy": 0.77515625
        },
        "0.01": null
      },
      "auroc": 0.9135304402669272
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1536,
          "fn": 64,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9649935221354168
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1389,
          "fn": 211,
          "accuracy": 0.868125
        },
        "0.01": null
      },
      "auroc": 0.9659246582031249
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2925,
          "fn": 275,
          "accuracy": 0.9140625
        },
        "0.01": null
      },
      "auroc": 0.9654590901692708
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1497,
          "fn": 103,
          "accuracy": 0.935625
        },
        "0.01": null
      },
      "auroc": 0.9601748046875
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1379,
          "fn": 221,
          "accuracy": 0.861875
        },
        "0.01": null
      },
      "auroc": 0.9693862304687499
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2876,
          "fn": 324,
          "accuracy": 0.89875
        },
        "0.01": null
      },
      "auroc": 0.9647805175781251
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3033,
          "fn": 167,
          "accuracy": 0.9478125
        },
        "0.01": null
      },
      "auroc": 0.9625841634114584
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2768,
          "fn": 432,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9676554443359375
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 5801,
          "fn": 599,
          "accuracy": 0.90640625
        },
        "0.01": null
      },
      "auroc": 0.9651198038736979
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1594,
          "fn": 6,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.995882861328125
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1384,
          "fn": 216,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.9686874186197916
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2978,
          "fn": 222,
          "accuracy": 0.930625
        },
        "0.01": null
      },
      "auroc": 0.9822851399739583
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 210,
          "fn": 1390,
          "accuracy": 0.13125
        },
        "0.01": null
      },
      "auroc": 0.6489776367187501
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 441,
          "fn": 1159,
          "accuracy": 0.275625
        },
        "0.01": null
      },
      "auroc": 0.7593986328125001
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 651,
          "fn": 2549,
          "accuracy": 0.2034375
        },
        "0.01": null
      },
      "auroc": 0.7041881347656249
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1804,
          "fn": 1396,
          "accuracy": 0.56375
        },
        "0.01": null
      },
      "auroc": 0.8224302490234375
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1825,
          "fn": 1375,
          "accuracy": 0.5703125
        },
        "0.01": null
      },
      "auroc": 0.8640430257161459
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3629,
          "fn": 2771,
          "accuracy": 0.56703125
        },
        "0.01": null
      },
      "auroc": 0.8432366373697917
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1558,
          "fn": 42,
          "accuracy": 0.97375
        },
        "0.01": null
      },
      "auroc": 0.9923959309895833
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1206,
          "fn": 394,
          "accuracy": 0.75375
        },
        "0.01": null
      },
      "auroc": 0.9711087890625001
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2764,
          "fn": 436,
          "accuracy": 0.86375
        },
        "0.01": null
      },
      "auroc": 0.9817523600260416
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 452,
          "fn": 1148,
          "accuracy": 0.2825
        },
        "0.01": null
      },
      "auroc": 0.7278520182291667
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 990,
          "fn": 610,
          "accuracy": 0.61875
        },
        "0.01": null
      },
      "auroc": 0.8920087239583333
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1442,
          "fn": 1758,
          "accuracy": 0.450625
        },
        "0.01": null
      },
      "auroc": 0.8099303710937498
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2010,
          "fn": 1190,
          "accuracy": 0.628125
        },
        "0.01": null
      },
      "auroc": 0.860123974609375
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2196,
          "fn": 1004,
          "accuracy": 0.68625
        },
        "0.01": null
      },
      "auroc": 0.9315587565104166
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 4206,
          "fn": 2194,
          "accuracy": 0.6571875
        },
        "0.01": null
      },
      "auroc": 0.8958413655598958
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1576,
          "fn": 24,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9754794596354166
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1499,
          "fn": 101,
          "accuracy": 0.936875
        },
        "0.01": null
      },
      "auroc": 0.9605540852864584
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3075,
          "fn": 125,
          "accuracy": 0.9609375
        },
        "0.01": null
      },
      "auroc": 0.9680167724609375
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1478,
          "fn": 122,
          "accuracy": 0.92375
        },
        "0.01": null
      },
      "auroc": 0.9522462076822917
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1198,
          "fn": 402,
          "accuracy": 0.74875
        },
        "0.01": null
      },
      "auroc": 0.9267660970052083
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2676,
          "fn": 524,
          "accuracy": 0.83625
        },
        "0.01": null
      },
      "auroc": 0.9395061523437499
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3054,
          "fn": 146,
          "accuracy": 0.954375
        },
        "0.01": null
      },
      "auroc": 0.9638628336588542
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2697,
          "fn": 503,
          "accuracy": 0.8428125
        },
        "0.01": null
      },
      "auroc": 0.9436600911458334
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 5751,
          "fn": 649,
          "accuracy": 0.89859375
        },
        "0.01": null
      },
      "auroc": 0.9537614624023438
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1305,
          "fn": 295,
          "accuracy": 0.815625
        },
        "0.01": null
      },
      "auroc": 0.9044648600260417
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1305,
          "fn": 295,
          "accuracy": 0.815625
        },
        "0.01": null
      },
      "auroc": 0.9044648600260417
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1137,
          "fn": 463,
          "accuracy": 0.710625
        },
        "0.01": null
      },
      "auroc": 0.873514306640625
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1137,
          "fn": 463,
          "accuracy": 0.710625
        },
        "0.01": null
      },
      "auroc": 0.873514306640625
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2442,
          "fn": 758,
          "accuracy": 0.763125
        },
        "0.01": null
      },
      "auroc": 0.8889895833333333
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2442,
          "fn": 758,
          "accuracy": 0.763125
        },
        "0.01": null
      },
      "auroc": 0.8889895833333333
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 669,
          "fn": 931,
          "accuracy": 0.418125
        },
        "0.01": null
      },
      "auroc": 0.7591246093749999
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 669,
          "fn": 931,
          "accuracy": 0.418125
        },
        "0.01": null
      },
      "auroc": 0.7591246093749999
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 509,
          "fn": 1091,
          "accuracy": 0.318125
        },
        "0.01": null
      },
      "auroc": 0.7050634440104167
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 509,
          "fn": 1091,
          "accuracy": 0.318125
        },
        "0.01": null
      },
      "auroc": 0.7050634440104167
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1178,
          "fn": 2022,
          "accuracy": 0.368125
        },
        "0.01": null
      },
      "auroc": 0.7320940266927083
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1178,
          "fn": 2022,
          "accuracy": 0.368125
        },
        "0.01": null
      },
      "auroc": 0.7320940266927083
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1568,
          "fn": 32,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9628267903645833
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1568,
          "fn": 32,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9628267903645833
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1504,
          "fn": 96,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9448113932291666
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1504,
          "fn": 96,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9448113932291666
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3072,
          "fn": 128,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9538190917968749
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3072,
          "fn": 128,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9538190917968749
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1542,
          "fn": 58,
          "accuracy": 0.96375
        },
        "0.01": null
      },
      "auroc": 0.9674807454427082
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1542,
          "fn": 58,
          "accuracy": 0.96375
        },
        "0.01": null
      },
      "auroc": 0.9674807454427082
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1171,
          "fn": 429,
          "accuracy": 0.731875
        },
        "0.01": null
      },
      "auroc": 0.8556634440104167
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1171,
          "fn": 429,
          "accuracy": 0.731875
        },
        "0.01": null
      },
      "auroc": 0.8556634440104167
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2713,
          "fn": 487,
          "accuracy": 0.8478125
        },
        "0.01": null
      },
      "auroc": 0.9115720947265625
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2713,
          "fn": 487,
          "accuracy": 0.8478125
        },
        "0.01": null
      },
      "auroc": 0.9115720947265625
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 401,
          "accuracy": 0.749375
        },
        "0.01": null
      },
      "auroc": 0.900116796875
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 401,
          "accuracy": 0.749375
        },
        "0.01": null
      },
      "auroc": 0.900116796875
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1042,
          "fn": 558,
          "accuracy": 0.65125
        },
        "0.01": null
      },
      "auroc": 0.8537711100260418
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1042,
          "fn": 558,
          "accuracy": 0.65125
        },
        "0.01": null
      },
      "auroc": 0.8537711100260418
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2241,
          "fn": 959,
          "accuracy": 0.7003125
        },
        "0.01": null
      },
      "auroc": 0.8769439534505208
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2241,
          "fn": 959,
          "accuracy": 0.7003125
        },
        "0.01": null
      },
      "auroc": 0.8769439534505208
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 15656,
          "fn": 1944,
          "accuracy": 0.8895454545454545
        },
        "0.01": null
      },
      "auroc": 0.9449417628432766
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 8499,
          "fn": 1101,
          "accuracy": 0.8853125
        },
        "0.01": null
      },
      "auroc": 0.9702499864366321
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 24155,
          "fn": 3045,
          "accuracy": 0.8880514705882353
        },
        "0.01": null
      },
      "auroc": 0.9538740770526962
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 10832,
          "fn": 6768,
          "accuracy": 0.6154545454545455
        },
        "0.01": null
      },
      "auroc": 0.8340846694483901
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 6959,
          "fn": 2641,
          "accuracy": 0.7248958333333333
        },
        "0.01": null
      },
      "auroc": 0.9139394856770834
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 17791,
          "fn": 9409,
          "accuracy": 0.6540808823529412
        },
        "0.01": null
      },
      "auroc": 0.8622687222349876
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 26488,
          "fn": 8712,
          "accuracy": 0.7525
        },
        "0.01": null
      },
      "auroc": 0.8895132161458332
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 15458,
          "fn": 3742,
          "accuracy": 0.8051041666666666
        },
        "0.01": null
      },
      "auroc": 0.9420947360568578
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 41946,
          "fn": 12454,
          "accuracy": 0.7710661764705883
        },
        "0.01": null
      },
      "auroc": 0.908071399643842
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1503,
          "fn": 97,
          "accuracy": 0.939375
        },
        "0.01": null
      },
      "auroc": 0.9500966634114585
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1374,
          "fn": 226,
          "accuracy": 0.85875
        },
        "0.01": null
      },
      "auroc": 0.9264074544270833
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2877,
          "fn": 323,
          "accuracy": 0.8990625
        },
        "0.01": null
      },
      "auroc": 0.9382520589192707
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1465,
          "fn": 135,
          "accuracy": 0.915625
        },
        "0.01": null
      },
      "auroc": 0.9418388020833334
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1256,
          "fn": 344,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.9026215169270835
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2721,
          "fn": 479,
          "accuracy": 0.8503125
        },
        "0.01": null
      },
      "auroc": 0.9222301595052084
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2968,
          "fn": 232,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.9459677327473959
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2630,
          "fn": 570,
          "accuracy": 0.821875
        },
        "0.01": null
      },
      "auroc": 0.9145144856770834
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 5598,
          "fn": 802,
          "accuracy": 0.8746875
        },
        "0.01": null
      },
      "auroc": 0.9302411092122396
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1483,
          "fn": 117,
          "accuracy": 0.926875
        },
        "0.01": null
      },
      "auroc": 0.9814772949218751
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1550,
          "fn": 50,
          "accuracy": 0.96875
        },
        "0.01": null
      },
      "auroc": 0.9916371256510417
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3033,
          "fn": 167,
          "accuracy": 0.9478125
        },
        "0.01": null
      },
      "auroc": 0.9865572102864584
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 265,
          "fn": 1335,
          "accuracy": 0.165625
        },
        "0.01": null
      },
      "auroc": 0.6312007975260416
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1548,
          "fn": 52,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9915311686197916
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1813,
          "fn": 1387,
          "accuracy": 0.5665625
        },
        "0.01": null
      },
      "auroc": 0.8113659830729167
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1748,
          "fn": 1452,
          "accuracy": 0.54625
        },
        "0.01": null
      },
      "auroc": 0.8063390462239584
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3098,
          "fn": 102,
          "accuracy": 0.968125
        },
        "0.01": null
      },
      "auroc": 0.9915841471354165
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 4846,
          "fn": 1554,
          "accuracy": 0.7571875
        },
        "0.01": null
      },
      "auroc": 0.8989615966796876
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1457,
          "fn": 143,
          "accuracy": 0.910625
        },
        "0.01": null
      },
      "auroc": 0.9306488769531249
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1308,
          "fn": 292,
          "accuracy": 0.8175
        },
        "0.01": null
      },
      "auroc": 0.9455252115885416
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2765,
          "fn": 435,
          "accuracy": 0.8640625
        },
        "0.01": null
      },
      "auroc": 0.9380870442708332
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1375,
          "fn": 225,
          "accuracy": 0.859375
        },
        "0.01": null
      },
      "auroc": 0.9206362467447916
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1291,
          "fn": 309,
          "accuracy": 0.806875
        },
        "0.01": null
      },
      "auroc": 0.9548747395833335
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2666,
          "fn": 534,
          "accuracy": 0.833125
        },
        "0.01": null
      },
      "auroc": 0.9377554931640625
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2832,
          "fn": 368,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9256425618489583
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2599,
          "fn": 601,
          "accuracy": 0.8121875
        },
        "0.01": null
      },
      "auroc": 0.9501999755859376
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 5431,
          "fn": 969,
          "accuracy": 0.84859375
        },
        "0.01": null
      },
      "auroc": 0.9379212687174477
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1584,
          "fn": 16,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.995468701171875
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 908,
          "fn": 692,
          "accuracy": 0.5675
        },
        "0.01": null
      },
      "auroc": 0.9070703776041666
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2492,
          "fn": 708,
          "accuracy": 0.77875
        },
        "0.01": null
      },
      "auroc": 0.9512695393880207
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 1433,
          "accuracy": 0.104375
        },
        "0.01": null
      },
      "auroc": 0.6095452311197916
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 462,
          "fn": 1138,
          "accuracy": 0.28875
        },
        "0.01": null
      },
      "auroc": 0.76696953125
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 629,
          "fn": 2571,
          "accuracy": 0.1965625
        },
        "0.01": null
      },
      "auroc": 0.6882573811848959
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1751,
          "fn": 1449,
          "accuracy": 0.5471875
        },
        "0.01": null
      },
      "auroc": 0.8025069661458334
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1370,
          "fn": 1830,
          "accuracy": 0.428125
        },
        "0.01": null
      },
      "auroc": 0.8370199544270833
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3121,
          "fn": 3279,
          "accuracy": 0.48765625
        },
        "0.01": null
      },
      "auroc": 0.8197634602864583
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1502,
          "fn": 98,
          "accuracy": 0.93875
        },
        "0.01": null
      },
      "auroc": 0.9864687825520834
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1045,
          "fn": 555,
          "accuracy": 0.653125
        },
        "0.01": null
      },
      "auroc": 0.9421262858072916
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2547,
          "fn": 653,
          "accuracy": 0.7959375
        },
        "0.01": null
      },
      "auroc": 0.9642975341796874
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 403,
          "fn": 1197,
          "accuracy": 0.251875
        },
        "0.01": null
      },
      "auroc": 0.6919521484375
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1022,
          "fn": 578,
          "accuracy": 0.63875
        },
        "0.01": null
      },
      "auroc": 0.9024572591145834
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1425,
          "fn": 1775,
          "accuracy": 0.4453125
        },
        "0.01": null
      },
      "auroc": 0.7972047037760417
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1905,
          "fn": 1295,
          "accuracy": 0.5953125
        },
        "0.01": null
      },
      "auroc": 0.8392104654947916
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2067,
          "fn": 1133,
          "accuracy": 0.6459375
        },
        "0.01": null
      },
      "auroc": 0.9222917724609375
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3972,
          "fn": 2428,
          "accuracy": 0.620625
        },
        "0.01": null
      },
      "auroc": 0.8807511189778645
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1506,
          "fn": 94,
          "accuracy": 0.94125
        },
        "0.01": null
      },
      "auroc": 0.9445157389322917
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1351,
          "fn": 249,
          "accuracy": 0.844375
        },
        "0.01": null
      },
      "auroc": 0.9266818196614584
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2857,
          "fn": 343,
          "accuracy": 0.8928125
        },
        "0.01": null
      },
      "auroc": 0.935598779296875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1312,
          "fn": 288,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.9145750651041666
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 980,
          "fn": 620,
          "accuracy": 0.6125
        },
        "0.01": null
      },
      "auroc": 0.8865141438802084
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2292,
          "fn": 908,
          "accuracy": 0.71625
        },
        "0.01": null
      },
      "auroc": 0.9005446044921875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2818,
          "fn": 382,
          "accuracy": 0.880625
        },
        "0.01": null
      },
      "auroc": 0.9295454020182291
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2331,
          "fn": 869,
          "accuracy": 0.7284375
        },
        "0.01": null
      },
      "auroc": 0.9065979817708333
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 5149,
          "fn": 1251,
          "accuracy": 0.80453125
        },
        "0.01": null
      },
      "auroc": 0.9180716918945312
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 900,
          "fn": 700,
          "accuracy": 0.5625
        },
        "0.01": null
      },
      "auroc": 0.8468470377604168
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 900,
          "fn": 700,
          "accuracy": 0.5625
        },
        "0.01": null
      },
      "auroc": 0.8468470377604168
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 740,
          "fn": 860,
          "accuracy": 0.4625
        },
        "0.01": null
      },
      "auroc": 0.8039301432291666
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 740,
          "fn": 860,
          "accuracy": 0.4625
        },
        "0.01": null
      },
      "auroc": 0.8039301432291666
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1640,
          "fn": 1560,
          "accuracy": 0.5125
        },
        "0.01": null
      },
      "auroc": 0.8253885904947917
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1640,
          "fn": 1560,
          "accuracy": 0.5125
        },
        "0.01": null
      },
      "auroc": 0.8253885904947917
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 1230,
          "accuracy": 0.23125
        },
        "0.01": null
      },
      "auroc": 0.6937889811197917
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 1230,
          "accuracy": 0.23125
        },
        "0.01": null
      },
      "auroc": 0.6937889811197917
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 289,
          "fn": 1311,
          "accuracy": 0.180625
        },
        "0.01": null
      },
      "auroc": 0.6460348795572917
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 289,
          "fn": 1311,
          "accuracy": 0.180625
        },
        "0.01": null
      },
      "auroc": 0.6460348795572917
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 659,
          "fn": 2541,
          "accuracy": 0.2059375
        },
        "0.01": null
      },
      "auroc": 0.6699119303385417
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 659,
          "fn": 2541,
          "accuracy": 0.2059375
        },
        "0.01": null
      },
      "auroc": 0.6699119303385417
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1481,
          "fn": 119,
          "accuracy": 0.925625
        },
        "0.01": null
      },
      "auroc": 0.9243252441406251
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1481,
          "fn": 119,
          "accuracy": 0.925625
        },
        "0.01": null
      },
      "auroc": 0.9243252441406251
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1366,
          "fn": 234,
          "accuracy": 0.85375
        },
        "0.01": null
      },
      "auroc": 0.9068927083333334
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1366,
          "fn": 234,
          "accuracy": 0.85375
        },
        "0.01": null
      },
      "auroc": 0.9068927083333334
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2847,
          "fn": 353,
          "accuracy": 0.8896875
        },
        "0.01": null
      },
      "auroc": 0.9156089762369791
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2847,
          "fn": 353,
          "accuracy": 0.8896875
        },
        "0.01": null
      },
      "auroc": 0.9156089762369791
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1424,
          "fn": 176,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9222153320312501
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1424,
          "fn": 176,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9222153320312501
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 869,
          "fn": 731,
          "accuracy": 0.543125
        },
        "0.01": null
      },
      "auroc": 0.8078974121093749
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 869,
          "fn": 731,
          "accuracy": 0.543125
        },
        "0.01": null
      },
      "auroc": 0.8078974121093749
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2293,
          "fn": 907,
          "accuracy": 0.7165625
        },
        "0.01": null
      },
      "auroc": 0.8650563720703124
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2293,
          "fn": 907,
          "accuracy": 0.7165625
        },
        "0.01": null
      },
      "auroc": 0.8650563720703124
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 962,
          "fn": 638,
          "accuracy": 0.60125
        },
        "0.01": null
      },
      "auroc": 0.8481458821614583
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 962,
          "fn": 638,
          "accuracy": 0.60125
        },
        "0.01": null
      },
      "auroc": 0.8481458821614583
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 804,
          "fn": 796,
          "accuracy": 0.5025
        },
        "0.01": null
      },
      "auroc": 0.7965258626302083
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 804,
          "fn": 796,
          "accuracy": 0.5025
        },
        "0.01": null
      },
      "auroc": 0.7965258626302083
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1766,
          "fn": 1434,
          "accuracy": 0.551875
        },
        "0.01": null
      },
      "auroc": 0.8223358723958333
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1766,
          "fn": 1434,
          "accuracy": 0.551875
        },
        "0.01": null
      },
      "auroc": 0.8223358723958333
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 14172,
          "fn": 3428,
          "accuracy": 0.8052272727272727
        },
        "0.01": null
      },
      "auroc": 0.9112725941051136
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 7536,
          "fn": 2064,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.9399080457899305
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 21708,
          "fn": 5492,
          "accuracy": 0.7980882352941177
        },
        "0.01": null
      },
      "auroc": 0.9213792241115196
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 9055,
          "fn": 8545,
          "accuracy": 0.5144886363636364
        },
        "0.01": null
      },
      "auroc": 0.788275390625
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 6559,
          "fn": 3041,
          "accuracy": 0.6832291666666667
        },
        "0.01": null
      },
      "auroc": 0.9008280598958334
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 15614,
          "fn": 11586,
          "accuracy": 0.5740441176470589
        },
        "0.01": null
      },
      "auroc": 0.827999862132353
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 23227,
          "fn": 11973,
          "accuracy": 0.6598579545454546
        },
        "0.01": null
      },
      "auroc": 0.8497739923650568
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 14095,
          "fn": 5105,
          "accuracy": 0.7341145833333333
        },
        "0.01": null
      },
      "auroc": 0.9203680528428819
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 37322,
          "fn": 17078,
          "accuracy": 0.6860661764705882
        },
        "0.01": null
      },
      "auroc": 0.8746895431219361
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1556,
          "fn": 44,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.982447900390625
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1481,
          "fn": 119,
          "accuracy": 0.925625
        },
        "0.01": null
      },
      "auroc": 0.9670388346354166
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3037,
          "fn": 163,
          "accuracy": 0.9490625
        },
        "0.01": null
      },
      "auroc": 0.9747433675130208
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1523,
          "fn": 77,
          "accuracy": 0.951875
        },
        "0.01": null
      },
      "auroc": 0.9754781087239583
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1407,
          "fn": 193,
          "accuracy": 0.879375
        },
        "0.01": null
      },
      "auroc": 0.946285791015625
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2930,
          "fn": 270,
          "accuracy": 0.915625
        },
        "0.01": null
      },
      "auroc": 0.9608819498697917
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3079,
          "fn": 121,
          "accuracy": 0.9621875
        },
        "0.01": null
      },
      "auroc": 0.9789630045572917
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2888,
          "fn": 312,
          "accuracy": 0.9025
        },
        "0.01": null
      },
      "auroc": 0.9566623128255208
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 5967,
          "fn": 433,
          "accuracy": 0.93234375
        },
        "0.01": null
      },
      "auroc": 0.9678126586914062
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1566,
          "fn": 34,
          "accuracy": 0.97875
        },
        "0.01": null
      },
      "auroc": 0.9932524414062501
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1538,
          "fn": 62,
          "accuracy": 0.96125
        },
        "0.01": null
      },
      "auroc": 0.9896237630208334
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3104,
          "fn": 96,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9914381022135417
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 303,
          "fn": 1297,
          "accuracy": 0.189375
        },
        "0.01": null
      },
      "auroc": 0.6554725260416667
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1545,
          "fn": 55,
          "accuracy": 0.965625
        },
        "0.01": null
      },
      "auroc": 0.9904599772135417
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1848,
          "fn": 1352,
          "accuracy": 0.5775
        },
        "0.01": null
      },
      "auroc": 0.8229662516276042
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1869,
          "fn": 1331,
          "accuracy": 0.5840625
        },
        "0.01": null
      },
      "auroc": 0.8243624837239584
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3083,
          "fn": 117,
          "accuracy": 0.9634375
        },
        "0.01": null
      },
      "auroc": 0.9900418701171875
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 4952,
          "fn": 1448,
          "accuracy": 0.77375
        },
        "0.01": null
      },
      "auroc": 0.9072021769205729
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1538,
          "fn": 62,
          "accuracy": 0.96125
        },
        "0.01": null
      },
      "auroc": 0.9700888020833333
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1390,
          "fn": 210,
          "accuracy": 0.86875
        },
        "0.01": null
      },
      "auroc": 0.9651627441406251
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2928,
          "fn": 272,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9676257731119791
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1498,
          "fn": 102,
          "accuracy": 0.93625
        },
        "0.01": null
      },
      "auroc": 0.9647477864583334
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1383,
          "fn": 217,
          "accuracy": 0.864375
        },
        "0.01": null
      },
      "auroc": 0.9690663899739583
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2881,
          "fn": 319,
          "accuracy": 0.9003125
        },
        "0.01": null
      },
      "auroc": 0.9669070882161459
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3036,
          "fn": 164,
          "accuracy": 0.94875
        },
        "0.01": null
      },
      "auroc": 0.9674182942708334
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2773,
          "fn": 427,
          "accuracy": 0.8665625
        },
        "0.01": null
      },
      "auroc": 0.9671145670572916
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 5809,
          "fn": 591,
          "accuracy": 0.90765625
        },
        "0.01": null
      },
      "auroc": 0.9672664306640626
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9959444661458332
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1378,
          "fn": 222,
          "accuracy": 0.86125
        },
        "0.01": null
      },
      "auroc": 0.9688471516927083
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2975,
          "fn": 225,
          "accuracy": 0.9296875
        },
        "0.01": null
      },
      "auroc": 0.9823958089192708
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 1410,
          "accuracy": 0.11875
        },
        "0.01": null
      },
      "auroc": 0.612300439453125
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 1240,
          "accuracy": 0.225
        },
        "0.01": null
      },
      "auroc": 0.6837974934895834
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 550,
          "fn": 2650,
          "accuracy": 0.171875
        },
        "0.01": null
      },
      "auroc": 0.6480489664713542
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1787,
          "fn": 1413,
          "accuracy": 0.5584375
        },
        "0.01": null
      },
      "auroc": 0.8041224527994792
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1738,
          "fn": 1462,
          "accuracy": 0.543125
        },
        "0.01": null
      },
      "auroc": 0.8263223225911458
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3525,
          "fn": 2875,
          "accuracy": 0.55078125
        },
        "0.01": null
      },
      "auroc": 0.8152223876953124
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1559,
          "fn": 41,
          "accuracy": 0.974375
        },
        "0.01": null
      },
      "auroc": 0.9928007812500002
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1207,
          "fn": 393,
          "accuracy": 0.754375
        },
        "0.01": null
      },
      "auroc": 0.9680034993489584
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2766,
          "fn": 434,
          "accuracy": 0.864375
        },
        "0.01": null
      },
      "auroc": 0.9804021402994791
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 439,
          "fn": 1161,
          "accuracy": 0.274375
        },
        "0.01": null
      },
      "auroc": 0.7095716471354168
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 930,
          "fn": 670,
          "accuracy": 0.58125
        },
        "0.01": null
      },
      "auroc": 0.8606098307291666
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1369,
          "fn": 1831,
          "accuracy": 0.4278125
        },
        "0.01": null
      },
      "auroc": 0.7850907389322916
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1998,
          "fn": 1202,
          "accuracy": 0.624375
        },
        "0.01": null
      },
      "auroc": 0.8511862141927085
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2137,
          "fn": 1063,
          "accuracy": 0.6678125
        },
        "0.01": null
      },
      "auroc": 0.9143066650390624
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 4135,
          "fn": 2265,
          "accuracy": 0.64609375
        },
        "0.01": null
      },
      "auroc": 0.8827464396158853
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1576,
          "fn": 24,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.979430615234375
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1498,
          "fn": 102,
          "accuracy": 0.93625
        },
        "0.01": null
      },
      "auroc": 0.9631297526041668
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3074,
          "fn": 126,
          "accuracy": 0.960625
        },
        "0.01": null
      },
      "auroc": 0.9712801839192708
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1486,
          "fn": 114,
          "accuracy": 0.92875
        },
        "0.01": null
      },
      "auroc": 0.9542864420572917
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1212,
          "fn": 388,
          "accuracy": 0.7575
        },
        "0.01": null
      },
      "auroc": 0.9213020996093749
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2698,
          "fn": 502,
          "accuracy": 0.843125
        },
        "0.01": null
      },
      "auroc": 0.9377942708333336
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3062,
          "fn": 138,
          "accuracy": 0.956875
        },
        "0.01": null
      },
      "auroc": 0.9668585286458332
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2710,
          "fn": 490,
          "accuracy": 0.846875
        },
        "0.01": null
      },
      "auroc": 0.9422159261067709
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 5772,
          "fn": 628,
          "accuracy": 0.901875
        },
        "0.01": null
      },
      "auroc": 0.954537227376302
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1338,
          "fn": 262,
          "accuracy": 0.83625
        },
        "0.01": null
      },
      "auroc": 0.9119646647135418
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1338,
          "fn": 262,
          "accuracy": 0.83625
        },
        "0.01": null
      },
      "auroc": 0.9119646647135418
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1186,
          "fn": 414,
          "accuracy": 0.74125
        },
        "0.01": null
      },
      "auroc": 0.8813605794270833
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1186,
          "fn": 414,
          "accuracy": 0.74125
        },
        "0.01": null
      },
      "auroc": 0.8813605794270833
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2524,
          "fn": 676,
          "accuracy": 0.78875
        },
        "0.01": null
      },
      "auroc": 0.8966626220703124
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2524,
          "fn": 676,
          "accuracy": 0.78875
        },
        "0.01": null
      },
      "auroc": 0.8966626220703124
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 702,
          "fn": 898,
          "accuracy": 0.43875
        },
        "0.01": null
      },
      "auroc": 0.7561251627604167
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 702,
          "fn": 898,
          "accuracy": 0.43875
        },
        "0.01": null
      },
      "auroc": 0.7561251627604167
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 538,
          "fn": 1062,
          "accuracy": 0.33625
        },
        "0.01": null
      },
      "auroc": 0.7024874837239583
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 538,
          "fn": 1062,
          "accuracy": 0.33625
        },
        "0.01": null
      },
      "auroc": 0.7024874837239583
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1240,
          "fn": 1960,
          "accuracy": 0.3875
        },
        "0.01": null
      },
      "auroc": 0.7293063232421875
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1240,
          "fn": 1960,
          "accuracy": 0.3875
        },
        "0.01": null
      },
      "auroc": 0.7293063232421875
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1565,
          "fn": 35,
          "accuracy": 0.978125
        },
        "0.01": null
      },
      "auroc": 0.9693366373697918
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1565,
          "fn": 35,
          "accuracy": 0.978125
        },
        "0.01": null
      },
      "auroc": 0.9693366373697918
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1502,
          "fn": 98,
          "accuracy": 0.93875
        },
        "0.01": null
      },
      "auroc": 0.9516246582031249
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1502,
          "fn": 98,
          "accuracy": 0.93875
        },
        "0.01": null
      },
      "auroc": 0.9516246582031249
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3067,
          "fn": 133,
          "accuracy": 0.9584375
        },
        "0.01": null
      },
      "auroc": 0.9604806477864584
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3067,
          "fn": 133,
          "accuracy": 0.9584375
        },
        "0.01": null
      },
      "auroc": 0.9604806477864584
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1532,
          "fn": 68,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9725677897135417
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1532,
          "fn": 68,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9725677897135417
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1176,
          "fn": 424,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.8595244628906249
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1176,
          "fn": 424,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.8595244628906249
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2708,
          "fn": 492,
          "accuracy": 0.84625
        },
        "0.01": null
      },
      "auroc": 0.9160461263020834
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2708,
          "fn": 492,
          "accuracy": 0.84625
        },
        "0.01": null
      },
      "auroc": 0.9160461263020834
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1203,
          "fn": 397,
          "accuracy": 0.751875
        },
        "0.01": null
      },
      "auroc": 0.9000301920572916
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1203,
          "fn": 397,
          "accuracy": 0.751875
        },
        "0.01": null
      },
      "auroc": 0.9000301920572916
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1050,
          "fn": 550,
          "accuracy": 0.65625
        },
        "0.01": null
      },
      "auroc": 0.8539999837239584
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1050,
          "fn": 550,
          "accuracy": 0.65625
        },
        "0.01": null
      },
      "auroc": 0.8539999837239584
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2253,
          "fn": 947,
          "accuracy": 0.7040625
        },
        "0.01": null
      },
      "auroc": 0.8770150878906249
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2253,
          "fn": 947,
          "accuracy": 0.7040625
        },
        "0.01": null
      },
      "auroc": 0.8770150878906249
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 15732,
          "fn": 1868,
          "accuracy": 0.8938636363636364
        },
        "0.01": null
      },
      "auroc": 0.9476354048295452
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 8492,
          "fn": 1108,
          "accuracy": 0.8845833333333334
        },
        "0.01": null
      },
      "auroc": 0.9703009575737848
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 24224,
          "fn": 2976,
          "accuracy": 0.8905882352941177
        },
        "0.01": null
      },
      "auroc": 0.9556350116804534
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 10891,
          "fn": 6709,
          "accuracy": 0.6188068181818182
        },
        "0.01": null
      },
      "auroc": 0.8291685561671401
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 6837,
          "fn": 2763,
          "accuracy": 0.7121875
        },
        "0.01": null
      },
      "auroc": 0.8952535970052082
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 17728,
          "fn": 9472,
          "accuracy": 0.6517647058823529
        },
        "0.01": null
      },
      "auroc": 0.8524926882276347
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 26623,
          "fn": 8577,
          "accuracy": 0.7563352272727273
        },
        "0.01": null
      },
      "auroc": 0.8884019804983428
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 15329,
          "fn": 3871,
          "accuracy": 0.7983854166666666
        },
        "0.01": null
      },
      "auroc": 0.9327772772894964
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 41952,
          "fn": 12448,
          "accuracy": 0.7711764705882352
        },
        "0.01": null
      },
      "auroc": 0.9040638499540441
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1371,
          "fn": 229,
          "accuracy": 0.856875
        },
        "0.01": null
      },
      "auroc": 0.9352903971354166
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1262,
          "fn": 338,
          "accuracy": 0.78875
        },
        "0.01": null
      },
      "auroc": 0.9101738606770834
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2633,
          "fn": 567,
          "accuracy": 0.8228125
        },
        "0.01": null
      },
      "auroc": 0.9227321289062499
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1353,
          "fn": 247,
          "accuracy": 0.845625
        },
        "0.01": null
      },
      "auroc": 0.9307900716145834
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1136,
          "fn": 464,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.8822480631510417
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2489,
          "fn": 711,
          "accuracy": 0.7778125
        },
        "0.01": null
      },
      "auroc": 0.9065190673828125
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2724,
          "fn": 476,
          "accuracy": 0.85125
        },
        "0.01": null
      },
      "auroc": 0.933040234375
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2398,
          "fn": 802,
          "accuracy": 0.749375
        },
        "0.01": null
      },
      "auroc": 0.8962109619140625
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 5122,
          "fn": 1278,
          "accuracy": 0.8003125
        },
        "0.01": null
      },
      "auroc": 0.9146255981445313
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1374,
          "fn": 226,
          "accuracy": 0.85875
        },
        "0.01": null
      },
      "auroc": 0.9707651692708332
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 509,
          "fn": 1091,
          "accuracy": 0.318125
        },
        "0.01": null
      },
      "auroc": 0.7808959147135417
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1883,
          "fn": 1317,
          "accuracy": 0.5884375
        },
        "0.01": null
      },
      "auroc": 0.8758305419921875
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 1227,
          "accuracy": 0.233125
        },
        "0.01": null
      },
      "auroc": 0.7137534830729166
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 477,
          "fn": 1123,
          "accuracy": 0.298125
        },
        "0.01": null
      },
      "auroc": 0.7615895670572917
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 850,
          "fn": 2350,
          "accuracy": 0.265625
        },
        "0.01": null
      },
      "auroc": 0.7376715250651042
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1747,
          "fn": 1453,
          "accuracy": 0.5459375
        },
        "0.01": null
      },
      "auroc": 0.8422593261718749
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 986,
          "fn": 2214,
          "accuracy": 0.308125
        },
        "0.01": null
      },
      "auroc": 0.7712427408854167
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2733,
          "fn": 3667,
          "accuracy": 0.42703125
        },
        "0.01": null
      },
      "auroc": 0.8067510335286456
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1298,
          "fn": 302,
          "accuracy": 0.81125
        },
        "0.01": null
      },
      "auroc": 0.9040451985677084
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 900,
          "fn": 700,
          "accuracy": 0.5625
        },
        "0.01": null
      },
      "auroc": 0.8425457031250001
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2198,
          "fn": 1002,
          "accuracy": 0.686875
        },
        "0.01": null
      },
      "auroc": 0.8732954508463542
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1250,
          "fn": 350,
          "accuracy": 0.78125
        },
        "0.01": null
      },
      "auroc": 0.8911732584635417
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 683,
          "fn": 917,
          "accuracy": 0.426875
        },
        "0.01": null
      },
      "auroc": 0.7911775878906251
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1933,
          "fn": 1267,
          "accuracy": 0.6040625
        },
        "0.01": null
      },
      "auroc": 0.8411754231770834
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2548,
          "fn": 652,
          "accuracy": 0.79625
        },
        "0.01": null
      },
      "auroc": 0.897609228515625
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1583,
          "fn": 1617,
          "accuracy": 0.4946875
        },
        "0.01": null
      },
      "auroc": 0.8168616455078126
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 4131,
          "fn": 2269,
          "accuracy": 0.64546875
        },
        "0.01": null
      },
      "auroc": 0.8572354370117188
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1358,
          "fn": 242,
          "accuracy": 0.84875
        },
        "0.01": null
      },
      "auroc": 0.9717658365885417
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1016,
          "fn": 584,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.8917118326822917
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2374,
          "fn": 826,
          "accuracy": 0.741875
        },
        "0.01": null
      },
      "auroc": 0.9317388346354167
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 1253,
          "accuracy": 0.216875
        },
        "0.01": null
      },
      "auroc": 0.717978173828125
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 1408,
          "accuracy": 0.12
        },
        "0.01": null
      },
      "auroc": 0.6341291503906249
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 539,
          "fn": 2661,
          "accuracy": 0.1684375
        },
        "0.01": null
      },
      "auroc": 0.676053662109375
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1705,
          "fn": 1495,
          "accuracy": 0.5328125
        },
        "0.01": null
      },
      "auroc": 0.8448720052083334
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1208,
          "fn": 1992,
          "accuracy": 0.3775
        },
        "0.01": null
      },
      "auroc": 0.7629204915364584
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2913,
          "fn": 3487,
          "accuracy": 0.45515625
        },
        "0.01": null
      },
      "auroc": 0.8038962483723958
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1361,
          "fn": 239,
          "accuracy": 0.850625
        },
        "0.01": null
      },
      "auroc": 0.9651424641927083
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 611,
          "fn": 989,
          "accuracy": 0.381875
        },
        "0.01": null
      },
      "auroc": 0.7889768717447916
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1972,
          "fn": 1228,
          "accuracy": 0.61625
        },
        "0.01": null
      },
      "auroc": 0.87705966796875
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 1259,
          "accuracy": 0.213125
        },
        "0.01": null
      },
      "auroc": 0.71370087890625
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 228,
          "fn": 1372,
          "accuracy": 0.1425
        },
        "0.01": null
      },
      "auroc": 0.6518216959635417
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 569,
          "fn": 2631,
          "accuracy": 0.1778125
        },
        "0.01": null
      },
      "auroc": 0.6827612874348958
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1702,
          "fn": 1498,
          "accuracy": 0.531875
        },
        "0.01": null
      },
      "auroc": 0.8394216715494791
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 839,
          "fn": 2361,
          "accuracy": 0.2621875
        },
        "0.01": null
      },
      "auroc": 0.7203992838541666
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2541,
          "fn": 3859,
          "accuracy": 0.39703125
        },
        "0.01": null
      },
      "auroc": 0.779910477701823
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1383,
          "fn": 217,
          "accuracy": 0.864375
        },
        "0.01": null
      },
      "auroc": 0.9311498535156251
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1243,
          "fn": 357,
          "accuracy": 0.776875
        },
        "0.01": null
      },
      "auroc": 0.90471630859375
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2626,
          "fn": 574,
          "accuracy": 0.820625
        },
        "0.01": null
      },
      "auroc": 0.9179330810546875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1282,
          "fn": 318,
          "accuracy": 0.80125
        },
        "0.01": null
      },
      "auroc": 0.907046533203125
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 918,
          "fn": 682,
          "accuracy": 0.57375
        },
        "0.01": null
      },
      "auroc": 0.8459295410156249
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2200,
          "fn": 1000,
          "accuracy": 0.6875
        },
        "0.01": null
      },
      "auroc": 0.876488037109375
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2665,
          "fn": 535,
          "accuracy": 0.8328125
        },
        "0.01": null
      },
      "auroc": 0.919098193359375
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2161,
          "fn": 1039,
          "accuracy": 0.6753125
        },
        "0.01": null
      },
      "auroc": 0.8753229248046875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 4826,
          "fn": 1574,
          "accuracy": 0.7540625
        },
        "0.01": null
      },
      "auroc": 0.8972105590820314
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 989,
          "fn": 611,
          "accuracy": 0.618125
        },
        "0.01": null
      },
      "auroc": 0.8555487141927083
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 989,
          "fn": 611,
          "accuracy": 0.618125
        },
        "0.01": null
      },
      "auroc": 0.8555487141927083
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 854,
          "fn": 746,
          "accuracy": 0.53375
        },
        "0.01": null
      },
      "auroc": 0.8293103027343751
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 854,
          "fn": 746,
          "accuracy": 0.53375
        },
        "0.01": null
      },
      "auroc": 0.8293103027343751
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1843,
          "fn": 1357,
          "accuracy": 0.5759375
        },
        "0.01": null
      },
      "auroc": 0.8424295084635416
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1843,
          "fn": 1357,
          "accuracy": 0.5759375
        },
        "0.01": null
      },
      "auroc": 0.8424295084635416
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 551,
          "fn": 1049,
          "accuracy": 0.344375
        },
        "0.01": null
      },
      "auroc": 0.7537819010416666
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 551,
          "fn": 1049,
          "accuracy": 0.344375
        },
        "0.01": null
      },
      "auroc": 0.7537819010416666
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 442,
          "fn": 1158,
          "accuracy": 0.27625
        },
        "0.01": null
      },
      "auroc": 0.7195696614583333
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 442,
          "fn": 1158,
          "accuracy": 0.27625
        },
        "0.01": null
      },
      "auroc": 0.7195696614583333
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 993,
          "fn": 2207,
          "accuracy": 0.3103125
        },
        "0.01": null
      },
      "auroc": 0.73667578125
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 993,
          "fn": 2207,
          "accuracy": 0.3103125
        },
        "0.01": null
      },
      "auroc": 0.73667578125
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1345,
          "fn": 255,
          "accuracy": 0.840625
        },
        "0.01": null
      },
      "auroc": 0.9089268717447916
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1345,
          "fn": 255,
          "accuracy": 0.840625
        },
        "0.01": null
      },
      "auroc": 0.9089268717447916
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1270,
          "fn": 330,
          "accuracy": 0.79375
        },
        "0.01": null
      },
      "auroc": 0.8919638834635417
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1270,
          "fn": 330,
          "accuracy": 0.79375
        },
        "0.01": null
      },
      "auroc": 0.8919638834635417
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2615,
          "fn": 585,
          "accuracy": 0.8171875
        },
        "0.01": null
      },
      "auroc": 0.9004453776041668
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2615,
          "fn": 585,
          "accuracy": 0.8171875
        },
        "0.01": null
      },
      "auroc": 0.9004453776041668
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1289,
          "fn": 311,
          "accuracy": 0.805625
        },
        "0.01": null
      },
      "auroc": 0.9036755859375001
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1289,
          "fn": 311,
          "accuracy": 0.805625
        },
        "0.01": null
      },
      "auroc": 0.9036755859375001
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 883,
          "fn": 717,
          "accuracy": 0.551875
        },
        "0.01": null
      },
      "auroc": 0.8099169270833334
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 883,
          "fn": 717,
          "accuracy": 0.551875
        },
        "0.01": null
      },
      "auroc": 0.8099169270833334
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2172,
          "fn": 1028,
          "accuracy": 0.67875
        },
        "0.01": null
      },
      "auroc": 0.8567962565104168
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2172,
          "fn": 1028,
          "accuracy": 0.67875
        },
        "0.01": null
      },
      "auroc": 0.8567962565104168
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 984,
          "fn": 616,
          "accuracy": 0.615
        },
        "0.01": null
      },
      "auroc": 0.8611189290364583
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 984,
          "fn": 616,
          "accuracy": 0.615
        },
        "0.01": null
      },
      "auroc": 0.8611189290364583
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 814,
          "fn": 786,
          "accuracy": 0.50875
        },
        "0.01": null
      },
      "auroc": 0.8144011555989585
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 814,
          "fn": 786,
          "accuracy": 0.50875
        },
        "0.01": null
      },
      "auroc": 0.8144011555989585
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1798,
          "fn": 1402,
          "accuracy": 0.561875
        },
        "0.01": null
      },
      "auroc": 0.8377600423177084
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1798,
          "fn": 1402,
          "accuracy": 0.561875
        },
        "0.01": null
      },
      "auroc": 0.8377600423177084
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 13303,
          "fn": 4297,
          "accuracy": 0.7558522727272727
        },
        "0.01": null
      },
      "auroc": 0.9055646292021782
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 5541,
          "fn": 4059,
          "accuracy": 0.5771875
        },
        "0.01": null
      },
      "auroc": 0.8531700819227431
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 18844,
          "fn": 8356,
          "accuracy": 0.6927941176470588
        },
        "0.01": null
      },
      "auroc": 0.8870724360447303
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 9209,
          "fn": 8391,
          "accuracy": 0.5232386363636363
        },
        "0.01": null
      },
      "auroc": 0.8126913026751894
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3634,
          "fn": 5966,
          "accuracy": 0.37854166666666667
        },
        "0.01": null
      },
      "auroc": 0.7611492675781248
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 12843,
          "fn": 14357,
          "accuracy": 0.4721691176470588
        },
        "0.01": null
      },
      "auroc": 0.7944999961703432
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 22512,
          "fn": 12688,
          "accuracy": 0.6395454545454545
        },
        "0.01": null
      },
      "auroc": 0.8591279659386838
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 9175,
          "fn": 10025,
          "accuracy": 0.4778645833333333
        },
        "0.01": null
      },
      "auroc": 0.8071596747504339
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 31687,
          "fn": 22713,
          "accuracy": 0.5824816176470589
        },
        "0.01": null
      },
      "auroc": 0.8407862161075369
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1579,
          "fn": 21,
          "accuracy": 0.986875
        },
        "0.01": null
      },
      "auroc": 0.9864264322916666
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1516,
          "fn": 84,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9743234863281249
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3095,
          "fn": 105,
          "accuracy": 0.9671875
        },
        "0.01": null
      },
      "auroc": 0.9803749593098958
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1556,
          "fn": 44,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.981279541015625
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1451,
          "fn": 149,
          "accuracy": 0.906875
        },
        "0.01": null
      },
      "auroc": 0.9564080891927084
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3007,
          "fn": 193,
          "accuracy": 0.9396875
        },
        "0.01": null
      },
      "auroc": 0.9688438151041667
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3135,
          "fn": 65,
          "accuracy": 0.9796875
        },
        "0.01": null
      },
      "auroc": 0.9838529866536458
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2967,
          "fn": 233,
          "accuracy": 0.9271875
        },
        "0.01": null
      },
      "auroc": 0.9653657877604166
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 6102,
          "fn": 298,
          "accuracy": 0.9534375
        },
        "0.01": null
      },
      "auroc": 0.9746093872070312
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1543,
          "fn": 57,
          "accuracy": 0.964375
        },
        "0.01": null
      },
      "auroc": 0.9933981119791668
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1552,
          "fn": 48,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9913214029947917
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3095,
          "fn": 105,
          "accuracy": 0.9671875
        },
        "0.01": null
      },
      "auroc": 0.9923597574869791
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 1205,
          "accuracy": 0.246875
        },
        "0.01": null
      },
      "auroc": 0.7311844401041667
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1548,
          "fn": 52,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9914928059895833
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1943,
          "fn": 1257,
          "accuracy": 0.6071875
        },
        "0.01": null
      },
      "auroc": 0.861338623046875
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1938,
          "fn": 1262,
          "accuracy": 0.605625
        },
        "0.01": null
      },
      "auroc": 0.8622912760416667
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3100,
          "fn": 100,
          "accuracy": 0.96875
        },
        "0.01": null
      },
      "auroc": 0.9914071044921876
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 5038,
          "fn": 1362,
          "accuracy": 0.7871875
        },
        "0.01": null
      },
      "auroc": 0.926849190266927
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1562,
          "fn": 38,
          "accuracy": 0.97625
        },
        "0.01": null
      },
      "auroc": 0.9738724772135418
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1431,
          "fn": 169,
          "accuracy": 0.894375
        },
        "0.01": null
      },
      "auroc": 0.972246728515625
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2993,
          "fn": 207,
          "accuracy": 0.9353125
        },
        "0.01": null
      },
      "auroc": 0.9730596028645834
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1521,
          "fn": 79,
          "accuracy": 0.950625
        },
        "0.01": null
      },
      "auroc": 0.9693740234375
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1416,
          "fn": 184,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.9743934733072916
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2937,
          "fn": 263,
          "accuracy": 0.9178125
        },
        "0.01": null
      },
      "auroc": 0.9718837483723957
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3083,
          "fn": 117,
          "accuracy": 0.9634375
        },
        "0.01": null
      },
      "auroc": 0.9716232503255209
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2847,
          "fn": 353,
          "accuracy": 0.8896875
        },
        "0.01": null
      },
      "auroc": 0.9733201009114583
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 5930,
          "fn": 470,
          "accuracy": 0.9265625
        },
        "0.01": null
      },
      "auroc": 0.9724716756184896
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1581,
          "fn": 19,
          "accuracy": 0.988125
        },
        "0.01": null
      },
      "auroc": 0.9956381184895833
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1474,
          "fn": 126,
          "accuracy": 0.92125
        },
        "0.01": null
      },
      "auroc": 0.9809755533854168
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3055,
          "fn": 145,
          "accuracy": 0.9546875
        },
        "0.01": null
      },
      "auroc": 0.9883068359375
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 1307,
          "accuracy": 0.183125
        },
        "0.01": null
      },
      "auroc": 0.7120302734375
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 462,
          "fn": 1138,
          "accuracy": 0.28875
        },
        "0.01": null
      },
      "auroc": 0.78874833984375
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 755,
          "fn": 2445,
          "accuracy": 0.2359375
        },
        "0.01": null
      },
      "auroc": 0.7503893066406249
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1874,
          "fn": 1326,
          "accuracy": 0.585625
        },
        "0.01": null
      },
      "auroc": 0.8538341959635418
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1936,
          "fn": 1264,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.8848619466145832
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3810,
          "fn": 2590,
          "accuracy": 0.5953125
        },
        "0.01": null
      },
      "auroc": 0.8693480712890624
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1555,
          "fn": 45,
          "accuracy": 0.971875
        },
        "0.01": null
      },
      "auroc": 0.9930249674479167
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1305,
          "fn": 295,
          "accuracy": 0.815625
        },
        "0.01": null
      },
      "auroc": 0.9796749348958332
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2860,
          "fn": 340,
          "accuracy": 0.89375
        },
        "0.01": null
      },
      "auroc": 0.986349951171875
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 512,
          "fn": 1088,
          "accuracy": 0.32
        },
        "0.01": null
      },
      "auroc": 0.7731313802083333
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 998,
          "fn": 602,
          "accuracy": 0.62375
        },
        "0.01": null
      },
      "auroc": 0.9019818684895833
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1510,
          "fn": 1690,
          "accuracy": 0.471875
        },
        "0.01": null
      },
      "auroc": 0.8375566243489582
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2067,
          "fn": 1133,
          "accuracy": 0.6459375
        },
        "0.01": null
      },
      "auroc": 0.883078173828125
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2303,
          "fn": 897,
          "accuracy": 0.7196875
        },
        "0.01": null
      },
      "auroc": 0.9408284016927084
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 4370,
          "fn": 2030,
          "accuracy": 0.6828125
        },
        "0.01": null
      },
      "auroc": 0.9119532877604166
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1587,
          "fn": 13,
          "accuracy": 0.991875
        },
        "0.01": null
      },
      "auroc": 0.981902197265625
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1533,
          "fn": 67,
          "accuracy": 0.958125
        },
        "0.01": null
      },
      "auroc": 0.969022998046875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3120,
          "fn": 80,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.97546259765625
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1523,
          "fn": 77,
          "accuracy": 0.951875
        },
        "0.01": null
      },
      "auroc": 0.961598681640625
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1264,
          "fn": 336,
          "accuracy": 0.79
        },
        "0.01": null
      },
      "auroc": 0.94019404296875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2787,
          "fn": 413,
          "accuracy": 0.8709375
        },
        "0.01": null
      },
      "auroc": 0.9508963623046875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3110,
          "fn": 90,
          "accuracy": 0.971875
        },
        "0.01": null
      },
      "auroc": 0.971750439453125
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2797,
          "fn": 403,
          "accuracy": 0.8740625
        },
        "0.01": null
      },
      "auroc": 0.9546085205078126
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 5907,
          "fn": 493,
          "accuracy": 0.92296875
        },
        "0.01": null
      },
      "auroc": 0.9631794799804687
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1332,
          "fn": 268,
          "accuracy": 0.8325
        },
        "0.01": null
      },
      "auroc": 0.9204991048177085
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1332,
          "fn": 268,
          "accuracy": 0.8325
        },
        "0.01": null
      },
      "auroc": 0.9204991048177085
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1214,
          "fn": 386,
          "accuracy": 0.75875
        },
        "0.01": null
      },
      "auroc": 0.894913037109375
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1214,
          "fn": 386,
          "accuracy": 0.75875
        },
        "0.01": null
      },
      "auroc": 0.894913037109375
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2546,
          "fn": 654,
          "accuracy": 0.795625
        },
        "0.01": null
      },
      "auroc": 0.9077060709635416
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2546,
          "fn": 654,
          "accuracy": 0.795625
        },
        "0.01": null
      },
      "auroc": 0.9077060709635416
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 816,
          "fn": 784,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.7989043294270834
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 816,
          "fn": 784,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.7989043294270834
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 621,
          "fn": 979,
          "accuracy": 0.388125
        },
        "0.01": null
      },
      "auroc": 0.7467968098958333
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 621,
          "fn": 979,
          "accuracy": 0.388125
        },
        "0.01": null
      },
      "auroc": 0.7467968098958333
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1437,
          "fn": 1763,
          "accuracy": 0.4490625
        },
        "0.01": null
      },
      "auroc": 0.7728505696614584
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1437,
          "fn": 1763,
          "accuracy": 0.4490625
        },
        "0.01": null
      },
      "auroc": 0.7728505696614584
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1584,
          "fn": 16,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9741269205729167
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1584,
          "fn": 16,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9741269205729167
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1535,
          "fn": 65,
          "accuracy": 0.959375
        },
        "0.01": null
      },
      "auroc": 0.958916259765625
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1535,
          "fn": 65,
          "accuracy": 0.959375
        },
        "0.01": null
      },
      "auroc": 0.958916259765625
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3119,
          "fn": 81,
          "accuracy": 0.9746875
        },
        "0.01": null
      },
      "auroc": 0.9665215901692709
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3119,
          "fn": 81,
          "accuracy": 0.9746875
        },
        "0.01": null
      },
      "auroc": 0.9665215901692709
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1568,
          "fn": 32,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9778895833333333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1568,
          "fn": 32,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9778895833333333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1261,
          "fn": 339,
          "accuracy": 0.788125
        },
        "0.01": null
      },
      "auroc": 0.8849320475260417
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1261,
          "fn": 339,
          "accuracy": 0.788125
        },
        "0.01": null
      },
      "auroc": 0.8849320475260417
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2829,
          "fn": 371,
          "accuracy": 0.8840625
        },
        "0.01": null
      },
      "auroc": 0.9314108154296874
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2829,
          "fn": 371,
          "accuracy": 0.8840625
        },
        "0.01": null
      },
      "auroc": 0.9314108154296874
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1270,
          "fn": 330,
          "accuracy": 0.79375
        },
        "0.01": null
      },
      "auroc": 0.9216183105468749
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1270,
          "fn": 330,
          "accuracy": 0.79375
        },
        "0.01": null
      },
      "auroc": 0.9216183105468749
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1125,
          "fn": 475,
          "accuracy": 0.703125
        },
        "0.01": null
      },
      "auroc": 0.8830588378906251
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1125,
          "fn": 475,
          "accuracy": 0.703125
        },
        "0.01": null
      },
      "auroc": 0.8830588378906251
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2395,
          "fn": 805,
          "accuracy": 0.7484375
        },
        "0.01": null
      },
      "auroc": 0.9023385742187502
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2395,
          "fn": 805,
          "accuracy": 0.7484375
        },
        "0.01": null
      },
      "auroc": 0.9023385742187502
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 15977,
          "fn": 1623,
          "accuracy": 0.907784090909091
        },
        "0.01": null
      },
      "auroc": 0.9561182321259468
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 8811,
          "fn": 789,
          "accuracy": 0.9178125
        },
        "0.01": null
      },
      "auroc": 0.977927517361111
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 24788,
          "fn": 2412,
          "accuracy": 0.9113235294117648
        },
        "0.01": null
      },
      "auroc": 0.9638156269148285
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 11556,
          "fn": 6044,
          "accuracy": 0.6565909090909091
        },
        "0.01": null
      },
      "auroc": 0.8633832120028408
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 7139,
          "fn": 2461,
          "accuracy": 0.7436458333333333
        },
        "0.01": null
      },
      "auroc": 0.9255364366319444
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 18695,
          "fn": 8505,
          "accuracy": 0.6873161764705882
        },
        "0.01": null
      },
      "auroc": 0.8853196442248775
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 27533,
          "fn": 7667,
          "accuracy": 0.7821875
        },
        "0.01": null
      },
      "auroc": 0.909750722064394
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 15950,
          "fn": 3250,
          "accuracy": 0.8307291666666666
        },
        "0.01": null
      },
      "auroc": 0.9517319769965279
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 43483,
          "fn": 10917,
          "accuracy": 0.7993198529411765
        },
        "0.01": null
      },
      "auroc": 0.924567635569853
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1579,
          "fn": 21,
          "accuracy": 0.986875
        },
        "0.01": null
      },
      "auroc": 0.9865582682291667
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1516,
          "fn": 84,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.97450029296875
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3095,
          "fn": 105,
          "accuracy": 0.9671875
        },
        "0.01": null
      },
      "auroc": 0.9805292805989583
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1556,
          "fn": 44,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9816396158854166
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1455,
          "fn": 145,
          "accuracy": 0.909375
        },
        "0.01": null
      },
      "auroc": 0.9566344075520833
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3011,
          "fn": 189,
          "accuracy": 0.9409375
        },
        "0.01": null
      },
      "auroc": 0.96913701171875
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3135,
          "fn": 65,
          "accuracy": 0.9796875
        },
        "0.01": null
      },
      "auroc": 0.9840989420572916
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2971,
          "fn": 229,
          "accuracy": 0.9284375
        },
        "0.01": null
      },
      "auroc": 0.9655673502604167
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 6106,
          "fn": 294,
          "accuracy": 0.9540625
        },
        "0.01": null
      },
      "auroc": 0.9748331461588542
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1581,
          "fn": 19,
          "accuracy": 0.988125
        },
        "0.01": null
      },
      "auroc": 0.9942276041666667
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1551,
          "fn": 49,
          "accuracy": 0.969375
        },
        "0.01": null
      },
      "auroc": 0.9912218098958334
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3132,
          "fn": 68,
          "accuracy": 0.97875
        },
        "0.01": null
      },
      "auroc": 0.99272470703125
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 414,
          "fn": 1186,
          "accuracy": 0.25875
        },
        "0.01": null
      },
      "auroc": 0.7378898925781249
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1547,
          "fn": 53,
          "accuracy": 0.966875
        },
        "0.01": null
      },
      "auroc": 0.991334423828125
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1961,
          "fn": 1239,
          "accuracy": 0.6128125
        },
        "0.01": null
      },
      "auroc": 0.864612158203125
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1995,
          "fn": 1205,
          "accuracy": 0.6234375
        },
        "0.01": null
      },
      "auroc": 0.8660587483723958
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3098,
          "fn": 102,
          "accuracy": 0.968125
        },
        "0.01": null
      },
      "auroc": 0.9912781168619792
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 5093,
          "fn": 1307,
          "accuracy": 0.79578125
        },
        "0.01": null
      },
      "auroc": 0.9286684326171875
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1565,
          "fn": 35,
          "accuracy": 0.978125
        },
        "0.01": null
      },
      "auroc": 0.9741643554687501
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1430,
          "fn": 170,
          "accuracy": 0.89375
        },
        "0.01": null
      },
      "auroc": 0.9723981445312501
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2995,
          "fn": 205,
          "accuracy": 0.9359375
        },
        "0.01": null
      },
      "auroc": 0.9732812499999999
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1524,
          "fn": 76,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.96953349609375
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1415,
          "fn": 185,
          "accuracy": 0.884375
        },
        "0.01": null
      },
      "auroc": 0.9745145996093751
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2939,
          "fn": 261,
          "accuracy": 0.9184375
        },
        "0.01": null
      },
      "auroc": 0.9720240478515626
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3089,
          "fn": 111,
          "accuracy": 0.9653125
        },
        "0.01": null
      },
      "auroc": 0.9718489257812499
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2845,
          "fn": 355,
          "accuracy": 0.8890625
        },
        "0.01": null
      },
      "auroc": 0.9734563720703125
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 5934,
          "fn": 466,
          "accuracy": 0.9271875
        },
        "0.01": null
      },
      "auroc": 0.9726526489257813
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9959988118489582
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1535,
          "fn": 65,
          "accuracy": 0.959375
        },
        "0.01": null
      },
      "auroc": 0.982658203125
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3135,
          "fn": 65,
          "accuracy": 0.9796875
        },
        "0.01": null
      },
      "auroc": 0.9893285074869792
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 1300,
          "accuracy": 0.1875
        },
        "0.01": null
      },
      "auroc": 0.7188445312499999
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 457,
          "fn": 1143,
          "accuracy": 0.285625
        },
        "0.01": null
      },
      "auroc": 0.7883962076822916
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 757,
          "fn": 2443,
          "accuracy": 0.2365625
        },
        "0.01": null
      },
      "auroc": 0.7536203694661459
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1900,
          "fn": 1300,
          "accuracy": 0.59375
        },
        "0.01": null
      },
      "auroc": 0.8574216715494791
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1992,
          "fn": 1208,
          "accuracy": 0.6225
        },
        "0.01": null
      },
      "auroc": 0.8855272054036458
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3892,
          "fn": 2508,
          "accuracy": 0.608125
        },
        "0.01": null
      },
      "auroc": 0.8714744384765626
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1579,
          "fn": 21,
          "accuracy": 0.986875
        },
        "0.01": null
      },
      "auroc": 0.9937651204427084
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1349,
          "fn": 251,
          "accuracy": 0.843125
        },
        "0.01": null
      },
      "auroc": 0.9818822591145835
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2928,
          "fn": 272,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9878236897786459
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 536,
          "fn": 1064,
          "accuracy": 0.335
        },
        "0.01": null
      },
      "auroc": 0.779423583984375
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 997,
          "fn": 603,
          "accuracy": 0.623125
        },
        "0.01": null
      },
      "auroc": 0.9008230794270833
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1533,
          "fn": 1667,
          "accuracy": 0.4790625
        },
        "0.01": null
      },
      "auroc": 0.8401233317057291
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2115,
          "fn": 1085,
          "accuracy": 0.6609375
        },
        "0.01": null
      },
      "auroc": 0.8865943522135417
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2346,
          "fn": 854,
          "accuracy": 0.733125
        },
        "0.01": null
      },
      "auroc": 0.9413526692708333
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 4461,
          "fn": 1939,
          "accuracy": 0.69703125
        },
        "0.01": null
      },
      "auroc": 0.9139735107421876
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1588,
          "fn": 12,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9820826497395834
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1535,
          "fn": 65,
          "accuracy": 0.959375
        },
        "0.01": null
      },
      "auroc": 0.9691384440104167
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3123,
          "fn": 77,
          "accuracy": 0.9759375
        },
        "0.01": null
      },
      "auroc": 0.9756105468749999
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1529,
          "fn": 71,
          "accuracy": 0.955625
        },
        "0.01": null
      },
      "auroc": 0.9620583333333333
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1284,
          "fn": 316,
          "accuracy": 0.8025
        },
        "0.01": null
      },
      "auroc": 0.9417297200520833
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2813,
          "fn": 387,
          "accuracy": 0.8790625
        },
        "0.01": null
      },
      "auroc": 0.9518940266927083
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3117,
          "fn": 83,
          "accuracy": 0.9740625
        },
        "0.01": null
      },
      "auroc": 0.9720704915364583
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2819,
          "fn": 381,
          "accuracy": 0.8809375
        },
        "0.01": null
      },
      "auroc": 0.9554340820312499
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 5936,
          "fn": 464,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.9637522867838542
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1431,
          "fn": 169,
          "accuracy": 0.894375
        },
        "0.01": null
      },
      "auroc": 0.923121044921875
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1431,
          "fn": 169,
          "accuracy": 0.894375
        },
        "0.01": null
      },
      "auroc": 0.923121044921875
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1308,
          "fn": 292,
          "accuracy": 0.8175
        },
        "0.01": null
      },
      "auroc": 0.89863837890625
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1308,
          "fn": 292,
          "accuracy": 0.8175
        },
        "0.01": null
      },
      "auroc": 0.89863837890625
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2739,
          "fn": 461,
          "accuracy": 0.8559375
        },
        "0.01": null
      },
      "auroc": 0.9108797119140625
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2739,
          "fn": 461,
          "accuracy": 0.8559375
        },
        "0.01": null
      },
      "auroc": 0.9108797119140625
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 889,
          "fn": 711,
          "accuracy": 0.555625
        },
        "0.01": null
      },
      "auroc": 0.80269619140625
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 889,
          "fn": 711,
          "accuracy": 0.555625
        },
        "0.01": null
      },
      "auroc": 0.80269619140625
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 705,
          "fn": 895,
          "accuracy": 0.440625
        },
        "0.01": null
      },
      "auroc": 0.7509581868489583
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 705,
          "fn": 895,
          "accuracy": 0.440625
        },
        "0.01": null
      },
      "auroc": 0.7509581868489583
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1594,
          "fn": 1606,
          "accuracy": 0.498125
        },
        "0.01": null
      },
      "auroc": 0.7768271891276041
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1594,
          "fn": 1606,
          "accuracy": 0.498125
        },
        "0.01": null
      },
      "auroc": 0.7768271891276041
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1584,
          "fn": 16,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9742848795572916
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1584,
          "fn": 16,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9742848795572916
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1535,
          "fn": 65,
          "accuracy": 0.959375
        },
        "0.01": null
      },
      "auroc": 0.9590642415364583
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1535,
          "fn": 65,
          "accuracy": 0.959375
        },
        "0.01": null
      },
      "auroc": 0.9590642415364583
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3119,
          "fn": 81,
          "accuracy": 0.9746875
        },
        "0.01": null
      },
      "auroc": 0.966674560546875
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3119,
          "fn": 81,
          "accuracy": 0.9746875
        },
        "0.01": null
      },
      "auroc": 0.966674560546875
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1575,
          "fn": 25,
          "accuracy": 0.984375
        },
        "0.01": null
      },
      "auroc": 0.9780458658854166
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1575,
          "fn": 25,
          "accuracy": 0.984375
        },
        "0.01": null
      },
      "auroc": 0.9780458658854166
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1274,
          "fn": 326,
          "accuracy": 0.79625
        },
        "0.01": null
      },
      "auroc": 0.8853105305989583
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1274,
          "fn": 326,
          "accuracy": 0.79625
        },
        "0.01": null
      },
      "auroc": 0.8853105305989583
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2849,
          "fn": 351,
          "accuracy": 0.8903125
        },
        "0.01": null
      },
      "auroc": 0.9316781982421874
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2849,
          "fn": 351,
          "accuracy": 0.8903125
        },
        "0.01": null
      },
      "auroc": 0.9316781982421874
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1298,
          "fn": 302,
          "accuracy": 0.81125
        },
        "0.01": null
      },
      "auroc": 0.9234960286458332
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1298,
          "fn": 302,
          "accuracy": 0.81125
        },
        "0.01": null
      },
      "auroc": 0.9234960286458332
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1172,
          "fn": 428,
          "accuracy": 0.7325
        },
        "0.01": null
      },
      "auroc": 0.8851503092447918
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1172,
          "fn": 428,
          "accuracy": 0.7325
        },
        "0.01": null
      },
      "auroc": 0.8851503092447918
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2470,
          "fn": 730,
          "accuracy": 0.771875
        },
        "0.01": null
      },
      "auroc": 0.9043231689453125
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2470,
          "fn": 730,
          "accuracy": 0.771875
        },
        "0.01": null
      },
      "auroc": 0.9043231689453125
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 16269,
          "fn": 1331,
          "accuracy": 0.924375
        },
        "0.01": null
      },
      "auroc": 0.9571309836647728
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 8916,
          "fn": 684,
          "accuracy": 0.92875
        },
        "0.01": null
      },
      "auroc": 0.9786331922743056
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 25185,
          "fn": 2015,
          "accuracy": 0.9259191176470588
        },
        "0.01": null
      },
      "auroc": 0.9647199984681374
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 11853,
          "fn": 5747,
          "accuracy": 0.6734659090909091
        },
        "0.01": null
      },
      "auroc": 0.8662282818418561
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 7155,
          "fn": 2445,
          "accuracy": 0.7453125
        },
        "0.01": null
      },
      "auroc": 0.9255720730251735
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 19008,
          "fn": 8192,
          "accuracy": 0.6988235294117647
        },
        "0.01": null
      },
      "auroc": 0.8871731493183211
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 28122,
          "fn": 7078,
          "accuracy": 0.7989204545454546
        },
        "0.01": null
      },
      "auroc": 0.9116796327533143
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 16071,
          "fn": 3129,
          "accuracy": 0.83703125
        },
        "0.01": null
      },
      "auroc": 0.9521026326497396
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 44193,
          "fn": 10207,
          "accuracy": 0.8123713235294118
        },
        "0.01": null
      },
      "auroc": 0.9259465738932293
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 52,
          "fn": 1548,
          "accuracy": 0.0325
        },
        "0.01": null
      },
      "auroc": 0.39772900390625
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 1574,
          "accuracy": 0.01625
        },
        "0.01": null
      },
      "auroc": 0.3732414225260416
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 3122,
          "accuracy": 0.024375
        },
        "0.01": null
      },
      "auroc": 0.3854852132161458
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 65,
          "fn": 1535,
          "accuracy": 0.040625
        },
        "0.01": null
      },
      "auroc": 0.391139404296875
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 1578,
          "accuracy": 0.01375
        },
        "0.01": null
      },
      "auroc": 0.3569185709635417
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 3113,
          "accuracy": 0.0271875
        },
        "0.01": null
      },
      "auroc": 0.37402898763020825
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 3083,
          "accuracy": 0.0365625
        },
        "0.01": null
      },
      "auroc": 0.3944342041015625
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 3152,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.3650799967447916
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 6235,
          "accuracy": 0.02578125
        },
        "0.01": null
      },
      "auroc": 0.3797571004231771
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 656,
          "fn": 944,
          "accuracy": 0.41
        },
        "0.01": null
      },
      "auroc": 0.6551396484375
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 1450,
          "accuracy": 0.09375
        },
        "0.01": null
      },
      "auroc": 0.4496435384114583
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 806,
          "fn": 2394,
          "accuracy": 0.251875
        },
        "0.01": null
      },
      "auroc": 0.5523915934244791
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 1490,
          "accuracy": 0.06875
        },
        "0.01": null
      },
      "auroc": 0.386222119140625
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 1399,
          "accuracy": 0.125625
        },
        "0.01": null
      },
      "auroc": 0.455906201171875
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 311,
          "fn": 2889,
          "accuracy": 0.0971875
        },
        "0.01": null
      },
      "auroc": 0.42106416015625003
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 766,
          "fn": 2434,
          "accuracy": 0.239375
        },
        "0.01": null
      },
      "auroc": 0.5206808837890625
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 2849,
          "accuracy": 0.1096875
        },
        "0.01": null
      },
      "auroc": 0.4527748697916666
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1117,
          "fn": 5283,
          "accuracy": 0.17453125
        },
        "0.01": null
      },
      "auroc": 0.4867278767903646
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 1538,
          "accuracy": 0.03875
        },
        "0.01": null
      },
      "auroc": 0.39273330078124996
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 1585,
          "accuracy": 0.009375
        },
        "0.01": null
      },
      "auroc": 0.32691318359374993
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 77,
          "fn": 3123,
          "accuracy": 0.0240625
        },
        "0.01": null
      },
      "auroc": 0.35982324218750006
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 1552,
          "accuracy": 0.03
        },
        "0.01": null
      },
      "auroc": 0.3656978841145833
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 1588,
          "accuracy": 0.0075
        },
        "0.01": null
      },
      "auroc": 0.31443212890625
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 60,
          "fn": 3140,
          "accuracy": 0.01875
        },
        "0.01": null
      },
      "auroc": 0.3400650065104166
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 3090,
          "accuracy": 0.034375
        },
        "0.01": null
      },
      "auroc": 0.3792155924479168
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 3173,
          "accuracy": 0.0084375
        },
        "0.01": null
      },
      "auroc": 0.32067265624999997
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 6263,
          "accuracy": 0.02140625
        },
        "0.01": null
      },
      "auroc": 0.3499441243489584
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1018,
          "fn": 582,
          "accuracy": 0.63625
        },
        "0.01": null
      },
      "auroc": 0.8308890950520833
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 1360,
          "accuracy": 0.15
        },
        "0.01": null
      },
      "auroc": 0.5053953450520833
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1258,
          "fn": 1942,
          "accuracy": 0.393125
        },
        "0.01": null
      },
      "auroc": 0.6681422200520833
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 60,
          "fn": 1540,
          "accuracy": 0.0375
        },
        "0.01": null
      },
      "auroc": 0.36264995117187504
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 1551,
          "accuracy": 0.030625
        },
        "0.01": null
      },
      "auroc": 0.346257470703125
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 3091,
          "accuracy": 0.0340625
        },
        "0.01": null
      },
      "auroc": 0.35445371093750005
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1078,
          "fn": 2122,
          "accuracy": 0.336875
        },
        "0.01": null
      },
      "auroc": 0.5967695231119793
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 289,
          "fn": 2911,
          "accuracy": 0.0903125
        },
        "0.01": null
      },
      "auroc": 0.4258264078776042
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1367,
          "fn": 5033,
          "accuracy": 0.21359375
        },
        "0.01": null
      },
      "auroc": 0.5112979654947917
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 784,
          "fn": 816,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.7599538899739582
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 873,
          "fn": 727,
          "accuracy": 0.545625
        },
        "0.01": null
      },
      "auroc": 0.7413400716145833
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1657,
          "fn": 1543,
          "accuracy": 0.5178125
        },
        "0.01": null
      },
      "auroc": 0.7506469807942708
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 291,
          "fn": 1309,
          "accuracy": 0.181875
        },
        "0.01": null
      },
      "auroc": 0.4857547200520833
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 1265,
          "accuracy": 0.209375
        },
        "0.01": null
      },
      "auroc": 0.47810330403645834
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 626,
          "fn": 2574,
          "accuracy": 0.195625
        },
        "0.01": null
      },
      "auroc": 0.48192901204427085
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1075,
          "fn": 2125,
          "accuracy": 0.3359375
        },
        "0.01": null
      },
      "auroc": 0.6228543050130207
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1208,
          "fn": 1992,
          "accuracy": 0.3775
        },
        "0.01": null
      },
      "auroc": 0.6097216878255208
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2283,
          "fn": 4117,
          "accuracy": 0.35671875
        },
        "0.01": null
      },
      "auroc": 0.6162879964192708
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 1456,
          "accuracy": 0.09
        },
        "0.01": null
      },
      "auroc": 0.41813575846354156
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 30,
          "fn": 1570,
          "accuracy": 0.01875
        },
        "0.01": null
      },
      "auroc": 0.35185960286458323
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 3026,
          "accuracy": 0.054375
        },
        "0.01": null
      },
      "auroc": 0.3849976806640625
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 1541,
          "accuracy": 0.036875
        },
        "0.01": null
      },
      "auroc": 0.36789296875000005
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 1583,
          "accuracy": 0.010625
        },
        "0.01": null
      },
      "auroc": 0.3173545247395833
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 3124,
          "accuracy": 0.02375
        },
        "0.01": null
      },
      "auroc": 0.34262374674479174
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 203,
          "fn": 2997,
          "accuracy": 0.0634375
        },
        "0.01": null
      },
      "auroc": 0.3930143636067708
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 47,
          "fn": 3153,
          "accuracy": 0.0146875
        },
        "0.01": null
      },
      "auroc": 0.3346070638020833
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 6150,
          "accuracy": 0.0390625
        },
        "0.01": null
      },
      "auroc": 0.3638107137044271
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 1389,
          "accuracy": 0.131875
        },
        "0.01": null
      },
      "auroc": 0.4749138834635417
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 1389,
          "accuracy": 0.131875
        },
        "0.01": null
      },
      "auroc": 0.4749138834635417
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 1459,
          "accuracy": 0.088125
        },
        "0.01": null
      },
      "auroc": 0.43750265299479174
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 1459,
          "accuracy": 0.088125
        },
        "0.01": null
      },
      "auroc": 0.43750265299479174
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 2848,
          "accuracy": 0.11
        },
        "0.01": null
      },
      "auroc": 0.45620826822916666
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 2848,
          "accuracy": 0.11
        },
        "0.01": null
      },
      "auroc": 0.45620826822916666
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 1569,
          "accuracy": 0.019375
        },
        "0.01": null
      },
      "auroc": 0.34388164062500004
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 1569,
          "accuracy": 0.019375
        },
        "0.01": null
      },
      "auroc": 0.34388164062500004
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 1574,
          "accuracy": 0.01625
        },
        "0.01": null
      },
      "auroc": 0.32890522460937505
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 1574,
          "accuracy": 0.01625
        },
        "0.01": null
      },
      "auroc": 0.32890522460937505
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 3143,
          "accuracy": 0.0178125
        },
        "0.01": null
      },
      "auroc": 0.33639343261718746
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 3143,
          "accuracy": 0.0178125
        },
        "0.01": null
      },
      "auroc": 0.33639343261718746
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 1543,
          "accuracy": 0.035625
        },
        "0.01": null
      },
      "auroc": 0.3411075846354166
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 1543,
          "accuracy": 0.035625
        },
        "0.01": null
      },
      "auroc": 0.3411075846354166
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 1569,
          "accuracy": 0.019375
        },
        "0.01": null
      },
      "auroc": 0.31487739257812497
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 1569,
          "accuracy": 0.019375
        },
        "0.01": null
      },
      "auroc": 0.31487739257812497
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 3112,
          "accuracy": 0.0275
        },
        "0.01": null
      },
      "auroc": 0.32799248860677077
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 3112,
          "accuracy": 0.0275
        },
        "0.01": null
      },
      "auroc": 0.32799248860677077
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 1581,
          "accuracy": 0.011875
        },
        "0.01": null
      },
      "auroc": 0.3332293294270833
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 1581,
          "accuracy": 0.011875
        },
        "0.01": null
      },
      "auroc": 0.3332293294270833
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 1599,
          "accuracy": 0.000625
        },
        "0.01": null
      },
      "auroc": 0.26607096354166665
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 1599,
          "accuracy": 0.000625
        },
        "0.01": null
      },
      "auroc": 0.26607096354166665
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 20,
          "fn": 3180,
          "accuracy": 0.00625
        },
        "0.01": null
      },
      "auroc": 0.299650146484375
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 20,
          "fn": 3180,
          "accuracy": 0.00625
        },
        "0.01": null
      },
      "auroc": 0.299650146484375
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 1532,
          "accuracy": 0.0425
        },
        "0.01": null
      },
      "auroc": 0.38053831380208336
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 1532,
          "accuracy": 0.0425
        },
        "0.01": null
      },
      "auroc": 0.38053831380208336
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 1547,
          "accuracy": 0.033125
        },
        "0.01": null
      },
      "auroc": 0.35791590169270837
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 1547,
          "accuracy": 0.033125
        },
        "0.01": null
      },
      "auroc": 0.35791590169270837
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 3079,
          "accuracy": 0.0378125
        },
        "0.01": null
      },
      "auroc": 0.36922710774739587
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 3079,
          "accuracy": 0.0378125
        },
        "0.01": null
      },
      "auroc": 0.36922710774739587
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3102,
          "fn": 14498,
          "accuracy": 0.17625
        },
        "0.01": null
      },
      "auroc": 0.4843864953243372
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1334,
          "fn": 8266,
          "accuracy": 0.13895833333333332
        },
        "0.01": null
      },
      "auroc": 0.45806552734374995
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4436,
          "fn": 22764,
          "accuracy": 0.16308823529411764
        },
        "0.01": null
      },
      "auroc": 0.47509674191942397
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 885,
          "fn": 16715,
          "accuracy": 0.05028409090909091
        },
        "0.01": null
      },
      "auroc": 0.3695117439038826
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 636,
          "fn": 8964,
          "accuracy": 0.06625
        },
        "0.01": null
      },
      "auroc": 0.3781620334201389
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1521,
          "fn": 25679,
          "accuracy": 0.05591911764705882
        },
        "0.01": null
      },
      "auroc": 0.3725647872625613
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3987,
          "fn": 31213,
          "accuracy": 0.11326704545454545
        },
        "0.01": null
      },
      "auroc": 0.42694911961410986
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1970,
          "fn": 17230,
          "accuracy": 0.10260416666666666
        },
        "0.01": null
      },
      "auroc": 0.4181137803819444
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5957,
          "fn": 48443,
          "accuracy": 0.10950367647058823
        },
        "0.01": null
      },
      "auroc": 0.42383076459099267
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1544,
          "fn": 56,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9629774576822917
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1457,
          "fn": 143,
          "accuracy": 0.910625
        },
        "0.01": null
      },
      "auroc": 0.9498584472656251
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3001,
          "fn": 199,
          "accuracy": 0.9378125
        },
        "0.01": null
      },
      "auroc": 0.9564179524739583
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1514,
          "fn": 86,
          "accuracy": 0.94625
        },
        "0.01": null
      },
      "auroc": 0.9542705240885417
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1396,
          "fn": 204,
          "accuracy": 0.8725
        },
        "0.01": null
      },
      "auroc": 0.9336835937500001
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2910,
          "fn": 290,
          "accuracy": 0.909375
        },
        "0.01": null
      },
      "auroc": 0.9439770589192709
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3058,
          "fn": 142,
          "accuracy": 0.955625
        },
        "0.01": null
      },
      "auroc": 0.9586239908854167
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2853,
          "fn": 347,
          "accuracy": 0.8915625
        },
        "0.01": null
      },
      "auroc": 0.9417710205078125
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 5911,
          "fn": 489,
          "accuracy": 0.92359375
        },
        "0.01": null
      },
      "auroc": 0.9501975056966147
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1519,
          "fn": 81,
          "accuracy": 0.949375
        },
        "0.01": null
      },
      "auroc": 0.986244091796875
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1552,
          "fn": 48,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9912793131510417
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3071,
          "fn": 129,
          "accuracy": 0.9596875
        },
        "0.01": null
      },
      "auroc": 0.9887617024739582
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 297,
          "fn": 1303,
          "accuracy": 0.185625
        },
        "0.01": null
      },
      "auroc": 0.6784090332031251
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1547,
          "fn": 53,
          "accuracy": 0.966875
        },
        "0.01": null
      },
      "auroc": 0.9914383626302083
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1844,
          "fn": 1356,
          "accuracy": 0.57625
        },
        "0.01": null
      },
      "auroc": 0.8349236979166665
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1816,
          "fn": 1384,
          "accuracy": 0.5675
        },
        "0.01": null
      },
      "auroc": 0.8323265625000001
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3099,
          "fn": 101,
          "accuracy": 0.9684375
        },
        "0.01": null
      },
      "auroc": 0.9913588378906251
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 4915,
          "fn": 1485,
          "accuracy": 0.76796875
        },
        "0.01": null
      },
      "auroc": 0.9118427001953125
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1509,
          "fn": 91,
          "accuracy": 0.943125
        },
        "0.01": null
      },
      "auroc": 0.9469980305989583
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1424,
          "fn": 176,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.9680070800781251
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2933,
          "fn": 267,
          "accuracy": 0.9165625
        },
        "0.01": null
      },
      "auroc": 0.9575025553385418
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1465,
          "fn": 135,
          "accuracy": 0.915625
        },
        "0.01": null
      },
      "auroc": 0.9410276041666666
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1411,
          "fn": 189,
          "accuracy": 0.881875
        },
        "0.01": null
      },
      "auroc": 0.97253876953125
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2876,
          "fn": 324,
          "accuracy": 0.89875
        },
        "0.01": null
      },
      "auroc": 0.9567831868489584
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2974,
          "fn": 226,
          "accuracy": 0.929375
        },
        "0.01": null
      },
      "auroc": 0.9440128173828126
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2835,
          "fn": 365,
          "accuracy": 0.8859375
        },
        "0.01": null
      },
      "auroc": 0.9702729248046874
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 5809,
          "fn": 591,
          "accuracy": 0.90765625
        },
        "0.01": null
      },
      "auroc": 0.95714287109375
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9958540201822916
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1386,
          "fn": 214,
          "accuracy": 0.86625
        },
        "0.01": null
      },
      "auroc": 0.9603214518229167
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2981,
          "fn": 219,
          "accuracy": 0.9315625
        },
        "0.01": null
      },
      "auroc": 0.9780877360026041
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 1395,
          "accuracy": 0.128125
        },
        "0.01": null
      },
      "auroc": 0.6574857421875001
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 515,
          "fn": 1085,
          "accuracy": 0.321875
        },
        "0.01": null
      },
      "auroc": 0.8025465494791666
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 720,
          "fn": 2480,
          "accuracy": 0.225
        },
        "0.01": null
      },
      "auroc": 0.7300161458333333
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1800,
          "fn": 1400,
          "accuracy": 0.5625
        },
        "0.01": null
      },
      "auroc": 0.8266698811848958
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1901,
          "fn": 1299,
          "accuracy": 0.5940625
        },
        "0.01": null
      },
      "auroc": 0.8814340006510417
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3701,
          "fn": 2699,
          "accuracy": 0.57828125
        },
        "0.01": null
      },
      "auroc": 0.8540519409179685
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1549,
          "fn": 51,
          "accuracy": 0.968125
        },
        "0.01": null
      },
      "auroc": 0.9901690917968752
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1256,
          "fn": 344,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.9745996744791667
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2805,
          "fn": 395,
          "accuracy": 0.8765625
        },
        "0.01": null
      },
      "auroc": 0.9823843831380208
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 446,
          "fn": 1154,
          "accuracy": 0.27875
        },
        "0.01": null
      },
      "auroc": 0.7332670247395835
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1057,
          "fn": 543,
          "accuracy": 0.660625
        },
        "0.01": null
      },
      "auroc": 0.9141622721354167
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1503,
          "fn": 1697,
          "accuracy": 0.4696875
        },
        "0.01": null
      },
      "auroc": 0.8237146484374999
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1995,
          "fn": 1205,
          "accuracy": 0.6234375
        },
        "0.01": null
      },
      "auroc": 0.8617180582682292
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2313,
          "fn": 887,
          "accuracy": 0.7228125
        },
        "0.01": null
      },
      "auroc": 0.9443809733072916
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 4308,
          "fn": 2092,
          "accuracy": 0.673125
        },
        "0.01": null
      },
      "auroc": 0.9030495157877605
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1545,
          "fn": 55,
          "accuracy": 0.965625
        },
        "0.01": null
      },
      "auroc": 0.9556282063802084
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1479,
          "fn": 121,
          "accuracy": 0.924375
        },
        "0.01": null
      },
      "auroc": 0.9493339680989582
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3024,
          "fn": 176,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9524810872395832
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1436,
          "fn": 164,
          "accuracy": 0.8975
        },
        "0.01": null
      },
      "auroc": 0.9339742024739583
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1218,
          "fn": 382,
          "accuracy": 0.76125
        },
        "0.01": null
      },
      "auroc": 0.9299664713541668
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2654,
          "fn": 546,
          "accuracy": 0.829375
        },
        "0.01": null
      },
      "auroc": 0.9319703369140626
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2981,
          "fn": 219,
          "accuracy": 0.9315625
        },
        "0.01": null
      },
      "auroc": 0.9448012044270833
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2697,
          "fn": 503,
          "accuracy": 0.8428125
        },
        "0.01": null
      },
      "auroc": 0.9396502197265624
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 5678,
          "fn": 722,
          "accuracy": 0.8871875
        },
        "0.01": null
      },
      "auroc": 0.9422257120768229
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1181,
          "fn": 419,
          "accuracy": 0.738125
        },
        "0.01": null
      },
      "auroc": 0.8717380045572918
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1181,
          "fn": 419,
          "accuracy": 0.738125
        },
        "0.01": null
      },
      "auroc": 0.8717380045572918
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1030,
          "fn": 570,
          "accuracy": 0.64375
        },
        "0.01": null
      },
      "auroc": 0.8415040201822916
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1030,
          "fn": 570,
          "accuracy": 0.64375
        },
        "0.01": null
      },
      "auroc": 0.8415040201822916
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2211,
          "fn": 989,
          "accuracy": 0.6909375
        },
        "0.01": null
      },
      "auroc": 0.8566210123697917
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2211,
          "fn": 989,
          "accuracy": 0.6909375
        },
        "0.01": null
      },
      "auroc": 0.8566210123697917
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 586,
          "fn": 1014,
          "accuracy": 0.36625
        },
        "0.01": null
      },
      "auroc": 0.7402468912760417
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 586,
          "fn": 1014,
          "accuracy": 0.36625
        },
        "0.01": null
      },
      "auroc": 0.7402468912760417
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 464,
          "fn": 1136,
          "accuracy": 0.29
        },
        "0.01": null
      },
      "auroc": 0.6913923990885418
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 464,
          "fn": 1136,
          "accuracy": 0.29
        },
        "0.01": null
      },
      "auroc": 0.6913923990885418
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1050,
          "fn": 2150,
          "accuracy": 0.328125
        },
        "0.01": null
      },
      "auroc": 0.7158196451822918
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1050,
          "fn": 2150,
          "accuracy": 0.328125
        },
        "0.01": null
      },
      "auroc": 0.7158196451822918
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1537,
          "fn": 63,
          "accuracy": 0.960625
        },
        "0.01": null
      },
      "auroc": 0.9409932942708332
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1537,
          "fn": 63,
          "accuracy": 0.960625
        },
        "0.01": null
      },
      "auroc": 0.9409932942708332
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1471,
          "fn": 129,
          "accuracy": 0.919375
        },
        "0.01": null
      },
      "auroc": 0.9277317057291666
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1471,
          "fn": 129,
          "accuracy": 0.919375
        },
        "0.01": null
      },
      "auroc": 0.9277317057291666
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3008,
          "fn": 192,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9343625
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3008,
          "fn": 192,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9343625
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1472,
          "fn": 128,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9437063802083333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1472,
          "fn": 128,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9437063802083333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1139,
          "fn": 461,
          "accuracy": 0.711875
        },
        "0.01": null
      },
      "auroc": 0.8497347493489584
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1139,
          "fn": 461,
          "accuracy": 0.711875
        },
        "0.01": null
      },
      "auroc": 0.8497347493489584
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2611,
          "fn": 589,
          "accuracy": 0.8159375
        },
        "0.01": null
      },
      "auroc": 0.8967205647786458
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2611,
          "fn": 589,
          "accuracy": 0.8159375
        },
        "0.01": null
      },
      "auroc": 0.8967205647786458
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1137,
          "fn": 463,
          "accuracy": 0.710625
        },
        "0.01": null
      },
      "auroc": 0.8823378255208333
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1137,
          "fn": 463,
          "accuracy": 0.710625
        },
        "0.01": null
      },
      "auroc": 0.8823378255208333
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 974,
          "fn": 626,
          "accuracy": 0.60875
        },
        "0.01": null
      },
      "auroc": 0.8366607584635416
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 974,
          "fn": 626,
          "accuracy": 0.60875
        },
        "0.01": null
      },
      "auroc": 0.8366607584635416
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2111,
          "fn": 1089,
          "accuracy": 0.6596875
        },
        "0.01": null
      },
      "auroc": 0.8594992919921874
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2111,
          "fn": 1089,
          "accuracy": 0.6596875
        },
        "0.01": null
      },
      "auroc": 0.8594992919921874
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 15174,
          "fn": 2426,
          "accuracy": 0.8621590909090909
        },
        "0.01": null
      },
      "auroc": 0.9288084812973485
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 8554,
          "fn": 1046,
          "accuracy": 0.8910416666666666
        },
        "0.01": null
      },
      "auroc": 0.9655666558159722
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 23728,
          "fn": 3472,
          "accuracy": 0.8723529411764706
        },
        "0.01": null
      },
      "auroc": 0.9417819546568629
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 10441,
          "fn": 7159,
          "accuracy": 0.5932386363636364
        },
        "0.01": null
      },
      "auroc": 0.8223143421519885
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 7144,
          "fn": 2456,
          "accuracy": 0.7441666666666666
        },
        "0.01": null
      },
      "auroc": 0.9240560031467013
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 17585,
          "fn": 9615,
          "accuracy": 0.6465073529411764
        },
        "0.01": null
      },
      "auroc": 0.8582231636795341
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 25615,
          "fn": 9585,
          "accuracy": 0.7276988636363636
        },
        "0.01": null
      },
      "auroc": 0.8755614117246687
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 15698,
          "fn": 3502,
          "accuracy": 0.8176041666666667
        },
        "0.01": null
      },
      "auroc": 0.9448113294813367
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 41313,
          "fn": 13087,
          "accuracy": 0.7594301470588235
        },
        "0.01": null
      },
      "auroc": 0.9000025591681986
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1552,
          "fn": 48,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9825631022135418
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1476,
          "fn": 124,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.9680328776041667
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3028,
          "fn": 172,
          "accuracy": 0.94625
        },
        "0.01": null
      },
      "auroc": 0.9752979899088542
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1529,
          "fn": 71,
          "accuracy": 0.955625
        },
        "0.01": null
      },
      "auroc": 0.9772486165364583
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1432,
          "fn": 168,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.9496388509114583
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2961,
          "fn": 239,
          "accuracy": 0.9253125
        },
        "0.01": null
      },
      "auroc": 0.9634437337239583
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3081,
          "fn": 119,
          "accuracy": 0.9628125
        },
        "0.01": null
      },
      "auroc": 0.9799058593750001
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2908,
          "fn": 292,
          "accuracy": 0.90875
        },
        "0.01": null
      },
      "auroc": 0.9588358642578125
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 5989,
          "fn": 411,
          "accuracy": 0.93578125
        },
        "0.01": null
      },
      "auroc": 0.9693708618164063
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1574,
          "fn": 26,
          "accuracy": 0.98375
        },
        "0.01": null
      },
      "auroc": 0.9936195638020834
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1550,
          "fn": 50,
          "accuracy": 0.96875
        },
        "0.01": null
      },
      "auroc": 0.9906187174479166
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3124,
          "fn": 76,
          "accuracy": 0.97625
        },
        "0.01": null
      },
      "auroc": 0.9921191406250001
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 1235,
          "accuracy": 0.228125
        },
        "0.01": null
      },
      "auroc": 0.7094449544270833
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1548,
          "fn": 52,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9912088704427084
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1913,
          "fn": 1287,
          "accuracy": 0.5978125
        },
        "0.01": null
      },
      "auroc": 0.8503269124348959
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1939,
          "fn": 1261,
          "accuracy": 0.6059375
        },
        "0.01": null
      },
      "auroc": 0.8515322591145833
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3098,
          "fn": 102,
          "accuracy": 0.968125
        },
        "0.01": null
      },
      "auroc": 0.9909137939453125
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 5037,
          "fn": 1363,
          "accuracy": 0.78703125
        },
        "0.01": null
      },
      "auroc": 0.9212230265299479
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1545,
          "fn": 55,
          "accuracy": 0.965625
        },
        "0.01": null
      },
      "auroc": 0.9710363444010417
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1413,
          "fn": 187,
          "accuracy": 0.883125
        },
        "0.01": null
      },
      "auroc": 0.9682191406249999
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2958,
          "fn": 242,
          "accuracy": 0.924375
        },
        "0.01": null
      },
      "auroc": 0.9696277425130208
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1506,
          "fn": 94,
          "accuracy": 0.94125
        },
        "0.01": null
      },
      "auroc": 0.965259130859375
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1397,
          "fn": 203,
          "accuracy": 0.873125
        },
        "0.01": null
      },
      "auroc": 0.9711025065104167
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2903,
          "fn": 297,
          "accuracy": 0.9071875
        },
        "0.01": null
      },
      "auroc": 0.9681808186848959
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3051,
          "fn": 149,
          "accuracy": 0.9534375
        },
        "0.01": null
      },
      "auroc": 0.9681477376302082
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2810,
          "fn": 390,
          "accuracy": 0.878125
        },
        "0.01": null
      },
      "auroc": 0.9696608235677084
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 5861,
          "fn": 539,
          "accuracy": 0.91578125
        },
        "0.01": null
      },
      "auroc": 0.9689042805989584
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1599,
          "fn": 1,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9959799967447915
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1500,
          "fn": 100,
          "accuracy": 0.9375
        },
        "0.01": null
      },
      "auroc": 0.9793058919270834
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3099,
          "fn": 101,
          "accuracy": 0.9684375
        },
        "0.01": null
      },
      "auroc": 0.9876429443359375
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 263,
          "fn": 1337,
          "accuracy": 0.164375
        },
        "0.01": null
      },
      "auroc": 0.685273193359375
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 408,
          "fn": 1192,
          "accuracy": 0.255
        },
        "0.01": null
      },
      "auroc": 0.7447440917968751
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 671,
          "fn": 2529,
          "accuracy": 0.2096875
        },
        "0.01": null
      },
      "auroc": 0.7150086425781249
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1862,
          "fn": 1338,
          "accuracy": 0.581875
        },
        "0.01": null
      },
      "auroc": 0.8406265950520834
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1908,
          "fn": 1292,
          "accuracy": 0.59625
        },
        "0.01": null
      },
      "auroc": 0.8620249918619791
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3770,
          "fn": 2630,
          "accuracy": 0.5890625
        },
        "0.01": null
      },
      "auroc": 0.8513257934570313
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1574,
          "fn": 26,
          "accuracy": 0.98375
        },
        "0.01": null
      },
      "auroc": 0.9930369140624999
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1294,
          "fn": 306,
          "accuracy": 0.80875
        },
        "0.01": null
      },
      "auroc": 0.9771807779947916
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2868,
          "fn": 332,
          "accuracy": 0.89625
        },
        "0.01": null
      },
      "auroc": 0.9851088460286459
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 503,
          "fn": 1097,
          "accuracy": 0.314375
        },
        "0.01": null
      },
      "auroc": 0.7585749511718749
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 965,
          "fn": 635,
          "accuracy": 0.603125
        },
        "0.01": null
      },
      "auroc": 0.8873993489583334
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1468,
          "fn": 1732,
          "accuracy": 0.45875
        },
        "0.01": null
      },
      "auroc": 0.822987150065104
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2077,
          "fn": 1123,
          "accuracy": 0.6490625
        },
        "0.01": null
      },
      "auroc": 0.8758059326171874
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2259,
          "fn": 941,
          "accuracy": 0.7059375
        },
        "0.01": null
      },
      "auroc": 0.9322900634765626
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 4336,
          "fn": 2064,
          "accuracy": 0.6775
        },
        "0.01": null
      },
      "auroc": 0.9040479980468751
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1581,
          "fn": 19,
          "accuracy": 0.988125
        },
        "0.01": null
      },
      "auroc": 0.9799111490885418
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1520,
          "fn": 80,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9651012532552083
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3101,
          "fn": 99,
          "accuracy": 0.9690625
        },
        "0.01": null
      },
      "auroc": 0.972506201171875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1506,
          "fn": 94,
          "accuracy": 0.94125
        },
        "0.01": null
      },
      "auroc": 0.9574501790364583
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1250,
          "fn": 350,
          "accuracy": 0.78125
        },
        "0.01": null
      },
      "auroc": 0.9327918782552085
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2756,
          "fn": 444,
          "accuracy": 0.86125
        },
        "0.01": null
      },
      "auroc": 0.9451210286458334
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3087,
          "fn": 113,
          "accuracy": 0.9646875
        },
        "0.01": null
      },
      "auroc": 0.9686806640625001
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2770,
          "fn": 430,
          "accuracy": 0.865625
        },
        "0.01": null
      },
      "auroc": 0.9489465657552083
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 5857,
          "fn": 543,
          "accuracy": 0.91515625
        },
        "0.01": null
      },
      "auroc": 0.9588136149088542
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1409,
          "fn": 191,
          "accuracy": 0.880625
        },
        "0.01": null
      },
      "auroc": 0.91800771484375
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1409,
          "fn": 191,
          "accuracy": 0.880625
        },
        "0.01": null
      },
      "auroc": 0.91800771484375
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1266,
          "fn": 334,
          "accuracy": 0.79125
        },
        "0.01": null
      },
      "auroc": 0.8909932454427083
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1266,
          "fn": 334,
          "accuracy": 0.79125
        },
        "0.01": null
      },
      "auroc": 0.8909932454427083
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2675,
          "fn": 525,
          "accuracy": 0.8359375
        },
        "0.01": null
      },
      "auroc": 0.904500480143229
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2675,
          "fn": 525,
          "accuracy": 0.8359375
        },
        "0.01": null
      },
      "auroc": 0.904500480143229
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 814,
          "fn": 786,
          "accuracy": 0.50875
        },
        "0.01": null
      },
      "auroc": 0.7841508951822916
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 814,
          "fn": 786,
          "accuracy": 0.50875
        },
        "0.01": null
      },
      "auroc": 0.7841508951822916
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 641,
          "fn": 959,
          "accuracy": 0.400625
        },
        "0.01": null
      },
      "auroc": 0.7295224609375
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 641,
          "fn": 959,
          "accuracy": 0.400625
        },
        "0.01": null
      },
      "auroc": 0.7295224609375
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1455,
          "fn": 1745,
          "accuracy": 0.4546875
        },
        "0.01": null
      },
      "auroc": 0.7568366780598959
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1455,
          "fn": 1745,
          "accuracy": 0.4546875
        },
        "0.01": null
      },
      "auroc": 0.7568366780598959
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1573,
          "fn": 27,
          "accuracy": 0.983125
        },
        "0.01": null
      },
      "auroc": 0.9704869140625001
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1573,
          "fn": 27,
          "accuracy": 0.983125
        },
        "0.01": null
      },
      "auroc": 0.9704869140625001
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1505,
          "fn": 95,
          "accuracy": 0.940625
        },
        "0.01": null
      },
      "auroc": 0.9527910319010416
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1505,
          "fn": 95,
          "accuracy": 0.940625
        },
        "0.01": null
      },
      "auroc": 0.9527910319010416
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3078,
          "fn": 122,
          "accuracy": 0.961875
        },
        "0.01": null
      },
      "auroc": 0.9616389729817708
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3078,
          "fn": 122,
          "accuracy": 0.961875
        },
        "0.01": null
      },
      "auroc": 0.9616389729817708
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1527,
          "fn": 73,
          "accuracy": 0.954375
        },
        "0.01": null
      },
      "auroc": 0.9721221028645832
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1527,
          "fn": 73,
          "accuracy": 0.954375
        },
        "0.01": null
      },
      "auroc": 0.9721221028645832
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1216,
          "fn": 384,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.8643766438802084
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1216,
          "fn": 384,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.8643766438802084
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2743,
          "fn": 457,
          "accuracy": 0.8571875
        },
        "0.01": null
      },
      "auroc": 0.9182493733723959
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2743,
          "fn": 457,
          "accuracy": 0.8571875
        },
        "0.01": null
      },
      "auroc": 0.9182493733723959
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1248,
          "fn": 352,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9127160156249998
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1248,
          "fn": 352,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.9127160156249998
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1120,
          "fn": 480,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.8703707845052083
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1120,
          "fn": 480,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.8703707845052083
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2368,
          "fn": 832,
          "accuracy": 0.74
        },
        "0.01": null
      },
      "auroc": 0.8915434000651042
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2368,
          "fn": 832,
          "accuracy": 0.74
        },
        "0.01": null
      },
      "auroc": 0.8915434000651042
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 15996,
          "fn": 1604,
          "accuracy": 0.9088636363636363
        },
        "0.01": null
      },
      "auroc": 0.9521482466264203
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 8753,
          "fn": 847,
          "accuracy": 0.9117708333333333
        },
        "0.01": null
      },
      "auroc": 0.9747431098090278
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 24749,
          "fn": 2451,
          "accuracy": 0.909889705882353
        },
        "0.01": null
      },
      "auroc": 0.9601229042202818
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 11420,
          "fn": 6180,
          "accuracy": 0.6488636363636363
        },
        "0.01": null
      },
      "auroc": 0.8510277447324811
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 7000,
          "fn": 2600,
          "accuracy": 0.7291666666666666
        },
        "0.01": null
      },
      "auroc": 0.9128142578125
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 18420,
          "fn": 8780,
          "accuracy": 0.6772058823529412
        },
        "0.01": null
      },
      "auroc": 0.8728347493489585
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 27416,
          "fn": 7784,
          "accuracy": 0.7788636363636363
        },
        "0.01": null
      },
      "auroc": 0.9015879956794507
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 15753,
          "fn": 3447,
          "accuracy": 0.82046875
        },
        "0.01": null
      },
      "auroc": 0.9437786838107638
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 43169,
          "fn": 11231,
          "accuracy": 0.7935477941176471
        },
        "0.01": null
      },
      "auroc": 0.9164788267846201
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 1483,
          "accuracy": 0.073125
        },
        "0.01": null
      },
      "auroc": 0.3583211263020833
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 1491,
          "accuracy": 0.068125
        },
        "0.01": null
      },
      "auroc": 0.35256591796875003
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 2974,
          "accuracy": 0.070625
        },
        "0.01": null
      },
      "auroc": 0.3554435221354166
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 1473,
          "accuracy": 0.079375
        },
        "0.01": null
      },
      "auroc": 0.360299658203125
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 1495,
          "accuracy": 0.065625
        },
        "0.01": null
      },
      "auroc": 0.34937565104166673
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 232,
          "fn": 2968,
          "accuracy": 0.0725
        },
        "0.01": null
      },
      "auroc": 0.3548376546223958
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 2956,
          "accuracy": 0.07625
        },
        "0.01": null
      },
      "auroc": 0.3593103922526042
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 214,
          "fn": 2986,
          "accuracy": 0.066875
        },
        "0.01": null
      },
      "auroc": 0.35097078450520836
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 458,
          "fn": 5942,
          "accuracy": 0.0715625
        },
        "0.01": null
      },
      "auroc": 0.3551405883789062
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 538,
          "fn": 1062,
          "accuracy": 0.33625
        },
        "0.01": null
      },
      "auroc": 0.5900391601562499
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 1395,
          "accuracy": 0.128125
        },
        "0.01": null
      },
      "auroc": 0.4600598795572917
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 743,
          "fn": 2457,
          "accuracy": 0.2321875
        },
        "0.01": null
      },
      "auroc": 0.5250495198567707
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 1447,
          "accuracy": 0.095625
        },
        "0.01": null
      },
      "auroc": 0.4070698079427083
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 242,
          "fn": 1358,
          "accuracy": 0.15125
        },
        "0.01": null
      },
      "auroc": 0.48192109375000003
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 2805,
          "accuracy": 0.1234375
        },
        "0.01": null
      },
      "auroc": 0.44449545084635417
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 691,
          "fn": 2509,
          "accuracy": 0.2159375
        },
        "0.01": null
      },
      "auroc": 0.49855448404947916
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 447,
          "fn": 2753,
          "accuracy": 0.1396875
        },
        "0.01": null
      },
      "auroc": 0.47099048665364585
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1138,
          "fn": 5262,
          "accuracy": 0.1778125
        },
        "0.01": null
      },
      "auroc": 0.48477248535156253
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 1508,
          "accuracy": 0.0575
        },
        "0.01": null
      },
      "auroc": 0.3578964029947917
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 60,
          "fn": 1540,
          "accuracy": 0.0375
        },
        "0.01": null
      },
      "auroc": 0.33965698242187503
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 3048,
          "accuracy": 0.0475
        },
        "0.01": null
      },
      "auroc": 0.3487766927083334
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 1507,
          "accuracy": 0.058125
        },
        "0.01": null
      },
      "auroc": 0.3539053059895833
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 70,
          "fn": 1530,
          "accuracy": 0.04375
        },
        "0.01": null
      },
      "auroc": 0.348662060546875
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 3037,
          "accuracy": 0.0509375
        },
        "0.01": null
      },
      "auroc": 0.3512836832682291
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 3015,
          "accuracy": 0.0578125
        },
        "0.01": null
      },
      "auroc": 0.35590085449218745
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 3070,
          "accuracy": 0.040625
        },
        "0.01": null
      },
      "auroc": 0.34415952148437495
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 315,
          "fn": 6085,
          "accuracy": 0.04921875
        },
        "0.01": null
      },
      "auroc": 0.3500301879882813
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 874,
          "fn": 726,
          "accuracy": 0.54625
        },
        "0.01": null
      },
      "auroc": 0.7396616536458334
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 1472,
          "accuracy": 0.08
        },
        "0.01": null
      },
      "auroc": 0.4196647135416666
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1002,
          "fn": 2198,
          "accuracy": 0.313125
        },
        "0.01": null
      },
      "auroc": 0.57966318359375
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 1461,
          "accuracy": 0.086875
        },
        "0.01": null
      },
      "auroc": 0.3894744954427083
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 1455,
          "accuracy": 0.090625
        },
        "0.01": null
      },
      "auroc": 0.3864925130208334
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 2916,
          "accuracy": 0.08875
        },
        "0.01": null
      },
      "auroc": 0.38798350423177086
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1013,
          "fn": 2187,
          "accuracy": 0.3165625
        },
        "0.01": null
      },
      "auroc": 0.5645680745442708
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 273,
          "fn": 2927,
          "accuracy": 0.0853125
        },
        "0.01": null
      },
      "auroc": 0.40307861328125
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1286,
          "fn": 5114,
          "accuracy": 0.2009375
        },
        "0.01": null
      },
      "auroc": 0.4838233439127604
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 744,
          "fn": 856,
          "accuracy": 0.465
        },
        "0.01": null
      },
      "auroc": 0.710903271484375
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 869,
          "fn": 731,
          "accuracy": 0.543125
        },
        "0.01": null
      },
      "auroc": 0.7234473958333333
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1613,
          "fn": 1587,
          "accuracy": 0.5040625
        },
        "0.01": null
      },
      "auroc": 0.7171753336588541
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 1273,
          "accuracy": 0.204375
        },
        "0.01": null
      },
      "auroc": 0.5049325358072916
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 1225,
          "accuracy": 0.234375
        },
        "0.01": null
      },
      "auroc": 0.514707568359375
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 702,
          "fn": 2498,
          "accuracy": 0.219375
        },
        "0.01": null
      },
      "auroc": 0.5098200520833333
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1071,
          "fn": 2129,
          "accuracy": 0.3346875
        },
        "0.01": null
      },
      "auroc": 0.6079179036458333
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1244,
          "fn": 1956,
          "accuracy": 0.38875
        },
        "0.01": null
      },
      "auroc": 0.6190774820963543
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2315,
          "fn": 4085,
          "accuracy": 0.36171875
        },
        "0.01": null
      },
      "auroc": 0.6134976928710938
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 1467,
          "accuracy": 0.083125
        },
        "0.01": null
      },
      "auroc": 0.38067819010416665
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 66,
          "fn": 1534,
          "accuracy": 0.04125
        },
        "0.01": null
      },
      "auroc": 0.3390871907552083
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 3001,
          "accuracy": 0.0621875
        },
        "0.01": null
      },
      "auroc": 0.3598826904296875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 1494,
          "accuracy": 0.06625
        },
        "0.01": null
      },
      "auroc": 0.3591730794270833
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 1520,
          "accuracy": 0.05
        },
        "0.01": null
      },
      "auroc": 0.34299925130208336
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 3014,
          "accuracy": 0.058125
        },
        "0.01": null
      },
      "auroc": 0.3510861653645833
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 2961,
          "accuracy": 0.0746875
        },
        "0.01": null
      },
      "auroc": 0.36992563476562496
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 3054,
          "accuracy": 0.045625
        },
        "0.01": null
      },
      "auroc": 0.3410432210286458
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 6015,
          "accuracy": 0.06015625
        },
        "0.01": null
      },
      "auroc": 0.3554844278971354
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 1414,
          "accuracy": 0.11625
        },
        "0.01": null
      },
      "auroc": 0.4324506022135417
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 1414,
          "accuracy": 0.11625
        },
        "0.01": null
      },
      "auroc": 0.4324506022135417
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 1456,
          "accuracy": 0.09
        },
        "0.01": null
      },
      "auroc": 0.409389501953125
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 1456,
          "accuracy": 0.09
        },
        "0.01": null
      },
      "auroc": 0.409389501953125
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 330,
          "fn": 2870,
          "accuracy": 0.103125
        },
        "0.01": null
      },
      "auroc": 0.4209200520833333
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 330,
          "fn": 2870,
          "accuracy": 0.103125
        },
        "0.01": null
      },
      "auroc": 0.4209200520833333
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 1494,
          "accuracy": 0.06625
        },
        "0.01": null
      },
      "auroc": 0.37526171875000003
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 1494,
          "accuracy": 0.06625
        },
        "0.01": null
      },
      "auroc": 0.37526171875000003
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 1496,
          "accuracy": 0.065
        },
        "0.01": null
      },
      "auroc": 0.37197062174479173
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 1496,
          "accuracy": 0.065
        },
        "0.01": null
      },
      "auroc": 0.37197062174479173
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 210,
          "fn": 2990,
          "accuracy": 0.065625
        },
        "0.01": null
      },
      "auroc": 0.3736161702473959
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 210,
          "fn": 2990,
          "accuracy": 0.065625
        },
        "0.01": null
      },
      "auroc": 0.3736161702473959
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 1492,
          "accuracy": 0.0675
        },
        "0.01": null
      },
      "auroc": 0.3476840657552084
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 1492,
          "accuracy": 0.0675
        },
        "0.01": null
      },
      "auroc": 0.3476840657552084
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 1503,
          "accuracy": 0.060625
        },
        "0.01": null
      },
      "auroc": 0.3402412434895833
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 1503,
          "accuracy": 0.060625
        },
        "0.01": null
      },
      "auroc": 0.3402412434895833
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 2995,
          "accuracy": 0.0640625
        },
        "0.01": null
      },
      "auroc": 0.34396265462239584
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 2995,
          "accuracy": 0.0640625
        },
        "0.01": null
      },
      "auroc": 0.34396265462239584
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 1568,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.3355291992187499
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 1568,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.3355291992187499
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 1552,
          "accuracy": 0.03
        },
        "0.01": null
      },
      "auroc": 0.32648450520833333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 1552,
          "accuracy": 0.03
        },
        "0.01": null
      },
      "auroc": 0.32648450520833333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 3120,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.33100685221354165
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 3120,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.33100685221354165
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 1474,
          "accuracy": 0.07875
        },
        "0.01": null
      },
      "auroc": 0.37687021484374994
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 1474,
          "accuracy": 0.07875
        },
        "0.01": null
      },
      "auroc": 0.37687021484374994
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 1476,
          "accuracy": 0.0775
        },
        "0.01": null
      },
      "auroc": 0.37308487955729164
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 1476,
          "accuracy": 0.0775
        },
        "0.01": null
      },
      "auroc": 0.37308487955729164
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 2950,
          "accuracy": 0.078125
        },
        "0.01": null
      },
      "auroc": 0.37497754720052084
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 2950,
          "accuracy": 0.078125
        },
        "0.01": null
      },
      "auroc": 0.37497754720052084
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3056,
          "fn": 14544,
          "accuracy": 0.17363636363636364
        },
        "0.01": null
      },
      "auroc": 0.45502687322443186
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1437,
          "fn": 8163,
          "accuracy": 0.1496875
        },
        "0.01": null
      },
      "auroc": 0.4390803466796874
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4493,
          "fn": 22707,
          "accuracy": 0.16518382352941177
        },
        "0.01": null
      },
      "auroc": 0.44939868738511035
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1462,
          "fn": 16138,
          "accuracy": 0.08306818181818182
        },
        "0.01": null
      },
      "auroc": 0.38145687588778415
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1017,
          "fn": 8583,
          "accuracy": 0.1059375
        },
        "0.01": null
      },
      "auroc": 0.4040263563368055
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2479,
          "fn": 24721,
          "accuracy": 0.09113970588235294
        },
        "0.01": null
      },
      "auroc": 0.38942257486979165
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4518,
          "fn": 30682,
          "accuracy": 0.12835227272727273
        },
        "0.01": null
      },
      "auroc": 0.41824187455610795
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2454,
          "fn": 16746,
          "accuracy": 0.1278125
        },
        "0.01": null
      },
      "auroc": 0.4215533515082464
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 6972,
          "fn": 47428,
          "accuracy": 0.12816176470588236
        },
        "0.01": null
      },
      "auroc": 0.419410631127451
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 15568,
          "fn": 3632,
          "accuracy": 0.8108333333333333
        },
        "0.01": null
      },
      "auroc": 0.8745877888997395
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 14718,
          "fn": 4482,
          "accuracy": 0.7665625
        },
        "0.01": null
      },
      "auroc": 0.8591049587673611
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 30286,
          "fn": 8114,
          "accuracy": 0.7886979166666667
        },
        "0.01": null
      },
      "auroc": 0.8668463738335503
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 15323,
          "fn": 3877,
          "accuracy": 0.7980729166666667
        },
        "0.01": null
      },
      "auroc": 0.8692059638129341
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 13973,
          "fn": 5227,
          "accuracy": 0.7277604166666667
        },
        "0.01": null
      },
      "auroc": 0.8409887356228299
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 29296,
          "fn": 9104,
          "accuracy": 0.7629166666666667
        },
        "0.01": null
      },
      "auroc": 0.8550973497178819
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 30891,
          "fn": 7509,
          "accuracy": 0.804453125
        },
        "0.01": null
      },
      "auroc": 0.8718968763563368
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 28691,
          "fn": 9709,
          "accuracy": 0.7471614583333334
        },
        "0.01": null
      },
      "auroc": 0.8500468471950955
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 59582,
          "fn": 17218,
          "accuracy": 0.7758072916666666
        },
        "0.01": null
      },
      "auroc": 0.860971861775716
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 16548,
          "fn": 2652,
          "accuracy": 0.861875
        },
        "0.01": null
      },
      "auroc": 0.9282237942165797
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 14813,
          "fn": 4387,
          "accuracy": 0.7715104166666666
        },
        "0.01": null
      },
      "auroc": 0.8841641615125868
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 31361,
          "fn": 7039,
          "accuracy": 0.8166927083333333
        },
        "0.01": null
      },
      "auroc": 0.9061939778645833
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3812,
          "fn": 15388,
          "accuracy": 0.19854166666666667
        },
        "0.01": null
      },
      "auroc": 0.6505561672634549
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 14845,
          "fn": 4355,
          "accuracy": 0.7731770833333333
        },
        "0.01": null
      },
      "auroc": 0.8850879123263888
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18657,
          "fn": 19743,
          "accuracy": 0.485859375
        },
        "0.01": null
      },
      "auroc": 0.7678220397949217
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 20360,
          "fn": 18040,
          "accuracy": 0.5302083333333333
        },
        "0.01": null
      },
      "auroc": 0.7893899807400172
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 29658,
          "fn": 8742,
          "accuracy": 0.77234375
        },
        "0.01": null
      },
      "auroc": 0.8846260369194878
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 50018,
          "fn": 26782,
          "accuracy": 0.6512760416666666
        },
        "0.01": null
      },
      "auroc": 0.8370080088297527
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 15294,
          "fn": 3906,
          "accuracy": 0.7965625
        },
        "0.01": null
      },
      "auroc": 0.8612338351779514
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 13624,
          "fn": 5576,
          "accuracy": 0.7095833333333333
        },
        "0.01": null
      },
      "auroc": 0.8509531399197049
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 28918,
          "fn": 9482,
          "accuracy": 0.7530729166666666
        },
        "0.01": null
      },
      "auroc": 0.856093487548828
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 14825,
          "fn": 4375,
          "accuracy": 0.7721354166666666
        },
        "0.01": null
      },
      "auroc": 0.8533834350585937
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 13289,
          "fn": 5911,
          "accuracy": 0.6921354166666667
        },
        "0.01": null
      },
      "auroc": 0.8491035658094618
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 28114,
          "fn": 10286,
          "accuracy": 0.7321354166666667
        },
        "0.01": null
      },
      "auroc": 0.8512435004340277
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 30119,
          "fn": 8281,
          "accuracy": 0.7843489583333333
        },
        "0.01": null
      },
      "auroc": 0.8573086351182726
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 26913,
          "fn": 11487,
          "accuracy": 0.700859375
        },
        "0.01": null
      },
      "auroc": 0.8500283528645833
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 57032,
          "fn": 19768,
          "accuracy": 0.7426041666666666
        },
        "0.01": null
      },
      "auroc": 0.8536684939914279
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 17600,
          "fn": 1600,
          "accuracy": 0.9166666666666666
        },
        "0.01": null
      },
      "auroc": 0.958756765407986
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 14019,
          "fn": 5181,
          "accuracy": 0.73015625
        },
        "0.01": null
      },
      "auroc": 0.8774963799370659
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 31619,
          "fn": 6781,
          "accuracy": 0.8234114583333333
        },
        "0.01": null
      },
      "auroc": 0.918126572672526
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2774,
          "fn": 16426,
          "accuracy": 0.14447916666666666
        },
        "0.01": null
      },
      "auroc": 0.6293672607421874
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4407,
          "fn": 14793,
          "accuracy": 0.22953125
        },
        "0.01": null
      },
      "auroc": 0.6899421766493055
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7181,
          "fn": 31219,
          "accuracy": 0.18700520833333334
        },
        "0.01": null
      },
      "auroc": 0.6596547186957464
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 20374,
          "fn": 18026,
          "accuracy": 0.5305729166666666
        },
        "0.01": null
      },
      "auroc": 0.7940620130750868
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18426,
          "fn": 19974,
          "accuracy": 0.47984375
        },
        "0.01": null
      },
      "auroc": 0.7837192782931859
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 38800,
          "fn": 38000,
          "accuracy": 0.5052083333333334
        },
        "0.01": null
      },
      "auroc": 0.7888906456841361
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 16923,
          "fn": 2277,
          "accuracy": 0.88140625
        },
        "0.01": null
      },
      "auroc": 0.9470992879231772
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 13713,
          "fn": 5487,
          "accuracy": 0.71421875
        },
        "0.01": null
      },
      "auroc": 0.9176749565972222
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 30636,
          "fn": 7764,
          "accuracy": 0.7978125
        },
        "0.01": null
      },
      "auroc": 0.9323871222601996
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5322,
          "fn": 13878,
          "accuracy": 0.2771875
        },
        "0.01": null
      },
      "auroc": 0.7031014472113716
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9893,
          "fn": 9307,
          "accuracy": 0.5152604166666667
        },
        "0.01": null
      },
      "auroc": 0.8088770982530382
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 15215,
          "fn": 23185,
          "accuracy": 0.39622395833333335
        },
        "0.01": null
      },
      "auroc": 0.7559892727322048
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 22245,
          "fn": 16155,
          "accuracy": 0.579296875
        },
        "0.01": null
      },
      "auroc": 0.8251003675672742
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 23606,
          "fn": 14794,
          "accuracy": 0.6147395833333333
        },
        "0.01": null
      },
      "auroc": 0.8632760274251302
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 45851,
          "fn": 30949,
          "accuracy": 0.5970182291666667
        },
        "0.01": null
      },
      "auroc": 0.8441881974962024
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 15795,
          "fn": 3405,
          "accuracy": 0.82265625
        },
        "0.01": null
      },
      "auroc": 0.8744232598198783
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 14824,
          "fn": 4376,
          "accuracy": 0.7720833333333333
        },
        "0.01": null
      },
      "auroc": 0.8530750854492186
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 30619,
          "fn": 7781,
          "accuracy": 0.7973697916666667
        },
        "0.01": null
      },
      "auroc": 0.8637491726345486
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 14775,
          "fn": 4425,
          "accuracy": 0.76953125
        },
        "0.01": null
      },
      "auroc": 0.8495351793077257
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 11989,
          "fn": 7211,
          "accuracy": 0.6244270833333333
        },
        "0.01": null
      },
      "auroc": 0.8224192776150173
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 26764,
          "fn": 11636,
          "accuracy": 0.6969791666666667
        },
        "0.01": null
      },
      "auroc": 0.8359772284613715
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 30570,
          "fn": 7830,
          "accuracy": 0.79609375
        },
        "0.01": null
      },
      "auroc": 0.8619792195638021
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 26813,
          "fn": 11587,
          "accuracy": 0.6982552083333333
        },
        "0.01": null
      },
      "auroc": 0.837747181532118
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 57383,
          "fn": 19417,
          "accuracy": 0.7471744791666667
        },
        "0.01": null
      },
      "auroc": 0.84986320054796
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 13144,
          "fn": 6056,
          "accuracy": 0.6845833333333333
        },
        "0.01": null
      },
      "auroc": 0.8254831190321181
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 13144,
          "fn": 6056,
          "accuracy": 0.6845833333333333
        },
        "0.01": null
      },
      "auroc": 0.8254831190321181
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 11636,
          "fn": 7564,
          "accuracy": 0.6060416666666667
        },
        "0.01": null
      },
      "auroc": 0.7965291259765626
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 11636,
          "fn": 7564,
          "accuracy": 0.6060416666666667
        },
        "0.01": null
      },
      "auroc": 0.7965291259765626
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 24780,
          "fn": 13620,
          "accuracy": 0.6453125
        },
        "0.01": null
      },
      "auroc": 0.8110061225043405
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 24780,
          "fn": 13620,
          "accuracy": 0.6453125
        },
        "0.01": null
      },
      "auroc": 0.8110061225043405
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7312,
          "fn": 11888,
          "accuracy": 0.38083333333333336
        },
        "0.01": null
      },
      "auroc": 0.701113266330295
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7312,
          "fn": 11888,
          "accuracy": 0.38083333333333336
        },
        "0.01": null
      },
      "auroc": 0.701113266330295
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5749,
          "fn": 13451,
          "accuracy": 0.29942708333333334
        },
        "0.01": null
      },
      "auroc": 0.6578900444878472
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5749,
          "fn": 13451,
          "accuracy": 0.29942708333333334
        },
        "0.01": null
      },
      "auroc": 0.6578900444878472
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 13061,
          "fn": 25339,
          "accuracy": 0.34013020833333335
        },
        "0.01": null
      },
      "auroc": 0.6795016554090711
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 13061,
          "fn": 25339,
          "accuracy": 0.34013020833333335
        },
        "0.01": null
      },
      "auroc": 0.6795016554090711
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 15570,
          "fn": 3630,
          "accuracy": 0.8109375
        },
        "0.01": null
      },
      "auroc": 0.8552222981770832
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 15570,
          "fn": 3630,
          "accuracy": 0.8109375
        },
        "0.01": null
      },
      "auroc": 0.8552222981770832
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 14886,
          "fn": 4314,
          "accuracy": 0.7753125
        },
        "0.01": null
      },
      "auroc": 0.8389204033745661
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 14886,
          "fn": 4314,
          "accuracy": 0.7753125
        },
        "0.01": null
      },
      "auroc": 0.8389204033745661
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 30456,
          "fn": 7944,
          "accuracy": 0.793125
        },
        "0.01": null
      },
      "auroc": 0.8470713507758246
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 30456,
          "fn": 7944,
          "accuracy": 0.793125
        },
        "0.01": null
      },
      "auroc": 0.8470713507758246
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 15130,
          "fn": 4070,
          "accuracy": 0.7880208333333333
        },
        "0.01": null
      },
      "auroc": 0.8552123562282986
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 15130,
          "fn": 4070,
          "accuracy": 0.7880208333333333
        },
        "0.01": null
      },
      "auroc": 0.8552123562282986
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 11586,
          "fn": 7614,
          "accuracy": 0.6034375
        },
        "0.01": null
      },
      "auroc": 0.7650441786024305
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 11586,
          "fn": 7614,
          "accuracy": 0.6034375
        },
        "0.01": null
      },
      "auroc": 0.7650441786024305
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 26716,
          "fn": 11684,
          "accuracy": 0.6957291666666666
        },
        "0.01": null
      },
      "auroc": 0.8101282674153647
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 26716,
          "fn": 11684,
          "accuracy": 0.6957291666666666
        },
        "0.01": null
      },
      "auroc": 0.8101282674153647
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 12091,
          "fn": 7109,
          "accuracy": 0.6297395833333334
        },
        "0.01": null
      },
      "auroc": 0.8128342692057291
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 12091,
          "fn": 7109,
          "accuracy": 0.6297395833333334
        },
        "0.01": null
      },
      "auroc": 0.8128342692057291
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 10622,
          "fn": 8578,
          "accuracy": 0.5532291666666667
        },
        "0.01": null
      },
      "auroc": 0.7746129720052083
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 10622,
          "fn": 8578,
          "accuracy": 0.5532291666666667
        },
        "0.01": null
      },
      "auroc": 0.7746129720052083
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 22713,
          "fn": 15687,
          "accuracy": 0.591484375
        },
        "0.01": null
      },
      "auroc": 0.7937236206054687
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 22713,
          "fn": 15687,
          "accuracy": 0.591484375
        },
        "0.01": null
      },
      "auroc": 0.7937236206054687
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 160975,
          "fn": 50225,
          "accuracy": 0.7621922348484849
        },
        "0.01": null
      },
      "auroc": 0.8631081854926215
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 85711,
          "fn": 29489,
          "accuracy": 0.7440190972222223
        },
        "0.01": null
      },
      "auroc": 0.8737447803638598
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 246686,
          "fn": 79714,
          "accuracy": 0.7557781862745098
        },
        "0.01": null
      },
      "auroc": 0.8668622778001176
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 111310,
          "fn": 99890,
          "accuracy": 0.5270359848484848
        },
        "0.01": null
      },
      "auroc": 0.762558743440262
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 68396,
          "fn": 46804,
          "accuracy": 0.5937152777777778
        },
        "0.01": null
      },
      "auroc": 0.8160697943793402
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 179706,
          "fn": 146694,
          "accuracy": 0.5505698529411764
        },
        "0.01": null
      },
      "auroc": 0.7814449967128778
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 272285,
          "fn": 150115,
          "accuracy": 0.6446141098484849
        },
        "0.01": null
      },
      "auroc": 0.8128334644664417
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 154107,
          "fn": 76293,
          "accuracy": 0.6688671875
        },
        "0.01": null
      },
      "auroc": 0.8449072873716
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 426392,
          "fn": 226408,
          "accuracy": 0.6531740196078432
        },
        "0.01": null
      },
      "auroc": 0.8241536372564977
    }
  ],
  "thresholds": {
    "0.05": {
      "abstracts": 4.742145538361164e-06,
      "books": 0.00020091772079466885,
      "news": 0.9945951375365256,
      "poetry": 0.8824219462275504,
      "recipes": 0.9991116306185721,
      "reddit": 0.8586341679096221,
      "reviews": 0.9995117191784084,
      "wiki": 0.9990234375325964
    },
    "0.01": {
      "abstracts": 0.00016972780227664241,
      "books": 0.5099512839317322,
      "news": 0.9994779500365256,
      "poetry": 0.9999999998416751,
      "recipes": 0.999999999955762,
      "reddit": 0.9970703125931322,
      "reviews": 0.9999999999627471,
      "wiki": 0.9999999999743887
    }
  },
  "fpr": {
    "0.05": {
      "abstracts": 0.050000000000000044,
      "books": 0.050000000000000044,
      "news": 0.050000000000000044,
      "poetry": 0.050000000000000044,
      "recipes": 0.050000000000000044,
      "reddit": 0.050000000000000044,
      "reviews": 0.030000000000000027,
      "wiki": 0.04500000000000004
    },
    "0.01": {
      "abstracts": 0.010000000000000009,
      "books": 0.010000000000000009,
      "news": 0.010000000000000009,
      "poetry": 0.015000000000000013,
      "recipes": 0.015000000000000013,
      "reddit": 0.0050000000000000044,
      "reviews": 0.030000000000000027,
      "wiki": 0.020000000000000018
    }
  }
}