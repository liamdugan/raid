{
  "_autogen_note": "This file is automatically generated by the RAID leaderboard submission script. Do not edit this file manually or include it in new submissions' PRs.",
  "_submission_hash": "5273512309458c82e61438154b21b3a633deb9eaa8624a9d518bcf53068ffe41",
  "_results_hash": "4036ba4857d5e1494278185b014f9d9c1391a74d203b5092ecaac86daee55300",
  "date_released": "2023-10-08",
  "detector_name": "FastDetectGPT",
  "contact_info": "Submitted by RAID Authors",
  "website": "http://region-9.autodl.pro:21504/#/home",
  "paper_link": "https://arxiv.org/abs/2310.05130",
  "huggingface_link": null,
  "github_link": "https://github.com/baoguangsheng/fast-detect-gpt",
  "additional_metadata": null,
  "score_agg": {
    "all": {
      "_note": "No aggregate score across all settings is reported here as some domains/generator models/decoding strategies/repetition penalties/adversarial attacks were not included in the submission. This submission will not appear in the main leaderboard; it will only be visible within the splits in which all samples were evaluated."
    },
    "no_adversarial": {
      "_note": "No aggregate score across all non-adversarial settings is reported here as some domains/generator models/decoding strategies/repetition penalties were not included in the submission."
    }
  },
  "scores": [
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9554166666666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9547145833333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.955065625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9551802083333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.933878125
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9445291666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9552984375
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9442963541666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 792,
          "fn": 8,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        }
      },
      "auroc": 0.9497973958333336
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9549624999999999
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.055627083333333355
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.5052947916666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.8912302083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.036640625000000024
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        },
        "0.01": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        }
      },
      "auroc": 0.4639354166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9230963541666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.04613385416666668
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 420,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 343,
          "fn": 457,
          "accuracy": 0.42875
        }
      },
      "auroc": 0.4846151041666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9313760416666668
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        }
      },
      "auroc": 0.67186875
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        },
        "0.01": {
          "tp": 228,
          "fn": 172,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8016223958333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9258822916666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.33123125000000003
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        }
      },
      "auroc": 0.6285567708333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        }
      },
      "auroc": 0.9286291666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 280,
          "accuracy": 0.3
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.50155
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 509,
          "fn": 291,
          "accuracy": 0.63625
        },
        "0.01": {
          "tp": 413,
          "fn": 387,
          "accuracy": 0.51625
        }
      },
      "auroc": 0.7150895833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.95285
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9545791666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9537145833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9116166666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.3013510416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        },
        "0.01": {
          "tp": 163,
          "fn": 237,
          "accuracy": 0.4075
        }
      },
      "auroc": 0.6064838541666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9322333333333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.6279651041666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 584,
          "fn": 216,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 561,
          "fn": 239,
          "accuracy": 0.70125
        }
      },
      "auroc": 0.78009921875
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9551729166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9219031249999999
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9385380208333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        }
      },
      "auroc": 0.8357031250000001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.04507395833333335
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 263,
          "accuracy": 0.3425
        },
        "0.01": {
          "tp": 95,
          "fn": 305,
          "accuracy": 0.2375
        }
      },
      "auroc": 0.4403885416666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 295,
          "fn": 105,
          "accuracy": 0.7375
        }
      },
      "auroc": 0.8954380208333335
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        }
      },
      "auroc": 0.4834885416666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 524,
          "fn": 276,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 465,
          "fn": 335,
          "accuracy": 0.58125
        }
      },
      "auroc": 0.68946328125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9545479166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.95035625
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9524520833333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9501583333333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.952353125
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9463250000000001
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9463250000000001
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9365354166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9365354166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9414302083333335
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9414302083333335
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.916628125
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.916628125
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        }
      },
      "auroc": 0.8745927083333334
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        }
      },
      "auroc": 0.8745927083333334
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        }
      },
      "auroc": 0.8956104166666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        }
      },
      "auroc": 0.8956104166666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9514812500000001
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9514812500000001
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9417249999999999
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9417249999999999
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.946603125
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.946603125
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9445479166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9445479166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.668146875
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.668146875
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        },
        "0.01": {
          "tp": 210,
          "fn": 190,
          "accuracy": 0.525
        }
      },
      "auroc": 0.8063473958333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        },
        "0.01": {
          "tp": 210,
          "fn": 190,
          "accuracy": 0.525
        }
      },
      "auroc": 0.8063473958333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9438895833333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9438895833333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9242291666666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9242291666666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9340593749999999
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9340593749999999
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2174,
          "fn": 26,
          "accuracy": 0.9881818181818182
        },
        "0.01": {
          "tp": 2110,
          "fn": 90,
          "accuracy": 0.9590909090909091
        }
      },
      "auroc": 0.9461089015151515
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 885,
          "fn": 315,
          "accuracy": 0.7375
        },
        "0.01": {
          "tp": 814,
          "fn": 386,
          "accuracy": 0.6783333333333333
        }
      },
      "auroc": 0.7515081597222222
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3059,
          "fn": 341,
          "accuracy": 0.8997058823529411
        },
        "0.01": {
          "tp": 2924,
          "fn": 476,
          "accuracy": 0.86
        }
      },
      "auroc": 0.8774262867647058
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1888,
          "fn": 312,
          "accuracy": 0.8581818181818182
        },
        "0.01": {
          "tp": 1679,
          "fn": 521,
          "accuracy": 0.7631818181818182
        }
      },
      "auroc": 0.8922727272727273
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 4062,
          "fn": 338,
          "accuracy": 0.9231818181818182
        },
        "0.01": {
          "tp": 3789,
          "fn": 611,
          "accuracy": 0.8611363636363636
        }
      },
      "auroc": 0.9191908143939392
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9545729166666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9493739583333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9519734375
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9545625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9162385416666669
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9354005208333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9545677083333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.93280625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 788,
          "fn": 12,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 766,
          "fn": 34,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9436869791666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9537125000000001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.049410416666666686
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        }
      },
      "auroc": 0.5015614583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        }
      },
      "auroc": 0.8606260416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.036727083333333355
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 240,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        }
      },
      "auroc": 0.4486765625
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9071692708333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.04306875000000002
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 440,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 312,
          "fn": 488,
          "accuracy": 0.39
        }
      },
      "auroc": 0.47511901041666665
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.8926999999999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.5741166666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 151,
          "fn": 249,
          "accuracy": 0.3775
        }
      },
      "auroc": 0.7334083333333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.8874354166666668
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.27580208333333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 214,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 137,
          "fn": 263,
          "accuracy": 0.3425
        }
      },
      "auroc": 0.5816187500000001
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        }
      },
      "auroc": 0.8900677083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 351,
          "accuracy": 0.1225
        },
        "0.01": {
          "tp": 12,
          "fn": 388,
          "accuracy": 0.03
        }
      },
      "auroc": 0.42495937500000003
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 402,
          "fn": 398,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 288,
          "fn": 512,
          "accuracy": 0.36
        }
      },
      "auroc": 0.6575135416666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9434395833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9501833333333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9468114583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.8901291666666665
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.3288760416666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        },
        "0.01": {
          "tp": 153,
          "fn": 247,
          "accuracy": 0.3825
        }
      },
      "auroc": 0.6095026041666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        }
      },
      "auroc": 0.9167843749999999
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        }
      },
      "auroc": 0.6395296875
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 573,
          "fn": 227,
          "accuracy": 0.71625
        },
        "0.01": {
          "tp": 543,
          "fn": 257,
          "accuracy": 0.67875
        }
      },
      "auroc": 0.77815703125
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9506020833333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.8784645833333332
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        }
      },
      "auroc": 0.9145333333333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.7886322916666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.04926875000000003
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 80,
          "fn": 320,
          "accuracy": 0.2
        }
      },
      "auroc": 0.41895052083333345
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        }
      },
      "auroc": 0.8696171874999999
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        },
        "0.01": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        }
      },
      "auroc": 0.4638666666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 482,
          "fn": 318,
          "accuracy": 0.6025
        },
        "0.01": {
          "tp": 405,
          "fn": 395,
          "accuracy": 0.50625
        }
      },
      "auroc": 0.6667419270833332
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9476541666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9278833333333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.93776875
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9393895833333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.943521875
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9235312499999999
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9235312499999999
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9068645833333332
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9068645833333332
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9151979166666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9151979166666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        }
      },
      "auroc": 0.86765
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        }
      },
      "auroc": 0.86765
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8221802083333332
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8221802083333332
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        }
      },
      "auroc": 0.8449151041666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        }
      },
      "auroc": 0.8449151041666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.93271875
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.93271875
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9084145833333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9084145833333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9205666666666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9205666666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.919090625
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.919090625
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.5841687499999999
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.5841687499999999
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.7516296875
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.7516296875
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9253666666666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9253666666666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.890140625
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.890140625
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9077536458333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9077536458333333
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2086,
          "fn": 114,
          "accuracy": 0.9481818181818182
        },
        "0.01": {
          "tp": 1932,
          "fn": 268,
          "accuracy": 0.8781818181818182
        }
      },
      "auroc": 0.9282762310606061
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 802,
          "fn": 398,
          "accuracy": 0.6683333333333333
        },
        "0.01": {
          "tp": 711,
          "fn": 489,
          "accuracy": 0.5925
        }
      },
      "auroc": 0.721572048611111
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2888,
          "fn": 512,
          "accuracy": 0.8494117647058823
        },
        "0.01": {
          "tp": 2643,
          "fn": 757,
          "accuracy": 0.7773529411764706
        }
      },
      "auroc": 0.8553218137254902
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1740,
          "fn": 460,
          "accuracy": 0.7909090909090909
        },
        "0.01": {
          "tp": 1439,
          "fn": 761,
          "accuracy": 0.6540909090909091
        }
      },
      "auroc": 0.8575039772727273
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3826,
          "fn": 574,
          "accuracy": 0.8695454545454545
        },
        "0.01": {
          "tp": 3371,
          "fn": 1029,
          "accuracy": 0.7661363636363636
        }
      },
      "auroc": 0.8928901041666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.95408125
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9393135416666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9466973958333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9518041666666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.8807052083333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9162546874999998
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9529427083333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        }
      },
      "auroc": 0.9100093749999999
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 766,
          "fn": 34,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 728,
          "fn": 72,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9314760416666665
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9492239583333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.048404166666666686
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        }
      },
      "auroc": 0.49881406250000004
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.801615625
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.03547604166666669
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 284,
          "accuracy": 0.29
        },
        "0.01": {
          "tp": 66,
          "fn": 334,
          "accuracy": 0.165
        }
      },
      "auroc": 0.4185458333333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        },
        "0.01": {
          "tp": 259,
          "fn": 141,
          "accuracy": 0.6475
        }
      },
      "auroc": 0.8754197916666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.041940104166666686
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 316,
          "fn": 484,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 260,
          "fn": 540,
          "accuracy": 0.325
        }
      },
      "auroc": 0.45867994791666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        }
      },
      "auroc": 0.8707614583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.49099374999999995
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 123,
          "fn": 277,
          "accuracy": 0.3075
        }
      },
      "auroc": 0.6808776041666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8618125
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.23019687500000002
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 236,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 103,
          "fn": 297,
          "accuracy": 0.2575
        }
      },
      "auroc": 0.5460046875000001
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        }
      },
      "auroc": 0.8662869791666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 378,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        }
      },
      "auroc": 0.36059531250000004
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 452,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 226,
          "fn": 574,
          "accuracy": 0.2825
        }
      },
      "auroc": 0.6134411458333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9344177083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9464593750000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9404385416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.8826510416666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.2510354166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 235,
          "accuracy": 0.4125
        },
        "0.01": {
          "tp": 141,
          "fn": 259,
          "accuracy": 0.3525
        }
      },
      "auroc": 0.5668432291666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9085343750000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        }
      },
      "auroc": 0.5987473958333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 558,
          "fn": 242,
          "accuracy": 0.6975
        },
        "0.01": {
          "tp": 516,
          "fn": 284,
          "accuracy": 0.645
        }
      },
      "auroc": 0.7536408854166667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9476552083333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        }
      },
      "auroc": 0.8270208333333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        }
      },
      "auroc": 0.8873380208333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.7011166666666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.03959270833333336
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 316,
          "accuracy": 0.21
        },
        "0.01": {
          "tp": 33,
          "fn": 367,
          "accuracy": 0.0825
        }
      },
      "auroc": 0.3703546875
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        },
        "0.01": {
          "tp": 228,
          "fn": 172,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8243859375
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 259,
          "accuracy": 0.3525
        },
        "0.01": {
          "tp": 90,
          "fn": 310,
          "accuracy": 0.225
        }
      },
      "auroc": 0.4333067708333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 424,
          "fn": 376,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 318,
          "fn": 482,
          "accuracy": 0.3975
        }
      },
      "auroc": 0.6288463541666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9437083333333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.8988489583333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        }
      },
      "auroc": 0.9212786458333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9209052083333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9323067708333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.927528125
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.927528125
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.8938302083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.8938302083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9106791666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9106791666666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8383416666666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8383416666666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.765890625
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.765890625
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        },
        "0.01": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        }
      },
      "auroc": 0.8021161458333332
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        },
        "0.01": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        }
      },
      "auroc": 0.8021161458333332
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9187802083333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9187802083333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        }
      },
      "auroc": 0.8775989583333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        }
      },
      "auroc": 0.8775989583333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        }
      },
      "auroc": 0.8981895833333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        }
      },
      "auroc": 0.8981895833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.8919593750000001
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.8919593750000001
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.46756875
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.46756875
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 140,
          "fn": 260,
          "accuracy": 0.35
        }
      },
      "auroc": 0.6797640625000001
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 140,
          "fn": 260,
          "accuracy": 0.35
        }
      },
      "auroc": 0.6797640625000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9056291666666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9056291666666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        }
      },
      "auroc": 0.8619145833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        }
      },
      "auroc": 0.8619145833333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        }
      },
      "auroc": 0.8837718750000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        }
      },
      "auroc": 0.8837718750000001
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2039,
          "fn": 161,
          "accuracy": 0.9268181818181818
        },
        "0.01": {
          "tp": 1822,
          "fn": 378,
          "accuracy": 0.8281818181818181
        }
      },
      "auroc": 0.9165533143939394
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 736,
          "fn": 464,
          "accuracy": 0.6133333333333333
        },
        "0.01": {
          "tp": 615,
          "fn": 585,
          "accuracy": 0.5125
        }
      },
      "auroc": 0.6918401041666667
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2775,
          "fn": 625,
          "accuracy": 0.8161764705882353
        },
        "0.01": {
          "tp": 2437,
          "fn": 963,
          "accuracy": 0.716764705882353
        }
      },
      "auroc": 0.8372427696078432
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1558,
          "fn": 642,
          "accuracy": 0.7081818181818181
        },
        "0.01": {
          "tp": 1168,
          "fn": 1032,
          "accuracy": 0.5309090909090909
        }
      },
      "auroc": 0.8169734848484849
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3597,
          "fn": 803,
          "accuracy": 0.8175
        },
        "0.01": {
          "tp": 2990,
          "fn": 1410,
          "accuracy": 0.6795454545454546
        }
      },
      "auroc": 0.8667633996212122
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9390270833333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.8603406250000001
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        }
      },
      "auroc": 0.8996838541666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9363385416666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.7407052083333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        }
      },
      "auroc": 0.838521875
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9376828125000001
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 267,
          "fn": 133,
          "accuracy": 0.6675
        },
        "0.01": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        }
      },
      "auroc": 0.8005229166666669
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 660,
          "fn": 140,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 555,
          "fn": 245,
          "accuracy": 0.69375
        }
      },
      "auroc": 0.8691028645833334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9538604166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.03919375000000002
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        }
      },
      "auroc": 0.49652708333333323
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.659665625
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.03514375000000002
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 60,
          "fn": 340,
          "accuracy": 0.15
        },
        "0.01": {
          "tp": 26,
          "fn": 374,
          "accuracy": 0.065
        }
      },
      "auroc": 0.3474046875
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        }
      },
      "auroc": 0.8067630208333332
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.03716875000000002
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 261,
          "fn": 539,
          "accuracy": 0.32625
        },
        "0.01": {
          "tp": 226,
          "fn": 574,
          "accuracy": 0.2825
        }
      },
      "auroc": 0.4219658854166667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.7706916666666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.3042125
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.5374520833333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.7467447916666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.14666875000000001
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 303,
          "accuracy": 0.2425
        },
        "0.01": {
          "tp": 43,
          "fn": 357,
          "accuracy": 0.1075
        }
      },
      "auroc": 0.4467067708333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 96,
          "fn": 304,
          "accuracy": 0.24
        }
      },
      "auroc": 0.7587182291666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.22544062499999998
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 589,
          "accuracy": 0.26375
        },
        "0.01": {
          "tp": 97,
          "fn": 703,
          "accuracy": 0.12125
        }
      },
      "auroc": 0.4920794270833334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9510739583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9124187500000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9317463541666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.8235145833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.11050104166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 103,
          "fn": 297,
          "accuracy": 0.2575
        }
      },
      "auroc": 0.4670078125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        }
      },
      "auroc": 0.8872942708333332
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 164,
          "fn": 236,
          "accuracy": 0.41
        }
      },
      "auroc": 0.5114598958333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 529,
          "fn": 271,
          "accuracy": 0.66125
        },
        "0.01": {
          "tp": 463,
          "fn": 337,
          "accuracy": 0.57875
        }
      },
      "auroc": 0.6993770833333335
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9435666666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.6757624999999999
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        },
        "0.01": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        }
      },
      "auroc": 0.8096645833333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.57481875
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.03358437500000002
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 369,
          "accuracy": 0.0775
        },
        "0.01": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        }
      },
      "auroc": 0.3042015625000001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 228,
          "fn": 172,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        }
      },
      "auroc": 0.7591927083333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        },
        "0.01": {
          "tp": 21,
          "fn": 379,
          "accuracy": 0.0525
        }
      },
      "auroc": 0.3546734375
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 516,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 228,
          "fn": 572,
          "accuracy": 0.285
        }
      },
      "auroc": 0.5569330729166666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9066781250000001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.7854374999999999
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        }
      },
      "auroc": 0.8460578125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        }
      },
      "auroc": 0.8530739583333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        }
      },
      "auroc": 0.8798760416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        }
      },
      "auroc": 0.8812729166666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        }
      },
      "auroc": 0.8812729166666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.8498729166666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.8498729166666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.8655729166666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.8655729166666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.7771833333333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.7771833333333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.6928802083333332
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.6928802083333332
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 107,
          "fn": 293,
          "accuracy": 0.2675
        }
      },
      "auroc": 0.7350317708333334
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 107,
          "fn": 293,
          "accuracy": 0.2675
        }
      },
      "auroc": 0.7350317708333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        }
      },
      "auroc": 0.8302364583333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        }
      },
      "auroc": 0.8302364583333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        }
      },
      "auroc": 0.7696625
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        }
      },
      "auroc": 0.7696625
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 133,
          "fn": 267,
          "accuracy": 0.3325
        }
      },
      "auroc": 0.7999494791666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 133,
          "fn": 267,
          "accuracy": 0.3325
        }
      },
      "auroc": 0.7999494791666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.7843291666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.7843291666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.3697364583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.3697364583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 294,
          "accuracy": 0.265
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.5770328125
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 294,
          "accuracy": 0.265
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.5770328125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8696885416666666
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8696885416666666
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.8039875
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.8039875
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        },
        "0.01": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        }
      },
      "auroc": 0.8368380208333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        },
        "0.01": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        }
      },
      "auroc": 0.8368380208333334
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1760,
          "fn": 440,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 1450,
          "fn": 750,
          "accuracy": 0.6590909090909091
        }
      },
      "auroc": 0.8734189393939393
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 512,
          "fn": 688,
          "accuracy": 0.4266666666666667
        },
        "0.01": {
          "tp": 370,
          "fn": 830,
          "accuracy": 0.30833333333333335
        }
      },
      "auroc": 0.5962276041666668
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2272,
          "fn": 1128,
          "accuracy": 0.668235294117647
        },
        "0.01": {
          "tp": 1820,
          "fn": 1580,
          "accuracy": 0.5352941176470588
        }
      },
      "auroc": 0.7755867034313726
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1125,
          "fn": 1075,
          "accuracy": 0.5113636363636364
        },
        "0.01": {
          "tp": 750,
          "fn": 1450,
          "accuracy": 0.3409090909090909
        }
      },
      "auroc": 0.7345723484848485
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2885,
          "fn": 1515,
          "accuracy": 0.6556818181818181
        },
        "0.01": {
          "tp": 2200,
          "fn": 2200,
          "accuracy": 0.5
        }
      },
      "auroc": 0.803995643939394
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9554166666666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9535010416666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9544588541666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9544072916666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.92790625
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9411567708333332
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9549119791666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9407036458333332
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 789,
          "fn": 11,
          "accuracy": 0.98625
        },
        "0.01": {
          "tp": 782,
          "fn": 18,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9478078125000001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9541375000000001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.05482083333333336
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        }
      },
      "auroc": 0.5044791666666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.8797833333333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.036640625000000024
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        },
        "0.01": {
          "tp": 133,
          "fn": 267,
          "accuracy": 0.3325
        }
      },
      "auroc": 0.4582119791666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9169604166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.04573072916666669
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 427,
          "accuracy": 0.46625
        },
        "0.01": {
          "tp": 333,
          "fn": 467,
          "accuracy": 0.41625
        }
      },
      "auroc": 0.4813455729166666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9264791666666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.655425
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        },
        "0.01": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        }
      },
      "auroc": 0.7909520833333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9211979166666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.3115010416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 210,
          "fn": 190,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        }
      },
      "auroc": 0.6163494791666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9238385416666668
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 294,
          "accuracy": 0.265
        },
        "0.01": {
          "tp": 46,
          "fn": 354,
          "accuracy": 0.115
        }
      },
      "auroc": 0.48346302083333337
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 491,
          "fn": 309,
          "accuracy": 0.61375
        },
        "0.01": {
          "tp": 388,
          "fn": 412,
          "accuracy": 0.485
        }
      },
      "auroc": 0.70365078125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.95285
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.954321875
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9535859375000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9051125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.268315625
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        },
        "0.01": {
          "tp": 157,
          "fn": 243,
          "accuracy": 0.3925
        }
      },
      "auroc": 0.5867140624999999
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9289812500000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.6113187499999999
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 578,
          "fn": 222,
          "accuracy": 0.7225
        },
        "0.01": {
          "tp": 555,
          "fn": 245,
          "accuracy": 0.69375
        }
      },
      "auroc": 0.7701500000000002
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9551729166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9107375
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9329552083333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        }
      },
      "auroc": 0.8177302083333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.04314375000000002
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 270,
          "accuracy": 0.325
        },
        "0.01": {
          "tp": 87,
          "fn": 313,
          "accuracy": 0.2175
        }
      },
      "auroc": 0.4304369791666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        }
      },
      "auroc": 0.8864515625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        },
        "0.01": {
          "tp": 160,
          "fn": 240,
          "accuracy": 0.4
        }
      },
      "auroc": 0.476940625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 507,
          "fn": 293,
          "accuracy": 0.63375
        },
        "0.01": {
          "tp": 447,
          "fn": 353,
          "accuracy": 0.55875
        }
      },
      "auroc": 0.6816960937500001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9542114583333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9453072916666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9497593750000001
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9489458333333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9515786458333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9446177083333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9446177083333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9309541666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9309541666666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9377859375000002
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9377859375000002
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9050312500000001
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9050312500000001
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        }
      },
      "auroc": 0.854228125
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        }
      },
      "auroc": 0.854228125
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        }
      },
      "auroc": 0.8796296875
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        }
      },
      "auroc": 0.8796296875
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.94920625
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.94920625
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9355781249999999
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9355781249999999
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9423921875
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9423921875
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9433395833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9433395833333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.6421489583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.6421489583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        }
      },
      "auroc": 0.7927442708333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        }
      },
      "auroc": 0.7927442708333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9398
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9398
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9184302083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9184302083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9291151041666668
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9291151041666668
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2161,
          "fn": 39,
          "accuracy": 0.9822727272727273
        },
        "0.01": {
          "tp": 2086,
          "fn": 114,
          "accuracy": 0.9481818181818182
        }
      },
      "auroc": 0.9436602272727272
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 863,
          "fn": 337,
          "accuracy": 0.7191666666666666
        },
        "0.01": {
          "tp": 790,
          "fn": 410,
          "accuracy": 0.6583333333333333
        }
      },
      "auroc": 0.7456855902777777
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3024,
          "fn": 376,
          "accuracy": 0.8894117647058823
        },
        "0.01": {
          "tp": 2876,
          "fn": 524,
          "accuracy": 0.8458823529411764
        }
      },
      "auroc": 0.8737868259803921
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1843,
          "fn": 357,
          "accuracy": 0.8377272727272728
        },
        "0.01": {
          "tp": 1608,
          "fn": 592,
          "accuracy": 0.730909090909091
        }
      },
      "auroc": 0.8825924242424241
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 4004,
          "fn": 396,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 3694,
          "fn": 706,
          "accuracy": 0.8395454545454546
        }
      },
      "auroc": 0.9131263257575757
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9538187499999999
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9447177083333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9492682291666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9523468749999999
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9251479166666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9387473958333332
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9530828124999999
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9349328125
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        },
        "0.01": {
          "tp": 763,
          "fn": 37,
          "accuracy": 0.95375
        }
      },
      "auroc": 0.9440078125000001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9500229166666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.5585708333333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        }
      },
      "auroc": 0.7542968750000002
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.8877583333333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.43765625
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        }
      },
      "auroc": 0.6627072916666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        }
      },
      "auroc": 0.9188906250000001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 63,
          "fn": 337,
          "accuracy": 0.1575
        },
        "0.01": {
          "tp": 28,
          "fn": 372,
          "accuracy": 0.07
        }
      },
      "auroc": 0.49811354166666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 430,
          "fn": 370,
          "accuracy": 0.5375
        },
        "0.01": {
          "tp": 365,
          "fn": 435,
          "accuracy": 0.45625
        }
      },
      "auroc": 0.7085020833333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.8982302083333332
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.8203354166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        }
      },
      "auroc": 0.8592828124999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.8970020833333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.7348281250000002
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        }
      },
      "auroc": 0.8159151041666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        }
      },
      "auroc": 0.8976161458333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        },
        "0.01": {
          "tp": 120,
          "fn": 280,
          "accuracy": 0.3
        }
      },
      "auroc": 0.7775817708333332
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 588,
          "fn": 212,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 423,
          "fn": 377,
          "accuracy": 0.52875
        }
      },
      "auroc": 0.8375989583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9370739583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9410708333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9390723958333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9117322916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9244031250000001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9443333333333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        }
      },
      "auroc": 0.8604864583333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        }
      },
      "auroc": 0.9024098958333332
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.8376572916666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.5061593750000001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        }
      },
      "auroc": 0.6719083333333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 301,
          "fn": 99,
          "accuracy": 0.7525
        }
      },
      "auroc": 0.8909953125000001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        },
        "0.01": {
          "tp": 133,
          "fn": 267,
          "accuracy": 0.3325
        }
      },
      "auroc": 0.6833229166666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 542,
          "fn": 258,
          "accuracy": 0.6775
        },
        "0.01": {
          "tp": 434,
          "fn": 366,
          "accuracy": 0.5425
        }
      },
      "auroc": 0.7871591145833333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9462979166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9310177083333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        }
      },
      "auroc": 0.9386578125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9397208333333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.84808125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        }
      },
      "auroc": 0.8939010416666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.943009375
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        }
      },
      "auroc": 0.8895494791666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 741,
          "fn": 59,
          "accuracy": 0.92625
        },
        "0.01": {
          "tp": 663,
          "fn": 137,
          "accuracy": 0.82875
        }
      },
      "auroc": 0.9162794270833335
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9090510416666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9090510416666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        }
      },
      "auroc": 0.8934135416666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        }
      },
      "auroc": 0.8934135416666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9012322916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9012322916666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.8770291666666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.8770291666666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        }
      },
      "auroc": 0.852084375
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        }
      },
      "auroc": 0.852084375
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        }
      },
      "auroc": 0.8645567708333332
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        }
      },
      "auroc": 0.8645567708333332
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9317
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9317
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9188604166666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9188604166666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        }
      },
      "auroc": 0.9252802083333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        }
      },
      "auroc": 0.9252802083333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9228875000000001
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9228875000000001
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.8485375
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.8485375
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 274,
          "fn": 126,
          "accuracy": 0.685
        }
      },
      "auroc": 0.8857124999999999
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 274,
          "fn": 126,
          "accuracy": 0.685
        }
      },
      "auroc": 0.8857124999999999
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9167385416666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9167385416666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        }
      },
      "auroc": 0.897528125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        }
      },
      "auroc": 0.897528125
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 320,
          "fn": 80,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9071333333333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 320,
          "fn": 80,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9071333333333333
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2092,
          "fn": 108,
          "accuracy": 0.9509090909090909
        },
        "0.01": {
          "tp": 1941,
          "fn": 259,
          "accuracy": 0.8822727272727273
        }
      },
      "auroc": 0.9261075757575757
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 928,
          "fn": 272,
          "accuracy": 0.7733333333333333
        },
        "0.01": {
          "tp": 773,
          "fn": 427,
          "accuracy": 0.6441666666666667
        }
      },
      "auroc": 0.8426998263888889
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3020,
          "fn": 380,
          "accuracy": 0.888235294117647
        },
        "0.01": {
          "tp": 2714,
          "fn": 686,
          "accuracy": 0.798235294117647
        }
      },
      "auroc": 0.8966695465686275
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1926,
          "fn": 274,
          "accuracy": 0.8754545454545455
        },
        "0.01": {
          "tp": 1637,
          "fn": 563,
          "accuracy": 0.7440909090909091
        }
      },
      "auroc": 0.8942401515151516
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 4018,
          "fn": 382,
          "accuracy": 0.9131818181818182
        },
        "0.01": {
          "tp": 3578,
          "fn": 822,
          "accuracy": 0.8131818181818182
        }
      },
      "auroc": 0.9101738636363637
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9554166666666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9541052083333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9547609375
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9549083333333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.93183125
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9433697916666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9551624999999999
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9429682291666668
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        },
        "0.01": {
          "tp": 784,
          "fn": 16,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9490653645833335
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9545802083333335
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.05097500000000002
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        }
      },
      "auroc": 0.5027776041666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        }
      },
      "auroc": 0.8441260416666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.03604375000000002
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 244,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 119,
          "fn": 281,
          "accuracy": 0.2975
        }
      },
      "auroc": 0.4400848958333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        }
      },
      "auroc": 0.8993531249999999
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.04350937500000003
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 443,
          "accuracy": 0.44625
        },
        "0.01": {
          "tp": 319,
          "fn": 481,
          "accuracy": 0.39875
        }
      },
      "auroc": 0.47143125
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9294177083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        }
      },
      "auroc": 0.6683979166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        }
      },
      "auroc": 0.7989078125
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9230583333333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.3261927083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        },
        "0.01": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        }
      },
      "auroc": 0.6246255208333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9262380208333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 281,
          "accuracy": 0.2975
        },
        "0.01": {
          "tp": 53,
          "fn": 347,
          "accuracy": 0.1325
        }
      },
      "auroc": 0.49729531250000003
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 507,
          "fn": 293,
          "accuracy": 0.63375
        },
        "0.01": {
          "tp": 405,
          "fn": 395,
          "accuracy": 0.50625
        }
      },
      "auroc": 0.7117666666666665
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9519187499999999
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.952103125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9520109375
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.8990895833333331
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.2772614583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        },
        "0.01": {
          "tp": 159,
          "fn": 241,
          "accuracy": 0.3975
        }
      },
      "auroc": 0.5881755208333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        }
      },
      "auroc": 0.9255041666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        }
      },
      "auroc": 0.6146822916666668
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 575,
          "fn": 225,
          "accuracy": 0.71875
        },
        "0.01": {
          "tp": 555,
          "fn": 245,
          "accuracy": 0.69375
        }
      },
      "auroc": 0.7700932291666668
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9497947916666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9072749999999999
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        }
      },
      "auroc": 0.9285348958333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.7890260416666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.044477083333333355
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 277,
          "accuracy": 0.3075
        },
        "0.01": {
          "tp": 78,
          "fn": 322,
          "accuracy": 0.195
        }
      },
      "auroc": 0.4167515625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        }
      },
      "auroc": 0.8694104166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 222,
          "accuracy": 0.445
        },
        "0.01": {
          "tp": 159,
          "fn": 241,
          "accuracy": 0.3975
        }
      },
      "auroc": 0.4758760416666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 500,
          "fn": 300,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 435,
          "fn": 365,
          "accuracy": 0.54375
        }
      },
      "auroc": 0.6726432291666665
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9539552083333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9501135416666666
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.952034375
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9488927083333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9514239583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9441583333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9441583333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9345645833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9345645833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9393614583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9393614583333334
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9053385416666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9053385416666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        }
      },
      "auroc": 0.8603510416666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        }
      },
      "auroc": 0.8603510416666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        }
      },
      "auroc": 0.8828447916666667
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        }
      },
      "auroc": 0.8828447916666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9512447916666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9512447916666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9404770833333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9404770833333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9458609375
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9458609375
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9432770833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9432770833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.6607895833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.6607895833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8020333333333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8020333333333334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9408958333333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9408958333333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9196416666666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9196416666666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.93026875
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.93026875
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2164,
          "fn": 36,
          "accuracy": 0.9836363636363636
        },
        "0.01": {
          "tp": 2088,
          "fn": 112,
          "accuracy": 0.9490909090909091
        }
      },
      "auroc": 0.9436361742424243
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 875,
          "fn": 325,
          "accuracy": 0.7291666666666666
        },
        "0.01": {
          "tp": 799,
          "fn": 401,
          "accuracy": 0.6658333333333334
        }
      },
      "auroc": 0.7471616319444444
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3039,
          "fn": 361,
          "accuracy": 0.8938235294117647
        },
        "0.01": {
          "tp": 2887,
          "fn": 513,
          "accuracy": 0.8491176470588235
        }
      },
      "auroc": 0.874292218137255
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1832,
          "fn": 368,
          "accuracy": 0.8327272727272728
        },
        "0.01": {
          "tp": 1614,
          "fn": 586,
          "accuracy": 0.7336363636363636
        }
      },
      "auroc": 0.8795386363636364
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3996,
          "fn": 404,
          "accuracy": 0.9081818181818182
        },
        "0.01": {
          "tp": 3702,
          "fn": 698,
          "accuracy": 0.8413636363636363
        }
      },
      "auroc": 0.9115874053030302
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9554166666666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9542510416666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9548338541666668
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9550802083333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9325968750000001
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9438385416666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9552484375000001
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9434239583333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 792,
          "fn": 8,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        }
      },
      "auroc": 0.9493361979166669
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9551895833333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.05464479166666669
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.5049171875
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        }
      },
      "auroc": 0.89888125
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.036640625000000024
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 219,
          "accuracy": 0.4525
        },
        "0.01": {
          "tp": 151,
          "fn": 249,
          "accuracy": 0.3775
        }
      },
      "auroc": 0.4677609375000001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9270354166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.04564270833333335
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 418,
          "accuracy": 0.4775
        },
        "0.01": {
          "tp": 352,
          "fn": 448,
          "accuracy": 0.44
        }
      },
      "auroc": 0.4863390624999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9258312500000001
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.6542802083333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        },
        "0.01": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        }
      },
      "auroc": 0.7900557291666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9210489583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        }
      },
      "auroc": 0.3182625
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        },
        "0.01": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        }
      },
      "auroc": 0.6196557291666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9234401041666668
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 47,
          "fn": 353,
          "accuracy": 0.1175
        }
      },
      "auroc": 0.48627135416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 496,
          "fn": 304,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 393,
          "fn": 407,
          "accuracy": 0.49125
        }
      },
      "auroc": 0.7048557291666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.952428125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9548072916666668
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9536177083333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9133291666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.3432447916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 165,
          "fn": 235,
          "accuracy": 0.4125
        }
      },
      "auroc": 0.6282869791666666
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9328786458333336
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.6490260416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 586,
          "fn": 214,
          "accuracy": 0.7325
        },
        "0.01": {
          "tp": 563,
          "fn": 237,
          "accuracy": 0.70375
        }
      },
      "auroc": 0.7909523437499999
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9539864583333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.92325
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9386182291666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.8334677083333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.05124583333333335
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 262,
          "accuracy": 0.345
        },
        "0.01": {
          "tp": 89,
          "fn": 311,
          "accuracy": 0.2225
        }
      },
      "auroc": 0.44235677083333336
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        }
      },
      "auroc": 0.8937270833333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 165,
          "fn": 235,
          "accuracy": 0.4125
        }
      },
      "auroc": 0.48724791666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 528,
          "fn": 272,
          "accuracy": 0.66
        },
        "0.01": {
          "tp": 452,
          "fn": 348,
          "accuracy": 0.565
        }
      },
      "auroc": 0.6904875
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9543041666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9473239583333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9508140625
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9499166666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9521104166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9449114583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9449114583333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9353125
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9353125
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9401119791666668
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9401119791666668
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9153958333333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9153958333333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8738458333333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8738458333333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        }
      },
      "auroc": 0.8946208333333334
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        }
      },
      "auroc": 0.8946208333333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.950740625
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.950740625
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9379135416666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9379135416666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9443270833333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9443270833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9435052083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9435052083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.6738895833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.6738895833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8086973958333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8086973958333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9417895833333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9417895833333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.920984375
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.920984375
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9313869791666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9313869791666667
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2173,
          "fn": 27,
          "accuracy": 0.9877272727272727
        },
        "0.01": {
          "tp": 2097,
          "fn": 103,
          "accuracy": 0.9531818181818181
        }
      },
      "auroc": 0.9448635416666668
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 876,
          "fn": 324,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 799,
          "fn": 401,
          "accuracy": 0.6658333333333334
        }
      },
      "auroc": 0.7480928819444445
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3049,
          "fn": 351,
          "accuracy": 0.8967647058823529
        },
        "0.01": {
          "tp": 2896,
          "fn": 504,
          "accuracy": 0.851764705882353
        }
      },
      "auroc": 0.8754150735294118
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1891,
          "fn": 309,
          "accuracy": 0.8595454545454545
        },
        "0.01": {
          "tp": 1668,
          "fn": 532,
          "accuracy": 0.7581818181818182
        }
      },
      "auroc": 0.8921517992424244
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 4064,
          "fn": 336,
          "accuracy": 0.9236363636363636
        },
        "0.01": {
          "tp": 3765,
          "fn": 635,
          "accuracy": 0.8556818181818182
        }
      },
      "auroc": 0.9185076704545454
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9093218750000001
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.7955427083333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        }
      },
      "auroc": 0.8524322916666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9001145833333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.714771875
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 256,
          "fn": 144,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 186,
          "fn": 214,
          "accuracy": 0.465
        }
      },
      "auroc": 0.8074432291666668
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        }
      },
      "auroc": 0.9047182291666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 102,
          "fn": 298,
          "accuracy": 0.255
        }
      },
      "auroc": 0.7551572916666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 554,
          "fn": 246,
          "accuracy": 0.6925
        },
        "0.01": {
          "tp": 415,
          "fn": 385,
          "accuracy": 0.51875
        }
      },
      "auroc": 0.8299377604166667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9498229166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.149903125
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.5498630208333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.8671093750000001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.19452708333333335
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        },
        "0.01": {
          "tp": 124,
          "fn": 276,
          "accuracy": 0.31
        }
      },
      "auroc": 0.5308182291666668
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9084661458333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 384,
          "accuracy": 0.04
        },
        "0.01": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        }
      },
      "auroc": 0.17221510416666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 424,
          "accuracy": 0.47
        },
        "0.01": {
          "tp": 323,
          "fn": 477,
          "accuracy": 0.40375
        }
      },
      "auroc": 0.540340625
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        }
      },
      "auroc": 0.7574635416666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.4036374999999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 307,
          "accuracy": 0.2325
        },
        "0.01": {
          "tp": 45,
          "fn": 355,
          "accuracy": 0.1125
        }
      },
      "auroc": 0.5805505208333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.7294541666666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.29115
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 313,
          "accuracy": 0.2175
        },
        "0.01": {
          "tp": 28,
          "fn": 372,
          "accuracy": 0.07
        }
      },
      "auroc": 0.5103020833333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        },
        "0.01": {
          "tp": 72,
          "fn": 328,
          "accuracy": 0.18
        }
      },
      "auroc": 0.7434588541666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 389,
          "accuracy": 0.0275
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.34739375000000006
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 620,
          "accuracy": 0.225
        },
        "0.01": {
          "tp": 73,
          "fn": 727,
          "accuracy": 0.09125
        }
      },
      "auroc": 0.5454263020833333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9418489583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.8367979166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 274,
          "fn": 126,
          "accuracy": 0.685
        }
      },
      "auroc": 0.8893234375
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.8870197916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.4996020833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        },
        "0.01": {
          "tp": 145,
          "fn": 255,
          "accuracy": 0.3625
        }
      },
      "auroc": 0.6933109375
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        }
      },
      "auroc": 0.914434375
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        },
        "0.01": {
          "tp": 87,
          "fn": 313,
          "accuracy": 0.2175
        }
      },
      "auroc": 0.6682
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 516,
          "fn": 284,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 419,
          "fn": 381,
          "accuracy": 0.52375
        }
      },
      "auroc": 0.7913171875
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9227468750000001
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.5071979166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        },
        "0.01": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        }
      },
      "auroc": 0.7149723958333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.8114604166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.2708770833333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 266,
          "accuracy": 0.335
        },
        "0.01": {
          "tp": 78,
          "fn": 322,
          "accuracy": 0.195
        }
      },
      "auroc": 0.54116875
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        },
        "0.01": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        }
      },
      "auroc": 0.8671036458333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 20,
          "fn": 380,
          "accuracy": 0.05
        },
        "0.01": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        }
      },
      "auroc": 0.3890375
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 463,
          "accuracy": 0.42125
        },
        "0.01": {
          "tp": 255,
          "fn": 545,
          "accuracy": 0.31875
        }
      },
      "auroc": 0.6280705729166667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8572489583333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.6671114583333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 122,
          "fn": 278,
          "accuracy": 0.305
        }
      },
      "auroc": 0.7621802083333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.8153427083333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        }
      },
      "auroc": 0.8362958333333335
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.7555479166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.7555479166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.7366458333333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.7366458333333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        },
        "0.01": {
          "tp": 110,
          "fn": 290,
          "accuracy": 0.275
        }
      },
      "auroc": 0.7460968749999999
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        },
        "0.01": {
          "tp": 110,
          "fn": 290,
          "accuracy": 0.275
        }
      },
      "auroc": 0.7460968749999999
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.8028479166666668
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.8028479166666668
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.7612031249999999
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.7612031249999999
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 121,
          "fn": 279,
          "accuracy": 0.3025
        }
      },
      "auroc": 0.7820255208333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 121,
          "fn": 279,
          "accuracy": 0.3025
        }
      },
      "auroc": 0.7820255208333333
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.790140625
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.790140625
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.766990625
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.766990625
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 82,
          "fn": 318,
          "accuracy": 0.205
        }
      },
      "auroc": 0.7785656250000001
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 82,
          "fn": 318,
          "accuracy": 0.205
        }
      },
      "auroc": 0.7785656250000001
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.7393677083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.7393677083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        },
        "0.01": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        }
      },
      "auroc": 0.6165239583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        },
        "0.01": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        }
      },
      "auroc": 0.6165239583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 283,
          "accuracy": 0.2925
        },
        "0.01": {
          "tp": 38,
          "fn": 362,
          "accuracy": 0.095
        }
      },
      "auroc": 0.6779458333333332
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 283,
          "accuracy": 0.2925
        },
        "0.01": {
          "tp": 38,
          "fn": 362,
          "accuracy": 0.095
        }
      },
      "auroc": 0.6779458333333332
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8444864583333332
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8444864583333332
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.8062354166666666
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.8062354166666666
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 274,
          "fn": 126,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        }
      },
      "auroc": 0.8253609375
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 274,
          "fn": 126,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        }
      },
      "auroc": 0.8253609375
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1560,
          "fn": 640,
          "accuracy": 0.7090909090909091
        },
        "0.01": {
          "tp": 1175,
          "fn": 1025,
          "accuracy": 0.5340909090909091
        }
      },
      "auroc": 0.8428039772727274
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 878,
          "accuracy": 0.2683333333333333
        },
        "0.01": {
          "tp": 184,
          "fn": 1016,
          "accuracy": 0.15333333333333332
        }
      },
      "auroc": 0.5600317708333334
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1882,
          "fn": 1518,
          "accuracy": 0.5535294117647059
        },
        "0.01": {
          "tp": 1359,
          "fn": 2041,
          "accuracy": 0.3997058823529412
        }
      },
      "auroc": 0.7430020220588236
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1310,
          "fn": 890,
          "accuracy": 0.5954545454545455
        },
        "0.01": {
          "tp": 827,
          "fn": 1373,
          "accuracy": 0.3759090909090909
        }
      },
      "auroc": 0.7907363636363635
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2870,
          "fn": 1530,
          "accuracy": 0.6522727272727272
        },
        "0.01": {
          "tp": 2002,
          "fn": 2398,
          "accuracy": 0.455
        }
      },
      "auroc": 0.8167701704545455
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9542760416666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9490624999999999
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9516692708333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9541604166666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9146687499999999
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.9344145833333333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9542182291666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9318656249999999
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 786,
          "fn": 14,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 766,
          "fn": 34,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9430419270833335
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.954375
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.055627083333333355
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.5050010416666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        }
      },
      "auroc": 0.8072635416666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.036640625000000024
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 272,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 68,
          "fn": 332,
          "accuracy": 0.17
        }
      },
      "auroc": 0.42195208333333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        }
      },
      "auroc": 0.8808192708333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.04613385416666668
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 471,
          "accuracy": 0.41125
        },
        "0.01": {
          "tp": 269,
          "fn": 531,
          "accuracy": 0.33625
        }
      },
      "auroc": 0.46347656249999997
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.8884895833333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.6142052083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": {
          "tp": 159,
          "fn": 241,
          "accuracy": 0.3975
        }
      },
      "auroc": 0.7513473958333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.8779364583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.28611458333333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        },
        "0.01": {
          "tp": 130,
          "fn": 270,
          "accuracy": 0.325
        }
      },
      "auroc": 0.5820255208333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        },
        "0.01": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        }
      },
      "auroc": 0.8832130208333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 67,
          "fn": 333,
          "accuracy": 0.1675
        },
        "0.01": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        }
      },
      "auroc": 0.4501598958333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 410,
          "fn": 390,
          "accuracy": 0.5125
        },
        "0.01": {
          "tp": 289,
          "fn": 511,
          "accuracy": 0.36125
        }
      },
      "auroc": 0.6666864583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.950678125
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9494052083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9500416666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.8567718750000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.22433020833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 240,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        }
      },
      "auroc": 0.5405510416666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9037250000000001
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        }
      },
      "auroc": 0.5868677083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 559,
          "fn": 241,
          "accuracy": 0.69875
        },
        "0.01": {
          "tp": 526,
          "fn": 274,
          "accuracy": 0.6575
        }
      },
      "auroc": 0.7452963541666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.95135625
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8768791666666668
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        }
      },
      "auroc": 0.9141177083333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        },
        "0.01": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        }
      },
      "auroc": 0.7100770833333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.04097604166666669
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 77,
          "fn": 323,
          "accuracy": 0.1925
        },
        "0.01": {
          "tp": 31,
          "fn": 369,
          "accuracy": 0.0775
        }
      },
      "auroc": 0.37552656250000005
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        }
      },
      "auroc": 0.8307166666666665
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        },
        "0.01": {
          "tp": 130,
          "fn": 270,
          "accuracy": 0.325
        }
      },
      "auroc": 0.45892760416666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 446,
          "fn": 354,
          "accuracy": 0.5575
        },
        "0.01": {
          "tp": 360,
          "fn": 440,
          "accuracy": 0.45
        }
      },
      "auroc": 0.6448221354166668
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9481124999999999
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9265802083333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.9373463541666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9301625
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9391375
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9296541666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9296541666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9065708333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9065708333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        }
      },
      "auroc": 0.9181125
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        }
      },
      "auroc": 0.9181125
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.8538906249999999
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.8538906249999999
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.770346875
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.770346875
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        }
      },
      "auroc": 0.81211875
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        }
      },
      "auroc": 0.81211875
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9337385416666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9337385416666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9076062500000001
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9076062500000001
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        }
      },
      "auroc": 0.9206723958333334
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        }
      },
      "auroc": 0.9206723958333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.913703125
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.913703125
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.5291427083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.5291427083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        }
      },
      "auroc": 0.7214229166666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        }
      },
      "auroc": 0.7214229166666666
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.919134375
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.919134375
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.8745375
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.8745375
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        }
      },
      "auroc": 0.8968359375000001
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        }
      },
      "auroc": 0.8968359375000001
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2084,
          "fn": 116,
          "accuracy": 0.9472727272727273
        },
        "0.01": {
          "tp": 1940,
          "fn": 260,
          "accuracy": 0.8818181818181818
        }
      },
      "auroc": 0.9270371212121211
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 817,
          "fn": 383,
          "accuracy": 0.6808333333333333
        },
        "0.01": {
          "tp": 719,
          "fn": 481,
          "accuracy": 0.5991666666666666
        }
      },
      "auroc": 0.7286265625
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2901,
          "fn": 499,
          "accuracy": 0.8532352941176471
        },
        "0.01": {
          "tp": 2659,
          "fn": 741,
          "accuracy": 0.7820588235294118
        }
      },
      "auroc": 0.8570098651960785
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1588,
          "fn": 612,
          "accuracy": 0.7218181818181818
        },
        "0.01": {
          "tp": 1259,
          "fn": 941,
          "accuracy": 0.5722727272727273
        }
      },
      "auroc": 0.829506912878788
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3672,
          "fn": 728,
          "accuracy": 0.8345454545454546
        },
        "0.01": {
          "tp": 3199,
          "fn": 1201,
          "accuracy": 0.7270454545454546
        }
      },
      "auroc": 0.8782720170454545
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9551447916666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9532854166666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9542151041666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.95484375
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9259864583333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9404151041666666
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9549942708333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9396359375000001
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        },
        "0.01": {
          "tp": 779,
          "fn": 21,
          "accuracy": 0.97375
        }
      },
      "auroc": 0.9473151041666669
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9549624999999999
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.05363333333333335
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.5042979166666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.870703125
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.03716666666666669
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        },
        "0.01": {
          "tp": 123,
          "fn": 277,
          "accuracy": 0.3075
        }
      },
      "auroc": 0.4539348958333334
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        }
      },
      "auroc": 0.9128328125
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.045400000000000024
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 432,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 324,
          "fn": 476,
          "accuracy": 0.405
        }
      },
      "auroc": 0.4791164062499999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9230364583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.6333729166666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        },
        "0.01": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        }
      },
      "auroc": 0.7782046874999999
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.917340625
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.29618229166666665
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": {
          "tp": 178,
          "fn": 222,
          "accuracy": 0.445
        }
      },
      "auroc": 0.6067614583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        }
      },
      "auroc": 0.9201885416666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 301,
          "accuracy": 0.2475
        },
        "0.01": {
          "tp": 42,
          "fn": 358,
          "accuracy": 0.105
        }
      },
      "auroc": 0.4647776041666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 486,
          "fn": 314,
          "accuracy": 0.6075
        },
        "0.01": {
          "tp": 385,
          "fn": 415,
          "accuracy": 0.48125
        }
      },
      "auroc": 0.6924830729166667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.951225
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9521958333333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9517104166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9017979166666668
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.26219895833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        },
        "0.01": {
          "tp": 158,
          "fn": 242,
          "accuracy": 0.395
        }
      },
      "auroc": 0.5819984375
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9265114583333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        }
      },
      "auroc": 0.6071973958333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 577,
          "fn": 223,
          "accuracy": 0.72125
        },
        "0.01": {
          "tp": 551,
          "fn": 249,
          "accuracy": 0.68875
        }
      },
      "auroc": 0.7668544270833333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9551729166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9004229166666666
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        }
      },
      "auroc": 0.9277979166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.805696875
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.04054895833333335
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 278,
          "accuracy": 0.305
        },
        "0.01": {
          "tp": 79,
          "fn": 321,
          "accuracy": 0.1975
        }
      },
      "auroc": 0.4231229166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        }
      },
      "auroc": 0.8804348958333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        },
        "0.01": {
          "tp": 149,
          "fn": 251,
          "accuracy": 0.3725
        }
      },
      "auroc": 0.4704859375
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 501,
          "fn": 299,
          "accuracy": 0.62625
        },
        "0.01": {
          "tp": 428,
          "fn": 372,
          "accuracy": 0.535
        }
      },
      "auroc": 0.6754604166666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9538187499999999
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.944646875
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9492328125
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9469989583333334
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9504088541666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9396708333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9396708333333333
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9289197916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9289197916666666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9342953125000001
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9342953125000001
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9036677083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9036677083333333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        }
      },
      "auroc": 0.8499625
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        }
      },
      "auroc": 0.8499625
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        }
      },
      "auroc": 0.8768151041666666
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        }
      },
      "auroc": 0.8768151041666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9486947916666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9486947916666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9330854166666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9330854166666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9408901041666666
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9408901041666666
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9367520833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9367520833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.6208375
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.6208375
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        }
      },
      "auroc": 0.7787947916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        }
      },
      "auroc": 0.7787947916666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9386895833333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9386895833333333
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9096291666666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9096291666666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        }
      },
      "auroc": 0.924159375
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        }
      },
      "auroc": 0.924159375
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2155,
          "fn": 45,
          "accuracy": 0.9795454545454545
        },
        "0.01": {
          "tp": 2062,
          "fn": 138,
          "accuracy": 0.9372727272727273
        }
      },
      "auroc": 0.9418941287878787
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 862,
          "fn": 338,
          "accuracy": 0.7183333333333334
        },
        "0.01": {
          "tp": 774,
          "fn": 426,
          "accuracy": 0.645
        }
      },
      "auroc": 0.7395928819444445
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3017,
          "fn": 383,
          "accuracy": 0.8873529411764706
        },
        "0.01": {
          "tp": 2836,
          "fn": 564,
          "accuracy": 0.8341176470588235
        }
      },
      "auroc": 0.8704936887254903
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1819,
          "fn": 381,
          "accuracy": 0.8268181818181818
        },
        "0.01": {
          "tp": 1579,
          "fn": 621,
          "accuracy": 0.7177272727272728
        }
      },
      "auroc": 0.876346875
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3974,
          "fn": 426,
          "accuracy": 0.9031818181818182
        },
        "0.01": {
          "tp": 3641,
          "fn": 759,
          "accuracy": 0.8275
        }
      },
      "auroc": 0.9091205018939394
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9554166666666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9554166666666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9554166666666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9554166666666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9554166666666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9554166666666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9554166666666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9554166666666667
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9554166666666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9550802083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.952534375
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9538072916666667
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.95365
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9384968749999999
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9460734375000001
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9543651041666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.945515625
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9499403645833334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9542895833333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9529604166666666
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9536250000000001
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.953434375
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.951165625
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9523
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9538619791666667
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9520630208333334
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        }
      },
      "auroc": 0.9529624999999999
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.953609375
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9551802083333333
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9543947916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9521895833333334
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9457479166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.94896875
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9528994791666667
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9504640625
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        },
        "0.01": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9516817708333333
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9550010416666668
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9539552083333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.954478125
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9541041666666668
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9520833333333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.95309375
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9545526041666667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9530192708333334
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        }
      },
      "auroc": 0.9537859375
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9554166666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9554166666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9554166666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9554166666666667
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9554166666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.948990625
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.948990625
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9494947916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9494947916666667
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9492427083333334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9492427083333334
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9550343750000001
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9550343750000001
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9549624999999999
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9549624999999999
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9549984375
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9549984375
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9554166666666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9554166666666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9554166666666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9554166666666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9554166666666667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9554166666666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9551729166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9551729166666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9550510416666668
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9550510416666668
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9551119791666667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9551119791666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9545072916666666
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9545072916666666
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9537604166666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9537604166666667
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9541338541666666
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9541338541666666
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2191,
          "fn": 9,
          "accuracy": 0.9959090909090909
        }
      },
      "auroc": 0.9543577651515152
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1199,
          "fn": 1,
          "accuracy": 0.9991666666666666
        },
        "0.01": {
          "tp": 1195,
          "fn": 5,
          "accuracy": 0.9958333333333333
        }
      },
      "auroc": 0.9542439236111111
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3399,
          "fn": 1,
          "accuracy": 0.9997058823529412
        },
        "0.01": {
          "tp": 3386,
          "fn": 14,
          "accuracy": 0.9958823529411764
        }
      },
      "auroc": 0.9543175857843137
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2198,
          "fn": 2,
          "accuracy": 0.9990909090909091
        },
        "0.01": {
          "tp": 2191,
          "fn": 9,
          "accuracy": 0.9959090909090909
        }
      },
      "auroc": 0.953899715909091
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4398,
          "fn": 2,
          "accuracy": 0.9995454545454545
        },
        "0.01": {
          "tp": 4382,
          "fn": 18,
          "accuracy": 0.9959090909090909
        }
      },
      "auroc": 0.954128740530303
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2379,
          "fn": 21,
          "accuracy": 0.99125
        },
        "0.01": {
          "tp": 2346,
          "fn": 54,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.949777170138889
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2268,
          "fn": 132,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 2152,
          "fn": 248,
          "accuracy": 0.8966666666666666
        }
      },
      "auroc": 0.9303020833333334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4647,
          "fn": 153,
          "accuracy": 0.968125
        },
        "0.01": {
          "tp": 4498,
          "fn": 302,
          "accuracy": 0.9370833333333334
        }
      },
      "auroc": 0.9400396267361111
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2375,
          "fn": 25,
          "accuracy": 0.9895833333333334
        },
        "0.01": {
          "tp": 2328,
          "fn": 72,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9482636284722222
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2074,
          "fn": 326,
          "accuracy": 0.8641666666666666
        },
        "0.01": {
          "tp": 1886,
          "fn": 514,
          "accuracy": 0.7858333333333334
        }
      },
      "auroc": 0.8916544270833333
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4449,
          "fn": 351,
          "accuracy": 0.926875
        },
        "0.01": {
          "tp": 4214,
          "fn": 586,
          "accuracy": 0.8779166666666667
        }
      },
      "auroc": 0.9199590277777778
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4754,
          "fn": 46,
          "accuracy": 0.9904166666666666
        },
        "0.01": {
          "tp": 4674,
          "fn": 126,
          "accuracy": 0.97375
        }
      },
      "auroc": 0.9490203993055556
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4342,
          "fn": 458,
          "accuracy": 0.9045833333333333
        },
        "0.01": {
          "tp": 4038,
          "fn": 762,
          "accuracy": 0.84125
        }
      },
      "auroc": 0.9109782552083334
    },
    {
      "domain": "abstracts",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9096,
          "fn": 504,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 8712,
          "fn": 888,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9299993272569445
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2396,
          "fn": 4,
          "accuracy": 0.9983333333333333
        },
        "0.01": {
          "tp": 2383,
          "fn": 17,
          "accuracy": 0.9929166666666667
        }
      },
      "auroc": 0.953327517361111
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 254,
          "fn": 2146,
          "accuracy": 0.10583333333333333
        },
        "0.01": {
          "tp": 230,
          "fn": 2170,
          "accuracy": 0.09583333333333334
        }
      },
      "auroc": 0.17694539930555558
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2650,
          "fn": 2150,
          "accuracy": 0.5520833333333334
        },
        "0.01": {
          "tp": 2613,
          "fn": 2187,
          "accuracy": 0.544375
        }
      },
      "auroc": 0.5651364583333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1848,
          "fn": 552,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 1398,
          "fn": 1002,
          "accuracy": 0.5825
        }
      },
      "auroc": 0.8518677083333333
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 228,
          "fn": 2172,
          "accuracy": 0.095
        },
        "0.01": {
          "tp": 205,
          "fn": 2195,
          "accuracy": 0.08541666666666667
        }
      },
      "auroc": 0.15815000000000004
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2076,
          "fn": 2724,
          "accuracy": 0.4325
        },
        "0.01": {
          "tp": 1603,
          "fn": 3197,
          "accuracy": 0.33395833333333336
        }
      },
      "auroc": 0.5050088541666666
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4244,
          "fn": 556,
          "accuracy": 0.8841666666666667
        },
        "0.01": {
          "tp": 3781,
          "fn": 1019,
          "accuracy": 0.7877083333333333
        }
      },
      "auroc": 0.9025976128472221
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 482,
          "fn": 4318,
          "accuracy": 0.10041666666666667
        },
        "0.01": {
          "tp": 435,
          "fn": 4365,
          "accuracy": 0.090625
        }
      },
      "auroc": 0.16754769965277783
    },
    {
      "domain": "abstracts",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4726,
          "fn": 4874,
          "accuracy": 0.4922916666666667
        },
        "0.01": {
          "tp": 4216,
          "fn": 5384,
          "accuracy": 0.43916666666666665
        }
      },
      "auroc": 0.53507265625
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2076,
          "fn": 324,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 1735,
          "fn": 665,
          "accuracy": 0.7229166666666667
        }
      },
      "auroc": 0.8890638888888888
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 902,
          "fn": 1498,
          "accuracy": 0.37583333333333335
        },
        "0.01": {
          "tp": 513,
          "fn": 1887,
          "accuracy": 0.21375
        }
      },
      "auroc": 0.6203171875
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2978,
          "fn": 1822,
          "accuracy": 0.6204166666666666
        },
        "0.01": {
          "tp": 2248,
          "fn": 2552,
          "accuracy": 0.4683333333333333
        }
      },
      "auroc": 0.7546905381944445
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2019,
          "fn": 381,
          "accuracy": 0.84125
        },
        "0.01": {
          "tp": 1636,
          "fn": 764,
          "accuracy": 0.6816666666666666
        }
      },
      "auroc": 0.8801956597222222
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 430,
          "fn": 1970,
          "accuracy": 0.17916666666666667
        },
        "0.01": {
          "tp": 291,
          "fn": 2109,
          "accuracy": 0.12125
        }
      },
      "auroc": 0.3749413194444445
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2449,
          "fn": 2351,
          "accuracy": 0.5102083333333334
        },
        "0.01": {
          "tp": 1927,
          "fn": 2873,
          "accuracy": 0.4014583333333333
        }
      },
      "auroc": 0.6275684895833333
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4095,
          "fn": 705,
          "accuracy": 0.853125
        },
        "0.01": {
          "tp": 3371,
          "fn": 1429,
          "accuracy": 0.7022916666666666
        }
      },
      "auroc": 0.8846297743055556
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1332,
          "fn": 3468,
          "accuracy": 0.2775
        },
        "0.01": {
          "tp": 804,
          "fn": 3996,
          "accuracy": 0.1675
        }
      },
      "auroc": 0.4976292534722222
    },
    {
      "domain": "abstracts",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5427,
          "fn": 4173,
          "accuracy": 0.5653125
        },
        "0.01": {
          "tp": 4175,
          "fn": 5425,
          "accuracy": 0.4348958333333333
        }
      },
      "auroc": 0.6911295138888889
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2372,
          "fn": 28,
          "accuracy": 0.9883333333333333
        },
        "0.01": {
          "tp": 2338,
          "fn": 62,
          "accuracy": 0.9741666666666666
        }
      },
      "auroc": 0.9477844618055556
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2313,
          "fn": 87,
          "accuracy": 0.96375
        },
        "0.01": {
          "tp": 2213,
          "fn": 187,
          "accuracy": 0.9220833333333334
        }
      },
      "auroc": 0.9382935763888889
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4685,
          "fn": 115,
          "accuracy": 0.9760416666666667
        },
        "0.01": {
          "tp": 4551,
          "fn": 249,
          "accuracy": 0.948125
        }
      },
      "auroc": 0.9430390190972221
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2098,
          "fn": 302,
          "accuracy": 0.8741666666666666
        },
        "0.01": {
          "tp": 1839,
          "fn": 561,
          "accuracy": 0.76625
        }
      },
      "auroc": 0.8945795138888887
    },
    {
      "domain": "abstracts",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4470,
          "fn": 330,
          "accuracy": 0.93125
        },
        "0.01": {
          "tp": 4177,
          "fn": 623,
          "accuracy": 0.8702083333333334
        }
      },
      "auroc": 0.9211819878472223
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2375,
          "fn": 25,
          "accuracy": 0.9895833333333334
        },
        "0.01": {
          "tp": 2337,
          "fn": 63,
          "accuracy": 0.97375
        }
      },
      "auroc": 0.9487134548611111
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1830,
          "fn": 570,
          "accuracy": 0.7625
        },
        "0.01": {
          "tp": 1499,
          "fn": 901,
          "accuracy": 0.6245833333333334
        }
      },
      "auroc": 0.8452796006944444
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4205,
          "fn": 595,
          "accuracy": 0.8760416666666667
        },
        "0.01": {
          "tp": 3836,
          "fn": 964,
          "accuracy": 0.7991666666666667
        }
      },
      "auroc": 0.8969965277777777
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1438,
          "fn": 962,
          "accuracy": 0.5991666666666666
        },
        "0.01": {
          "tp": 977,
          "fn": 1423,
          "accuracy": 0.40708333333333335
        }
      },
      "auroc": 0.7882908854166667
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 231,
          "fn": 2169,
          "accuracy": 0.09625
        },
        "0.01": {
          "tp": 209,
          "fn": 2191,
          "accuracy": 0.08708333333333333
        }
      },
      "auroc": 0.17641927083333336
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1669,
          "fn": 3131,
          "accuracy": 0.34770833333333334
        },
        "0.01": {
          "tp": 1186,
          "fn": 3614,
          "accuracy": 0.24708333333333332
        }
      },
      "auroc": 0.48235507812499995
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3813,
          "fn": 987,
          "accuracy": 0.794375
        },
        "0.01": {
          "tp": 3314,
          "fn": 1486,
          "accuracy": 0.6904166666666667
        }
      },
      "auroc": 0.8685021701388889
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2061,
          "fn": 2739,
          "accuracy": 0.429375
        },
        "0.01": {
          "tp": 1708,
          "fn": 3092,
          "accuracy": 0.35583333333333333
        }
      },
      "auroc": 0.510849435763889
    },
    {
      "domain": "abstracts",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5874,
          "fn": 3726,
          "accuracy": 0.611875
        },
        "0.01": {
          "tp": 5022,
          "fn": 4578,
          "accuracy": 0.523125
        }
      },
      "auroc": 0.6896758029513889
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2336,
          "fn": 64,
          "accuracy": 0.9733333333333334
        },
        "0.01": {
          "tp": 2225,
          "fn": 175,
          "accuracy": 0.9270833333333334
        }
      },
      "auroc": 0.9396628472222223
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2110,
          "fn": 290,
          "accuracy": 0.8791666666666667
        },
        "0.01": {
          "tp": 1936,
          "fn": 464,
          "accuracy": 0.8066666666666666
        }
      },
      "auroc": 0.9025036458333333
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4446,
          "fn": 354,
          "accuracy": 0.92625
        },
        "0.01": {
          "tp": 4161,
          "fn": 639,
          "accuracy": 0.866875
        }
      },
      "auroc": 0.9210832465277777
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2264,
          "fn": 136,
          "accuracy": 0.9433333333333334
        },
        "0.01": {
          "tp": 2079,
          "fn": 321,
          "accuracy": 0.86625
        }
      },
      "auroc": 0.9249103298611111
    },
    {
      "domain": "abstracts",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4600,
          "fn": 200,
          "accuracy": 0.9583333333333334
        },
        "0.01": {
          "tp": 4304,
          "fn": 496,
          "accuracy": 0.8966666666666666
        }
      },
      "auroc": 0.9322865885416666
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2209,
          "fn": 191,
          "accuracy": 0.9204166666666667
        },
        "0.01": {
          "tp": 2034,
          "fn": 366,
          "accuracy": 0.8475
        }
      },
      "auroc": 0.9162716145833334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2209,
          "fn": 191,
          "accuracy": 0.9204166666666667
        },
        "0.01": {
          "tp": 2034,
          "fn": 366,
          "accuracy": 0.8475
        }
      },
      "auroc": 0.9162716145833334
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2148,
          "fn": 252,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 1919,
          "fn": 481,
          "accuracy": 0.7995833333333333
        }
      },
      "auroc": 0.900248263888889
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2148,
          "fn": 252,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 1919,
          "fn": 481,
          "accuracy": 0.7995833333333333
        }
      },
      "auroc": 0.900248263888889
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4357,
          "fn": 443,
          "accuracy": 0.9077083333333333
        },
        "0.01": {
          "tp": 3953,
          "fn": 847,
          "accuracy": 0.8235416666666666
        }
      },
      "auroc": 0.908259939236111
    },
    {
      "domain": "abstracts",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4357,
          "fn": 443,
          "accuracy": 0.9077083333333333
        },
        "0.01": {
          "tp": 3953,
          "fn": 847,
          "accuracy": 0.8235416666666666
        }
      },
      "auroc": 0.908259939236111
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1933,
          "fn": 467,
          "accuracy": 0.8054166666666667
        },
        "0.01": {
          "tp": 1595,
          "fn": 805,
          "accuracy": 0.6645833333333333
        }
      },
      "auroc": 0.8765032118055556
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1933,
          "fn": 467,
          "accuracy": 0.8054166666666667
        },
        "0.01": {
          "tp": 1595,
          "fn": 805,
          "accuracy": 0.6645833333333333
        }
      },
      "auroc": 0.8765032118055556
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1684,
          "fn": 716,
          "accuracy": 0.7016666666666667
        },
        "0.01": {
          "tp": 1273,
          "fn": 1127,
          "accuracy": 0.5304166666666666
        }
      },
      "auroc": 0.8277106770833333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1684,
          "fn": 716,
          "accuracy": 0.7016666666666667
        },
        "0.01": {
          "tp": 1273,
          "fn": 1127,
          "accuracy": 0.5304166666666666
        }
      },
      "auroc": 0.8277106770833333
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3617,
          "fn": 1183,
          "accuracy": 0.7535416666666667
        },
        "0.01": {
          "tp": 2868,
          "fn": 1932,
          "accuracy": 0.5975
        }
      },
      "auroc": 0.8521069444444445
    },
    {
      "domain": "abstracts",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3617,
          "fn": 1183,
          "accuracy": 0.7535416666666667
        },
        "0.01": {
          "tp": 2868,
          "fn": 1932,
          "accuracy": 0.5975
        }
      },
      "auroc": 0.8521069444444445
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2211,
          "fn": 189,
          "accuracy": 0.92125
        },
        "0.01": {
          "tp": 2019,
          "fn": 381,
          "accuracy": 0.84125
        }
      },
      "auroc": 0.9203415798611112
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2211,
          "fn": 189,
          "accuracy": 0.92125
        },
        "0.01": {
          "tp": 2019,
          "fn": 381,
          "accuracy": 0.84125
        }
      },
      "auroc": 0.9203415798611112
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2118,
          "fn": 282,
          "accuracy": 0.8825
        },
        "0.01": {
          "tp": 1818,
          "fn": 582,
          "accuracy": 0.7575
        }
      },
      "auroc": 0.8994440972222222
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2118,
          "fn": 282,
          "accuracy": 0.8825
        },
        "0.01": {
          "tp": 1818,
          "fn": 582,
          "accuracy": 0.7575
        }
      },
      "auroc": 0.8994440972222222
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4329,
          "fn": 471,
          "accuracy": 0.901875
        },
        "0.01": {
          "tp": 3837,
          "fn": 963,
          "accuracy": 0.799375
        }
      },
      "auroc": 0.9098928385416667
    },
    {
      "domain": "abstracts",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4329,
          "fn": 471,
          "accuracy": 0.901875
        },
        "0.01": {
          "tp": 3837,
          "fn": 963,
          "accuracy": 0.799375
        }
      },
      "auroc": 0.9098928385416667
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2131,
          "fn": 269,
          "accuracy": 0.8879166666666667
        },
        "0.01": {
          "tp": 1861,
          "fn": 539,
          "accuracy": 0.7754166666666666
        }
      },
      "auroc": 0.9031610243055556
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2131,
          "fn": 269,
          "accuracy": 0.8879166666666667
        },
        "0.01": {
          "tp": 1861,
          "fn": 539,
          "accuracy": 0.7754166666666666
        }
      },
      "auroc": 0.9031610243055556
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 690,
          "fn": 1710,
          "accuracy": 0.2875
        },
        "0.01": {
          "tp": 407,
          "fn": 1993,
          "accuracy": 0.16958333333333334
        }
      },
      "auroc": 0.6363784722222222
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 690,
          "fn": 1710,
          "accuracy": 0.2875
        },
        "0.01": {
          "tp": 407,
          "fn": 1993,
          "accuracy": 0.16958333333333334
        }
      },
      "auroc": 0.6363784722222222
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2821,
          "fn": 1979,
          "accuracy": 0.5877083333333334
        },
        "0.01": {
          "tp": 2268,
          "fn": 2532,
          "accuracy": 0.4725
        }
      },
      "auroc": 0.7697697482638888
    },
    {
      "domain": "abstracts",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2821,
          "fn": 1979,
          "accuracy": 0.5877083333333334
        },
        "0.01": {
          "tp": 2268,
          "fn": 2532,
          "accuracy": 0.4725
        }
      },
      "auroc": 0.7697697482638888
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2230,
          "fn": 170,
          "accuracy": 0.9291666666666667
        },
        "0.01": {
          "tp": 2021,
          "fn": 379,
          "accuracy": 0.8420833333333333
        }
      },
      "auroc": 0.9200513020833332
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2230,
          "fn": 170,
          "accuracy": 0.9291666666666667
        },
        "0.01": {
          "tp": 2021,
          "fn": 379,
          "accuracy": 0.8420833333333333
        }
      },
      "auroc": 0.9200513020833332
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2036,
          "fn": 364,
          "accuracy": 0.8483333333333334
        },
        "0.01": {
          "tp": 1745,
          "fn": 655,
          "accuracy": 0.7270833333333333
        }
      },
      "auroc": 0.8900848958333332
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2036,
          "fn": 364,
          "accuracy": 0.8483333333333334
        },
        "0.01": {
          "tp": 1745,
          "fn": 655,
          "accuracy": 0.7270833333333333
        }
      },
      "auroc": 0.8900848958333332
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4266,
          "fn": 534,
          "accuracy": 0.88875
        },
        "0.01": {
          "tp": 3766,
          "fn": 1034,
          "accuracy": 0.7845833333333333
        }
      },
      "auroc": 0.9050680989583334
    },
    {
      "domain": "abstracts",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4266,
          "fn": 534,
          "accuracy": 0.88875
        },
        "0.01": {
          "tp": 3766,
          "fn": 1034,
          "accuracy": 0.7845833333333333
        }
      },
      "auroc": 0.9050680989583334
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 24648,
          "fn": 1752,
          "accuracy": 0.9336363636363636
        },
        "0.01": {
          "tp": 22894,
          "fn": 3506,
          "accuracy": 0.8671969696969697
        }
      },
      "auroc": 0.924059824810606
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9677,
          "fn": 4723,
          "accuracy": 0.6720138888888889
        },
        "0.01": {
          "tp": 8543,
          "fn": 5857,
          "accuracy": 0.5932638888888889
        }
      },
      "auroc": 0.7356069155092593
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 34325,
          "fn": 6475,
          "accuracy": 0.8412990196078431
        },
        "0.01": {
          "tp": 31437,
          "fn": 9363,
          "accuracy": 0.7705147058823529
        }
      },
      "auroc": 0.8575470332924837
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 20718,
          "fn": 5682,
          "accuracy": 0.7847727272727273
        },
        "0.01": {
          "tp": 17419,
          "fn": 8981,
          "accuracy": 0.659810606060606
        }
      },
      "auroc": 0.8583612847222222
    },
    {
      "domain": "abstracts",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 45366,
          "fn": 7434,
          "accuracy": 0.8592045454545455
        },
        "0.01": {
          "tp": 40313,
          "fn": 12487,
          "accuracy": 0.7635037878787879
        }
      },
      "auroc": 0.8912105547664141
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.8542093749999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.8977296874999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.8977296874999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 772,
          "fn": 28,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9194898437500001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.055055208333333314
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.4981526041666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9136010416666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.032812499999999974
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        },
        "0.01": null
      },
      "auroc": 0.4732067708333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9274255208333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.04393385416666664
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 409,
          "accuracy": 0.48875
        },
        "0.01": null
      },
      "auroc": 0.4856796875
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.94094375
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": null
      },
      "auroc": 0.6903145833333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": null
      },
      "auroc": 0.8156291666666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.938590625
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        },
        "0.01": null
      },
      "auroc": 0.2927197916666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        },
        "0.01": null
      },
      "auroc": 0.6156552083333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9397671875
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 270,
          "accuracy": 0.325
        },
        "0.01": null
      },
      "auroc": 0.49151718749999995
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 529,
          "fn": 271,
          "accuracy": 0.66125
        },
        "0.01": null
      },
      "auroc": 0.7156421875
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9272072916666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": null
      },
      "auroc": 0.42942395833333336
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": null
      },
      "auroc": 0.678315625
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9342286458333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": null
      },
      "auroc": 0.6853369791666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 601,
          "fn": 199,
          "accuracy": 0.75125
        },
        "0.01": null
      },
      "auroc": 0.8097828125
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": null
      },
      "auroc": 0.8347239583333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.8879869791666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.901371875
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.04993854166666664
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        },
        "0.01": null
      },
      "auroc": 0.4756552083333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9213109375
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 242,
          "accuracy": 0.395
        },
        "0.01": null
      },
      "auroc": 0.44233125
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 540,
          "fn": 260,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.6818210937499999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.940521875
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9408859375
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.94045625
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": null
      },
      "auroc": 0.854996875
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.8977265625000002
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9408531250000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.8977593750000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 758,
          "fn": 42,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9193062500000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9341145833333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9341145833333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9247885416666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9247885416666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9294515624999999
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9294515624999999
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9258031250000001
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9258031250000001
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.8877375000000001
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.8877375000000001
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9067703125000001
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9067703125000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9353552083333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9353552083333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9383026041666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9383026041666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": null
      },
      "auroc": 0.724071875
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": null
      },
      "auroc": 0.724071875
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 271,
          "fn": 129,
          "accuracy": 0.6775
        },
        "0.01": null
      },
      "auroc": 0.8326609375
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 271,
          "fn": 129,
          "accuracy": 0.6775
        },
        "0.01": null
      },
      "auroc": 0.8326609375
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.928321875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.928321875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9094708333333332
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9094708333333332
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9188963541666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9188963541666666
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2183,
          "fn": 17,
          "accuracy": 0.9922727272727273
        },
        "0.01": null
      },
      "auroc": 0.9379939393939394
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 863,
          "fn": 337,
          "accuracy": 0.7191666666666666
        },
        "0.01": null
      },
      "auroc": 0.7338526041666668
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3046,
          "fn": 354,
          "accuracy": 0.8958823529411765
        },
        "0.01": null
      },
      "auroc": 0.8659440563725492
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1984,
          "fn": 216,
          "accuracy": 0.9018181818181819
        },
        "0.01": null
      },
      "auroc": 0.9039910037878788
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 838,
          "accuracy": 0.3016666666666667
        },
        "0.01": null
      },
      "auroc": 0.41901684027777775
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2346,
          "fn": 1054,
          "accuracy": 0.69
        },
        "0.01": null
      },
      "auroc": 0.7328236519607843
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 4167,
          "fn": 233,
          "accuracy": 0.9470454545454545
        },
        "0.01": null
      },
      "auroc": 0.9209924715909092
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1225,
          "fn": 1175,
          "accuracy": 0.5104166666666666
        },
        "0.01": null
      },
      "auroc": 0.5764347222222221
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 5392,
          "fn": 1408,
          "accuracy": 0.7929411764705883
        },
        "0.01": null
      },
      "auroc": 0.7993838541666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9402916666666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9407708333333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9408281249999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.8327229166666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.8867755208333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9410390625
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.8865072916666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 764,
          "fn": 36,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9137731770833333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.045324999999999976
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.4932875
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.8880145833333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.03087291666666664
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        },
        "0.01": null
      },
      "auroc": 0.45944375
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": null
      },
      "auroc": 0.9146322916666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.03809895833333331
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 427,
          "accuracy": 0.46625
        },
        "0.01": null
      },
      "auroc": 0.4763656249999999
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9223052083333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        },
        "0.01": null
      },
      "auroc": 0.596778125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        },
        "0.01": null
      },
      "auroc": 0.7595416666666668
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.915815625
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": null
      },
      "auroc": 0.23652291666666664
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": null
      },
      "auroc": 0.5761692708333332
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9190604166666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 75,
          "fn": 325,
          "accuracy": 0.1875
        },
        "0.01": null
      },
      "auroc": 0.4166505208333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 465,
          "fn": 335,
          "accuracy": 0.58125
        },
        "0.01": null
      },
      "auroc": 0.66785546875
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9363052083333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9392958333333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9378005208333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9188333333333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.4169677083333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        },
        "0.01": null
      },
      "auroc": 0.6679005208333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9275692708333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.6781317708333332
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 590,
          "fn": 210,
          "accuracy": 0.7375
        },
        "0.01": null
      },
      "auroc": 0.8028505208333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9406875
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": null
      },
      "auroc": 0.782765625
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.8617265625000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.8680020833333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.048442708333333306
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 236,
          "accuracy": 0.41
        },
        "0.01": null
      },
      "auroc": 0.45822239583333335
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.9043447916666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 264,
          "accuracy": 0.34
        },
        "0.01": null
      },
      "auroc": 0.41560416666666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 500,
          "fn": 300,
          "accuracy": 0.625
        },
        "0.01": null
      },
      "auroc": 0.6599744791666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9401854166666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9342656249999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9372255208333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9361645833333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": null
      },
      "auroc": 0.7947760416666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        },
        "0.01": null
      },
      "auroc": 0.8654703125000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.938175
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        },
        "0.01": null
      },
      "auroc": 0.8645208333333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 713,
          "fn": 87,
          "accuracy": 0.89125
        },
        "0.01": null
      },
      "auroc": 0.9013479166666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9072437500000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9072437500000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.8941885416666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.8941885416666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": null
      },
      "auroc": 0.9007161458333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": null
      },
      "auroc": 0.9007161458333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9062802083333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9062802083333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.8588875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.8588875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.8825838541666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.8825838541666666
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9367958333333334
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9367958333333334
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9242854166666666
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9242854166666666
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.930540625
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.930540625
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9401958333333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9401958333333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        },
        "0.01": null
      },
      "auroc": 0.6900458333333332
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        },
        "0.01": null
      },
      "auroc": 0.6900458333333332
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        },
        "0.01": null
      },
      "auroc": 0.8151208333333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        },
        "0.01": null
      },
      "auroc": 0.8151208333333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9184583333333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9184583333333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": null
      },
      "auroc": 0.8794052083333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": null
      },
      "auroc": 0.8794052083333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.8989317708333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.8989317708333333
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2145,
          "fn": 55,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9300870265151515
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 801,
          "fn": 399,
          "accuracy": 0.6675
        },
        "0.01": null
      },
      "auroc": 0.7064536458333334
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2946,
          "fn": 454,
          "accuracy": 0.8664705882352941
        },
        "0.01": null
      },
      "auroc": 0.8511575980392158
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1865,
          "fn": 335,
          "accuracy": 0.8477272727272728
        },
        "0.01": null
      },
      "auroc": 0.8831337121212121
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 291,
          "fn": 909,
          "accuracy": 0.2425
        },
        "0.01": null
      },
      "auroc": 0.39338420138888897
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2156,
          "fn": 1244,
          "accuracy": 0.6341176470588236
        },
        "0.01": null
      },
      "auroc": 0.710280943627451
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 4010,
          "fn": 390,
          "accuracy": 0.9113636363636364
        },
        "0.01": null
      },
      "auroc": 0.906610369318182
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1092,
          "fn": 1308,
          "accuracy": 0.455
        },
        "0.01": null
      },
      "auroc": 0.549918923611111
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 5102,
          "fn": 1698,
          "accuracy": 0.7502941176470588
        },
        "0.01": null
      },
      "auroc": 0.7807192708333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.939278125
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9402640625
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9408281249999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.7963479166666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.8685880208333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9410390625
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.8678130208333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 752,
          "fn": 48,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9044260416666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9400166666666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.04920416666666664
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.4946104166666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": null
      },
      "auroc": 0.8423791666666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.03194999999999997
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 257,
          "accuracy": 0.3575
        },
        "0.01": null
      },
      "auroc": 0.4371645833333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        },
        "0.01": null
      },
      "auroc": 0.8911979166666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.04057708333333331
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 343,
          "fn": 457,
          "accuracy": 0.42875
        },
        "0.01": null
      },
      "auroc": 0.4658875
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9136645833333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        },
        "0.01": null
      },
      "auroc": 0.5486125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        },
        "0.01": null
      },
      "auroc": 0.7311385416666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.8941416666666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.2063520833333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        },
        "0.01": null
      },
      "auroc": 0.550246875
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.903903125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 358,
          "accuracy": 0.105
        },
        "0.01": null
      },
      "auroc": 0.37748229166666664
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 412,
          "fn": 388,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.6406927083333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.932521875
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9405739583333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9365479166666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9151322916666665
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.30970833333333336
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        },
        "0.01": null
      },
      "auroc": 0.6124203125000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9238270833333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.6251411458333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 589,
          "fn": 211,
          "accuracy": 0.73625
        },
        "0.01": null
      },
      "auroc": 0.7744841145833333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9398187499999999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.7255197916666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.8326692708333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.8209677083333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.03839270833333331
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 270,
          "accuracy": 0.325
        },
        "0.01": null
      },
      "auroc": 0.42968020833333337
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.8803932291666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 294,
          "accuracy": 0.265
        },
        "0.01": null
      },
      "auroc": 0.38195625000000005
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 436,
          "fn": 364,
          "accuracy": 0.545
        },
        "0.01": null
      },
      "auroc": 0.6311747395833334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9389270833333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.929340625
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9341338541666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9321197916666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": null
      },
      "auroc": 0.7338166666666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 291,
          "fn": 109,
          "accuracy": 0.7275
        },
        "0.01": null
      },
      "auroc": 0.8329682291666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9355234375
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        },
        "0.01": null
      },
      "auroc": 0.8315786458333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 684,
          "fn": 116,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.8835510416666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9084114583333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9084114583333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.8784270833333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.8784270833333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.8934192708333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.8934192708333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.8794208333333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.8794208333333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": null
      },
      "auroc": 0.8030270833333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": null
      },
      "auroc": 0.8030270833333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.8412239583333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.8412239583333334
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9313781250000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9313781250000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.8930416666666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.8930416666666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9122098958333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9122098958333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9351760416666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9351760416666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": null
      },
      "auroc": 0.558165625
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": null
      },
      "auroc": 0.558165625
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": null
      },
      "auroc": 0.7466708333333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": null
      },
      "auroc": 0.7466708333333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9068604166666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9068604166666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.8576083333333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.8576083333333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": null
      },
      "auroc": 0.882234375
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": null
      },
      "auroc": 0.882234375
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2116,
          "fn": 84,
          "accuracy": 0.9618181818181818
        },
        "0.01": null
      },
      "auroc": 0.9243132575757576
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 739,
          "fn": 461,
          "accuracy": 0.6158333333333333
        },
        "0.01": null
      },
      "auroc": 0.6887548611111111
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2855,
          "fn": 545,
          "accuracy": 0.8397058823529412
        },
        "0.01": null
      },
      "auroc": 0.841175
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1691,
          "fn": 509,
          "accuracy": 0.7686363636363637
        },
        "0.01": null
      },
      "auroc": 0.8487125946969697
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 951,
          "accuracy": 0.2075
        },
        "0.01": null
      },
      "auroc": 0.35276128472222223
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1940,
          "fn": 1460,
          "accuracy": 0.5705882352941176
        },
        "0.01": null
      },
      "auroc": 0.6736709558823529
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3807,
          "fn": 593,
          "accuracy": 0.8652272727272727
        },
        "0.01": null
      },
      "auroc": 0.8865129261363637
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 988,
          "fn": 1412,
          "accuracy": 0.4116666666666667
        },
        "0.01": null
      },
      "auroc": 0.5207580729166666
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 4795,
          "fn": 2005,
          "accuracy": 0.7051470588235295
        },
        "0.01": null
      },
      "auroc": 0.7574229779411765
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9215187499999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": null
      },
      "auroc": 0.7837770833333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 301,
          "fn": 99,
          "accuracy": 0.7525
        },
        "0.01": null
      },
      "auroc": 0.8526479166666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9067218750000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        },
        "0.01": null
      },
      "auroc": 0.5345145833333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 228,
          "fn": 172,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.7206182291666665
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9141203124999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 251,
          "accuracy": 0.3725
        },
        "0.01": null
      },
      "auroc": 0.6591458333333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 529,
          "fn": 271,
          "accuracy": 0.66125
        },
        "0.01": null
      },
      "auroc": 0.7866330729166667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9392010416666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.03205104166666664
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": null
      },
      "auroc": 0.4856260416666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": null
      },
      "auroc": 0.6243333333333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.03214062499999997
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 355,
          "accuracy": 0.1125
        },
        "0.01": null
      },
      "auroc": 0.32823697916666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        },
        "0.01": null
      },
      "auroc": 0.7817671875000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.03209583333333331
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 556,
          "accuracy": 0.305
        },
        "0.01": null
      },
      "auroc": 0.4069315104166667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": null
      },
      "auroc": 0.7605072916666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.28841666666666665
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 289,
          "accuracy": 0.2775
        },
        "0.01": null
      },
      "auroc": 0.5244619791666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        },
        "0.01": null
      },
      "auroc": 0.7182614583333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.10396979166666664
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 319,
          "accuracy": 0.2025
        },
        "0.01": null
      },
      "auroc": 0.4111156250000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        },
        "0.01": null
      },
      "auroc": 0.739384375
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": null
      },
      "auroc": 0.19619322916666665
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 608,
          "accuracy": 0.24
        },
        "0.01": null
      },
      "auroc": 0.4677888020833333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9390958333333332
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9127697916666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9259328125
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": null
      },
      "auroc": 0.812875
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.10426666666666665
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 274,
          "accuracy": 0.315
        },
        "0.01": null
      },
      "auroc": 0.45857083333333337
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.8759854166666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": null
      },
      "auroc": 0.5085182291666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 513,
          "fn": 287,
          "accuracy": 0.64125
        },
        "0.01": null
      },
      "auroc": 0.6922518229166666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9396135416666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": null
      },
      "auroc": 0.35090416666666663
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        },
        "0.01": null
      },
      "auroc": 0.6452588541666665
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        },
        "0.01": null
      },
      "auroc": 0.6152885416666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.028755208333333306
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 362,
          "accuracy": 0.095
        },
        "0.01": null
      },
      "auroc": 0.322021875
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": null
      },
      "auroc": 0.7774510416666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.18982968749999998
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 246,
          "fn": 554,
          "accuracy": 0.3075
        },
        "0.01": null
      },
      "auroc": 0.4836403645833334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.904959375
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": null
      },
      "auroc": 0.77015
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        },
        "0.01": null
      },
      "auroc": 0.8375546875000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.8455406250000002
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        },
        "0.01": null
      },
      "auroc": 0.39886666666666665
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 237,
          "accuracy": 0.4075
        },
        "0.01": null
      },
      "auroc": 0.6222036458333332
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": null
      },
      "auroc": 0.8752500000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        },
        "0.01": null
      },
      "auroc": 0.5845083333333332
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 449,
          "fn": 351,
          "accuracy": 0.56125
        },
        "0.01": null
      },
      "auroc": 0.7298791666666669
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.8059208333333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.8059208333333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        },
        "0.01": null
      },
      "auroc": 0.7514031250000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        },
        "0.01": null
      },
      "auroc": 0.7514031250000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        },
        "0.01": null
      },
      "auroc": 0.7786619791666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        },
        "0.01": null
      },
      "auroc": 0.7786619791666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        },
        "0.01": null
      },
      "auroc": 0.721978125
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        },
        "0.01": null
      },
      "auroc": 0.721978125
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        },
        "0.01": null
      },
      "auroc": 0.622059375
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        },
        "0.01": null
      },
      "auroc": 0.622059375
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 272,
          "accuracy": 0.32
        },
        "0.01": null
      },
      "auroc": 0.67201875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 272,
          "accuracy": 0.32
        },
        "0.01": null
      },
      "auroc": 0.67201875
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": null
      },
      "auroc": 0.8158624999999999
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": null
      },
      "auroc": 0.8158624999999999
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        },
        "0.01": null
      },
      "auroc": 0.6722041666666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        },
        "0.01": null
      },
      "auroc": 0.6722041666666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        },
        "0.01": null
      },
      "auroc": 0.7440333333333334
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        },
        "0.01": null
      },
      "auroc": 0.7440333333333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": null
      },
      "auroc": 0.8350770833333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": null
      },
      "auroc": 0.8350770833333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.33850625000000006
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.33850625000000006
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        },
        "0.01": null
      },
      "auroc": 0.5867916666666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        },
        "0.01": null
      },
      "auroc": 0.5867916666666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": null
      },
      "auroc": 0.8102458333333332
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": null
      },
      "auroc": 0.8102458333333332
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.7368177083333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.7368177083333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": null
      },
      "auroc": 0.7735317708333335
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": null
      },
      "auroc": 0.7735317708333335
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1683,
          "fn": 517,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.8539982007575757
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 411,
          "fn": 789,
          "accuracy": 0.3425
        },
        "0.01": null
      },
      "auroc": 0.5230114583333334
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2094,
          "fn": 1306,
          "accuracy": 0.6158823529411764
        },
        "0.01": null
      },
      "auroc": 0.7371793504901961
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 917,
          "fn": 1283,
          "accuracy": 0.4168181818181818
        },
        "0.01": null
      },
      "auroc": 0.6949101325757576
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 50,
          "fn": 1150,
          "accuracy": 0.041666666666666664
        },
        "0.01": null
      },
      "auroc": 0.20041892361111108
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 967,
          "fn": 2433,
          "accuracy": 0.28441176470588236
        },
        "0.01": null
      },
      "auroc": 0.5203838235294117
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2600,
          "fn": 1800,
          "accuracy": 0.5909090909090909
        },
        "0.01": null
      },
      "auroc": 0.7744541666666667
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 461,
          "fn": 1939,
          "accuracy": 0.19208333333333333
        },
        "0.01": null
      },
      "auroc": 0.3617151909722222
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3061,
          "fn": 3739,
          "accuracy": 0.4501470588235294
        },
        "0.01": null
      },
      "auroc": 0.6287815870098039
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": null
      },
      "auroc": 0.8412833333333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.8912666666666665
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.8912666666666665
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 767,
          "fn": 33,
          "accuracy": 0.95875
        },
        "0.01": null
      },
      "auroc": 0.9162583333333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.05498229166666664
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.4981161458333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.8983020833333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.032812499999999974
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 219,
          "accuracy": 0.4525
        },
        "0.01": null
      },
      "auroc": 0.4655572916666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9197760416666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.04389739583333331
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 419,
          "accuracy": 0.47625
        },
        "0.01": null
      },
      "auroc": 0.48183671874999995
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.938734375
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        },
        "0.01": null
      },
      "auroc": 0.6634770833333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.8011057291666668
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9352520833333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        },
        "0.01": null
      },
      "auroc": 0.27389062499999994
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        },
        "0.01": null
      },
      "auroc": 0.6045713541666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9369932291666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 285,
          "accuracy": 0.2875
        },
        "0.01": null
      },
      "auroc": 0.46868385416666664
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 512,
          "fn": 288,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.7028385416666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9250406250000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.3588708333333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.6419557291666668
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9331453125
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.6500604166666668
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 596,
          "fn": 204,
          "accuracy": 0.745
        },
        "0.01": null
      },
      "auroc": 0.7916028645833333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.7990697916666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": null
      },
      "auroc": 0.8701598958333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.8816385416666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.04473437499999998
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        },
        "0.01": null
      },
      "auroc": 0.46318645833333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": null
      },
      "auroc": 0.9114442708333332
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 255,
          "accuracy": 0.3625
        },
        "0.01": null
      },
      "auroc": 0.4219020833333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 520,
          "fn": 280,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.6666731770833335
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9380854166666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9396677083333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9403614583333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.8274364583333332
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.8838989583333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9408057291666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": null
      },
      "auroc": 0.8827609375000002
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 741,
          "fn": 59,
          "accuracy": 0.92625
        },
        "0.01": null
      },
      "auroc": 0.9117833333333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9272052083333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9272052083333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9138437500000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9138437500000001
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9205244791666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9205244791666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9184083333333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9184083333333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": null
      },
      "auroc": 0.8682510416666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": null
      },
      "auroc": 0.8682510416666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": null
      },
      "auroc": 0.8933296875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": null
      },
      "auroc": 0.8933296875
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.939628125
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.939628125
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9318052083333332
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9318052083333332
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9357166666666665
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9357166666666665
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9409145833333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9409145833333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": null
      },
      "auroc": 0.687840625
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": null
      },
      "auroc": 0.687840625
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        },
        "0.01": null
      },
      "auroc": 0.8143776041666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        },
        "0.01": null
      },
      "auroc": 0.8143776041666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.92455
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.92455
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.8985749999999999
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.8985749999999999
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9115625
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9115625
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2175,
          "fn": 25,
          "accuracy": 0.9886363636363636
        },
        "0.01": null
      },
      "auroc": 0.935971875
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 838,
          "fn": 362,
          "accuracy": 0.6983333333333334
        },
        "0.01": null
      },
      "auroc": 0.723019097222222
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3013,
          "fn": 387,
          "accuracy": 0.8861764705882353
        },
        "0.01": null
      },
      "auroc": 0.8608120710784314
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1919,
          "fn": 281,
          "accuracy": 0.8722727272727273
        },
        "0.01": null
      },
      "auroc": 0.8929236742424242
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 868,
          "accuracy": 0.27666666666666667
        },
        "0.01": null
      },
      "auroc": 0.3965046875
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2251,
          "fn": 1149,
          "accuracy": 0.6620588235294118
        },
        "0.01": null
      },
      "auroc": 0.7177169730392157
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 4094,
          "fn": 306,
          "accuracy": 0.9304545454545454
        },
        "0.01": null
      },
      "auroc": 0.9144477746212121
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1170,
          "fn": 1230,
          "accuracy": 0.4875
        },
        "0.01": null
      },
      "auroc": 0.5597618923611111
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 5264,
          "fn": 1536,
          "accuracy": 0.7741176470588236
        },
        "0.01": null
      },
      "auroc": 0.7892645220588235
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9367510416666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9390005208333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9405666666666668
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.91225625
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9264114583333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9409083333333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9245036458333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 785,
          "fn": 15,
          "accuracy": 0.98125
        },
        "0.01": null
      },
      "auroc": 0.9327059895833333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.94094375
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": null
      },
      "auroc": 0.6323822916666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        },
        "0.01": null
      },
      "auroc": 0.7866630208333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9229385416666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9319411458333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.8967624999999999
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": null
      },
      "auroc": 0.8020197916666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 289,
          "fn": 111,
          "accuracy": 0.7225
        },
        "0.01": null
      },
      "auroc": 0.8493911458333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.8895854166666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        },
        "0.01": null
      },
      "auroc": 0.7170916666666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": null
      },
      "auroc": 0.8033385416666668
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.8931739583333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        },
        "0.01": null
      },
      "auroc": 0.7595557291666668
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 538,
          "fn": 262,
          "accuracy": 0.6725
        },
        "0.01": null
      },
      "auroc": 0.82636484375
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9375333333333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.935784375
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9366588541666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.91798125
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": null
      },
      "auroc": 0.7824666666666666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 301,
          "fn": 99,
          "accuracy": 0.7525
        },
        "0.01": null
      },
      "auroc": 0.8502239583333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9277572916666668
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": null
      },
      "auroc": 0.8591255208333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 696,
          "fn": 104,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.89344140625
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9407625
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.885434375
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9130984375
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9062364583333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        },
        "0.01": null
      },
      "auroc": 0.5815343749999999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        },
        "0.01": null
      },
      "auroc": 0.7438854166666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.9234994791666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.7334843750000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 591,
          "fn": 209,
          "accuracy": 0.73875
        },
        "0.01": null
      },
      "auroc": 0.8284919270833334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.934703125
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9260354166666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9303692708333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9287739583333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.8798250000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": null
      },
      "auroc": 0.9042994791666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9317385416666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": null
      },
      "auroc": 0.9029302083333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 756,
          "fn": 44,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9173343749999999
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.8905010416666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.8905010416666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.8760989583333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.8760989583333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": null
      },
      "auroc": 0.8833
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": null
      },
      "auroc": 0.8833
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9099364583333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9099364583333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.8830385416666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.8830385416666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.8964875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.8964875
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9253031250000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9253031250000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9147354166666668
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9147354166666668
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9200192708333332
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9200192708333332
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.933221875
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.933221875
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.8715312500000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.8715312500000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": null
      },
      "auroc": 0.9023765625000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": null
      },
      "auroc": 0.9023765625000001
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9145239583333332
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9145239583333332
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.889125
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.889125
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": null
      },
      "auroc": 0.9018244791666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": null
      },
      "auroc": 0.9018244791666666
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2113,
          "fn": 87,
          "accuracy": 0.9604545454545454
        },
        "0.01": null
      },
      "auroc": 0.9241310606060605
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 923,
          "fn": 277,
          "accuracy": 0.7691666666666667
        },
        "0.01": null
      },
      "auroc": 0.8530678819444444
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3036,
          "fn": 364,
          "accuracy": 0.8929411764705882
        },
        "0.01": null
      },
      "auroc": 0.8990499387254903
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2011,
          "fn": 189,
          "accuracy": 0.9140909090909091
        },
        "0.01": null
      },
      "auroc": 0.9036919507575757
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 4124,
          "fn": 276,
          "accuracy": 0.9372727272727273
        },
        "0.01": null
      },
      "auroc": 0.9139115056818181
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.853784375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.8975171875
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.8975171875
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 771,
          "fn": 29,
          "accuracy": 0.96375
        },
        "0.01": null
      },
      "auroc": 0.9193835937500001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.054543749999999974
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.4978968749999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.9057968750000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.03054166666666664
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        },
        "0.01": null
      },
      "auroc": 0.4681692708333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9235234374999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.042542708333333304
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 416,
          "accuracy": 0.48
        },
        "0.01": null
      },
      "auroc": 0.48303307291666664
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.94094375
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": null
      },
      "auroc": 0.6894031249999999
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": null
      },
      "auroc": 0.8151734375
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.938590625
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        },
        "0.01": null
      },
      "auroc": 0.29233020833333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        },
        "0.01": null
      },
      "auroc": 0.6154604166666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9397671875
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 270,
          "accuracy": 0.325
        },
        "0.01": null
      },
      "auroc": 0.49086666666666673
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 529,
          "fn": 271,
          "accuracy": 0.66125
        },
        "0.01": null
      },
      "auroc": 0.7153169270833333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9381177083333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.94094375
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9395307291666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9247739583333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.3911395833333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": null
      },
      "auroc": 0.6579567708333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9314458333333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        },
        "0.01": null
      },
      "auroc": 0.6660416666666668
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 598,
          "fn": 202,
          "accuracy": 0.7475
        },
        "0.01": null
      },
      "auroc": 0.79874375
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.940521875
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.8270979166666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": null
      },
      "auroc": 0.8838098958333332
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.8942770833333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.05022187499999998
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        },
        "0.01": null
      },
      "auroc": 0.47224947916666665
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9173994791666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 245,
          "accuracy": 0.3875
        },
        "0.01": null
      },
      "auroc": 0.4386598958333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 532,
          "fn": 268,
          "accuracy": 0.665
        },
        "0.01": null
      },
      "auroc": 0.6780296875
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.940521875
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9408859375
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.94045625
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": null
      },
      "auroc": 0.8553427083333335
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.8978994791666668
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9408531250000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.8979322916666668
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 758,
          "fn": 42,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9193927083333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9314145833333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9314145833333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9224739583333332
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9224739583333332
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9269442708333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9269442708333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9228927083333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9228927083333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": null
      },
      "auroc": 0.8798520833333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": null
      },
      "auroc": 0.8798520833333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": null
      },
      "auroc": 0.9013723958333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": null
      },
      "auroc": 0.9013723958333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9353552083333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9353552083333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9383026041666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9383026041666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": null
      },
      "auroc": 0.7223666666666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": null
      },
      "auroc": 0.7223666666666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 271,
          "fn": 129,
          "accuracy": 0.6775
        },
        "0.01": null
      },
      "auroc": 0.8318083333333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 271,
          "fn": 129,
          "accuracy": 0.6775
        },
        "0.01": null
      },
      "auroc": 0.8318083333333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.92869375
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.92869375
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9092208333333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9092208333333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9189572916666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9189572916666666
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2179,
          "fn": 21,
          "accuracy": 0.9904545454545455
        },
        "0.01": null
      },
      "auroc": 0.9371667613636364
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 860,
          "fn": 340,
          "accuracy": 0.7166666666666667
        },
        "0.01": null
      },
      "auroc": 0.7322934027777777
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3039,
          "fn": 361,
          "accuracy": 0.8938235294117647
        },
        "0.01": null
      },
      "auroc": 0.8648585171568628
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1967,
          "fn": 233,
          "accuracy": 0.894090909090909
        },
        "0.01": null
      },
      "auroc": 0.901310321969697
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 841,
          "accuracy": 0.2991666666666667
        },
        "0.01": null
      },
      "auroc": 0.4122267361111111
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2326,
          "fn": 1074,
          "accuracy": 0.6841176470588235
        },
        "0.01": null
      },
      "auroc": 0.7286925857843138
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 4146,
          "fn": 254,
          "accuracy": 0.9422727272727273
        },
        "0.01": null
      },
      "auroc": 0.9192385416666666
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1219,
          "fn": 1181,
          "accuracy": 0.5079166666666667
        },
        "0.01": null
      },
      "auroc": 0.5722600694444444
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 5365,
          "fn": 1435,
          "accuracy": 0.7889705882352941
        },
        "0.01": null
      },
      "auroc": 0.7967755514705882
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.855521875
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.8983859375000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.8983859375000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 771,
          "fn": 29,
          "accuracy": 0.96375
        },
        "0.01": null
      },
      "auroc": 0.91981796875
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.05729479166666665
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.49927239583333327
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9164958333333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.03351354166666664
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        },
        "0.01": null
      },
      "auroc": 0.4750046874999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9288729166666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.04540416666666664
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 407,
          "accuracy": 0.49125
        },
        "0.01": null
      },
      "auroc": 0.4871385416666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.94094375
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        },
        "0.01": null
      },
      "auroc": 0.693390625
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": null
      },
      "auroc": 0.8171671875000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9376572916666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        },
        "0.01": null
      },
      "auroc": 0.2910635416666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.6143604166666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9393005208333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 265,
          "accuracy": 0.3375
        },
        "0.01": null
      },
      "auroc": 0.49222708333333337
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 533,
          "fn": 267,
          "accuracy": 0.66625
        },
        "0.01": null
      },
      "auroc": 0.7157638020833333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9268000000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": null
      },
      "auroc": 0.462515625
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.6946578124999999
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9340250000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        },
        "0.01": null
      },
      "auroc": 0.7018828125
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 602,
          "fn": 198,
          "accuracy": 0.7525
        },
        "0.01": null
      },
      "auroc": 0.81795390625
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.828465625
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": null
      },
      "auroc": 0.8848578125
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.8979770833333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.049663541666666644
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 219,
          "accuracy": 0.4525
        },
        "0.01": null
      },
      "auroc": 0.4738203125
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9196135416666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 245,
          "accuracy": 0.3875
        },
        "0.01": null
      },
      "auroc": 0.43906458333333326
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 536,
          "fn": 264,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.6793390625
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9403812500000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.940815625
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9406875
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.852028125
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": null
      },
      "auroc": 0.8963578125000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9409687500000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": null
      },
      "auroc": 0.8962046875000002
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 757,
          "fn": 43,
          "accuracy": 0.94625
        },
        "0.01": null
      },
      "auroc": 0.9185867187500002
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9331885416666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9331885416666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9255895833333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9255895833333333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9293890625
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9293890625
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9259468750000001
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9259468750000001
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.890853125
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.890853125
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.9084000000000001
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.9084000000000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9409239583333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9409239583333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9352854166666666
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9352854166666666
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9381046875
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9381046875
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.94094375
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.94094375
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        },
        "0.01": null
      },
      "auroc": 0.7002552083333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        },
        "0.01": null
      },
      "auroc": 0.7002552083333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        },
        "0.01": null
      },
      "auroc": 0.8205994791666666
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        },
        "0.01": null
      },
      "auroc": 0.8205994791666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9302489583333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9302489583333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.908546875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.908546875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9193979166666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9193979166666667
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2185,
          "fn": 15,
          "accuracy": 0.9931818181818182
        },
        "0.01": null
      },
      "auroc": 0.9380405303030304
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 862,
          "fn": 338,
          "accuracy": 0.7183333333333334
        },
        "0.01": null
      },
      "auroc": 0.7336720486111111
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3047,
          "fn": 353,
          "accuracy": 0.8961764705882352
        },
        "0.01": null
      },
      "auroc": 0.8659104779411764
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1972,
          "fn": 228,
          "accuracy": 0.8963636363636364
        },
        "0.01": null
      },
      "auroc": 0.9019452651515152
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 836,
          "accuracy": 0.30333333333333334
        },
        "0.01": null
      },
      "auroc": 0.4240510416666666
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2336,
          "fn": 1064,
          "accuracy": 0.6870588235294117
        },
        "0.01": null
      },
      "auroc": 0.7332767156862745
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 4157,
          "fn": 243,
          "accuracy": 0.9447727272727273
        },
        "0.01": null
      },
      "auroc": 0.9199928977272729
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1226,
          "fn": 1174,
          "accuracy": 0.5108333333333334
        },
        "0.01": null
      },
      "auroc": 0.5788615451388889
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 5383,
          "fn": 1417,
          "accuracy": 0.7916176470588235
        },
        "0.01": null
      },
      "auroc": 0.7995935968137255
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": null
      },
      "auroc": 0.8610052083333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        },
        "0.01": null
      },
      "auroc": 0.738221875
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        },
        "0.01": null
      },
      "auroc": 0.7996135416666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.8460770833333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        },
        "0.01": null
      },
      "auroc": 0.5985145833333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        },
        "0.01": null
      },
      "auroc": 0.7222958333333332
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": null
      },
      "auroc": 0.8535411458333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 107,
          "fn": 293,
          "accuracy": 0.2675
        },
        "0.01": null
      },
      "auroc": 0.6683682291666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 412,
          "fn": 388,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.7609546875000002
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.936965625
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": null
      },
      "auroc": 0.25534375
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.5961546875000001
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.8987062499999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        },
        "0.01": null
      },
      "auroc": 0.316184375
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.6074453125
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9178359375
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 371,
          "accuracy": 0.0725
        },
        "0.01": null
      },
      "auroc": 0.28576406249999997
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 408,
          "fn": 392,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.6017999999999999
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": null
      },
      "auroc": 0.6989364583333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": null
      },
      "auroc": 0.42087187499999995
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 72,
          "fn": 328,
          "accuracy": 0.18
        },
        "0.01": null
      },
      "auroc": 0.5599041666666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        },
        "0.01": null
      },
      "auroc": 0.6890197916666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.24469375
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        },
        "0.01": null
      },
      "auroc": 0.4668567708333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 274,
          "accuracy": 0.315
        },
        "0.01": null
      },
      "auroc": 0.6939781250000001
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.33278281249999997
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 666,
          "accuracy": 0.1675
        },
        "0.01": null
      },
      "auroc": 0.51338046875
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9278635416666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.86535625
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": null
      },
      "auroc": 0.8966098958333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.877721875
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        },
        "0.01": null
      },
      "auroc": 0.636803125
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        },
        "0.01": null
      },
      "auroc": 0.7572625000000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9027927083333335
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        },
        "0.01": null
      },
      "auroc": 0.7510796875
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 569,
          "fn": 231,
          "accuracy": 0.71125
        },
        "0.01": null
      },
      "auroc": 0.8269361979166666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9357875
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": null
      },
      "auroc": 0.5714791666666668
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.7536333333333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.8808427083333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.347253125
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 228,
          "accuracy": 0.43
        },
        "0.01": null
      },
      "auroc": 0.6140479166666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9083151041666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 369,
          "accuracy": 0.0775
        },
        "0.01": null
      },
      "auroc": 0.4593661458333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 402,
          "accuracy": 0.4975
        },
        "0.01": null
      },
      "auroc": 0.683840625
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": null
      },
      "auroc": 0.8151645833333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        },
        "0.01": null
      },
      "auroc": 0.7001958333333332
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        },
        "0.01": null
      },
      "auroc": 0.7576802083333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.78163125
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": null
      },
      "auroc": 0.5498145833333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 277,
          "accuracy": 0.3075
        },
        "0.01": null
      },
      "auroc": 0.6657229166666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        },
        "0.01": null
      },
      "auroc": 0.7983979166666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 312,
          "accuracy": 0.22
        },
        "0.01": null
      },
      "auroc": 0.6250052083333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 482,
          "accuracy": 0.3975
        },
        "0.01": null
      },
      "auroc": 0.7117015624999998
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": null
      },
      "auroc": 0.7811802083333332
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": null
      },
      "auroc": 0.7811802083333332
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.74529375
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.74529375
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.7632369791666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.7632369791666666
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.8640166666666668
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.8640166666666668
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.8260625
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.8260625
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.8450395833333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.8450395833333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": null
      },
      "auroc": 0.727646875
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": null
      },
      "auroc": 0.727646875
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        },
        "0.01": null
      },
      "auroc": 0.6705197916666666
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        },
        "0.01": null
      },
      "auroc": 0.6705197916666666
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 269,
          "accuracy": 0.3275
        },
        "0.01": null
      },
      "auroc": 0.6990833333333334
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 269,
          "accuracy": 0.3275
        },
        "0.01": null
      },
      "auroc": 0.6990833333333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.7836718749999999
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.7836718749999999
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        },
        "0.01": null
      },
      "auroc": 0.6084458333333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        },
        "0.01": null
      },
      "auroc": 0.6084458333333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 266,
          "accuracy": 0.335
        },
        "0.01": null
      },
      "auroc": 0.6960588541666668
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 266,
          "accuracy": 0.335
        },
        "0.01": null
      },
      "auroc": 0.6960588541666668
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": null
      },
      "auroc": 0.8260145833333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": null
      },
      "auroc": 0.8260145833333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": null
      },
      "auroc": 0.7826166666666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": null
      },
      "auroc": 0.7826166666666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 256,
          "fn": 144,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.8043156250000001
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 256,
          "fn": 144,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.8043156250000001
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1539,
          "fn": 661,
          "accuracy": 0.6995454545454546
        },
        "0.01": null
      },
      "auroc": 0.8325684659090908
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 853,
          "accuracy": 0.2891666666666667
        },
        "0.01": null
      },
      "auroc": 0.5919114583333334
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1886,
          "fn": 1514,
          "accuracy": 0.5547058823529412
        },
        "0.01": null
      },
      "auroc": 0.7476306985294118
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1269,
          "fn": 931,
          "accuracy": 0.5768181818181818
        },
        "0.01": null
      },
      "auroc": 0.7824488636363637
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 1089,
          "accuracy": 0.0925
        },
        "0.01": null
      },
      "auroc": 0.4488772569444444
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1380,
          "fn": 2020,
          "accuracy": 0.40588235294117647
        },
        "0.01": null
      },
      "auroc": 0.6647177083333333
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2808,
          "fn": 1592,
          "accuracy": 0.6381818181818182
        },
        "0.01": null
      },
      "auroc": 0.8075086647727272
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 458,
          "fn": 1942,
          "accuracy": 0.19083333333333333
        },
        "0.01": null
      },
      "auroc": 0.5203943576388889
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3266,
          "fn": 3534,
          "accuracy": 0.4802941176470588
        },
        "0.01": null
      },
      "auroc": 0.7061742034313725
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9362395833333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9387447916666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9400250000000001
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.792521875
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.8662734375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9406375
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": null
      },
      "auroc": 0.8643807291666668
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 744,
          "fn": 56,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9025091145833333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.055055208333333314
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.4981526041666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.831365625
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.032812499999999974
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        },
        "0.01": null
      },
      "auroc": 0.4320890625
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.8863078125
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.04393385416666664
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 458,
          "accuracy": 0.4275
        },
        "0.01": null
      },
      "auroc": 0.4651208333333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9007166666666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        },
        "0.01": null
      },
      "auroc": 0.5651895833333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": null
      },
      "auroc": 0.732953125
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.87236875
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": null
      },
      "auroc": 0.22591145833333331
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        },
        "0.01": null
      },
      "auroc": 0.5491401041666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.8865427083333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 341,
          "accuracy": 0.1475
        },
        "0.01": null
      },
      "auroc": 0.39555052083333336
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 403,
          "fn": 397,
          "accuracy": 0.50375
        },
        "0.01": null
      },
      "auroc": 0.6410466145833333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9380927083333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9397552083333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9389239583333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9035114583333335
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.31267083333333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        },
        "0.01": null
      },
      "auroc": 0.6080911458333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9208020833333335
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.6262130208333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 581,
          "fn": 219,
          "accuracy": 0.72625
        },
        "0.01": null
      },
      "auroc": 0.7735075520833334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.940265625
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.7803739583333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": null
      },
      "auroc": 0.8603197916666665
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.7940104166666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.03760416666666664
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 287,
          "accuracy": 0.2825
        },
        "0.01": null
      },
      "auroc": 0.41580729166666663
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": null
      },
      "auroc": 0.8671380208333332
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 265,
          "accuracy": 0.3375
        },
        "0.01": null
      },
      "auroc": 0.40898906249999994
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 448,
          "fn": 352,
          "accuracy": 0.56
        },
        "0.01": null
      },
      "auroc": 0.6380635416666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9352958333333332
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9244458333333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9298708333333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9251218750000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": null
      },
      "auroc": 0.7506937499999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 289,
          "fn": 111,
          "accuracy": 0.7225
        },
        "0.01": null
      },
      "auroc": 0.8379078125000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9302088541666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.8375697916666668
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 681,
          "fn": 119,
          "accuracy": 0.85125
        },
        "0.01": null
      },
      "auroc": 0.8838893229166667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.8975020833333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.8975020833333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": null
      },
      "auroc": 0.8656927083333332
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": null
      },
      "auroc": 0.8656927083333332
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": null
      },
      "auroc": 0.8815973958333334
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": null
      },
      "auroc": 0.8815973958333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.869575
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.869575
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": null
      },
      "auroc": 0.7771802083333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": null
      },
      "auroc": 0.7771802083333333
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        },
        "0.01": null
      },
      "auroc": 0.8233776041666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        },
        "0.01": null
      },
      "auroc": 0.8233776041666667
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.91959375
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.91959375
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.8716968749999999
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.8716968749999999
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.8956453125
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.8956453125
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.92968125
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.92968125
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": null
      },
      "auroc": 0.5311947916666666
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": null
      },
      "auroc": 0.5311947916666666
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        },
        "0.01": null
      },
      "auroc": 0.7304380208333334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        },
        "0.01": null
      },
      "auroc": 0.7304380208333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.898990625
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.898990625
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.8440458333333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.8440458333333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.8715182291666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.8715182291666667
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2081,
          "fn": 119,
          "accuracy": 0.9459090909090909
        },
        "0.01": null
      },
      "auroc": 0.9192921401515153
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 781,
          "fn": 419,
          "accuracy": 0.6508333333333334
        },
        "0.01": null
      },
      "auroc": 0.7001765625
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2862,
          "fn": 538,
          "accuracy": 0.841764705882353
        },
        "0.01": null
      },
      "auroc": 0.841957230392157
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1603,
          "fn": 597,
          "accuracy": 0.7286363636363636
        },
        "0.01": null
      },
      "auroc": 0.8323830492424242
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 952,
          "accuracy": 0.20666666666666667
        },
        "0.01": null
      },
      "auroc": 0.3587024305555555
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1851,
          "fn": 1549,
          "accuracy": 0.5444117647058824
        },
        "0.01": null
      },
      "auroc": 0.6652016544117647
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3684,
          "fn": 716,
          "accuracy": 0.8372727272727273
        },
        "0.01": null
      },
      "auroc": 0.8758375946969698
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1029,
          "fn": 1371,
          "accuracy": 0.42875
        },
        "0.01": null
      },
      "auroc": 0.5294394965277778
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 4713,
          "fn": 2087,
          "accuracy": 0.6930882352941177
        },
        "0.01": null
      },
      "auroc": 0.7535794424019608
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9409239583333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9410869791666666
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.8430989583333334
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.8921744791666667
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.8920114583333333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 768,
          "fn": 32,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9166307291666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.05143854166666664
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.49634427083333327
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.9046270833333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.03214999999999997
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        },
        "0.01": null
      },
      "auroc": 0.4683885416666666
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9229385416666668
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.041794270833333313
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 417,
          "accuracy": 0.47875
        },
        "0.01": null
      },
      "auroc": 0.48236640625
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9382583333333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.6658583333333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        },
        "0.01": null
      },
      "auroc": 0.8020583333333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.933271875
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": null
      },
      "auroc": 0.27491770833333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        },
        "0.01": null
      },
      "auroc": 0.6040947916666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9357651041666666
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 278,
          "accuracy": 0.305
        },
        "0.01": null
      },
      "auroc": 0.4703880208333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 519,
          "fn": 281,
          "accuracy": 0.64875
        },
        "0.01": null
      },
      "auroc": 0.7030765625000001
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9257979166666668
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": null
      },
      "auroc": 0.3823385416666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": null
      },
      "auroc": 0.6540682291666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9335239583333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        },
        "0.01": null
      },
      "auroc": 0.6617942708333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 599,
          "fn": 201,
          "accuracy": 0.74875
        },
        "0.01": null
      },
      "auroc": 0.7976591145833334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": null
      },
      "auroc": 0.80458125
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": null
      },
      "auroc": 0.8729156249999999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.8889291666666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.041859374999999976
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        },
        "0.01": null
      },
      "auroc": 0.4653942708333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9150895833333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 251,
          "accuracy": 0.3725
        },
        "0.01": null
      },
      "auroc": 0.42322031249999986
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 523,
          "fn": 277,
          "accuracy": 0.65375
        },
        "0.01": null
      },
      "auroc": 0.6691549479166666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9396114583333334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9404307291666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9405739583333333
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": null
      },
      "auroc": 0.8379510416666668
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": null
      },
      "auroc": 0.8892625000000001
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9409119791666667
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": null
      },
      "auroc": 0.8887812500000002
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 751,
          "fn": 49,
          "accuracy": 0.93875
        },
        "0.01": null
      },
      "auroc": 0.9148466145833333
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9309291666666668
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9309291666666668
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9192510416666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9192510416666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9250901041666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9250901041666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9213322916666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9213322916666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": null
      },
      "auroc": 0.8736125
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": null
      },
      "auroc": 0.8736125
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.8974723958333334
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.8974723958333334
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.94094375
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.94094375
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.931078125
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.931078125
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9360109375000001
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9360109375000001
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.94094375
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.94094375
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        },
        "0.01": null
      },
      "auroc": 0.6774666666666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        },
        "0.01": null
      },
      "auroc": 0.6774666666666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        },
        "0.01": null
      },
      "auroc": 0.8092052083333332
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        },
        "0.01": null
      },
      "auroc": 0.8092052083333332
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9238052083333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9238052083333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.90291875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.90291875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.9133619791666666
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.9133619791666666
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2177,
          "fn": 23,
          "accuracy": 0.9895454545454545
        },
        "0.01": null
      },
      "auroc": 0.9365875
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 849,
          "fn": 351,
          "accuracy": 0.7075
        },
        "0.01": null
      },
      "auroc": 0.7239439236111112
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3026,
          "fn": 374,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.8615368259803922
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1928,
          "fn": 272,
          "accuracy": 0.8763636363636363
        },
        "0.01": null
      },
      "auroc": 0.8944342803030304
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 854,
          "accuracy": 0.28833333333333333
        },
        "0.01": null
      },
      "auroc": 0.40205260416666666
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2274,
          "fn": 1126,
          "accuracy": 0.6688235294117647
        },
        "0.01": null
      },
      "auroc": 0.7206525122549019
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 4105,
          "fn": 295,
          "accuracy": 0.9329545454545455
        },
        "0.01": null
      },
      "auroc": 0.9155108901515151
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1195,
          "fn": 1205,
          "accuracy": 0.4979166666666667
        },
        "0.01": null
      },
      "auroc": 0.5629982638888889
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 5300,
          "fn": 1500,
          "accuracy": 0.7794117647058824
        },
        "0.01": null
      },
      "auroc": 0.791094669117647
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9357447916666667
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9384973958333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9405666666666668
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.930784375
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9356755208333334
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9409083333333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9332645833333333
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        },
        "0.01": null
      },
      "auroc": 0.9370864583333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9373072916666667
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9392786458333333
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.93766875
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.939459375
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9374880208333334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9393690104166666
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.94094375
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9410968749999999
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9346229166666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9372010416666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9359119791666667
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9377833333333334
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9392255208333333
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.9385044270833334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9356958333333334
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9384729166666667
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9379906250000001
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9396203125
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9368432291666666
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9390466145833334
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9329572916666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9329572916666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9306885416666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9306885416666666
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9318229166666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9318229166666667
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9412499999999999
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9406875
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9406875
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9409239583333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9409239583333333
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9408057291666667
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9408057291666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9406875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9406875
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9404802083333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9404802083333333
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9405838541666667
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9405838541666667
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2197,
          "fn": 3,
          "accuracy": 0.9986363636363637
        },
        "0.01": null
      },
      "auroc": 0.9403660037878787
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1197,
          "fn": 3,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9387496527777778
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3394,
          "fn": 6,
          "accuracy": 0.9982352941176471
        },
        "0.01": null
      },
      "auroc": 0.9397955269607843
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2194,
          "fn": 6,
          "accuracy": 0.9972727272727273
        },
        "0.01": null
      },
      "auroc": 0.9395256628787879
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1194,
          "fn": 6,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.937690798611111
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3388,
          "fn": 12,
          "accuracy": 0.9964705882352941
        },
        "0.01": null
      },
      "auroc": 0.9388780637254901
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4391,
          "fn": 9,
          "accuracy": 0.9979545454545454
        },
        "0.01": null
      },
      "auroc": 0.9399458333333334
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2391,
          "fn": 9,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.9382202256944444
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 6782,
          "fn": 18,
          "accuracy": 0.9973529411764706
        },
        "0.01": null
      },
      "auroc": 0.9393367953431373
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2342,
          "fn": 58,
          "accuracy": 0.9758333333333333
        },
        "0.01": null
      },
      "auroc": 0.9329186631944445
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2186,
          "fn": 214,
          "accuracy": 0.9108333333333334
        },
        "0.01": null
      },
      "auroc": 0.9101444444444444
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4528,
          "fn": 272,
          "accuracy": 0.9433333333333334
        },
        "0.01": null
      },
      "auroc": 0.9215315538194444
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2342,
          "fn": 58,
          "accuracy": 0.9758333333333333
        },
        "0.01": null
      },
      "auroc": 0.9302122395833333
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1765,
          "fn": 635,
          "accuracy": 0.7354166666666667
        },
        "0.01": null
      },
      "auroc": 0.8046688368055556
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4107,
          "fn": 693,
          "accuracy": 0.855625
        },
        "0.01": null
      },
      "auroc": 0.8674405381944444
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4684,
          "fn": 116,
          "accuracy": 0.9758333333333333
        },
        "0.01": null
      },
      "auroc": 0.931565451388889
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3951,
          "fn": 849,
          "accuracy": 0.823125
        },
        "0.01": null
      },
      "auroc": 0.857406640625
    },
    {
      "domain": "books",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8635,
          "fn": 965,
          "accuracy": 0.8994791666666667
        },
        "0.01": null
      },
      "auroc": 0.8944860460069444
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2399,
          "fn": 1,
          "accuracy": 0.9995833333333334
        },
        "0.01": null
      },
      "auroc": 0.9405939236111112
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 256,
          "fn": 2144,
          "accuracy": 0.10666666666666667
        },
        "0.01": null
      },
      "auroc": 0.18986840277777778
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2655,
          "fn": 2145,
          "accuracy": 0.553125
        },
        "0.01": null
      },
      "auroc": 0.5652311631944444
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2010,
          "fn": 390,
          "accuracy": 0.8375
        },
        "0.01": null
      },
      "auroc": 0.8739272569444443
    },
    {
      "domain": "books",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4409,
          "fn": 391,
          "accuracy": 0.9185416666666667
        },
        "0.01": null
      },
      "auroc": 0.9072605902777777
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2113,
          "fn": 287,
          "accuracy": 0.8804166666666666
        },
        "0.01": null
      },
      "auroc": 0.8944972222222223
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 992,
          "fn": 1408,
          "accuracy": 0.41333333333333333
        },
        "0.01": null
      },
      "auroc": 0.6301366319444444
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3105,
          "fn": 1695,
          "accuracy": 0.646875
        },
        "0.01": null
      },
      "auroc": 0.7623169270833334
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2050,
          "fn": 350,
          "accuracy": 0.8541666666666666
        },
        "0.01": null
      },
      "auroc": 0.8836504340277779
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 409,
          "fn": 1991,
          "accuracy": 0.17041666666666666
        },
        "0.01": null
      },
      "auroc": 0.34142769097222225
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2459,
          "fn": 2341,
          "accuracy": 0.5122916666666667
        },
        "0.01": null
      },
      "auroc": 0.6125390625
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4163,
          "fn": 637,
          "accuracy": 0.8672916666666667
        },
        "0.01": null
      },
      "auroc": 0.8890738281249999
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1401,
          "fn": 3399,
          "accuracy": 0.291875
        },
        "0.01": null
      },
      "auroc": 0.48578216145833336
    },
    {
      "domain": "books",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5564,
          "fn": 4036,
          "accuracy": 0.5795833333333333
        },
        "0.01": null
      },
      "auroc": 0.6874279947916668
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2390,
          "fn": 10,
          "accuracy": 0.9958333333333333
        },
        "0.01": null
      },
      "auroc": 0.9379561631944444
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2340,
          "fn": 60,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9317274305555556
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4730,
          "fn": 70,
          "accuracy": 0.9854166666666667
        },
        "0.01": null
      },
      "auroc": 0.934841796875
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2222,
          "fn": 178,
          "accuracy": 0.9258333333333333
        },
        "0.01": null
      },
      "auroc": 0.9091914930555557
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 2021,
          "accuracy": 0.15791666666666668
        },
        "0.01": null
      },
      "auroc": 0.46036440972222226
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2601,
          "fn": 2199,
          "accuracy": 0.541875
        },
        "0.01": null
      },
      "auroc": 0.684777951388889
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4612,
          "fn": 188,
          "accuracy": 0.9608333333333333
        },
        "0.01": null
      },
      "auroc": 0.923573828125
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2719,
          "fn": 2081,
          "accuracy": 0.5664583333333333
        },
        "0.01": null
      },
      "auroc": 0.6960459201388889
    },
    {
      "domain": "books",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7331,
          "fn": 2269,
          "accuracy": 0.7636458333333334
        },
        "0.01": null
      },
      "auroc": 0.8098098741319444
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2399,
          "fn": 1,
          "accuracy": 0.9995833333333334
        },
        "0.01": null
      },
      "auroc": 0.9403089409722223
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1547,
          "fn": 853,
          "accuracy": 0.6445833333333333
        },
        "0.01": null
      },
      "auroc": 0.7605092881944444
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3946,
          "fn": 854,
          "accuracy": 0.8220833333333334
        },
        "0.01": null
      },
      "auroc": 0.8504091145833332
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1887,
          "fn": 513,
          "accuracy": 0.78625
        },
        "0.01": null
      },
      "auroc": 0.8575659722222222
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 236,
          "fn": 2164,
          "accuracy": 0.09833333333333333
        },
        "0.01": null
      },
      "auroc": 0.18803255208333333
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2123,
          "fn": 2677,
          "accuracy": 0.4422916666666667
        },
        "0.01": null
      },
      "auroc": 0.5227992621527777
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4286,
          "fn": 514,
          "accuracy": 0.8929166666666667
        },
        "0.01": null
      },
      "auroc": 0.8989374565972222
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1783,
          "fn": 3017,
          "accuracy": 0.37145833333333333
        },
        "0.01": null
      },
      "auroc": 0.4742709201388889
    },
    {
      "domain": "books",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 6069,
          "fn": 3531,
          "accuracy": 0.6321875
        },
        "0.01": null
      },
      "auroc": 0.6866041883680556
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2307,
          "fn": 93,
          "accuracy": 0.96125
        },
        "0.01": null
      },
      "auroc": 0.9263946180555556
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2150,
          "fn": 250,
          "accuracy": 0.8958333333333334
        },
        "0.01": null
      },
      "auroc": 0.9020671006944445
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4457,
          "fn": 343,
          "accuracy": 0.9285416666666667
        },
        "0.01": null
      },
      "auroc": 0.9142308593749999
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2240,
          "fn": 160,
          "accuracy": 0.9333333333333333
        },
        "0.01": null
      },
      "auroc": 0.9160947916666666
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1469,
          "fn": 931,
          "accuracy": 0.6120833333333333
        },
        "0.01": null
      },
      "auroc": 0.7730664930555555
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3709,
          "fn": 1091,
          "accuracy": 0.7727083333333333
        },
        "0.01": null
      },
      "auroc": 0.8445806423611111
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4547,
          "fn": 253,
          "accuracy": 0.9472916666666666
        },
        "0.01": null
      },
      "auroc": 0.9212447048611111
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3619,
          "fn": 1181,
          "accuracy": 0.7539583333333333
        },
        "0.01": null
      },
      "auroc": 0.837566796875
    },
    {
      "domain": "books",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8166,
          "fn": 1434,
          "accuracy": 0.850625
        },
        "0.01": null
      },
      "auroc": 0.8794057508680555
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2127,
          "fn": 273,
          "accuracy": 0.88625
        },
        "0.01": null
      },
      "auroc": 0.8983807291666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2127,
          "fn": 273,
          "accuracy": 0.88625
        },
        "0.01": null
      },
      "auroc": 0.8983807291666667
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1998,
          "fn": 402,
          "accuracy": 0.8325
        },
        "0.01": null
      },
      "auroc": 0.8789782986111112
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1998,
          "fn": 402,
          "accuracy": 0.8325
        },
        "0.01": null
      },
      "auroc": 0.8789782986111112
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4125,
          "fn": 675,
          "accuracy": 0.859375
        },
        "0.01": null
      },
      "auroc": 0.8886795138888889
    },
    {
      "domain": "books",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4125,
          "fn": 675,
          "accuracy": 0.859375
        },
        "0.01": null
      },
      "auroc": 0.8886795138888889
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2106,
          "fn": 294,
          "accuracy": 0.8775
        },
        "0.01": null
      },
      "auroc": 0.89223671875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2106,
          "fn": 294,
          "accuracy": 0.8775
        },
        "0.01": null
      },
      "auroc": 0.89223671875
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1774,
          "fn": 626,
          "accuracy": 0.7391666666666666
        },
        "0.01": null
      },
      "auroc": 0.8426509548611112
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1774,
          "fn": 626,
          "accuracy": 0.7391666666666666
        },
        "0.01": null
      },
      "auroc": 0.8426509548611112
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3880,
          "fn": 920,
          "accuracy": 0.8083333333333333
        },
        "0.01": null
      },
      "auroc": 0.8674438368055556
    },
    {
      "domain": "books",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3880,
          "fn": 920,
          "accuracy": 0.8083333333333333
        },
        "0.01": null
      },
      "auroc": 0.8674438368055556
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2189,
          "fn": 211,
          "accuracy": 0.9120833333333334
        },
        "0.01": null
      },
      "auroc": 0.9084855034722222
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2189,
          "fn": 211,
          "accuracy": 0.9120833333333334
        },
        "0.01": null
      },
      "auroc": 0.9084855034722222
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2024,
          "fn": 376,
          "accuracy": 0.8433333333333334
        },
        "0.01": null
      },
      "auroc": 0.8797177083333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2024,
          "fn": 376,
          "accuracy": 0.8433333333333334
        },
        "0.01": null
      },
      "auroc": 0.8797177083333333
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4213,
          "fn": 587,
          "accuracy": 0.8777083333333333
        },
        "0.01": null
      },
      "auroc": 0.8941016059027778
    },
    {
      "domain": "books",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4213,
          "fn": 587,
          "accuracy": 0.8777083333333333
        },
        "0.01": null
      },
      "auroc": 0.8941016059027778
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2220,
          "fn": 180,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.916917795138889
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2220,
          "fn": 180,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.916917795138889
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 1603,
          "accuracy": 0.33208333333333334
        },
        "0.01": null
      },
      "auroc": 0.6709012152777778
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 1603,
          "accuracy": 0.33208333333333334
        },
        "0.01": null
      },
      "auroc": 0.6709012152777778
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3017,
          "fn": 1783,
          "accuracy": 0.6285416666666667
        },
        "0.01": null
      },
      "auroc": 0.7939095052083334
    },
    {
      "domain": "books",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3017,
          "fn": 1783,
          "accuracy": 0.6285416666666667
        },
        "0.01": null
      },
      "auroc": 0.7939095052083334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2181,
          "fn": 219,
          "accuracy": 0.90875
        },
        "0.01": null
      },
      "auroc": 0.9042834201388891
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2181,
          "fn": 219,
          "accuracy": 0.90875
        },
        "0.01": null
      },
      "auroc": 0.9042834201388891
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1976,
          "fn": 424,
          "accuracy": 0.8233333333333334
        },
        "0.01": null
      },
      "auroc": 0.8715692708333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1976,
          "fn": 424,
          "accuracy": 0.8233333333333334
        },
        "0.01": null
      },
      "auroc": 0.8715692708333334
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4157,
          "fn": 643,
          "accuracy": 0.8660416666666667
        },
        "0.01": null
      },
      "auroc": 0.8879263454861112
    },
    {
      "domain": "books",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4157,
          "fn": 643,
          "accuracy": 0.8660416666666667
        },
        "0.01": null
      },
      "auroc": 0.8879263454861112
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 24773,
          "fn": 1627,
          "accuracy": 0.9383712121212121
        },
        "0.01": null
      },
      "auroc": 0.9175430634469697
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9471,
          "fn": 4929,
          "accuracy": 0.6577083333333333
        },
        "0.01": null
      },
      "auroc": 0.7207422164351853
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 34244,
          "fn": 6556,
          "accuracy": 0.8393137254901961
        },
        "0.01": null
      },
      "auroc": 0.8480839409722223
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 21320,
          "fn": 5080,
          "accuracy": 0.8075757575757576
        },
        "0.01": null
      },
      "auroc": 0.8649508759469697
    },
    {
      "domain": "books",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 46093,
          "fn": 6707,
          "accuracy": 0.8729734848484848
        },
        "0.01": null
      },
      "auroc": 0.8912469696969697
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.934309375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9347588541666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.8352854166666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        }
      },
      "auroc": 0.885246875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        }
      },
      "auroc": 0.8847973958333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 769,
          "fn": 31,
          "accuracy": 0.96125
        },
        "0.01": {
          "tp": 754,
          "fn": 46,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9100028645833333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9315864583333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.10088645833333332
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        }
      },
      "auroc": 0.5162364583333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.8536864583333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.056656249999999984
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        },
        "0.01": {
          "tp": 105,
          "fn": 295,
          "accuracy": 0.2625
        }
      },
      "auroc": 0.45517135416666665
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        }
      },
      "auroc": 0.8926364583333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07877135416666664
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 435,
          "accuracy": 0.45625
        },
        "0.01": {
          "tp": 300,
          "fn": 500,
          "accuracy": 0.375
        }
      },
      "auroc": 0.48570390625
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9343281250000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.7155104166666668
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        }
      },
      "auroc": 0.8249192708333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9321239583333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.272865625
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.6024947916666668
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9332260416666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 266,
          "accuracy": 0.335
        },
        "0.01": {
          "tp": 72,
          "fn": 328,
          "accuracy": 0.18
        }
      },
      "auroc": 0.4941880208333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 533,
          "fn": 267,
          "accuracy": 0.66625
        },
        "0.01": {
          "tp": 465,
          "fn": 335,
          "accuracy": 0.58125
        }
      },
      "auroc": 0.71370703125
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9319302083333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9335692708333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.912715625
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.32133020833333337
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        },
        "0.01": {
          "tp": 172,
          "fn": 228,
          "accuracy": 0.43
        }
      },
      "auroc": 0.6170229166666668
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9223229166666668
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        }
      },
      "auroc": 0.6282692708333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 594,
          "fn": 206,
          "accuracy": 0.7425
        },
        "0.01": {
          "tp": 566,
          "fn": 234,
          "accuracy": 0.7075
        }
      },
      "auroc": 0.7752960937500001
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9340229166666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.8842145833333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9091187500000001
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.8062874999999999
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07086041666666665
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 265,
          "accuracy": 0.3375
        },
        "0.01": {
          "tp": 78,
          "fn": 322,
          "accuracy": 0.195
        }
      },
      "auroc": 0.43857395833333335
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        }
      },
      "auroc": 0.8701552083333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        },
        "0.01": {
          "tp": 128,
          "fn": 272,
          "accuracy": 0.32
        }
      },
      "auroc": 0.4775374999999999
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 513,
          "fn": 287,
          "accuracy": 0.64125
        },
        "0.01": {
          "tp": 404,
          "fn": 396,
          "accuracy": 0.505
        }
      },
      "auroc": 0.6738463541666666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9306239583333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.68096875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        },
        "0.01": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        }
      },
      "auroc": 0.8057963541666666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9329161458333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        },
        "0.01": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        }
      },
      "auroc": 0.8080885416666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 677,
          "fn": 123,
          "accuracy": 0.84625
        },
        "0.01": {
          "tp": 635,
          "fn": 165,
          "accuracy": 0.79375
        }
      },
      "auroc": 0.87050234375
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9347812500000001
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9347812500000001
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9273906249999999
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9273906249999999
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9310859375
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9310859375
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9256395833333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9256395833333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.8944395833333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.8944395833333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        }
      },
      "auroc": 0.9100395833333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        }
      },
      "auroc": 0.9100395833333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9332843749999999
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9332843749999999
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9342463541666666
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9342463541666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        }
      },
      "auroc": 0.8394708333333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        }
      },
      "auroc": 0.8394708333333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        }
      },
      "auroc": 0.8873395833333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        }
      },
      "auroc": 0.8873395833333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.8982906250000001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.8982906250000001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.8711770833333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.8711770833333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 289,
          "fn": 111,
          "accuracy": 0.7225
        }
      },
      "auroc": 0.8847338541666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 289,
          "fn": 111,
          "accuracy": 0.7225
        }
      },
      "auroc": 0.8847338541666667
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2175,
          "fn": 25,
          "accuracy": 0.9886363636363636
        },
        "0.01": {
          "tp": 2126,
          "fn": 74,
          "accuracy": 0.9663636363636363
        }
      },
      "auroc": 0.9301284090909091
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 895,
          "fn": 305,
          "accuracy": 0.7458333333333333
        },
        "0.01": {
          "tp": 794,
          "fn": 406,
          "accuracy": 0.6616666666666666
        }
      },
      "auroc": 0.7508895833333334
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3070,
          "fn": 330,
          "accuracy": 0.9029411764705882
        },
        "0.01": {
          "tp": 2920,
          "fn": 480,
          "accuracy": 0.8588235294117647
        }
      },
      "auroc": 0.8668676470588235
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1989,
          "fn": 211,
          "accuracy": 0.9040909090909091
        },
        "0.01": {
          "tp": 1698,
          "fn": 502,
          "accuracy": 0.7718181818181818
        }
      },
      "auroc": 0.8942189393939394
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 271,
          "fn": 929,
          "accuracy": 0.22583333333333333
        },
        "0.01": {
          "tp": 200,
          "fn": 1000,
          "accuracy": 0.16666666666666666
        }
      },
      "auroc": 0.37299444444444446
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2260,
          "fn": 1140,
          "accuracy": 0.6647058823529411
        },
        "0.01": {
          "tp": 1898,
          "fn": 1502,
          "accuracy": 0.558235294117647
        }
      },
      "auroc": 0.7102573529411764
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 4164,
          "fn": 236,
          "accuracy": 0.9463636363636364
        },
        "0.01": {
          "tp": 3824,
          "fn": 576,
          "accuracy": 0.8690909090909091
        }
      },
      "auroc": 0.9121736742424242
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1166,
          "fn": 1234,
          "accuracy": 0.48583333333333334
        },
        "0.01": {
          "tp": 994,
          "fn": 1406,
          "accuracy": 0.4141666666666667
        }
      },
      "auroc": 0.5619420138888889
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 5330,
          "fn": 1470,
          "accuracy": 0.7838235294117647
        },
        "0.01": {
          "tp": 4818,
          "fn": 1982,
          "accuracy": 0.7085294117647059
        }
      },
      "auroc": 0.7885625
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9301541666666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.93268125
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.798909375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        }
      },
      "auroc": 0.8670588541666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        }
      },
      "auroc": 0.8645317708333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 752,
          "fn": 48,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 725,
          "fn": 75,
          "accuracy": 0.90625
        }
      },
      "auroc": 0.8998700520833334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9260864583333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08537604166666665
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        }
      },
      "auroc": 0.50573125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.805234375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.052767708333333316
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 263,
          "accuracy": 0.3425
        },
        "0.01": {
          "tp": 63,
          "fn": 337,
          "accuracy": 0.1575
        }
      },
      "auroc": 0.42900104166666664
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        }
      },
      "auroc": 0.8656604166666668
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.06907187499999996
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 469,
          "accuracy": 0.41375
        },
        "0.01": {
          "tp": 252,
          "fn": 548,
          "accuracy": 0.315
        }
      },
      "auroc": 0.4673661458333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9285010416666668
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.6224802083333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        },
        "0.01": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        }
      },
      "auroc": 0.775490625
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9167354166666668
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.23449166666666665
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        }
      },
      "auroc": 0.5756135416666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.9226182291666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 329,
          "accuracy": 0.1775
        },
        "0.01": {
          "tp": 32,
          "fn": 368,
          "accuracy": 0.08
        }
      },
      "auroc": 0.42848593749999997
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 460,
          "fn": 340,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 403,
          "fn": 397,
          "accuracy": 0.50375
        }
      },
      "auroc": 0.6755520833333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.91880625
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9308031250000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9248046875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.8896447916666668
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.32686875000000004
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        },
        "0.01": {
          "tp": 147,
          "fn": 253,
          "accuracy": 0.3675
        }
      },
      "auroc": 0.6082567708333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        }
      },
      "auroc": 0.9042255208333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        },
        "0.01": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        }
      },
      "auroc": 0.6288359375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 575,
          "fn": 225,
          "accuracy": 0.71875
        },
        "0.01": {
          "tp": 519,
          "fn": 281,
          "accuracy": 0.64875
        }
      },
      "auroc": 0.7665307291666668
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9312739583333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.81629375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        }
      },
      "auroc": 0.8737838541666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        }
      },
      "auroc": 0.7592270833333332
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.06722499999999997
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 296,
          "accuracy": 0.26
        },
        "0.01": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        }
      },
      "auroc": 0.41322604166666654
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        }
      },
      "auroc": 0.8452505208333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        },
        "0.01": {
          "tp": 71,
          "fn": 329,
          "accuracy": 0.1775
        }
      },
      "auroc": 0.44175937499999995
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 427,
          "fn": 373,
          "accuracy": 0.53375
        },
        "0.01": {
          "tp": 319,
          "fn": 481,
          "accuracy": 0.39875
        }
      },
      "auroc": 0.6435049479166667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9341208333333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9230510416666666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9285859375000001
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9211302083333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        }
      },
      "auroc": 0.6118833333333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 210,
          "fn": 190,
          "accuracy": 0.525
        }
      },
      "auroc": 0.7665067708333331
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9276255208333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        }
      },
      "auroc": 0.7674671875000001
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 649,
          "fn": 151,
          "accuracy": 0.81125
        },
        "0.01": {
          "tp": 588,
          "fn": 212,
          "accuracy": 0.735
        }
      },
      "auroc": 0.8475463541666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9232427083333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9232427083333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.8935468750000001
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.8935468750000001
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        }
      },
      "auroc": 0.9083947916666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        }
      },
      "auroc": 0.9083947916666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.900703125
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.900703125
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.856415625
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.856415625
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        }
      },
      "auroc": 0.878559375
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        }
      },
      "auroc": 0.878559375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.934496875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.934496875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9265479166666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9265479166666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9305223958333335
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9305223958333335
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9348531250000001
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9348531250000001
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.7941437499999999
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.7941437499999999
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        }
      },
      "auroc": 0.8644984375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        }
      },
      "auroc": 0.8644984375
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.8695708333333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.8695708333333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        }
      },
      "auroc": 0.835071875
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        }
      },
      "auroc": 0.835071875
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 236,
          "fn": 164,
          "accuracy": 0.59
        }
      },
      "auroc": 0.8523213541666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 236,
          "fn": 164,
          "accuracy": 0.59
        }
      },
      "auroc": 0.8523213541666667
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2129,
          "fn": 71,
          "accuracy": 0.9677272727272728
        },
        "0.01": {
          "tp": 2025,
          "fn": 175,
          "accuracy": 0.9204545454545454
        }
      },
      "auroc": 0.9215330492424242
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 786,
          "fn": 414,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 668,
          "fn": 532,
          "accuracy": 0.5566666666666666
        }
      },
      "auroc": 0.7180263888888888
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2915,
          "fn": 485,
          "accuracy": 0.8573529411764705
        },
        "0.01": {
          "tp": 2693,
          "fn": 707,
          "accuracy": 0.7920588235294118
        }
      },
      "auroc": 0.8497071691176472
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1820,
          "fn": 380,
          "accuracy": 0.8272727272727273
        },
        "0.01": {
          "tp": 1425,
          "fn": 775,
          "accuracy": 0.6477272727272727
        }
      },
      "auroc": 0.8666278409090908
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 219,
          "fn": 981,
          "accuracy": 0.1825
        },
        "0.01": {
          "tp": 156,
          "fn": 1044,
          "accuracy": 0.13
        }
      },
      "auroc": 0.3486909722222222
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2039,
          "fn": 1361,
          "accuracy": 0.5997058823529412
        },
        "0.01": {
          "tp": 1581,
          "fn": 1819,
          "accuracy": 0.465
        }
      },
      "auroc": 0.6838265931372549
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3949,
          "fn": 451,
          "accuracy": 0.8975
        },
        "0.01": {
          "tp": 3450,
          "fn": 950,
          "accuracy": 0.7840909090909091
        }
      },
      "auroc": 0.8940804450757577
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1005,
          "fn": 1395,
          "accuracy": 0.41875
        },
        "0.01": {
          "tp": 824,
          "fn": 1576,
          "accuracy": 0.3433333333333333
        }
      },
      "auroc": 0.5333586805555556
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 4954,
          "fn": 1846,
          "accuracy": 0.7285294117647059
        },
        "0.01": {
          "tp": 4274,
          "fn": 2526,
          "accuracy": 0.6285294117647059
        }
      },
      "auroc": 0.7667668811274511
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9199979166666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.927603125
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        }
      },
      "auroc": 0.7505822916666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        }
      },
      "auroc": 0.8428953124999999
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        },
        "0.01": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        }
      },
      "auroc": 0.8352901041666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 729,
          "fn": 71,
          "accuracy": 0.91125
        },
        "0.01": {
          "tp": 668,
          "fn": 132,
          "accuracy": 0.835
        }
      },
      "auroc": 0.88524921875
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.91463125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08564791666666664
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        }
      },
      "auroc": 0.5001395833333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.7288791666666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.05771979166666664
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 312,
          "accuracy": 0.22
        },
        "0.01": {
          "tp": 27,
          "fn": 373,
          "accuracy": 0.0675
        }
      },
      "auroc": 0.39329947916666663
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        },
        "0.01": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        }
      },
      "auroc": 0.8217552083333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07168385416666664
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 280,
          "fn": 520,
          "accuracy": 0.35
        },
        "0.01": {
          "tp": 202,
          "fn": 598,
          "accuracy": 0.2525
        }
      },
      "auroc": 0.44671953124999997
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9272625
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        },
        "0.01": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        }
      },
      "auroc": 0.5297499999999999
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        },
        "0.01": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        }
      },
      "auroc": 0.7285062499999999
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.89673125
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.18334374999999997
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        },
        "0.01": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        }
      },
      "auroc": 0.5400375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        }
      },
      "auroc": 0.911996875
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 40,
          "fn": 360,
          "accuracy": 0.1
        },
        "0.01": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        }
      },
      "auroc": 0.35654687499999993
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 420,
          "fn": 380,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 345,
          "fn": 455,
          "accuracy": 0.43125
        }
      },
      "auroc": 0.634271875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9150041666666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9291427083333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9220734375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.8546218750000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.23827083333333332
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        },
        "0.01": {
          "tp": 111,
          "fn": 289,
          "accuracy": 0.2775
        }
      },
      "auroc": 0.5464463541666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        }
      },
      "auroc": 0.8848130208333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        },
        "0.01": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        }
      },
      "auroc": 0.5837067708333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 547,
          "fn": 253,
          "accuracy": 0.68375
        },
        "0.01": {
          "tp": 471,
          "fn": 329,
          "accuracy": 0.58875
        }
      },
      "auroc": 0.7342598958333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.93006875
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.677559375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        }
      },
      "auroc": 0.8038140625
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.6305437500000002
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.05954479166666664
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 351,
          "accuracy": 0.1225
        },
        "0.01": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        }
      },
      "auroc": 0.34504427083333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        }
      },
      "auroc": 0.78030625
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 60,
          "fn": 340,
          "accuracy": 0.15
        },
        "0.01": {
          "tp": 14,
          "fn": 386,
          "accuracy": 0.035
        }
      },
      "auroc": 0.3685520833333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 493,
          "accuracy": 0.38375
        },
        "0.01": {
          "tp": 217,
          "fn": 583,
          "accuracy": 0.27125
        }
      },
      "auroc": 0.5744291666666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9348531250000001
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.90056875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        }
      },
      "auroc": 0.9177109375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9097927083333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.4828364583333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 172,
          "fn": 228,
          "accuracy": 0.43
        }
      },
      "auroc": 0.6963145833333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.9223229166666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 150,
          "fn": 250,
          "accuracy": 0.375
        }
      },
      "auroc": 0.6917026041666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 597,
          "fn": 203,
          "accuracy": 0.74625
        },
        "0.01": {
          "tp": 517,
          "fn": 283,
          "accuracy": 0.64625
        }
      },
      "auroc": 0.8070127604166666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9085510416666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9085510416666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        }
      },
      "auroc": 0.8547302083333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        }
      },
      "auroc": 0.8547302083333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        }
      },
      "auroc": 0.8816406250000001
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        }
      },
      "auroc": 0.8816406250000001
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        }
      },
      "auroc": 0.844903125
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        }
      },
      "auroc": 0.844903125
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.7616875
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.7616875
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        },
        "0.01": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        }
      },
      "auroc": 0.8032953125
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        },
        "0.01": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        }
      },
      "auroc": 0.8032953125
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9332437499999999
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9332437499999999
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.910003125
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.910003125
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9216234375
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9216234375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.933421875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.933421875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.6933177083333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.6933177083333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        }
      },
      "auroc": 0.8133697916666665
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        }
      },
      "auroc": 0.8133697916666665
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.807115625
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.807115625
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.7562156250000001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.7562156250000001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": {
          "tp": 157,
          "fn": 243,
          "accuracy": 0.3925
        }
      },
      "auroc": 0.7816656249999999
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": {
          "tp": 157,
          "fn": 243,
          "accuracy": 0.3925
        }
      },
      "auroc": 0.7816656249999999
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2044,
          "fn": 156,
          "accuracy": 0.9290909090909091
        },
        "0.01": {
          "tp": 1864,
          "fn": 336,
          "accuracy": 0.8472727272727273
        }
      },
      "auroc": 0.9076603219696969
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 677,
          "fn": 523,
          "accuracy": 0.5641666666666667
        },
        "0.01": {
          "tp": 536,
          "fn": 664,
          "accuracy": 0.44666666666666666
        }
      },
      "auroc": 0.6737777777777778
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2721,
          "fn": 679,
          "accuracy": 0.8002941176470588
        },
        "0.01": {
          "tp": 2400,
          "fn": 1000,
          "accuracy": 0.7058823529411765
        }
      },
      "auroc": 0.8251135416666667
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1500,
          "fn": 700,
          "accuracy": 0.6818181818181818
        },
        "0.01": {
          "tp": 1064,
          "fn": 1136,
          "accuracy": 0.48363636363636364
        }
      },
      "auroc": 0.8119755681818182
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 1045,
          "accuracy": 0.12916666666666668
        },
        "0.01": {
          "tp": 96,
          "fn": 1104,
          "accuracy": 0.08
        }
      },
      "auroc": 0.2953829861111111
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1655,
          "fn": 1745,
          "accuracy": 0.48676470588235293
        },
        "0.01": {
          "tp": 1160,
          "fn": 2240,
          "accuracy": 0.3411764705882353
        }
      },
      "auroc": 0.6296487745098038
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3544,
          "fn": 856,
          "accuracy": 0.8054545454545454
        },
        "0.01": {
          "tp": 2928,
          "fn": 1472,
          "accuracy": 0.6654545454545454
        }
      },
      "auroc": 0.8598179450757576
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 832,
          "fn": 1568,
          "accuracy": 0.3466666666666667
        },
        "0.01": {
          "tp": 632,
          "fn": 1768,
          "accuracy": 0.2633333333333333
        }
      },
      "auroc": 0.4845803819444444
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 4376,
          "fn": 2424,
          "accuracy": 0.6435294117647059
        },
        "0.01": {
          "tp": 3560,
          "fn": 3240,
          "accuracy": 0.5235294117647059
        }
      },
      "auroc": 0.7273811580882352
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.8750166666666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.6262395833333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 139,
          "fn": 261,
          "accuracy": 0.3475
        }
      },
      "auroc": 0.750628125
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.8670052083333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.3804489583333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        },
        "0.01": {
          "tp": 122,
          "fn": 278,
          "accuracy": 0.305
        }
      },
      "auroc": 0.6237270833333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        }
      },
      "auroc": 0.8710109374999999
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 332,
          "accuracy": 0.17
        },
        "0.01": {
          "tp": 19,
          "fn": 381,
          "accuracy": 0.0475
        }
      },
      "auroc": 0.5033442708333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 407,
          "fn": 393,
          "accuracy": 0.50875
        },
        "0.01": {
          "tp": 261,
          "fn": 539,
          "accuracy": 0.32625
        }
      },
      "auroc": 0.6871776041666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9097135416666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.062427083333333314
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.4860703125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.3562208333333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.05240416666666664
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 393,
          "accuracy": 0.0175
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.2043125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        },
        "0.01": {
          "tp": 171,
          "fn": 229,
          "accuracy": 0.4275
        }
      },
      "auroc": 0.6329671875
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.05741562499999999
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 609,
          "accuracy": 0.23875
        },
        "0.01": {
          "tp": 171,
          "fn": 629,
          "accuracy": 0.21375
        }
      },
      "auroc": 0.34519140625
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.7722947916666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.19003854166666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 287,
          "accuracy": 0.2825
        },
        "0.01": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        }
      },
      "auroc": 0.48116666666666663
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        }
      },
      "auroc": 0.6839458333333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08095416666666663
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 336,
          "accuracy": 0.16
        },
        "0.01": {
          "tp": 24,
          "fn": 376,
          "accuracy": 0.06
        }
      },
      "auroc": 0.38244999999999996
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        },
        "0.01": {
          "tp": 86,
          "fn": 314,
          "accuracy": 0.215
        }
      },
      "auroc": 0.7281203125
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.13549635416666664
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 623,
          "accuracy": 0.22125
        },
        "0.01": {
          "tp": 86,
          "fn": 714,
          "accuracy": 0.1075
        }
      },
      "auroc": 0.43180833333333335
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.92308125
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        }
      },
      "auroc": 0.7334416666666668
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        },
        "0.01": {
          "tp": 232,
          "fn": 168,
          "accuracy": 0.58
        }
      },
      "auroc": 0.8282614583333332
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.5459625
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.0724583333333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 369,
          "accuracy": 0.0775
        },
        "0.01": {
          "tp": 11,
          "fn": 389,
          "accuracy": 0.0275
        }
      },
      "auroc": 0.3092104166666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        },
        "0.01": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        }
      },
      "auroc": 0.7345218749999999
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 295,
          "accuracy": 0.2625
        },
        "0.01": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        }
      },
      "auroc": 0.40295000000000003
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 330,
          "fn": 470,
          "accuracy": 0.4125
        },
        "0.01": {
          "tp": 243,
          "fn": 557,
          "accuracy": 0.30375
        }
      },
      "auroc": 0.5687359375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9161916666666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.17517083333333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        },
        "0.01": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        }
      },
      "auroc": 0.54568125
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.28120625
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.04945937499999997
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.16533281249999998
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 171,
          "fn": 229,
          "accuracy": 0.4275
        }
      },
      "auroc": 0.5986989583333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.11231510416666665
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 604,
          "accuracy": 0.245
        },
        "0.01": {
          "tp": 171,
          "fn": 629,
          "accuracy": 0.21375
        }
      },
      "auroc": 0.35550703125000005
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.8779833333333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.5463114583333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 134,
          "fn": 266,
          "accuracy": 0.335
        }
      },
      "auroc": 0.7121473958333332
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.75023125
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.18081041666666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 94,
          "fn": 306,
          "accuracy": 0.235
        },
        "0.01": {
          "tp": 39,
          "fn": 361,
          "accuracy": 0.0975
        }
      },
      "auroc": 0.46552083333333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        }
      },
      "auroc": 0.8141072916666666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 28,
          "fn": 372,
          "accuracy": 0.07
        },
        "0.01": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        }
      },
      "auroc": 0.3635609375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 500,
          "accuracy": 0.375
        },
        "0.01": {
          "tp": 173,
          "fn": 627,
          "accuracy": 0.21625
        }
      },
      "auroc": 0.5888341145833332
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.6998489583333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.6998489583333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.5692833333333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.5692833333333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.6345661458333335
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.6345661458333335
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.5089354166666668
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.5089354166666668
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.394546875
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.394546875
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 371,
          "accuracy": 0.0725
        },
        "0.01": {
          "tp": 11,
          "fn": 389,
          "accuracy": 0.0275
        }
      },
      "auroc": 0.4517411458333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 371,
          "accuracy": 0.0725
        },
        "0.01": {
          "tp": 11,
          "fn": 389,
          "accuracy": 0.0275
        }
      },
      "auroc": 0.4517411458333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        }
      },
      "auroc": 0.7943666666666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        }
      },
      "auroc": 0.7943666666666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.6574229166666666
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.6574229166666666
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        },
        "0.01": {
          "tp": 73,
          "fn": 327,
          "accuracy": 0.1825
        }
      },
      "auroc": 0.7258947916666665
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        },
        "0.01": {
          "tp": 73,
          "fn": 327,
          "accuracy": 0.1825
        }
      },
      "auroc": 0.7258947916666665
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.816690625
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.816690625
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.32539375000000004
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.32539375000000004
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 265,
          "accuracy": 0.3375
        },
        "0.01": {
          "tp": 72,
          "fn": 328,
          "accuracy": 0.18
        }
      },
      "auroc": 0.5710421875
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 265,
          "accuracy": 0.3375
        },
        "0.01": {
          "tp": 72,
          "fn": 328,
          "accuracy": 0.18
        }
      },
      "auroc": 0.5710421875
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.5515854166666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.5515854166666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.495053125
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.495053125
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 89,
          "fn": 311,
          "accuracy": 0.2225
        },
        "0.01": {
          "tp": 45,
          "fn": 355,
          "accuracy": 0.1125
        }
      },
      "auroc": 0.5233192708333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 89,
          "fn": 311,
          "accuracy": 0.2225
        },
        "0.01": {
          "tp": 45,
          "fn": 355,
          "accuracy": 0.1125
        }
      },
      "auroc": 0.5233192708333333
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1450,
          "fn": 750,
          "accuracy": 0.6590909090909091
        },
        "0.01": {
          "tp": 1046,
          "fn": 1154,
          "accuracy": 0.47545454545454546
        }
      },
      "auroc": 0.7859734848484848
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 1013,
          "accuracy": 0.15583333333333332
        },
        "0.01": {
          "tp": 70,
          "fn": 1130,
          "accuracy": 0.058333333333333334
        }
      },
      "auroc": 0.38893819444444444
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1637,
          "fn": 1763,
          "accuracy": 0.4814705882352941
        },
        "0.01": {
          "tp": 1116,
          "fn": 2284,
          "accuracy": 0.32823529411764707
        }
      },
      "auroc": 0.6458433823529411
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 517,
          "fn": 1683,
          "accuracy": 0.235
        },
        "0.01": {
          "tp": 242,
          "fn": 1958,
          "accuracy": 0.11
        }
      },
      "auroc": 0.5387519886363636
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 1186,
          "accuracy": 0.011666666666666667
        },
        "0.01": {
          "tp": 4,
          "fn": 1196,
          "accuracy": 0.0033333333333333335
        }
      },
      "auroc": 0.13608923611111107
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 531,
          "fn": 2869,
          "accuracy": 0.1561764705882353
        },
        "0.01": {
          "tp": 246,
          "fn": 3154,
          "accuracy": 0.07235294117647059
        }
      },
      "auroc": 0.3966357230392156
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1967,
          "fn": 2433,
          "accuracy": 0.4470454545454545
        },
        "0.01": {
          "tp": 1288,
          "fn": 3112,
          "accuracy": 0.2927272727272727
        }
      },
      "auroc": 0.6623627367424242
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 2199,
          "accuracy": 0.08375
        },
        "0.01": {
          "tp": 74,
          "fn": 2326,
          "accuracy": 0.030833333333333334
        }
      },
      "auroc": 0.2625137152777778
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2168,
          "fn": 4632,
          "accuracy": 0.31882352941176473
        },
        "0.01": {
          "tp": 1362,
          "fn": 5438,
          "accuracy": 0.20029411764705882
        }
      },
      "auroc": 0.5212395526960785
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9332833333333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9342458333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.8087416666666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.871975
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        }
      },
      "auroc": 0.8710125
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 756,
          "fn": 44,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 737,
          "fn": 63,
          "accuracy": 0.92125
        }
      },
      "auroc": 0.9031104166666668
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9315864583333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.09185833333333332
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        }
      },
      "auroc": 0.5117223958333332
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        }
      },
      "auroc": 0.8222135416666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.05574791666666664
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 74,
          "fn": 326,
          "accuracy": 0.185
        }
      },
      "auroc": 0.4389807291666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        }
      },
      "auroc": 0.8769000000000001
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.073803125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 460,
          "accuracy": 0.425
        },
        "0.01": {
          "tp": 269,
          "fn": 531,
          "accuracy": 0.33625
        }
      },
      "auroc": 0.47535156250000005
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9338989583333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.662615625
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        }
      },
      "auroc": 0.7982572916666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9278718750000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.24606041666666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        },
        "0.01": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        }
      },
      "auroc": 0.5869661458333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9308854166666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 296,
          "accuracy": 0.26
        },
        "0.01": {
          "tp": 40,
          "fn": 360,
          "accuracy": 0.1
        }
      },
      "auroc": 0.4543380208333332
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 501,
          "fn": 299,
          "accuracy": 0.62625
        },
        "0.01": {
          "tp": 426,
          "fn": 374,
          "accuracy": 0.5325
        }
      },
      "auroc": 0.69261171875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9315645833333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.934871875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9332182291666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9005854166666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.27120833333333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 156,
          "fn": 244,
          "accuracy": 0.39
        }
      },
      "auroc": 0.585896875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        }
      },
      "auroc": 0.9160750000000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        },
        "0.01": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        }
      },
      "auroc": 0.6030401041666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 589,
          "fn": 211,
          "accuracy": 0.73625
        },
        "0.01": {
          "tp": 549,
          "fn": 251,
          "accuracy": 0.68625
        }
      },
      "auroc": 0.7595575520833333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9340229166666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.8508364583333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        }
      },
      "auroc": 0.8924296875
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        }
      },
      "auroc": 0.7629322916666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.06705104166666664
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        },
        "0.01": {
          "tp": 51,
          "fn": 349,
          "accuracy": 0.1275
        }
      },
      "auroc": 0.41499166666666665
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        },
        "0.01": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        }
      },
      "auroc": 0.8484776041666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 248,
          "accuracy": 0.38
        },
        "0.01": {
          "tp": 106,
          "fn": 294,
          "accuracy": 0.265
        }
      },
      "auroc": 0.45894375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 463,
          "fn": 337,
          "accuracy": 0.57875
        },
        "0.01": {
          "tp": 355,
          "fn": 445,
          "accuracy": 0.44375
        }
      },
      "auroc": 0.6537106770833333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9316666666666666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9334374999999999
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9261958333333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.6161729166666666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        },
        "0.01": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        }
      },
      "auroc": 0.7711843749999999
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9307020833333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        }
      },
      "auroc": 0.7739197916666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 655,
          "fn": 145,
          "accuracy": 0.81875
        },
        "0.01": {
          "tp": 614,
          "fn": 186,
          "accuracy": 0.7675
        }
      },
      "auroc": 0.8523109375000001
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9340895833333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9340895833333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9151822916666668
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9151822916666668
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9246359375000001
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9246359375000001
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9133083333333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9133083333333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.8751
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.8751
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 301,
          "fn": 99,
          "accuracy": 0.7525
        }
      },
      "auroc": 0.8942041666666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 301,
          "fn": 99,
          "accuracy": 0.7525
        }
      },
      "auroc": 0.8942041666666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.934871875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.934871875
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9285927083333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9285927083333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9317322916666666
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9317322916666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.7849552083333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.7849552083333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 320,
          "fn": 80,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        }
      },
      "auroc": 0.8600817708333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 320,
          "fn": 80,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        }
      },
      "auroc": 0.8600817708333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.8784458333333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.8784458333333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        }
      },
      "auroc": 0.8484562500000001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        }
      },
      "auroc": 0.8484562500000001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        }
      },
      "auroc": 0.8634510416666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        }
      },
      "auroc": 0.8634510416666666
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2157,
          "fn": 43,
          "accuracy": 0.9804545454545455
        },
        "0.01": {
          "tp": 2088,
          "fn": 112,
          "accuracy": 0.9490909090909091
        }
      },
      "auroc": 0.9270375946969697
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 843,
          "fn": 357,
          "accuracy": 0.7025
        },
        "0.01": {
          "tp": 733,
          "fn": 467,
          "accuracy": 0.6108333333333333
        }
      },
      "auroc": 0.7341887152777777
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3000,
          "fn": 400,
          "accuracy": 0.8823529411764706
        },
        "0.01": {
          "tp": 2821,
          "fn": 579,
          "accuracy": 0.8297058823529412
        }
      },
      "auroc": 0.8589732843137255
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1876,
          "fn": 324,
          "accuracy": 0.8527272727272728
        },
        "0.01": {
          "tp": 1511,
          "fn": 689,
          "accuracy": 0.6868181818181818
        }
      },
      "auroc": 0.8752085227272728
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 232,
          "fn": 968,
          "accuracy": 0.19333333333333333
        },
        "0.01": {
          "tp": 174,
          "fn": 1026,
          "accuracy": 0.145
        }
      },
      "auroc": 0.3441637152777777
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2108,
          "fn": 1292,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 1685,
          "fn": 1715,
          "accuracy": 0.49558823529411766
        }
      },
      "auroc": 0.6877809436274509
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 4033,
          "fn": 367,
          "accuracy": 0.9165909090909091
        },
        "0.01": {
          "tp": 3599,
          "fn": 801,
          "accuracy": 0.8179545454545455
        }
      },
      "auroc": 0.9011230587121213
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1075,
          "fn": 1325,
          "accuracy": 0.4479166666666667
        },
        "0.01": {
          "tp": 907,
          "fn": 1493,
          "accuracy": 0.3779166666666667
        }
      },
      "auroc": 0.5391762152777777
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 5108,
          "fn": 1692,
          "accuracy": 0.7511764705882353
        },
        "0.01": {
          "tp": 4506,
          "fn": 2294,
          "accuracy": 0.6626470588235294
        }
      },
      "auroc": 0.7733771139705883
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9342760416666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.930871875
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9325739583333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9346645833333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9097020833333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9221833333333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9344703125
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        }
      },
      "auroc": 0.9202869791666668
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 789,
          "fn": 11,
          "accuracy": 0.98625
        },
        "0.01": {
          "tp": 754,
          "fn": 46,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9273786458333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9306666666666668
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.5537072916666668
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        }
      },
      "auroc": 0.7421869791666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.8926635416666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.46793541666666677
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        },
        "0.01": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        }
      },
      "auroc": 0.6802994791666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9116651041666668
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 41,
          "fn": 359,
          "accuracy": 0.1025
        },
        "0.01": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        }
      },
      "auroc": 0.5108213541666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 427,
          "fn": 373,
          "accuracy": 0.53375
        },
        "0.01": {
          "tp": 358,
          "fn": 442,
          "accuracy": 0.4475
        }
      },
      "auroc": 0.7112432291666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9146937500000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.8272593749999999
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        }
      },
      "auroc": 0.8709765625000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        }
      },
      "auroc": 0.895928125
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.7106020833333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        },
        "0.01": {
          "tp": 178,
          "fn": 222,
          "accuracy": 0.445
        }
      },
      "auroc": 0.8032651041666665
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        }
      },
      "auroc": 0.9053109375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 228,
          "fn": 172,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 93,
          "fn": 307,
          "accuracy": 0.2325
        }
      },
      "auroc": 0.7689307291666665
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 606,
          "fn": 194,
          "accuracy": 0.7575
        },
        "0.01": {
          "tp": 416,
          "fn": 384,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8371208333333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9075739583333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9055114583333335
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9065427083333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.8970885416666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.7403250000000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        },
        "0.01": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        }
      },
      "auroc": 0.8187067708333332
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        }
      },
      "auroc": 0.90233125
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        },
        "0.01": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        }
      },
      "auroc": 0.8229182291666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 641,
          "fn": 159,
          "accuracy": 0.80125
        },
        "0.01": {
          "tp": 523,
          "fn": 277,
          "accuracy": 0.65375
        }
      },
      "auroc": 0.8626247395833333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9278541666666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.8578052083333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        }
      },
      "auroc": 0.8928296875
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        }
      },
      "auroc": 0.8645875000000001
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.5753104166666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 126,
          "fn": 274,
          "accuracy": 0.315
        }
      },
      "auroc": 0.7199489583333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.8962208333333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 109,
          "fn": 291,
          "accuracy": 0.2725
        }
      },
      "auroc": 0.7165578125000001
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 551,
          "fn": 249,
          "accuracy": 0.68875
        },
        "0.01": {
          "tp": 425,
          "fn": 375,
          "accuracy": 0.53125
        }
      },
      "auroc": 0.8063893229166667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.931546875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9105645833333335
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9210557291666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9114760416666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.8175489583333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        }
      },
      "auroc": 0.8645125
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        }
      },
      "auroc": 0.9215114583333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        }
      },
      "auroc": 0.8640567708333332
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 725,
          "fn": 75,
          "accuracy": 0.90625
        },
        "0.01": {
          "tp": 617,
          "fn": 183,
          "accuracy": 0.77125
        }
      },
      "auroc": 0.8927841145833333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.8990760416666668
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.8990760416666668
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.880640625
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.880640625
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        }
      },
      "auroc": 0.8898583333333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        }
      },
      "auroc": 0.8898583333333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9009447916666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9009447916666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.8847875000000001
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.8847875000000001
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 295,
          "fn": 105,
          "accuracy": 0.7375
        }
      },
      "auroc": 0.8928661458333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 295,
          "fn": 105,
          "accuracy": 0.7375
        }
      },
      "auroc": 0.8928661458333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9341833333333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9341833333333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.925025
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.925025
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9296041666666666
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9296041666666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9337979166666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9337979166666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9256989583333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9256989583333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9297484375
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9297484375
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        }
      },
      "auroc": 0.874775
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        }
      },
      "auroc": 0.874775
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        }
      },
      "auroc": 0.8650552083333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        }
      },
      "auroc": 0.8650552083333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        }
      },
      "auroc": 0.8699151041666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        }
      },
      "auroc": 0.8699151041666666
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2119,
          "fn": 81,
          "accuracy": 0.9631818181818181
        },
        "0.01": {
          "tp": 1959,
          "fn": 241,
          "accuracy": 0.8904545454545455
        }
      },
      "auroc": 0.9172171401515152
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 913,
          "fn": 287,
          "accuracy": 0.7608333333333334
        },
        "0.01": {
          "tp": 695,
          "fn": 505,
          "accuracy": 0.5791666666666667
        }
      },
      "auroc": 0.8309532986111111
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3032,
          "fn": 368,
          "accuracy": 0.8917647058823529
        },
        "0.01": {
          "tp": 2654,
          "fn": 746,
          "accuracy": 0.7805882352941177
        }
      },
      "auroc": 0.8867710784313726
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2018,
          "fn": 182,
          "accuracy": 0.9172727272727272
        },
        "0.01": {
          "tp": 1709,
          "fn": 491,
          "accuracy": 0.7768181818181819
        }
      },
      "auroc": 0.8979650568181818
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 546,
          "fn": 654,
          "accuracy": 0.455
        },
        "0.01": {
          "tp": 321,
          "fn": 879,
          "accuracy": 0.2675
        }
      },
      "auroc": 0.7035706597222222
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2564,
          "fn": 836,
          "accuracy": 0.7541176470588236
        },
        "0.01": {
          "tp": 2030,
          "fn": 1370,
          "accuracy": 0.5970588235294118
        }
      },
      "auroc": 0.8293552696078432
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 4137,
          "fn": 263,
          "accuracy": 0.9402272727272727
        },
        "0.01": {
          "tp": 3668,
          "fn": 732,
          "accuracy": 0.8336363636363636
        }
      },
      "auroc": 0.9075910984848485
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1459,
          "fn": 941,
          "accuracy": 0.6079166666666667
        },
        "0.01": {
          "tp": 1016,
          "fn": 1384,
          "accuracy": 0.42333333333333334
        }
      },
      "auroc": 0.7672619791666666
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 5596,
          "fn": 1204,
          "accuracy": 0.8229411764705883
        },
        "0.01": {
          "tp": 4684,
          "fn": 2116,
          "accuracy": 0.6888235294117647
        }
      },
      "auroc": 0.8580631740196079
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9334552083333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9343317708333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.8243322916666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        }
      },
      "auroc": 0.8797703124999999
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        }
      },
      "auroc": 0.87889375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 762,
          "fn": 38,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 745,
          "fn": 55,
          "accuracy": 0.93125
        }
      },
      "auroc": 0.9070510416666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9278927083333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.09683229166666665
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        },
        "0.01": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        }
      },
      "auroc": 0.5123625
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.8051354166666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.05644687499999997
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 262,
          "accuracy": 0.345
        },
        "0.01": {
          "tp": 69,
          "fn": 331,
          "accuracy": 0.1725
        }
      },
      "auroc": 0.43079114583333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        }
      },
      "auroc": 0.8665140625
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07663958333333332
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 465,
          "accuracy": 0.41875
        },
        "0.01": {
          "tp": 258,
          "fn": 542,
          "accuracy": 0.3225
        }
      },
      "auroc": 0.4715768229166667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.93369375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.6899666666666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 256,
          "fn": 144,
          "accuracy": 0.64
        }
      },
      "auroc": 0.8118302083333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9312572916666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.2600833333333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.5956703125
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9324755208333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 280,
          "accuracy": 0.3
        },
        "0.01": {
          "tp": 64,
          "fn": 336,
          "accuracy": 0.16
        }
      },
      "auroc": 0.475025
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 518,
          "fn": 282,
          "accuracy": 0.6475
        },
        "0.01": {
          "tp": 455,
          "fn": 345,
          "accuracy": 0.56875
        }
      },
      "auroc": 0.7037502604166669
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9279083333333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9342375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9310729166666668
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        }
      },
      "auroc": 0.8945322916666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.27548125
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 151,
          "fn": 249,
          "accuracy": 0.3775
        }
      },
      "auroc": 0.5850067708333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9112203125
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        },
        "0.01": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        }
      },
      "auroc": 0.6048593749999999
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 585,
          "fn": 215,
          "accuracy": 0.73125
        },
        "0.01": {
          "tp": 538,
          "fn": 262,
          "accuracy": 0.6725
        }
      },
      "auroc": 0.75803984375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.933353125
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        }
      },
      "auroc": 0.8418802083333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        }
      },
      "auroc": 0.8876166666666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.7450281249999999
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.06372708333333332
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 291,
          "accuracy": 0.2725
        },
        "0.01": {
          "tp": 49,
          "fn": 351,
          "accuracy": 0.1225
        }
      },
      "auroc": 0.4043776041666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        }
      },
      "auroc": 0.839190625
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 249,
          "accuracy": 0.3775
        },
        "0.01": {
          "tp": 96,
          "fn": 304,
          "accuracy": 0.24
        }
      },
      "auroc": 0.4528036458333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 458,
          "fn": 342,
          "accuracy": 0.5725
        },
        "0.01": {
          "tp": 342,
          "fn": 458,
          "accuracy": 0.4275
        }
      },
      "auroc": 0.6459971354166667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9324729166666668
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.932453125
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9324630208333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9273895833333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.6534833333333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        }
      },
      "auroc": 0.7904364583333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.92993125
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 273,
          "fn": 127,
          "accuracy": 0.6825
        },
        "0.01": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        }
      },
      "auroc": 0.7929682291666666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 668,
          "fn": 132,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 618,
          "fn": 182,
          "accuracy": 0.7725
        }
      },
      "auroc": 0.8614497395833333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9290406250000001
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9290406250000001
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9103739583333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9103739583333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        }
      },
      "auroc": 0.9197072916666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        }
      },
      "auroc": 0.9197072916666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9097625000000001
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9097625000000001
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.859128125
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.859128125
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        }
      },
      "auroc": 0.8844453125000001
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        }
      },
      "auroc": 0.8844453125000001
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9324270833333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9324270833333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9338177083333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9338177083333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        }
      },
      "auroc": 0.8147229166666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        }
      },
      "auroc": 0.8147229166666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        }
      },
      "auroc": 0.8749656250000001
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        }
      },
      "auroc": 0.8749656250000001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.87515625
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.87515625
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        }
      },
      "auroc": 0.837240625
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        }
      },
      "auroc": 0.837240625
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        }
      },
      "auroc": 0.8561984375
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        }
      },
      "auroc": 0.8561984375
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2146,
          "fn": 54,
          "accuracy": 0.9754545454545455
        },
        "0.01": {
          "tp": 2074,
          "fn": 126,
          "accuracy": 0.9427272727272727
        }
      },
      "auroc": 0.9249913825757574
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 857,
          "fn": 343,
          "accuracy": 0.7141666666666666
        },
        "0.01": {
          "tp": 746,
          "fn": 454,
          "accuracy": 0.6216666666666667
        }
      },
      "auroc": 0.7381375000000001
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3003,
          "fn": 397,
          "accuracy": 0.883235294117647
        },
        "0.01": {
          "tp": 2820,
          "fn": 580,
          "accuracy": 0.8294117647058824
        }
      },
      "auroc": 0.8590429534313726
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1868,
          "fn": 332,
          "accuracy": 0.8490909090909091
        },
        "0.01": {
          "tp": 1518,
          "fn": 682,
          "accuracy": 0.69
        }
      },
      "auroc": 0.8720403409090909
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 252,
          "fn": 948,
          "accuracy": 0.21
        },
        "0.01": {
          "tp": 186,
          "fn": 1014,
          "accuracy": 0.155
        }
      },
      "auroc": 0.35559236111111114
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2120,
          "fn": 1280,
          "accuracy": 0.6235294117647059
        },
        "0.01": {
          "tp": 1704,
          "fn": 1696,
          "accuracy": 0.5011764705882353
        }
      },
      "auroc": 0.6897645833333333
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 4014,
          "fn": 386,
          "accuracy": 0.9122727272727272
        },
        "0.01": {
          "tp": 3592,
          "fn": 808,
          "accuracy": 0.8163636363636364
        }
      },
      "auroc": 0.8985158617424243
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1109,
          "fn": 1291,
          "accuracy": 0.46208333333333335
        },
        "0.01": {
          "tp": 932,
          "fn": 1468,
          "accuracy": 0.3883333333333333
        }
      },
      "auroc": 0.5468649305555555
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 5123,
          "fn": 1677,
          "accuracy": 0.7533823529411765
        },
        "0.01": {
          "tp": 4524,
          "fn": 2276,
          "accuracy": 0.6652941176470588
        }
      },
      "auroc": 0.7744037683823529
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.933515625
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9343619791666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.8323822916666668
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        }
      },
      "auroc": 0.8837953125
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        }
      },
      "auroc": 0.8829489583333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 764,
          "fn": 36,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 746,
          "fn": 54,
          "accuracy": 0.9325
        }
      },
      "auroc": 0.9090786458333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.93195625
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.10040624999999997
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        }
      },
      "auroc": 0.5161812499999999
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        }
      },
      "auroc": 0.8409083333333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.05749583333333332
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 240,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 99,
          "fn": 301,
          "accuracy": 0.2475
        }
      },
      "auroc": 0.4492020833333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        }
      },
      "auroc": 0.8864322916666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07895104166666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 441,
          "accuracy": 0.44875
        },
        "0.01": {
          "tp": 294,
          "fn": 506,
          "accuracy": 0.3675
        }
      },
      "auroc": 0.48269166666666663
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.933503125
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        }
      },
      "auroc": 0.6977062500000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        }
      },
      "auroc": 0.8156046874999999
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9314749999999999
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.27638229166666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        },
        "0.01": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        }
      },
      "auroc": 0.6039286458333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9324890625
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 269,
          "accuracy": 0.3275
        },
        "0.01": {
          "tp": 59,
          "fn": 341,
          "accuracy": 0.1475
        }
      },
      "auroc": 0.48704427083333346
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 531,
          "fn": 269,
          "accuracy": 0.66375
        },
        "0.01": {
          "tp": 450,
          "fn": 350,
          "accuracy": 0.5625
        }
      },
      "auroc": 0.7097666666666668
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9300291666666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9329822916666668
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9315057291666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9033645833333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.337165625
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        },
        "0.01": {
          "tp": 162,
          "fn": 238,
          "accuracy": 0.405
        }
      },
      "auroc": 0.6202651041666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.916696875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        }
      },
      "auroc": 0.6350739583333335
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 591,
          "fn": 209,
          "accuracy": 0.73875
        },
        "0.01": {
          "tp": 548,
          "fn": 252,
          "accuracy": 0.685
        }
      },
      "auroc": 0.7758854166666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9340229166666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.8574239583333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        }
      },
      "auroc": 0.8957234375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.7844458333333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07392708333333332
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        },
        "0.01": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        }
      },
      "auroc": 0.42918645833333335
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        },
        "0.01": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8592343750000001
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 242,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 100,
          "fn": 300,
          "accuracy": 0.25
        }
      },
      "auroc": 0.46567552083333336
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 469,
          "fn": 331,
          "accuracy": 0.58625
        },
        "0.01": {
          "tp": 360,
          "fn": 440,
          "accuracy": 0.45
        }
      },
      "auroc": 0.6624549479166667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9343281250000001
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9347682291666666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9293604166666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        },
        "0.01": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        }
      },
      "auroc": 0.6697656249999999
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        }
      },
      "auroc": 0.7995630208333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.932284375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 273,
          "fn": 127,
          "accuracy": 0.6825
        },
        "0.01": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        }
      },
      "auroc": 0.802046875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 670,
          "fn": 130,
          "accuracy": 0.8375
        },
        "0.01": {
          "tp": 622,
          "fn": 178,
          "accuracy": 0.7775
        }
      },
      "auroc": 0.8671656249999999
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9339770833333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9339770833333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9149645833333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9149645833333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9244708333333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9244708333333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9173437500000001
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9173437500000001
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.8832229166666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.8832229166666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        }
      },
      "auroc": 0.9002833333333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        }
      },
      "auroc": 0.9002833333333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9329802083333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9329802083333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9340942708333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9340942708333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.8147083333333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.8147083333333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": {
          "tp": 259,
          "fn": 141,
          "accuracy": 0.6475
        }
      },
      "auroc": 0.8749583333333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": {
          "tp": 259,
          "fn": 141,
          "accuracy": 0.6475
        }
      },
      "auroc": 0.8749583333333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.8893916666666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.8893916666666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.8616697916666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.8616697916666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        }
      },
      "auroc": 0.8755307291666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        }
      },
      "auroc": 0.8755307291666666
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2171,
          "fn": 29,
          "accuracy": 0.9868181818181818
        },
        "0.01": {
          "tp": 2093,
          "fn": 107,
          "accuracy": 0.9513636363636364
        }
      },
      "auroc": 0.9282779356060606
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 870,
          "fn": 330,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 747,
          "fn": 453,
          "accuracy": 0.6225
        }
      },
      "auroc": 0.7427270833333335
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3041,
          "fn": 359,
          "accuracy": 0.8944117647058824
        },
        "0.01": {
          "tp": 2840,
          "fn": 560,
          "accuracy": 0.8352941176470589
        }
      },
      "auroc": 0.8627893995098038
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1915,
          "fn": 285,
          "accuracy": 0.8704545454545455
        },
        "0.01": {
          "tp": 1599,
          "fn": 601,
          "accuracy": 0.7268181818181818
        }
      },
      "auroc": 0.884755303030303
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 260,
          "fn": 940,
          "accuracy": 0.21666666666666667
        },
        "0.01": {
          "tp": 185,
          "fn": 1015,
          "accuracy": 0.15416666666666667
        }
      },
      "auroc": 0.3745197916666666
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2175,
          "fn": 1225,
          "accuracy": 0.6397058823529411
        },
        "0.01": {
          "tp": 1784,
          "fn": 1616,
          "accuracy": 0.5247058823529411
        }
      },
      "auroc": 0.7046721813725488
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 4086,
          "fn": 314,
          "accuracy": 0.9286363636363636
        },
        "0.01": {
          "tp": 3692,
          "fn": 708,
          "accuracy": 0.8390909090909091
        }
      },
      "auroc": 0.906516619318182
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1130,
          "fn": 1270,
          "accuracy": 0.4708333333333333
        },
        "0.01": {
          "tp": 932,
          "fn": 1468,
          "accuracy": 0.3883333333333333
        }
      },
      "auroc": 0.5586234375
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 5216,
          "fn": 1584,
          "accuracy": 0.7670588235294118
        },
        "0.01": {
          "tp": 4624,
          "fn": 2176,
          "accuracy": 0.68
        }
      },
      "auroc": 0.7837307904411764
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.8925760416666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.77939375
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        },
        "0.01": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        }
      },
      "auroc": 0.8359848958333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.8910552083333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.6393447916666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        }
      },
      "auroc": 0.7652000000000001
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        }
      },
      "auroc": 0.8918156250000001
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        },
        "0.01": {
          "tp": 64,
          "fn": 336,
          "accuracy": 0.16
        }
      },
      "auroc": 0.7093692708333332
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 533,
          "fn": 267,
          "accuracy": 0.66625
        },
        "0.01": {
          "tp": 346,
          "fn": 454,
          "accuracy": 0.4325
        }
      },
      "auroc": 0.8005924479166666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9084802083333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.2737239583333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        },
        "0.01": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.5911020833333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8436427083333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        }
      },
      "auroc": 0.2845833333333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 229,
          "accuracy": 0.4275
        },
        "0.01": {
          "tp": 111,
          "fn": 289,
          "accuracy": 0.2775
        }
      },
      "auroc": 0.5641130208333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 267,
          "fn": 133,
          "accuracy": 0.6675
        }
      },
      "auroc": 0.8760614583333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 378,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 13,
          "fn": 387,
          "accuracy": 0.0325
        }
      },
      "auroc": 0.27915364583333335
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 434,
          "accuracy": 0.4575
        },
        "0.01": {
          "tp": 280,
          "fn": 520,
          "accuracy": 0.35
        }
      },
      "auroc": 0.5776075520833334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        }
      },
      "auroc": 0.8086010416666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.45091041666666665
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        },
        "0.01": {
          "tp": 73,
          "fn": 327,
          "accuracy": 0.1825
        }
      },
      "auroc": 0.6297557291666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        }
      },
      "auroc": 0.7508239583333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.2538583333333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 41,
          "fn": 359,
          "accuracy": 0.1025
        }
      },
      "auroc": 0.5023411458333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        }
      },
      "auroc": 0.7797125
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 384,
          "accuracy": 0.04
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.35238437499999997
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 256,
          "fn": 544,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 114,
          "fn": 686,
          "accuracy": 0.1425
        }
      },
      "auroc": 0.5660484375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9053333333333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        }
      },
      "auroc": 0.7293666666666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        }
      },
      "auroc": 0.81735
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.8110520833333332
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.5041927083333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 240,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 84,
          "fn": 316,
          "accuracy": 0.21
        }
      },
      "auroc": 0.6576223958333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        }
      },
      "auroc": 0.8581927083333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        },
        "0.01": {
          "tp": 51,
          "fn": 349,
          "accuracy": 0.1275
        }
      },
      "auroc": 0.6167796875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 446,
          "fn": 354,
          "accuracy": 0.5575
        },
        "0.01": {
          "tp": 289,
          "fn": 511,
          "accuracy": 0.36125
        }
      },
      "auroc": 0.7374861979166667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.91460625
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.6014239583333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 181,
          "fn": 219,
          "accuracy": 0.4525
        }
      },
      "auroc": 0.7580151041666665
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.79671875
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.3933770833333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 267,
          "accuracy": 0.3325
        },
        "0.01": {
          "tp": 64,
          "fn": 336,
          "accuracy": 0.16
        }
      },
      "auroc": 0.5950479166666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 236,
          "fn": 164,
          "accuracy": 0.59
        }
      },
      "auroc": 0.8556625
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 363,
          "accuracy": 0.0925
        },
        "0.01": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        }
      },
      "auroc": 0.49740052083333336
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 451,
          "accuracy": 0.43625
        },
        "0.01": {
          "tp": 245,
          "fn": 555,
          "accuracy": 0.30625
        }
      },
      "auroc": 0.6765315104166667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        }
      },
      "auroc": 0.8852614583333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.7003104166666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        },
        "0.01": {
          "tp": 152,
          "fn": 248,
          "accuracy": 0.38
        }
      },
      "auroc": 0.7927859374999998
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        }
      },
      "auroc": 0.8394552083333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.5232520833333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 235,
          "accuracy": 0.4125
        },
        "0.01": {
          "tp": 92,
          "fn": 308,
          "accuracy": 0.23
        }
      },
      "auroc": 0.6813536458333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        }
      },
      "auroc": 0.8623583333333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 86,
          "fn": 314,
          "accuracy": 0.215
        },
        "0.01": {
          "tp": 27,
          "fn": 373,
          "accuracy": 0.0675
        }
      },
      "auroc": 0.6117812499999999
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 416,
          "fn": 384,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 244,
          "fn": 556,
          "accuracy": 0.305
        }
      },
      "auroc": 0.7370697916666668
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.8160375
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.8160375
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.7681614583333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.7681614583333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        },
        "0.01": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        }
      },
      "auroc": 0.7920994791666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        },
        "0.01": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        }
      },
      "auroc": 0.7920994791666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.8723156249999999
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.8723156249999999
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        }
      },
      "auroc": 0.8371979166666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        }
      },
      "auroc": 0.8371979166666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        },
        "0.01": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        }
      },
      "auroc": 0.8547567708333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        },
        "0.01": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        }
      },
      "auroc": 0.8547567708333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        }
      },
      "auroc": 0.85445625
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        }
      },
      "auroc": 0.85445625
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        }
      },
      "auroc": 0.8048645833333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        }
      },
      "auroc": 0.8048645833333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        },
        "0.01": {
          "tp": 159,
          "fn": 241,
          "accuracy": 0.3975
        }
      },
      "auroc": 0.8296604166666667
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        },
        "0.01": {
          "tp": 159,
          "fn": 241,
          "accuracy": 0.3975
        }
      },
      "auroc": 0.8296604166666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8774947916666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8774947916666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.7591333333333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.7591333333333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        },
        "0.01": {
          "tp": 152,
          "fn": 248,
          "accuracy": 0.38
        }
      },
      "auroc": 0.8183140624999999
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        },
        "0.01": {
          "tp": 152,
          "fn": 248,
          "accuracy": 0.38
        }
      },
      "auroc": 0.8183140624999999
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        }
      },
      "auroc": 0.7995583333333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        }
      },
      "auroc": 0.7995583333333334
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        }
      },
      "auroc": 0.7586916666666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        }
      },
      "auroc": 0.7586916666666667
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        },
        "0.01": {
          "tp": 107,
          "fn": 293,
          "accuracy": 0.2675
        }
      },
      "auroc": 0.779125
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        },
        "0.01": {
          "tp": 107,
          "fn": 293,
          "accuracy": 0.2675
        }
      },
      "auroc": 0.779125
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1816,
          "fn": 384,
          "accuracy": 0.8254545454545454
        },
        "0.01": {
          "tp": 1309,
          "fn": 891,
          "accuracy": 0.595
        }
      },
      "auroc": 0.866792803030303
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 319,
          "fn": 881,
          "accuracy": 0.2658333333333333
        },
        "0.01": {
          "tp": 128,
          "fn": 1072,
          "accuracy": 0.10666666666666667
        }
      },
      "auroc": 0.5891881944444444
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2135,
          "fn": 1265,
          "accuracy": 0.6279411764705882
        },
        "0.01": {
          "tp": 1437,
          "fn": 1963,
          "accuracy": 0.42264705882352943
        }
      },
      "auroc": 0.7688147058823529
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1456,
          "fn": 744,
          "accuracy": 0.6618181818181819
        },
        "0.01": {
          "tp": 829,
          "fn": 1371,
          "accuracy": 0.37681818181818183
        }
      },
      "auroc": 0.8055269886363636
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 1076,
          "accuracy": 0.10333333333333333
        },
        "0.01": {
          "tp": 36,
          "fn": 1164,
          "accuracy": 0.03
        }
      },
      "auroc": 0.4331013888888888
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1580,
          "fn": 1820,
          "accuracy": 0.4647058823529412
        },
        "0.01": {
          "tp": 865,
          "fn": 2535,
          "accuracy": 0.25441176470588234
        }
      },
      "auroc": 0.6740826593137255
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3272,
          "fn": 1128,
          "accuracy": 0.7436363636363637
        },
        "0.01": {
          "tp": 2138,
          "fn": 2262,
          "accuracy": 0.4859090909090909
        }
      },
      "auroc": 0.8361598958333334
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 443,
          "fn": 1957,
          "accuracy": 0.18458333333333332
        },
        "0.01": {
          "tp": 164,
          "fn": 2236,
          "accuracy": 0.06833333333333333
        }
      },
      "auroc": 0.5111447916666666
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3715,
          "fn": 3085,
          "accuracy": 0.5463235294117647
        },
        "0.01": {
          "tp": 2302,
          "fn": 4498,
          "accuracy": 0.33852941176470586
        }
      },
      "auroc": 0.7214486825980393
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.930690625
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9329494791666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        }
      },
      "auroc": 0.7817916666666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        }
      },
      "auroc": 0.8585
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        }
      },
      "auroc": 0.8562411458333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 749,
          "fn": 51,
          "accuracy": 0.93625
        },
        "0.01": {
          "tp": 710,
          "fn": 90,
          "accuracy": 0.8875
        }
      },
      "auroc": 0.8957247395833332
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9291979166666665
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.10088645833333332
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        }
      },
      "auroc": 0.5150421874999999
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.6846770833333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.056656249999999984
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        },
        "0.01": {
          "tp": 17,
          "fn": 383,
          "accuracy": 0.0425
        }
      },
      "auroc": 0.37066666666666664
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        },
        "0.01": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        }
      },
      "auroc": 0.8069375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07877135416666664
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 261,
          "fn": 539,
          "accuracy": 0.32625
        },
        "0.01": {
          "tp": 207,
          "fn": 593,
          "accuracy": 0.25875
        }
      },
      "auroc": 0.44285442708333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9302822916666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.6082958333333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        },
        "0.01": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        }
      },
      "auroc": 0.7692890624999998
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9125072916666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.22096458333333335
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        }
      },
      "auroc": 0.5667359375000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9213947916666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 75,
          "fn": 325,
          "accuracy": 0.1875
        },
        "0.01": {
          "tp": 21,
          "fn": 379,
          "accuracy": 0.0525
        }
      },
      "auroc": 0.4146302083333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 463,
          "fn": 337,
          "accuracy": 0.57875
        },
        "0.01": {
          "tp": 383,
          "fn": 417,
          "accuracy": 0.47875
        }
      },
      "auroc": 0.6680125
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9296333333333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9339729166666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9318031249999998
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        }
      },
      "auroc": 0.8346802083333335
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.21791562500000003
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 256,
          "accuracy": 0.36
        },
        "0.01": {
          "tp": 94,
          "fn": 306,
          "accuracy": 0.235
        }
      },
      "auroc": 0.5262979166666667
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        },
        "0.01": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        }
      },
      "auroc": 0.8821567708333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.5759442708333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 543,
          "fn": 257,
          "accuracy": 0.67875
        },
        "0.01": {
          "tp": 483,
          "fn": 317,
          "accuracy": 0.60375
        }
      },
      "auroc": 0.7290505208333332
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9340229166666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.8152677083333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        }
      },
      "auroc": 0.8746453125
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.6061229166666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.06255937499999997
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 43,
          "fn": 357,
          "accuracy": 0.1075
        },
        "0.01": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        }
      },
      "auroc": 0.33434114583333324
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        }
      },
      "auroc": 0.7700729166666668
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        },
        "0.01": {
          "tp": 72,
          "fn": 328,
          "accuracy": 0.18
        }
      },
      "auroc": 0.4389135416666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 429,
          "accuracy": 0.46375
        },
        "0.01": {
          "tp": 276,
          "fn": 524,
          "accuracy": 0.345
        }
      },
      "auroc": 0.6044932291666668
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.934871875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9266020833333334
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9307369791666666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9139250000000001
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.5622697916666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        },
        "0.01": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        }
      },
      "auroc": 0.7380973958333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9243984375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        }
      },
      "auroc": 0.7444359375
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 629,
          "fn": 171,
          "accuracy": 0.78625
        },
        "0.01": {
          "tp": 581,
          "fn": 219,
          "accuracy": 0.72625
        }
      },
      "auroc": 0.8344171875
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9251979166666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9251979166666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8835916666666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8835916666666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9043947916666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9043947916666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8552854166666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8552854166666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.7707833333333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.7707833333333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        },
        "0.01": {
          "tp": 147,
          "fn": 253,
          "accuracy": 0.3675
        }
      },
      "auroc": 0.8130343749999998
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        },
        "0.01": {
          "tp": 147,
          "fn": 253,
          "accuracy": 0.3675
        }
      },
      "auroc": 0.8130343749999998
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9336583333333335
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9336583333333335
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9185666666666668
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9185666666666668
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9261125000000001
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9261125000000001
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9343916666666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9343916666666666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.6981739583333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.6981739583333334
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        }
      },
      "auroc": 0.8162828125000001
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        }
      },
      "auroc": 0.8162828125000001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        }
      },
      "auroc": 0.8210468750000002
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        }
      },
      "auroc": 0.8210468750000002
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.757659375
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.757659375
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        }
      },
      "auroc": 0.7893531250000001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        }
      },
      "auroc": 0.7893531250000001
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2090,
          "fn": 110,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 1952,
          "fn": 248,
          "accuracy": 0.8872727272727273
        }
      },
      "auroc": 0.9147997159090909
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 403,
          "accuracy": 0.6641666666666667
        },
        "0.01": {
          "tp": 669,
          "fn": 531,
          "accuracy": 0.5575
        }
      },
      "auroc": 0.7192859375
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2887,
          "fn": 513,
          "accuracy": 0.8491176470588235
        },
        "0.01": {
          "tp": 2621,
          "fn": 779,
          "accuracy": 0.7708823529411765
        }
      },
      "auroc": 0.8457948529411763
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1494,
          "fn": 706,
          "accuracy": 0.6790909090909091
        },
        "0.01": {
          "tp": 1095,
          "fn": 1105,
          "accuracy": 0.49772727272727274
        }
      },
      "auroc": 0.8105359848484848
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 1005,
          "accuracy": 0.1625
        },
        "0.01": {
          "tp": 138,
          "fn": 1062,
          "accuracy": 0.115
        }
      },
      "auroc": 0.3170262152777778
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1689,
          "fn": 1711,
          "accuracy": 0.49676470588235294
        },
        "0.01": {
          "tp": 1233,
          "fn": 2167,
          "accuracy": 0.36264705882352943
        }
      },
      "auroc": 0.6363560661764706
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3584,
          "fn": 816,
          "accuracy": 0.8145454545454546
        },
        "0.01": {
          "tp": 3047,
          "fn": 1353,
          "accuracy": 0.6925
        }
      },
      "auroc": 0.8626678503787879
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 992,
          "fn": 1408,
          "accuracy": 0.41333333333333333
        },
        "0.01": {
          "tp": 807,
          "fn": 1593,
          "accuracy": 0.33625
        }
      },
      "auroc": 0.5181560763888888
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 4576,
          "fn": 2224,
          "accuracy": 0.6729411764705883
        },
        "0.01": {
          "tp": 3854,
          "fn": 2946,
          "accuracy": 0.566764705882353
        }
      },
      "auroc": 0.7410754595588236
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9341208333333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9346645833333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.8207614583333335
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        }
      },
      "auroc": 0.8779848958333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        }
      },
      "auroc": 0.8774411458333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 761,
          "fn": 39,
          "accuracy": 0.95125
        },
        "0.01": {
          "tp": 743,
          "fn": 57,
          "accuracy": 0.92875
        }
      },
      "auroc": 0.9063247395833333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9315864583333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.09327708333333332
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        }
      },
      "auroc": 0.5124317708333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        }
      },
      "auroc": 0.8376197916666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.05587812499999999
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 245,
          "accuracy": 0.3875
        },
        "0.01": {
          "tp": 94,
          "fn": 306,
          "accuracy": 0.235
        }
      },
      "auroc": 0.44674895833333333
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": {
          "tp": 289,
          "fn": 111,
          "accuracy": 0.7225
        }
      },
      "auroc": 0.8846031249999999
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07457760416666664
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 447,
          "accuracy": 0.44125
        },
        "0.01": {
          "tp": 289,
          "fn": 511,
          "accuracy": 0.36125
        }
      },
      "auroc": 0.4795903645833334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9327000000000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.6850656250000001
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        },
        "0.01": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        }
      },
      "auroc": 0.8088828124999999
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9288677083333334
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.252921875
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        }
      },
      "auroc": 0.5908947916666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9307838541666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 287,
          "accuracy": 0.2825
        },
        "0.01": {
          "tp": 63,
          "fn": 337,
          "accuracy": 0.1575
        }
      },
      "auroc": 0.46899375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 507,
          "fn": 293,
          "accuracy": 0.63375
        },
        "0.01": {
          "tp": 451,
          "fn": 349,
          "accuracy": 0.56375
        }
      },
      "auroc": 0.6998888020833335
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9319302083333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9343916666666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9331609375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9040593750000001
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.28208645833333335
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 163,
          "fn": 237,
          "accuracy": 0.4075
        }
      },
      "auroc": 0.5930729166666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        }
      },
      "auroc": 0.9179947916666668
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.6082390625
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 586,
          "fn": 214,
          "accuracy": 0.7325
        },
        "0.01": {
          "tp": 556,
          "fn": 244,
          "accuracy": 0.695
        }
      },
      "auroc": 0.7631169270833335
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9340229166666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        }
      },
      "auroc": 0.8611072916666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        }
      },
      "auroc": 0.8975651041666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        }
      },
      "auroc": 0.7781947916666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.06906979166666664
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 276,
          "accuracy": 0.31
        },
        "0.01": {
          "tp": 61,
          "fn": 339,
          "accuracy": 0.1525
        }
      },
      "auroc": 0.4236322916666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": {
          "tp": 259,
          "fn": 141,
          "accuracy": 0.6475
        }
      },
      "auroc": 0.8561088541666666
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 241,
          "accuracy": 0.3975
        },
        "0.01": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        }
      },
      "auroc": 0.46508854166666663
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 482,
          "fn": 318,
          "accuracy": 0.6025
        },
        "0.01": {
          "tp": 367,
          "fn": 433,
          "accuracy": 0.45875
        }
      },
      "auroc": 0.6605986979166666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9324802083333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9338442708333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9292687500000001
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.6475510416666665
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        },
        "0.01": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        }
      },
      "auroc": 0.7884098958333332
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9322385416666666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        }
      },
      "auroc": 0.7900156249999999
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 662,
          "fn": 138,
          "accuracy": 0.8275
        },
        "0.01": {
          "tp": 623,
          "fn": 177,
          "accuracy": 0.77875
        }
      },
      "auroc": 0.8611270833333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9329010416666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9329010416666667
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.922353125
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.922353125
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9276270833333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9276270833333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9175635416666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9175635416666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        }
      },
      "auroc": 0.8738281250000001
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        }
      },
      "auroc": 0.8738281250000001
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.8956958333333334
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.8956958333333334
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9307520833333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9307520833333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9329802083333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9329802083333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        }
      },
      "auroc": 0.8129447916666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        }
      },
      "auroc": 0.8129447916666667
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8740765625000001
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8740765625000001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.8907427083333335
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.8907427083333335
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        }
      },
      "auroc": 0.85535625
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        }
      },
      "auroc": 0.85535625
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 273,
          "fn": 127,
          "accuracy": 0.6825
        }
      },
      "auroc": 0.8730494791666668
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 273,
          "fn": 127,
          "accuracy": 0.6825
        }
      },
      "auroc": 0.8730494791666668
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2167,
          "fn": 33,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 2107,
          "fn": 93,
          "accuracy": 0.9577272727272728
        }
      },
      "auroc": 0.9283891098484849
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 859,
          "fn": 341,
          "accuracy": 0.7158333333333333
        },
        "0.01": {
          "tp": 760,
          "fn": 440,
          "accuracy": 0.6333333333333333
        }
      },
      "auroc": 0.7400737847222223
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3026,
          "fn": 374,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 2867,
          "fn": 533,
          "accuracy": 0.8432352941176471
        }
      },
      "auroc": 0.8619248774509806
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1921,
          "fn": 279,
          "accuracy": 0.8731818181818182
        },
        "0.01": {
          "tp": 1600,
          "fn": 600,
          "accuracy": 0.7272727272727273
        }
      },
      "auroc": 0.8825866477272728
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 242,
          "fn": 958,
          "accuracy": 0.20166666666666666
        },
        "0.01": {
          "tp": 182,
          "fn": 1018,
          "accuracy": 0.15166666666666667
        }
      },
      "auroc": 0.3547114583333333
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2163,
          "fn": 1237,
          "accuracy": 0.6361764705882353
        },
        "0.01": {
          "tp": 1782,
          "fn": 1618,
          "accuracy": 0.5241176470588236
        }
      },
      "auroc": 0.6962777573529412
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 4088,
          "fn": 312,
          "accuracy": 0.9290909090909091
        },
        "0.01": {
          "tp": 3707,
          "fn": 693,
          "accuracy": 0.8425
        }
      },
      "auroc": 0.9054878787878788
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1101,
          "fn": 1299,
          "accuracy": 0.45875
        },
        "0.01": {
          "tp": 942,
          "fn": 1458,
          "accuracy": 0.3925
        }
      },
      "auroc": 0.5473926215277778
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 5189,
          "fn": 1611,
          "accuracy": 0.7630882352941176
        },
        "0.01": {
          "tp": 4649,
          "fn": 2151,
          "accuracy": 0.6836764705882353
        }
      },
      "auroc": 0.7791013174019608
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9346645833333334
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9349364583333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.93483125
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9350197916666667
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9347479166666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.934978125
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9345645833333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9069229166666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        }
      },
      "auroc": 0.92074375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9336364583333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9041354166666666
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9188859375
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9341005208333334
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9055291666666667
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 775,
          "fn": 25,
          "accuracy": 0.96875
        },
        "0.01": {
          "tp": 716,
          "fn": 84,
          "accuracy": 0.895
        }
      },
      "auroc": 0.91981484375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9343916666666666
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9332333333333332
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9338125
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9334333333333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.92434375
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9288885416666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9339124999999999
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9287885416666667
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 774,
          "fn": 26,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9313505208333334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.932221875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.932246875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.932234375
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9335458333333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9288135416666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9311796875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9328838541666666
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9305302083333333
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 785,
          "fn": 15,
          "accuracy": 0.98125
        }
      },
      "auroc": 0.93170703125
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.934059375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9322874999999999
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9331734375
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9345166666666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9263760416666668
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9304463541666667
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9342880208333334
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9293317708333333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 779,
          "fn": 21,
          "accuracy": 0.97375
        }
      },
      "auroc": 0.9318098958333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.934871875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.93483125
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9348515625
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9324593750000001
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9338338541666666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9350401041666667
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9336453125
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        }
      },
      "auroc": 0.9343427083333333
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9321114583333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9321114583333334
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9319156249999999
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9319156249999999
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9320135416666666
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9320135416666666
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9345166666666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9345166666666667
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9348624999999999
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9348624999999999
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9352083333333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9347812500000001
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9347812500000001
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9349947916666668
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9349947916666668
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9333864583333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9333864583333333
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9318885416666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9318885416666666
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9326375
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9326375
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2184,
          "fn": 16,
          "accuracy": 0.9927272727272727
        }
      },
      "auroc": 0.9342218750000001
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1190,
          "fn": 10,
          "accuracy": 0.9916666666666667
        },
        "0.01": {
          "tp": 1145,
          "fn": 55,
          "accuracy": 0.9541666666666667
        }
      },
      "auroc": 0.9290310763888889
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3390,
          "fn": 10,
          "accuracy": 0.9970588235294118
        },
        "0.01": {
          "tp": 3329,
          "fn": 71,
          "accuracy": 0.9791176470588235
        }
      },
      "auroc": 0.9323898284313725
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2198,
          "fn": 2,
          "accuracy": 0.9990909090909091
        },
        "0.01": {
          "tp": 2181,
          "fn": 19,
          "accuracy": 0.9913636363636363
        }
      },
      "auroc": 0.933987215909091
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1177,
          "fn": 23,
          "accuracy": 0.9808333333333333
        },
        "0.01": {
          "tp": 1117,
          "fn": 83,
          "accuracy": 0.9308333333333333
        }
      },
      "auroc": 0.9251598958333334
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3375,
          "fn": 25,
          "accuracy": 0.9926470588235294
        },
        "0.01": {
          "tp": 3298,
          "fn": 102,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9308716911764706
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4398,
          "fn": 2,
          "accuracy": 0.9995454545454545
        },
        "0.01": {
          "tp": 4365,
          "fn": 35,
          "accuracy": 0.9920454545454546
        }
      },
      "auroc": 0.9341045454545455
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2367,
          "fn": 33,
          "accuracy": 0.98625
        },
        "0.01": {
          "tp": 2262,
          "fn": 138,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9270954861111111
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 6765,
          "fn": 35,
          "accuracy": 0.9948529411764706
        },
        "0.01": {
          "tp": 6627,
          "fn": 173,
          "accuracy": 0.9745588235294118
        }
      },
      "auroc": 0.9316307598039216
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2350,
          "fn": 50,
          "accuracy": 0.9791666666666666
        },
        "0.01": {
          "tp": 2264,
          "fn": 136,
          "accuracy": 0.9433333333333334
        }
      },
      "auroc": 0.9265619791666666
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2150,
          "fn": 250,
          "accuracy": 0.8958333333333334
        },
        "0.01": {
          "tp": 2001,
          "fn": 399,
          "accuracy": 0.83375
        }
      },
      "auroc": 0.8933914062499998
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4500,
          "fn": 300,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 4265,
          "fn": 535,
          "accuracy": 0.8885416666666667
        }
      },
      "auroc": 0.9099766927083333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2352,
          "fn": 48,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 2257,
          "fn": 143,
          "accuracy": 0.9404166666666667
        }
      },
      "auroc": 0.9258000000000001
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1719,
          "fn": 681,
          "accuracy": 0.71625
        },
        "0.01": {
          "tp": 1465,
          "fn": 935,
          "accuracy": 0.6104166666666667
        }
      },
      "auroc": 0.7764261284722223
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4071,
          "fn": 729,
          "accuracy": 0.848125
        },
        "0.01": {
          "tp": 3722,
          "fn": 1078,
          "accuracy": 0.7754166666666666
        }
      },
      "auroc": 0.8511130642361111
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4702,
          "fn": 98,
          "accuracy": 0.9795833333333334
        },
        "0.01": {
          "tp": 4521,
          "fn": 279,
          "accuracy": 0.941875
        }
      },
      "auroc": 0.9261809895833333
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3869,
          "fn": 931,
          "accuracy": 0.8060416666666667
        },
        "0.01": {
          "tp": 3466,
          "fn": 1334,
          "accuracy": 0.7220833333333333
        }
      },
      "auroc": 0.8349087673611111
    },
    {
      "domain": "news",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8571,
          "fn": 1029,
          "accuracy": 0.8928125
        },
        "0.01": {
          "tp": 7987,
          "fn": 1613,
          "accuracy": 0.8319791666666667
        }
      },
      "auroc": 0.8805448784722222
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2343,
          "fn": 57,
          "accuracy": 0.97625
        },
        "0.01": {
          "tp": 2249,
          "fn": 151,
          "accuracy": 0.9370833333333334
        }
      },
      "auroc": 0.9256624131944444
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 224,
          "fn": 2176,
          "accuracy": 0.09333333333333334
        },
        "0.01": {
          "tp": 167,
          "fn": 2233,
          "accuracy": 0.06958333333333333
        }
      },
      "auroc": 0.21266267361111107
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2567,
          "fn": 2233,
          "accuracy": 0.5347916666666667
        },
        "0.01": {
          "tp": 2416,
          "fn": 2384,
          "accuracy": 0.5033333333333333
        }
      },
      "auroc": 0.5691625434027777
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1602,
          "fn": 798,
          "accuracy": 0.6675
        },
        "0.01": {
          "tp": 1008,
          "fn": 1392,
          "accuracy": 0.42
        }
      },
      "auroc": 0.7837098090277779
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 214,
          "fn": 2186,
          "accuracy": 0.08916666666666667
        },
        "0.01": {
          "tp": 172,
          "fn": 2228,
          "accuracy": 0.07166666666666667
        }
      },
      "auroc": 0.1798689236111111
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1816,
          "fn": 2984,
          "accuracy": 0.37833333333333335
        },
        "0.01": {
          "tp": 1180,
          "fn": 3620,
          "accuracy": 0.24583333333333332
        }
      },
      "auroc": 0.48178936631944436
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3945,
          "fn": 855,
          "accuracy": 0.821875
        },
        "0.01": {
          "tp": 3257,
          "fn": 1543,
          "accuracy": 0.6785416666666667
        }
      },
      "auroc": 0.854686111111111
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 438,
          "fn": 4362,
          "accuracy": 0.09125
        },
        "0.01": {
          "tp": 339,
          "fn": 4461,
          "accuracy": 0.070625
        }
      },
      "auroc": 0.1962657986111111
    },
    {
      "domain": "news",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4383,
          "fn": 5217,
          "accuracy": 0.4565625
        },
        "0.01": {
          "tp": 3596,
          "fn": 6004,
          "accuracy": 0.3745833333333333
        }
      },
      "auroc": 0.525475954861111
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2232,
          "fn": 168,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 2064,
          "fn": 336,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9070125868055556
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1058,
          "fn": 1342,
          "accuracy": 0.44083333333333335
        },
        "0.01": {
          "tp": 598,
          "fn": 1802,
          "accuracy": 0.24916666666666668
        }
      },
      "auroc": 0.6344026909722222
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3290,
          "fn": 1510,
          "accuracy": 0.6854166666666667
        },
        "0.01": {
          "tp": 2662,
          "fn": 2138,
          "accuracy": 0.5545833333333333
        }
      },
      "auroc": 0.7707076388888888
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2108,
          "fn": 292,
          "accuracy": 0.8783333333333333
        },
        "0.01": {
          "tp": 1871,
          "fn": 529,
          "accuracy": 0.7795833333333333
        }
      },
      "auroc": 0.8868084201388889
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 2028,
          "accuracy": 0.155
        },
        "0.01": {
          "tp": 235,
          "fn": 2165,
          "accuracy": 0.09791666666666667
        }
      },
      "auroc": 0.32640598958333333
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2480,
          "fn": 2320,
          "accuracy": 0.5166666666666667
        },
        "0.01": {
          "tp": 2106,
          "fn": 2694,
          "accuracy": 0.43875
        }
      },
      "auroc": 0.6066072048611111
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4340,
          "fn": 460,
          "accuracy": 0.9041666666666667
        },
        "0.01": {
          "tp": 3935,
          "fn": 865,
          "accuracy": 0.8197916666666667
        }
      },
      "auroc": 0.8969105034722223
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1430,
          "fn": 3370,
          "accuracy": 0.29791666666666666
        },
        "0.01": {
          "tp": 833,
          "fn": 3967,
          "accuracy": 0.17354166666666668
        }
      },
      "auroc": 0.48040434027777773
    },
    {
      "domain": "news",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5770,
          "fn": 3830,
          "accuracy": 0.6010416666666667
        },
        "0.01": {
          "tp": 4768,
          "fn": 4832,
          "accuracy": 0.49666666666666665
        }
      },
      "auroc": 0.688657421875
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2348,
          "fn": 52,
          "accuracy": 0.9783333333333334
        },
        "0.01": {
          "tp": 2209,
          "fn": 191,
          "accuracy": 0.9204166666666667
        }
      },
      "auroc": 0.9237513888888889
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2178,
          "fn": 222,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 2034,
          "fn": 366,
          "accuracy": 0.8475
        }
      },
      "auroc": 0.8971814236111111
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4526,
          "fn": 274,
          "accuracy": 0.9429166666666666
        },
        "0.01": {
          "tp": 4243,
          "fn": 557,
          "accuracy": 0.8839583333333333
        }
      },
      "auroc": 0.9104664062499999
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1971,
          "fn": 429,
          "accuracy": 0.82125
        },
        "0.01": {
          "tp": 1597,
          "fn": 803,
          "accuracy": 0.6654166666666667
        }
      },
      "auroc": 0.8568210937500002
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 2073,
          "accuracy": 0.13625
        },
        "0.01": {
          "tp": 230,
          "fn": 2170,
          "accuracy": 0.09583333333333334
        }
      },
      "auroc": 0.3763430555555556
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2298,
          "fn": 2502,
          "accuracy": 0.47875
        },
        "0.01": {
          "tp": 1827,
          "fn": 2973,
          "accuracy": 0.380625
        }
      },
      "auroc": 0.6165820746527778
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4319,
          "fn": 481,
          "accuracy": 0.8997916666666667
        },
        "0.01": {
          "tp": 3806,
          "fn": 994,
          "accuracy": 0.7929166666666667
        }
      },
      "auroc": 0.8902862413194444
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2505,
          "fn": 2295,
          "accuracy": 0.521875
        },
        "0.01": {
          "tp": 2264,
          "fn": 2536,
          "accuracy": 0.4716666666666667
        }
      },
      "auroc": 0.6367622395833334
    },
    {
      "domain": "news",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 6824,
          "fn": 2776,
          "accuracy": 0.7108333333333333
        },
        "0.01": {
          "tp": 6070,
          "fn": 3530,
          "accuracy": 0.6322916666666667
        }
      },
      "auroc": 0.7635242404513889
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2369,
          "fn": 31,
          "accuracy": 0.9870833333333333
        },
        "0.01": {
          "tp": 2315,
          "fn": 85,
          "accuracy": 0.9645833333333333
        }
      },
      "auroc": 0.9297934895833333
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1503,
          "fn": 897,
          "accuracy": 0.62625
        },
        "0.01": {
          "tp": 1000,
          "fn": 1400,
          "accuracy": 0.4166666666666667
        }
      },
      "auroc": 0.7642725694444444
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3872,
          "fn": 928,
          "accuracy": 0.8066666666666666
        },
        "0.01": {
          "tp": 3315,
          "fn": 1485,
          "accuracy": 0.690625
        }
      },
      "auroc": 0.8470330295138888
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1277,
          "fn": 1123,
          "accuracy": 0.5320833333333334
        },
        "0.01": {
          "tp": 751,
          "fn": 1649,
          "accuracy": 0.3129166666666667
        }
      },
      "auroc": 0.7291509548611111
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 235,
          "fn": 2165,
          "accuracy": 0.09791666666666667
        },
        "0.01": {
          "tp": 194,
          "fn": 2206,
          "accuracy": 0.08083333333333333
        }
      },
      "auroc": 0.20654062499999995
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1512,
          "fn": 3288,
          "accuracy": 0.315
        },
        "0.01": {
          "tp": 945,
          "fn": 3855,
          "accuracy": 0.196875
        }
      },
      "auroc": 0.4678457899305555
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3646,
          "fn": 1154,
          "accuracy": 0.7595833333333334
        },
        "0.01": {
          "tp": 3066,
          "fn": 1734,
          "accuracy": 0.63875
        }
      },
      "auroc": 0.8294722222222222
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1738,
          "fn": 3062,
          "accuracy": 0.3620833333333333
        },
        "0.01": {
          "tp": 1194,
          "fn": 3606,
          "accuracy": 0.24875
        }
      },
      "auroc": 0.4854065972222222
    },
    {
      "domain": "news",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5384,
          "fn": 4216,
          "accuracy": 0.5608333333333333
        },
        "0.01": {
          "tp": 4260,
          "fn": 5340,
          "accuracy": 0.44375
        }
      },
      "auroc": 0.6574394097222223
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2355,
          "fn": 45,
          "accuracy": 0.98125
        },
        "0.01": {
          "tp": 2247,
          "fn": 153,
          "accuracy": 0.93625
        }
      },
      "auroc": 0.92556796875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2080,
          "fn": 320,
          "accuracy": 0.8666666666666667
        },
        "0.01": {
          "tp": 1891,
          "fn": 509,
          "accuracy": 0.7879166666666667
        }
      },
      "auroc": 0.8756980034722223
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4435,
          "fn": 365,
          "accuracy": 0.9239583333333333
        },
        "0.01": {
          "tp": 4138,
          "fn": 662,
          "accuracy": 0.8620833333333333
        }
      },
      "auroc": 0.9006329861111111
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2192,
          "fn": 208,
          "accuracy": 0.9133333333333333
        },
        "0.01": {
          "tp": 1990,
          "fn": 410,
          "accuracy": 0.8291666666666667
        }
      },
      "auroc": 0.9020047743055555
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 820,
          "fn": 1580,
          "accuracy": 0.3416666666666667
        },
        "0.01": {
          "tp": 499,
          "fn": 1901,
          "accuracy": 0.20791666666666667
        }
      },
      "auroc": 0.6149168402777778
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3012,
          "fn": 1788,
          "accuracy": 0.6275
        },
        "0.01": {
          "tp": 2489,
          "fn": 2311,
          "accuracy": 0.5185416666666667
        }
      },
      "auroc": 0.7584608072916666
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4547,
          "fn": 253,
          "accuracy": 0.9472916666666666
        },
        "0.01": {
          "tp": 4237,
          "fn": 563,
          "accuracy": 0.8827083333333333
        }
      },
      "auroc": 0.9137863715277778
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2900,
          "fn": 1900,
          "accuracy": 0.6041666666666666
        },
        "0.01": {
          "tp": 2390,
          "fn": 2410,
          "accuracy": 0.4979166666666667
        }
      },
      "auroc": 0.745307421875
    },
    {
      "domain": "news",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7447,
          "fn": 2153,
          "accuracy": 0.7757291666666667
        },
        "0.01": {
          "tp": 6627,
          "fn": 2973,
          "accuracy": 0.6903125
        }
      },
      "auroc": 0.8295468967013888
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2189,
          "fn": 211,
          "accuracy": 0.9120833333333334
        },
        "0.01": {
          "tp": 1977,
          "fn": 423,
          "accuracy": 0.82375
        }
      },
      "auroc": 0.8974046006944446
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2189,
          "fn": 211,
          "accuracy": 0.9120833333333334
        },
        "0.01": {
          "tp": 1977,
          "fn": 423,
          "accuracy": 0.82375
        }
      },
      "auroc": 0.8974046006944446
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2007,
          "fn": 393,
          "accuracy": 0.83625
        },
        "0.01": {
          "tp": 1668,
          "fn": 732,
          "accuracy": 0.695
        }
      },
      "auroc": 0.86434453125
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2007,
          "fn": 393,
          "accuracy": 0.83625
        },
        "0.01": {
          "tp": 1668,
          "fn": 732,
          "accuracy": 0.695
        }
      },
      "auroc": 0.86434453125
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4196,
          "fn": 604,
          "accuracy": 0.8741666666666666
        },
        "0.01": {
          "tp": 3645,
          "fn": 1155,
          "accuracy": 0.759375
        }
      },
      "auroc": 0.8808745659722222
    },
    {
      "domain": "news",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4196,
          "fn": 604,
          "accuracy": 0.8741666666666666
        },
        "0.01": {
          "tp": 3645,
          "fn": 1155,
          "accuracy": 0.759375
        }
      },
      "auroc": 0.8808745659722222
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2037,
          "fn": 363,
          "accuracy": 0.84875
        },
        "0.01": {
          "tp": 1729,
          "fn": 671,
          "accuracy": 0.7204166666666667
        }
      },
      "auroc": 0.8668261284722222
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2037,
          "fn": 363,
          "accuracy": 0.84875
        },
        "0.01": {
          "tp": 1729,
          "fn": 671,
          "accuracy": 0.7204166666666667
        }
      },
      "auroc": 0.8668261284722222
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1787,
          "fn": 613,
          "accuracy": 0.7445833333333334
        },
        "0.01": {
          "tp": 1308,
          "fn": 1092,
          "accuracy": 0.545
        }
      },
      "auroc": 0.8188045138888889
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1787,
          "fn": 613,
          "accuracy": 0.7445833333333334
        },
        "0.01": {
          "tp": 1308,
          "fn": 1092,
          "accuracy": 0.545
        }
      },
      "auroc": 0.8188045138888889
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3824,
          "fn": 976,
          "accuracy": 0.7966666666666666
        },
        "0.01": {
          "tp": 3037,
          "fn": 1763,
          "accuracy": 0.6327083333333333
        }
      },
      "auroc": 0.8428153211805556
    },
    {
      "domain": "news",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3824,
          "fn": 976,
          "accuracy": 0.7966666666666666
        },
        "0.01": {
          "tp": 3037,
          "fn": 1763,
          "accuracy": 0.6327083333333333
        }
      },
      "auroc": 0.8428153211805556
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2281,
          "fn": 119,
          "accuracy": 0.9504166666666667
        },
        "0.01": {
          "tp": 2149,
          "fn": 251,
          "accuracy": 0.8954166666666666
        }
      },
      "auroc": 0.9162765625
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2281,
          "fn": 119,
          "accuracy": 0.9504166666666667
        },
        "0.01": {
          "tp": 2149,
          "fn": 251,
          "accuracy": 0.8954166666666666
        }
      },
      "auroc": 0.9162765625
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2164,
          "fn": 236,
          "accuracy": 0.9016666666666666
        },
        "0.01": {
          "tp": 1966,
          "fn": 434,
          "accuracy": 0.8191666666666667
        }
      },
      "auroc": 0.8946395833333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2164,
          "fn": 236,
          "accuracy": 0.9016666666666666
        },
        "0.01": {
          "tp": 1966,
          "fn": 434,
          "accuracy": 0.8191666666666667
        }
      },
      "auroc": 0.8946395833333333
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4445,
          "fn": 355,
          "accuracy": 0.9260416666666667
        },
        "0.01": {
          "tp": 4115,
          "fn": 685,
          "accuracy": 0.8572916666666667
        }
      },
      "auroc": 0.9054580729166666
    },
    {
      "domain": "news",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4445,
          "fn": 355,
          "accuracy": 0.9260416666666667
        },
        "0.01": {
          "tp": 4115,
          "fn": 685,
          "accuracy": 0.8572916666666667
        }
      },
      "auroc": 0.9054580729166666
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2304,
          "fn": 96,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 2182,
          "fn": 218,
          "accuracy": 0.9091666666666667
        }
      },
      "auroc": 0.9201583333333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2304,
          "fn": 96,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 2182,
          "fn": 218,
          "accuracy": 0.9091666666666667
        }
      },
      "auroc": 0.9201583333333333
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1428,
          "fn": 972,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 810,
          "fn": 1590,
          "accuracy": 0.3375
        }
      },
      "auroc": 0.7664537326388889
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1428,
          "fn": 972,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 810,
          "fn": 1590,
          "accuracy": 0.3375
        }
      },
      "auroc": 0.7664537326388889
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3732,
          "fn": 1068,
          "accuracy": 0.7775
        },
        "0.01": {
          "tp": 2992,
          "fn": 1808,
          "accuracy": 0.6233333333333333
        }
      },
      "auroc": 0.8433060329861111
    },
    {
      "domain": "news",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3732,
          "fn": 1068,
          "accuracy": 0.7775
        },
        "0.01": {
          "tp": 2992,
          "fn": 1808,
          "accuracy": 0.6233333333333333
        }
      },
      "auroc": 0.8433060329861111
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1856,
          "fn": 544,
          "accuracy": 0.7733333333333333
        },
        "0.01": {
          "tp": 1442,
          "fn": 958,
          "accuracy": 0.6008333333333333
        }
      },
      "auroc": 0.8407554687500001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1856,
          "fn": 544,
          "accuracy": 0.7733333333333333
        },
        "0.01": {
          "tp": 1442,
          "fn": 958,
          "accuracy": 0.6008333333333333
        }
      },
      "auroc": 0.8407554687500001
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1684,
          "fn": 716,
          "accuracy": 0.7016666666666667
        },
        "0.01": {
          "tp": 1245,
          "fn": 1155,
          "accuracy": 0.51875
        }
      },
      "auroc": 0.8061279513888889
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1684,
          "fn": 716,
          "accuracy": 0.7016666666666667
        },
        "0.01": {
          "tp": 1245,
          "fn": 1155,
          "accuracy": 0.51875
        }
      },
      "auroc": 0.8061279513888889
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3540,
          "fn": 1260,
          "accuracy": 0.7375
        },
        "0.01": {
          "tp": 2687,
          "fn": 2113,
          "accuracy": 0.5597916666666667
        }
      },
      "auroc": 0.8234417100694444
    },
    {
      "domain": "news",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3540,
          "fn": 1260,
          "accuracy": 0.7375
        },
        "0.01": {
          "tp": 2687,
          "fn": 2113,
          "accuracy": 0.5597916666666667
        }
      },
      "auroc": 0.8234417100694444
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 24664,
          "fn": 1736,
          "accuracy": 0.9342424242424242
        },
        "0.01": {
          "tp": 22827,
          "fn": 3573,
          "accuracy": 0.8646590909090909
        }
      },
      "auroc": 0.9072519018308081
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9193,
          "fn": 5207,
          "accuracy": 0.6384027777777778
        },
        "0.01": {
          "tp": 7691,
          "fn": 6709,
          "accuracy": 0.5340972222222222
        }
      },
      "auroc": 0.7129347945601852
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 33857,
          "fn": 6943,
          "accuracy": 0.829828431372549
        },
        "0.01": {
          "tp": 30518,
          "fn": 10282,
          "accuracy": 0.7479901960784314
        }
      },
      "auroc": 0.838669393382353
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 20572,
          "fn": 5828,
          "accuracy": 0.7792424242424243
        },
        "0.01": {
          "tp": 16471,
          "fn": 9929,
          "accuracy": 0.6239015151515152
        }
      },
      "auroc": 0.8395150331439394
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3687,
          "fn": 10713,
          "accuracy": 0.25604166666666667
        },
        "0.01": {
          "tp": 2795,
          "fn": 11605,
          "accuracy": 0.19409722222222223
        }
      },
      "auroc": 0.41341692708333333
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 24259,
          "fn": 16541,
          "accuracy": 0.5945833333333334
        },
        "0.01": {
          "tp": 19266,
          "fn": 21534,
          "accuracy": 0.4722058823529412
        }
      },
      "auroc": 0.6891274662990197
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 45236,
          "fn": 7564,
          "accuracy": 0.8567424242424242
        },
        "0.01": {
          "tp": 39298,
          "fn": 13502,
          "accuracy": 0.744280303030303
        }
      },
      "auroc": 0.8733834674873736
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 12880,
          "fn": 15920,
          "accuracy": 0.44722222222222224
        },
        "0.01": {
          "tp": 10486,
          "fn": 18314,
          "accuracy": 0.36409722222222224
        }
      },
      "auroc": 0.5631758608217592
    },
    {
      "domain": "news",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 58116,
          "fn": 23484,
          "accuracy": 0.7122058823529411
        },
        "0.01": {
          "tp": 49784,
          "fn": 31816,
          "accuracy": 0.6100980392156863
        }
      },
      "auroc": 0.7638984298406862
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9449072916666668
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.8987625
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        }
      },
      "auroc": 0.9218348958333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9422458333333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.7639916666666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        },
        "0.01": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        }
      },
      "auroc": 0.8531187500000001
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9435765625
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.8313770833333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 638,
          "fn": 162,
          "accuracy": 0.7975
        },
        "0.01": {
          "tp": 592,
          "fn": 208,
          "accuracy": 0.74
        }
      },
      "auroc": 0.8874768229166666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9273156250000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.22537083333333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        }
      },
      "auroc": 0.5763432291666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9073552083333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.06033958333333332
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        },
        "0.01": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.4838473958333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9173354166666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.14285520833333332
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 437,
          "accuracy": 0.45375
        },
        "0.01": {
          "tp": 348,
          "fn": 452,
          "accuracy": 0.435
        }
      },
      "auroc": 0.5300953125
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9362135416666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.8851979166666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        },
        "0.01": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9107057291666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.9085822916666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9415770833333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        }
      },
      "auroc": 0.9250796875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9292989583333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.4171614583333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 178,
          "fn": 222,
          "accuracy": 0.445
        }
      },
      "auroc": 0.6732302083333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        }
      },
      "auroc": 0.9189406250000001
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        }
      },
      "auroc": 0.6793692708333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 549,
          "fn": 251,
          "accuracy": 0.68625
        },
        "0.01": {
          "tp": 521,
          "fn": 279,
          "accuracy": 0.65125
        }
      },
      "auroc": 0.7991549479166666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9181645833333332
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        }
      },
      "auroc": 0.7844041666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        }
      },
      "auroc": 0.8512843750000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.8851802083333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07700833333333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 250,
          "accuracy": 0.375
        },
        "0.01": {
          "tp": 132,
          "fn": 268,
          "accuracy": 0.33
        }
      },
      "auroc": 0.4810942708333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 295,
          "fn": 105,
          "accuracy": 0.7375
        }
      },
      "auroc": 0.9016723958333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        },
        "0.01": {
          "tp": 95,
          "fn": 305,
          "accuracy": 0.2375
        }
      },
      "auroc": 0.43070625
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 438,
          "fn": 362,
          "accuracy": 0.5475
        },
        "0.01": {
          "tp": 390,
          "fn": 410,
          "accuracy": 0.4875
        }
      },
      "auroc": 0.6661893229166667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9416854166666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.8697635416666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9057244791666665
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9147145833333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.535465625
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        }
      },
      "auroc": 0.7250901041666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9281999999999999
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 249,
          "accuracy": 0.3775
        },
        "0.01": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        }
      },
      "auroc": 0.7026145833333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 520,
          "fn": 280,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 466,
          "fn": 334,
          "accuracy": 0.5825
        }
      },
      "auroc": 0.8154072916666665
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9137010416666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9137010416666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.8906687499999999
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.8906687499999999
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        }
      },
      "auroc": 0.9021848958333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        }
      },
      "auroc": 0.9021848958333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9133375
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9133375
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        }
      },
      "auroc": 0.8657625000000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        }
      },
      "auroc": 0.8657625000000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        }
      },
      "auroc": 0.8895500000000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        }
      },
      "auroc": 0.8895500000000001
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9393552083333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9393552083333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        }
      },
      "auroc": 0.8839854166666666
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        }
      },
      "auroc": 0.8839854166666666
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9116703125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9116703125
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9418552083333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9418552083333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.7189020833333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.7189020833333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        }
      },
      "auroc": 0.8303786458333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        }
      },
      "auroc": 0.8303786458333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.8975677083333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.8975677083333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        }
      },
      "auroc": 0.824103125
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        }
      },
      "auroc": 0.824103125
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        },
        "0.01": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        }
      },
      "auroc": 0.8608354166666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        },
        "0.01": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        }
      },
      "auroc": 0.8608354166666667
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1999,
          "fn": 201,
          "accuracy": 0.9086363636363637
        },
        "0.01": {
          "tp": 1935,
          "fn": 265,
          "accuracy": 0.8795454545454545
        }
      },
      "auroc": 0.9256986742424242
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1616,
          "fn": 584,
          "accuracy": 0.7345454545454545
        },
        "0.01": {
          "tp": 1483,
          "fn": 717,
          "accuracy": 0.6740909090909091
        }
      },
      "auroc": 0.8770376893939393
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3615,
          "fn": 785,
          "accuracy": 0.821590909090909
        },
        "0.01": {
          "tp": 3418,
          "fn": 982,
          "accuracy": 0.7768181818181819
        }
      },
      "auroc": 0.9013681818181819
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9296604166666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.8173625
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        }
      },
      "auroc": 0.8735114583333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9266479166666668
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.647134375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        }
      },
      "auroc": 0.7868911458333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9281541666666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 248,
          "accuracy": 0.38
        },
        "0.01": {
          "tp": 102,
          "fn": 298,
          "accuracy": 0.255
        }
      },
      "auroc": 0.7322484375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 520,
          "fn": 280,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 456,
          "fn": 344,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8302013020833334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.8740406250000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.15035729166666664
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 262,
          "accuracy": 0.345
        },
        "0.01": {
          "tp": 113,
          "fn": 287,
          "accuracy": 0.2825
        }
      },
      "auroc": 0.5121989583333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        }
      },
      "auroc": 0.8731718750000002
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.04647708333333332
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        },
        "0.01": {
          "tp": 127,
          "fn": 273,
          "accuracy": 0.3175
        }
      },
      "auroc": 0.4598244791666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        }
      },
      "auroc": 0.87360625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.0984171875
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 286,
          "fn": 514,
          "accuracy": 0.3575
        },
        "0.01": {
          "tp": 240,
          "fn": 560,
          "accuracy": 0.3
        }
      },
      "auroc": 0.48601171875
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.910384375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.8074072916666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        },
        "0.01": {
          "tp": 228,
          "fn": 172,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8588958333333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.8490760416666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9269583333333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        }
      },
      "auroc": 0.8880171874999999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9030395833333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.3798666666666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        },
        "0.01": {
          "tp": 154,
          "fn": 246,
          "accuracy": 0.385
        }
      },
      "auroc": 0.641453125
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        },
        "0.01": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        }
      },
      "auroc": 0.8760578125
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        }
      },
      "auroc": 0.6534125000000001
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 465,
          "fn": 335,
          "accuracy": 0.58125
        },
        "0.01": {
          "tp": 416,
          "fn": 384,
          "accuracy": 0.52
        }
      },
      "auroc": 0.7647351562500001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.8600395833333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.6730833333333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        }
      },
      "auroc": 0.7665614583333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.8249614583333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.062195833333333325
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        },
        "0.01": {
          "tp": 86,
          "fn": 314,
          "accuracy": 0.215
        }
      },
      "auroc": 0.4435786458333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        }
      },
      "auroc": 0.8425005208333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 332,
          "accuracy": 0.17
        },
        "0.01": {
          "tp": 49,
          "fn": 351,
          "accuracy": 0.1225
        }
      },
      "auroc": 0.3676395833333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 308,
          "fn": 492,
          "accuracy": 0.385
        },
        "0.01": {
          "tp": 232,
          "fn": 568,
          "accuracy": 0.29
        }
      },
      "auroc": 0.6050700520833333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9217770833333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.7671718750000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        }
      },
      "auroc": 0.8444744791666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        }
      },
      "auroc": 0.8632645833333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.4067635416666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        },
        "0.01": {
          "tp": 116,
          "fn": 284,
          "accuracy": 0.29
        }
      },
      "auroc": 0.6350140625
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        },
        "0.01": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        }
      },
      "auroc": 0.8925208333333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 70,
          "fn": 330,
          "accuracy": 0.175
        },
        "0.01": {
          "tp": 43,
          "fn": 357,
          "accuracy": 0.1075
        }
      },
      "auroc": 0.5869677083333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 413,
          "accuracy": 0.48375
        },
        "0.01": {
          "tp": 325,
          "fn": 475,
          "accuracy": 0.40625
        }
      },
      "auroc": 0.7397442708333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        }
      },
      "auroc": 0.8549833333333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        }
      },
      "auroc": 0.8549833333333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.818940625
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.818940625
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        }
      },
      "auroc": 0.8369619791666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        }
      },
      "auroc": 0.8369619791666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.8660395833333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.8660395833333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.7910395833333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.7910395833333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        },
        "0.01": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        }
      },
      "auroc": 0.8285395833333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        },
        "0.01": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        }
      },
      "auroc": 0.8285395833333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9095833333333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9095833333333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        }
      },
      "auroc": 0.8061333333333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        }
      },
      "auroc": 0.8061333333333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 210,
          "fn": 190,
          "accuracy": 0.525
        }
      },
      "auroc": 0.8578583333333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 210,
          "fn": 190,
          "accuracy": 0.525
        }
      },
      "auroc": 0.8578583333333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9201677083333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9201677083333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.6162000000000001
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.6162000000000001
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        },
        "0.01": {
          "tp": 171,
          "fn": 229,
          "accuracy": 0.4275
        }
      },
      "auroc": 0.7681838541666666
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        },
        "0.01": {
          "tp": 171,
          "fn": 229,
          "accuracy": 0.4275
        }
      },
      "auroc": 0.7681838541666666
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.84626875
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.84626875
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.7542739583333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.7542739583333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        }
      },
      "auroc": 0.8002713541666666
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        }
      },
      "auroc": 0.8002713541666666
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1654,
          "fn": 546,
          "accuracy": 0.7518181818181818
        },
        "0.01": {
          "tp": 1456,
          "fn": 744,
          "accuracy": 0.6618181818181819
        }
      },
      "auroc": 0.8856382575757574
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1267,
          "fn": 933,
          "accuracy": 0.5759090909090909
        },
        "0.01": {
          "tp": 1049,
          "fn": 1151,
          "accuracy": 0.4768181818181818
        }
      },
      "auroc": 0.8168254734848484
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2921,
          "fn": 1479,
          "accuracy": 0.6638636363636363
        },
        "0.01": {
          "tp": 2505,
          "fn": 1895,
          "accuracy": 0.5693181818181818
        }
      },
      "auroc": 0.8512318655303031
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.93649375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.831446875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        }
      },
      "auroc": 0.8839703125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9293166666666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.659675
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        },
        "0.01": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        }
      },
      "auroc": 0.7944958333333335
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9329052083333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 251,
          "accuracy": 0.3725
        },
        "0.01": {
          "tp": 111,
          "fn": 289,
          "accuracy": 0.2775
        }
      },
      "auroc": 0.7455609375000001
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 525,
          "fn": 275,
          "accuracy": 0.65625
        },
        "0.01": {
          "tp": 477,
          "fn": 323,
          "accuracy": 0.59625
        }
      },
      "auroc": 0.8392330729166667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8830208333333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.19088229166666665
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        },
        "0.01": {
          "tp": 116,
          "fn": 284,
          "accuracy": 0.29
        }
      },
      "auroc": 0.5369515625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8576520833333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.056390624999999986
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 272,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 104,
          "fn": 296,
          "accuracy": 0.26
        }
      },
      "auroc": 0.45702135416666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 274,
          "fn": 126,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        }
      },
      "auroc": 0.8703364583333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.12363645833333331
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 524,
          "accuracy": 0.345
        },
        "0.01": {
          "tp": 220,
          "fn": 580,
          "accuracy": 0.275
        }
      },
      "auroc": 0.4969864583333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9164135416666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.824171875
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 274,
          "fn": 126,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        }
      },
      "auroc": 0.8702927083333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        }
      },
      "auroc": 0.8569583333333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9360270833333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        }
      },
      "auroc": 0.8964927083333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9091114583333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.34170520833333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 235,
          "accuracy": 0.4125
        },
        "0.01": {
          "tp": 153,
          "fn": 247,
          "accuracy": 0.3825
        }
      },
      "auroc": 0.6254083333333332
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        },
        "0.01": {
          "tp": 236,
          "fn": 164,
          "accuracy": 0.59
        }
      },
      "auroc": 0.8830348958333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        }
      },
      "auroc": 0.6388661458333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 469,
          "fn": 331,
          "accuracy": 0.58625
        },
        "0.01": {
          "tp": 423,
          "fn": 377,
          "accuracy": 0.52875
        }
      },
      "auroc": 0.7609505208333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.8665072916666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.6881666666666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        },
        "0.01": {
          "tp": 154,
          "fn": 246,
          "accuracy": 0.385
        }
      },
      "auroc": 0.7773369791666668
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.8271583333333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.06214479166666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 81,
          "fn": 319,
          "accuracy": 0.2025
        }
      },
      "auroc": 0.44465156249999993
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        },
        "0.01": {
          "tp": 181,
          "fn": 219,
          "accuracy": 0.4525
        }
      },
      "auroc": 0.8468328125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 332,
          "accuracy": 0.17
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.3751557291666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 311,
          "fn": 489,
          "accuracy": 0.38875
        },
        "0.01": {
          "tp": 235,
          "fn": 565,
          "accuracy": 0.29375
        }
      },
      "auroc": 0.6109942708333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.925534375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.7763541666666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        }
      },
      "auroc": 0.8509442708333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.8739979166666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.4035229166666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 126,
          "fn": 274,
          "accuracy": 0.315
        }
      },
      "auroc": 0.6387604166666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        },
        "0.01": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        }
      },
      "auroc": 0.8997661458333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 83,
          "fn": 317,
          "accuracy": 0.2075
        },
        "0.01": {
          "tp": 55,
          "fn": 345,
          "accuracy": 0.1375
        }
      },
      "auroc": 0.5899385416666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 400,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 349,
          "fn": 451,
          "accuracy": 0.43625
        }
      },
      "auroc": 0.74485234375
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.8814875
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.8814875
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8517000000000001
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8517000000000001
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        },
        "0.01": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        }
      },
      "auroc": 0.86659375
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        },
        "0.01": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        }
      },
      "auroc": 0.86659375
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.873978125
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.873978125
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        }
      },
      "auroc": 0.79856875
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        }
      },
      "auroc": 0.79856875
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        },
        "0.01": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        }
      },
      "auroc": 0.8362734375
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        },
        "0.01": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        }
      },
      "auroc": 0.8362734375
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9124104166666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9124104166666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        }
      },
      "auroc": 0.804446875
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        }
      },
      "auroc": 0.804446875
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        },
        "0.01": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        }
      },
      "auroc": 0.8584286458333332
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        },
        "0.01": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        }
      },
      "auroc": 0.8584286458333332
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9219520833333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9219520833333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.6167593750000001
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.6167593750000001
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        }
      },
      "auroc": 0.7693557291666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        }
      },
      "auroc": 0.7693557291666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        }
      },
      "auroc": 0.8505343750000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        }
      },
      "auroc": 0.8505343750000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.7547322916666668
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.7547322916666668
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        }
      },
      "auroc": 0.8026333333333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        }
      },
      "auroc": 0.8026333333333333
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1710,
          "fn": 490,
          "accuracy": 0.7772727272727272
        },
        "0.01": {
          "tp": 1534,
          "fn": 666,
          "accuracy": 0.6972727272727273
        }
      },
      "auroc": 0.8932082386363637
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1276,
          "fn": 924,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 1060,
          "fn": 1140,
          "accuracy": 0.4818181818181818
        }
      },
      "auroc": 0.8225105113636364
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2986,
          "fn": 1414,
          "accuracy": 0.6786363636363636
        },
        "0.01": {
          "tp": 2594,
          "fn": 1806,
          "accuracy": 0.5895454545454546
        }
      },
      "auroc": 0.857859375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        }
      },
      "auroc": 0.909434375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        }
      },
      "auroc": 0.7435489583333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        }
      },
      "auroc": 0.8264916666666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.8961208333333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.533890625
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 228,
          "accuracy": 0.43
        },
        "0.01": {
          "tp": 149,
          "fn": 251,
          "accuracy": 0.3725
        }
      },
      "auroc": 0.7150057291666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": {
          "tp": 289,
          "fn": 111,
          "accuracy": 0.7225
        }
      },
      "auroc": 0.9027776041666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 67,
          "fn": 333,
          "accuracy": 0.1675
        },
        "0.01": {
          "tp": 43,
          "fn": 357,
          "accuracy": 0.1075
        }
      },
      "auroc": 0.6387197916666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 412,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 332,
          "fn": 468,
          "accuracy": 0.415
        }
      },
      "auroc": 0.7707486979166668
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9116843750000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.11959375
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        },
        "0.01": {
          "tp": 160,
          "fn": 240,
          "accuracy": 0.4
        }
      },
      "auroc": 0.5156390625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        },
        "0.01": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        }
      },
      "auroc": 0.7335437499999999
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.04786354166666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 66,
          "fn": 334,
          "accuracy": 0.165
        },
        "0.01": {
          "tp": 44,
          "fn": 356,
          "accuracy": 0.11
        }
      },
      "auroc": 0.39070364583333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8226140625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08372864583333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 241,
          "fn": 559,
          "accuracy": 0.30125
        },
        "0.01": {
          "tp": 204,
          "fn": 596,
          "accuracy": 0.255
        }
      },
      "auroc": 0.4531713541666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.8586770833333335
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.7463239583333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        },
        "0.01": {
          "tp": 147,
          "fn": 253,
          "accuracy": 0.3675
        }
      },
      "auroc": 0.8025005208333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8942270833333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9266645833333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        }
      },
      "auroc": 0.9104458333333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.8484739583333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.14952500000000002
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        },
        "0.01": {
          "tp": 89,
          "fn": 311,
          "accuracy": 0.2225
        }
      },
      "auroc": 0.49899947916666665
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        }
      },
      "auroc": 0.8713505208333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        },
        "0.01": {
          "tp": 172,
          "fn": 228,
          "accuracy": 0.43
        }
      },
      "auroc": 0.5380947916666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 461,
          "fn": 339,
          "accuracy": 0.57625
        },
        "0.01": {
          "tp": 391,
          "fn": 409,
          "accuracy": 0.48875
        }
      },
      "auroc": 0.70472265625
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.8968906250000002
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.4909947916666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 222,
          "accuracy": 0.445
        },
        "0.01": {
          "tp": 158,
          "fn": 242,
          "accuracy": 0.395
        }
      },
      "auroc": 0.6939427083333335
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.7167802083333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.045686458333333325
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 341,
          "accuracy": 0.1475
        },
        "0.01": {
          "tp": 43,
          "fn": 357,
          "accuracy": 0.1075
        }
      },
      "auroc": 0.3812333333333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        }
      },
      "auroc": 0.8068354166666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 21,
          "fn": 379,
          "accuracy": 0.0525
        },
        "0.01": {
          "tp": 13,
          "fn": 387,
          "accuracy": 0.0325
        }
      },
      "auroc": 0.268340625
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 237,
          "fn": 563,
          "accuracy": 0.29625
        },
        "0.01": {
          "tp": 201,
          "fn": 599,
          "accuracy": 0.25125
        }
      },
      "auroc": 0.5375880208333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.8904208333333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.6105187499999999
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 238,
          "accuracy": 0.405
        },
        "0.01": {
          "tp": 141,
          "fn": 259,
          "accuracy": 0.3525
        }
      },
      "auroc": 0.7504697916666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.7913635416666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.3031635416666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 86,
          "fn": 314,
          "accuracy": 0.215
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.5472635416666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        }
      },
      "auroc": 0.8408921874999999
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        },
        "0.01": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        }
      },
      "auroc": 0.45684114583333335
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 552,
          "accuracy": 0.31
        },
        "0.01": {
          "tp": 197,
          "fn": 603,
          "accuracy": 0.24625
        }
      },
      "auroc": 0.6488666666666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        }
      },
      "auroc": 0.8217739583333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        }
      },
      "auroc": 0.8217739583333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.7476635416666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.7476635416666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 144,
          "fn": 256,
          "accuracy": 0.36
        }
      },
      "auroc": 0.78471875
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 144,
          "fn": 256,
          "accuracy": 0.36
        }
      },
      "auroc": 0.78471875
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.8131375000000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.8131375000000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.7256750000000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.7256750000000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 234,
          "accuracy": 0.415
        },
        "0.01": {
          "tp": 119,
          "fn": 281,
          "accuracy": 0.2975
        }
      },
      "auroc": 0.76940625
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 234,
          "accuracy": 0.415
        },
        "0.01": {
          "tp": 119,
          "fn": 281,
          "accuracy": 0.2975
        }
      },
      "auroc": 0.76940625
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.85690625
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.85690625
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.7447843749999999
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.7447843749999999
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        },
        "0.01": {
          "tp": 131,
          "fn": 269,
          "accuracy": 0.3275
        }
      },
      "auroc": 0.8008453125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        },
        "0.01": {
          "tp": 131,
          "fn": 269,
          "accuracy": 0.3275
        }
      },
      "auroc": 0.8008453125
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.90706875
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.90706875
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.627275
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.627275
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 158,
          "fn": 242,
          "accuracy": 0.395
        }
      },
      "auroc": 0.7671718750000001
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 158,
          "fn": 242,
          "accuracy": 0.395
        }
      },
      "auroc": 0.7671718750000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.8059427083333334
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.8059427083333334
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        }
      },
      "auroc": 0.7065041666666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        }
      },
      "auroc": 0.7065041666666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        },
        "0.01": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        }
      },
      "auroc": 0.7562234375
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        },
        "0.01": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        }
      },
      "auroc": 0.7562234375
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1544,
          "fn": 656,
          "accuracy": 0.7018181818181818
        },
        "0.01": {
          "tp": 1326,
          "fn": 874,
          "accuracy": 0.6027272727272728
        }
      },
      "auroc": 0.8696512310606062
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 833,
          "fn": 1367,
          "accuracy": 0.37863636363636366
        },
        "0.01": {
          "tp": 594,
          "fn": 1606,
          "accuracy": 0.27
        }
      },
      "auroc": 0.7531371212121212
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2377,
          "fn": 2023,
          "accuracy": 0.5402272727272728
        },
        "0.01": {
          "tp": 1920,
          "fn": 2480,
          "accuracy": 0.43636363636363634
        }
      },
      "auroc": 0.8113941761363637
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9443333333333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.89119375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9177635416666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9406708333333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.7522708333333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        }
      },
      "auroc": 0.8464708333333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9425020833333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        },
        "0.01": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        }
      },
      "auroc": 0.8217322916666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 623,
          "fn": 177,
          "accuracy": 0.77875
        },
        "0.01": {
          "tp": 575,
          "fn": 225,
          "accuracy": 0.71875
        }
      },
      "auroc": 0.8821171875
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9251958333333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.21188645833333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 176,
          "fn": 224,
          "accuracy": 0.44
        }
      },
      "auroc": 0.5685411458333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        }
      },
      "auroc": 0.8977437500000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.058098958333333325
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 236,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 154,
          "fn": 246,
          "accuracy": 0.385
        }
      },
      "auroc": 0.47792135416666665
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9114697916666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.13499270833333332
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 452,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 330,
          "fn": 470,
          "accuracy": 0.4125
        }
      },
      "auroc": 0.52323125
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9320916666666665
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.8756697916666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9038807291666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.909178125
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9415770833333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        }
      },
      "auroc": 0.9253776041666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.92435
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.36616145833333336
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 214,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        }
      },
      "auroc": 0.6452557291666665
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        }
      },
      "auroc": 0.9167640625
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        }
      },
      "auroc": 0.6538692708333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 545,
          "fn": 255,
          "accuracy": 0.68125
        },
        "0.01": {
          "tp": 517,
          "fn": 283,
          "accuracy": 0.64625
        }
      },
      "auroc": 0.7853166666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9174802083333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        }
      },
      "auroc": 0.758409375
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        },
        "0.01": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        }
      },
      "auroc": 0.8379447916666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.8761135416666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07310416666666665
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 255,
          "accuracy": 0.3625
        },
        "0.01": {
          "tp": 122,
          "fn": 278,
          "accuracy": 0.305
        }
      },
      "auroc": 0.4746088541666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 320,
          "fn": 80,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        }
      },
      "auroc": 0.8967968749999999
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 298,
          "accuracy": 0.255
        },
        "0.01": {
          "tp": 82,
          "fn": 318,
          "accuracy": 0.205
        }
      },
      "auroc": 0.41575677083333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 422,
          "fn": 378,
          "accuracy": 0.5275
        },
        "0.01": {
          "tp": 365,
          "fn": 435,
          "accuracy": 0.45625
        }
      },
      "auroc": 0.6562768229166667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9394114583333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        }
      },
      "auroc": 0.8504750000000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        }
      },
      "auroc": 0.8949432291666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        }
      },
      "auroc": 0.90700625
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.5109572916666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 214,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 158,
          "fn": 242,
          "accuracy": 0.395
        }
      },
      "auroc": 0.7089817708333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9232088541666668
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 267,
          "accuracy": 0.3325
        },
        "0.01": {
          "tp": 97,
          "fn": 303,
          "accuracy": 0.2425
        }
      },
      "auroc": 0.6807161458333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 494,
          "fn": 306,
          "accuracy": 0.6175
        },
        "0.01": {
          "tp": 437,
          "fn": 363,
          "accuracy": 0.54625
        }
      },
      "auroc": 0.8019624999999999
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9098739583333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9098739583333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.8723125
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.8723125
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        },
        "0.01": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        }
      },
      "auroc": 0.8910932291666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        },
        "0.01": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        }
      },
      "auroc": 0.8910932291666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        }
      },
      "auroc": 0.9024541666666668
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        }
      },
      "auroc": 0.9024541666666668
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        }
      },
      "auroc": 0.8549916666666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        }
      },
      "auroc": 0.8549916666666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        },
        "0.01": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        }
      },
      "auroc": 0.8787229166666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        },
        "0.01": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        }
      },
      "auroc": 0.8787229166666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9349447916666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9349447916666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8697583333333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8697583333333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 291,
          "fn": 109,
          "accuracy": 0.7275
        }
      },
      "auroc": 0.9023515624999999
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 291,
          "fn": 109,
          "accuracy": 0.7275
        }
      },
      "auroc": 0.9023515624999999
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9397677083333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9397677083333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.7079416666666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.7079416666666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 210,
          "fn": 190,
          "accuracy": 0.525
        }
      },
      "auroc": 0.8238546874999999
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 210,
          "fn": 190,
          "accuracy": 0.525
        }
      },
      "auroc": 0.8238546874999999
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.8911322916666666
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.8911322916666666
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8126677083333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8126677083333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        }
      },
      "auroc": 0.8519000000000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        }
      },
      "auroc": 0.8519000000000001
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1973,
          "fn": 227,
          "accuracy": 0.8968181818181818
        },
        "0.01": {
          "tp": 1894,
          "fn": 306,
          "accuracy": 0.860909090909091
        }
      },
      "auroc": 0.9223512310606061
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1556,
          "fn": 644,
          "accuracy": 0.7072727272727273
        },
        "0.01": {
          "tp": 1388,
          "fn": 812,
          "accuracy": 0.6309090909090909
        }
      },
      "auroc": 0.8672023674242424
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3529,
          "fn": 871,
          "accuracy": 0.8020454545454545
        },
        "0.01": {
          "tp": 3282,
          "fn": 1118,
          "accuracy": 0.7459090909090909
        }
      },
      "auroc": 0.8947767992424243
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.898903125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        }
      },
      "auroc": 0.8097541666666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        }
      },
      "auroc": 0.8543286458333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.9004645833333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        }
      },
      "auroc": 0.7687833333333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 172,
          "fn": 228,
          "accuracy": 0.43
        }
      },
      "auroc": 0.8346239583333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        },
        "0.01": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        }
      },
      "auroc": 0.8996838541666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 246,
          "accuracy": 0.385
        },
        "0.01": {
          "tp": 94,
          "fn": 306,
          "accuracy": 0.235
        }
      },
      "auroc": 0.78926875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 457,
          "fn": 343,
          "accuracy": 0.57125
        },
        "0.01": {
          "tp": 366,
          "fn": 434,
          "accuracy": 0.4575
        }
      },
      "auroc": 0.8444763020833334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.8519125
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        },
        "0.01": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        }
      },
      "auroc": 0.6075812499999999
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 253,
          "accuracy": 0.3675
        },
        "0.01": {
          "tp": 120,
          "fn": 280,
          "accuracy": 0.3
        }
      },
      "auroc": 0.7297468750000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.8904229166666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.4459864583333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 234,
          "accuracy": 0.415
        },
        "0.01": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        }
      },
      "auroc": 0.6682046875
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        }
      },
      "auroc": 0.8711677083333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 371,
          "accuracy": 0.0725
        },
        "0.01": {
          "tp": 13,
          "fn": 387,
          "accuracy": 0.0325
        }
      },
      "auroc": 0.5267838541666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 487,
          "accuracy": 0.39125
        },
        "0.01": {
          "tp": 268,
          "fn": 532,
          "accuracy": 0.335
        }
      },
      "auroc": 0.69897578125
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        }
      },
      "auroc": 0.8876729166666668
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.580634375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 248,
          "accuracy": 0.38
        },
        "0.01": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        }
      },
      "auroc": 0.7341536458333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        }
      },
      "auroc": 0.81695625
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.513390625
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 297,
          "accuracy": 0.2575
        },
        "0.01": {
          "tp": 72,
          "fn": 328,
          "accuracy": 0.18
        }
      },
      "auroc": 0.6651734375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        }
      },
      "auroc": 0.8523145833333334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 369,
          "accuracy": 0.0775
        },
        "0.01": {
          "tp": 17,
          "fn": 383,
          "accuracy": 0.0425
        }
      },
      "auroc": 0.5470124999999999
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 255,
          "fn": 545,
          "accuracy": 0.31875
        },
        "0.01": {
          "tp": 197,
          "fn": 603,
          "accuracy": 0.24625
        }
      },
      "auroc": 0.6996635416666668
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.930153125
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.8934666666666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.7099114583333335
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        }
      },
      "auroc": 0.8016890624999999
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        }
      },
      "auroc": 0.8200322916666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        }
      },
      "auroc": 0.8303218750000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        }
      },
      "auroc": 0.8002250000000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        },
        "0.01": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        }
      },
      "auroc": 0.8152734374999999
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.863059375
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.45752916666666665
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 265,
          "accuracy": 0.3375
        },
        "0.01": {
          "tp": 105,
          "fn": 295,
          "accuracy": 0.2625
        }
      },
      "auroc": 0.6602942708333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        }
      },
      "auroc": 0.8466906250000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 300,
          "accuracy": 0.25
        },
        "0.01": {
          "tp": 66,
          "fn": 334,
          "accuracy": 0.165
        }
      },
      "auroc": 0.6288770833333335
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 454,
          "accuracy": 0.4325
        },
        "0.01": {
          "tp": 266,
          "fn": 534,
          "accuracy": 0.3325
        }
      },
      "auroc": 0.7377838541666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.8981916666666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        },
        "0.01": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        }
      },
      "auroc": 0.8030052083333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        }
      },
      "auroc": 0.8505984375000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        }
      },
      "auroc": 0.8590979166666668
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.6912322916666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 237,
          "accuracy": 0.4075
        },
        "0.01": {
          "tp": 117,
          "fn": 283,
          "accuracy": 0.2925
        }
      },
      "auroc": 0.7751651041666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        }
      },
      "auroc": 0.8786447916666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 274,
          "accuracy": 0.315
        },
        "0.01": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        }
      },
      "auroc": 0.74711875
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 402,
          "fn": 398,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 304,
          "fn": 496,
          "accuracy": 0.38
        }
      },
      "auroc": 0.8128817708333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.8462333333333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.8462333333333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.8207291666666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.8207291666666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        },
        "0.01": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        }
      },
      "auroc": 0.8334812500000001
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        },
        "0.01": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        }
      },
      "auroc": 0.8334812500000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        }
      },
      "auroc": 0.8379395833333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        }
      },
      "auroc": 0.8379395833333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.7957916666666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.7957916666666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        },
        "0.01": {
          "tp": 143,
          "fn": 257,
          "accuracy": 0.3575
        }
      },
      "auroc": 0.816865625
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        },
        "0.01": {
          "tp": 143,
          "fn": 257,
          "accuracy": 0.3575
        }
      },
      "auroc": 0.816865625
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.893275
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.893275
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        }
      },
      "auroc": 0.8496666666666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        }
      },
      "auroc": 0.8496666666666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 259,
          "fn": 141,
          "accuracy": 0.6475
        },
        "0.01": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        }
      },
      "auroc": 0.8714708333333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 259,
          "fn": 141,
          "accuracy": 0.6475
        },
        "0.01": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        }
      },
      "auroc": 0.8714708333333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9153666666666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9153666666666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.8129270833333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.8129270833333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        }
      },
      "auroc": 0.864146875
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        }
      },
      "auroc": 0.864146875
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        }
      },
      "auroc": 0.828096875
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        }
      },
      "auroc": 0.828096875
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.764390625
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.764390625
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        },
        "0.01": {
          "tp": 145,
          "fn": 255,
          "accuracy": 0.3625
        }
      },
      "auroc": 0.7962437500000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        },
        "0.01": {
          "tp": 145,
          "fn": 255,
          "accuracy": 0.3625
        }
      },
      "auroc": 0.7962437500000001
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 488,
          "fn": 712,
          "accuracy": 0.4066666666666667
        },
        "0.01": {
          "tp": 367,
          "fn": 833,
          "accuracy": 0.30583333333333335
        }
      },
      "auroc": 0.7552255208333334
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1284,
          "fn": 916,
          "accuracy": 0.5836363636363636
        },
        "0.01": {
          "tp": 1047,
          "fn": 1153,
          "accuracy": 0.4759090909090909
        }
      },
      "auroc": 0.8424520833333333
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 1006,
          "accuracy": 0.16166666666666665
        },
        "0.01": {
          "tp": 96,
          "fn": 1104,
          "accuracy": 0.08
        }
      },
      "auroc": 0.5978055555555556
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1478,
          "fn": 1922,
          "accuracy": 0.43470588235294116
        },
        "0.01": {
          "tp": 1143,
          "fn": 2257,
          "accuracy": 0.3361764705882353
        }
      },
      "auroc": 0.75610625
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 682,
          "fn": 1718,
          "accuracy": 0.2841666666666667
        },
        "0.01": {
          "tp": 463,
          "fn": 1937,
          "accuracy": 0.19291666666666665
        }
      },
      "auroc": 0.6765155381944445
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.94428125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.8981583333333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        }
      },
      "auroc": 0.9212197916666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.941909375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.7637885416666668
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        },
        "0.01": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        }
      },
      "auroc": 0.8528489583333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9430953125000001
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        },
        "0.01": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        }
      },
      "auroc": 0.8309734375
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 636,
          "fn": 164,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 589,
          "fn": 211,
          "accuracy": 0.73625
        }
      },
      "auroc": 0.887034375
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9273156250000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.21879687499999997
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        }
      },
      "auroc": 0.57305625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.8987447916666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.05789270833333332
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        },
        "0.01": {
          "tp": 162,
          "fn": 238,
          "accuracy": 0.405
        }
      },
      "auroc": 0.47831874999999996
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        }
      },
      "auroc": 0.9130302083333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.13834479166666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 443,
          "accuracy": 0.44625
        },
        "0.01": {
          "tp": 341,
          "fn": 459,
          "accuracy": 0.42625
        }
      },
      "auroc": 0.5256875
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9362135416666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.8851979166666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        },
        "0.01": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9107057291666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.9087447916666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9415770833333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        }
      },
      "auroc": 0.9251609375000001
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9289760416666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.4072197916666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        }
      },
      "auroc": 0.6680979166666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9188604166666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        }
      },
      "auroc": 0.6743984375000001
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 548,
          "fn": 252,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 520,
          "fn": 280,
          "accuracy": 0.65
        }
      },
      "auroc": 0.7966294270833333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9181645833333332
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.7830114583333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        },
        "0.01": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        }
      },
      "auroc": 0.8505880208333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        }
      },
      "auroc": 0.8827760416666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07578958333333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 251,
          "accuracy": 0.3725
        },
        "0.01": {
          "tp": 131,
          "fn": 269,
          "accuracy": 0.3275
        }
      },
      "auroc": 0.4792828125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9004703125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 289,
          "accuracy": 0.2775
        },
        "0.01": {
          "tp": 92,
          "fn": 308,
          "accuracy": 0.23
        }
      },
      "auroc": 0.42940052083333335
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 436,
          "fn": 364,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 386,
          "fn": 414,
          "accuracy": 0.4825
        }
      },
      "auroc": 0.6649354166666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9416854166666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.8697635416666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9057244791666665
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9147145833333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.5353645833333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        },
        "0.01": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        }
      },
      "auroc": 0.7250395833333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9281999999999999
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 248,
          "accuracy": 0.38
        },
        "0.01": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        }
      },
      "auroc": 0.7025640625
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 521,
          "fn": 279,
          "accuracy": 0.65125
        },
        "0.01": {
          "tp": 466,
          "fn": 334,
          "accuracy": 0.5825
        }
      },
      "auroc": 0.8153820312499999
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9137010416666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9137010416666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.889709375
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.889709375
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        }
      },
      "auroc": 0.9017052083333332
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        }
      },
      "auroc": 0.9017052083333332
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9130010416666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9130010416666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        }
      },
      "auroc": 0.86464375
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        }
      },
      "auroc": 0.86464375
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        }
      },
      "auroc": 0.8888223958333333
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        }
      },
      "auroc": 0.8888223958333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9393552083333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9393552083333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        }
      },
      "auroc": 0.8839854166666666
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        }
      },
      "auroc": 0.8839854166666666
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9116703125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9116703125
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9418552083333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9418552083333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.7189020833333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.7189020833333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        }
      },
      "auroc": 0.8303786458333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        }
      },
      "auroc": 0.8303786458333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.8975677083333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.8975677083333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        }
      },
      "auroc": 0.8239406250000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        }
      },
      "auroc": 0.8239406250000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        },
        "0.01": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        }
      },
      "auroc": 0.8607541666666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        },
        "0.01": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        }
      },
      "auroc": 0.8607541666666667
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1997,
          "fn": 203,
          "accuracy": 0.9077272727272727
        },
        "0.01": {
          "tp": 1933,
          "fn": 267,
          "accuracy": 0.8786363636363637
        }
      },
      "auroc": 0.9256259469696969
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1609,
          "fn": 591,
          "accuracy": 0.7313636363636363
        },
        "0.01": {
          "tp": 1473,
          "fn": 727,
          "accuracy": 0.6695454545454546
        }
      },
      "auroc": 0.8757727272727271
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3606,
          "fn": 794,
          "accuracy": 0.8195454545454546
        },
        "0.01": {
          "tp": 3406,
          "fn": 994,
          "accuracy": 0.774090909090909
        }
      },
      "auroc": 0.9006993371212122
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9417572916666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.8670322916666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        }
      },
      "auroc": 0.9043947916666668
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9348322916666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.7098020833333335
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        }
      },
      "auroc": 0.8223171875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9382947916666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 157,
          "fn": 243,
          "accuracy": 0.3925
        }
      },
      "auroc": 0.7884171875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 578,
          "fn": 222,
          "accuracy": 0.7225
        },
        "0.01": {
          "tp": 534,
          "fn": 266,
          "accuracy": 0.6675
        }
      },
      "auroc": 0.8633559895833334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9201083333333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.22397083333333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 166,
          "fn": 234,
          "accuracy": 0.415
        }
      },
      "auroc": 0.5720395833333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        }
      },
      "auroc": 0.89864375
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.059271874999999995
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 234,
          "accuracy": 0.415
        },
        "0.01": {
          "tp": 154,
          "fn": 246,
          "accuracy": 0.385
        }
      },
      "auroc": 0.47895781249999997
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9093760416666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.14162135416666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 450,
          "accuracy": 0.4375
        },
        "0.01": {
          "tp": 320,
          "fn": 480,
          "accuracy": 0.4
        }
      },
      "auroc": 0.5254986979166666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.919421875
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.8397895833333333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        },
        "0.01": {
          "tp": 267,
          "fn": 133,
          "accuracy": 0.6675
        }
      },
      "auroc": 0.8796057291666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        }
      },
      "auroc": 0.9057552083333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.94070625
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        }
      },
      "auroc": 0.9232307291666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9205875
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.44288333333333335
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        }
      },
      "auroc": 0.6817354166666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        }
      },
      "auroc": 0.9131713541666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.6917947916666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 533,
          "fn": 267,
          "accuracy": 0.66625
        },
        "0.01": {
          "tp": 502,
          "fn": 298,
          "accuracy": 0.6275
        }
      },
      "auroc": 0.8024830729166665
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        }
      },
      "auroc": 0.9146145833333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.742771875
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        },
        "0.01": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        }
      },
      "auroc": 0.8286932291666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.8594697916666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.0768625
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        },
        "0.01": {
          "tp": 105,
          "fn": 295,
          "accuracy": 0.2625
        }
      },
      "auroc": 0.4681661458333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        }
      },
      "auroc": 0.8870421875000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 86,
          "fn": 314,
          "accuracy": 0.215
        },
        "0.01": {
          "tp": 71,
          "fn": 329,
          "accuracy": 0.1775
        }
      },
      "auroc": 0.40981718749999996
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 410,
          "accuracy": 0.4875
        },
        "0.01": {
          "tp": 335,
          "fn": 465,
          "accuracy": 0.41875
        }
      },
      "auroc": 0.6484296875
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9287010416666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        },
        "0.01": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        }
      },
      "auroc": 0.81229375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        },
        "0.01": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        }
      },
      "auroc": 0.8704973958333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.8866114583333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.45783958333333336
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        },
        "0.01": {
          "tp": 138,
          "fn": 262,
          "accuracy": 0.345
        }
      },
      "auroc": 0.6722255208333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        }
      },
      "auroc": 0.90765625
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 295,
          "accuracy": 0.2625
        },
        "0.01": {
          "tp": 72,
          "fn": 328,
          "accuracy": 0.18
        }
      },
      "auroc": 0.6350666666666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 440,
          "fn": 360,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 382,
          "fn": 418,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.7713614583333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9046635416666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9046635416666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.8719947916666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.8719947916666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        }
      },
      "auroc": 0.8883291666666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        }
      },
      "auroc": 0.8883291666666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.8893604166666668
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.8893604166666668
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.8287635416666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.8287635416666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        },
        "0.01": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        }
      },
      "auroc": 0.8590619791666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        },
        "0.01": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        }
      },
      "auroc": 0.8590619791666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9138229166666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9138229166666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.8158020833333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.8158020833333333
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        }
      },
      "auroc": 0.8648125
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        }
      },
      "auroc": 0.8648125
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9306385416666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9306385416666667
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.6455614583333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.6455614583333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        },
        "0.01": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        }
      },
      "auroc": 0.7881
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        },
        "0.01": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        }
      },
      "auroc": 0.7881
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.8772583333333334
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.8772583333333334
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        }
      },
      "auroc": 0.7929927083333334
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        }
      },
      "auroc": 0.7929927083333334
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        },
        "0.01": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        }
      },
      "auroc": 0.8351255208333332
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        },
        "0.01": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        }
      },
      "auroc": 0.8351255208333332
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1897,
          "fn": 303,
          "accuracy": 0.8622727272727273
        },
        "0.01": {
          "tp": 1787,
          "fn": 413,
          "accuracy": 0.8122727272727273
        }
      },
      "auroc": 0.9132820075757576
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1388,
          "fn": 812,
          "accuracy": 0.6309090909090909
        },
        "0.01": {
          "tp": 1229,
          "fn": 971,
          "accuracy": 0.5586363636363636
        }
      },
      "auroc": 0.8450044507575756
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3285,
          "fn": 1115,
          "accuracy": 0.7465909090909091
        },
        "0.01": {
          "tp": 3016,
          "fn": 1384,
          "accuracy": 0.6854545454545454
        }
      },
      "auroc": 0.8791432291666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.9017999999999999
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.7990635416666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        }
      },
      "auroc": 0.8504317708333332
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.8911395833333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.7408333333333335
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        },
        "0.01": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        }
      },
      "auroc": 0.8159864583333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        },
        "0.01": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        }
      },
      "auroc": 0.8964697916666666
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 259,
          "accuracy": 0.3525
        },
        "0.01": {
          "tp": 92,
          "fn": 308,
          "accuracy": 0.23
        }
      },
      "auroc": 0.7699484374999999
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 460,
          "fn": 340,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 367,
          "fn": 433,
          "accuracy": 0.45875
        }
      },
      "auroc": 0.8332091145833334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        }
      },
      "auroc": 0.8948135416666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.3063177083333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        },
        "0.01": {
          "tp": 153,
          "fn": 247,
          "accuracy": 0.3825
        }
      },
      "auroc": 0.6005656250000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9237906250000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.37680833333333336
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        }
      },
      "auroc": 0.6502994791666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9093020833333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 367,
          "accuracy": 0.0825
        },
        "0.01": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        }
      },
      "auroc": 0.3415630208333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 426,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 349,
          "fn": 451,
          "accuracy": 0.43625
        }
      },
      "auroc": 0.6254325520833334
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.8328718749999999
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.7797197916666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        },
        "0.01": {
          "tp": 136,
          "fn": 264,
          "accuracy": 0.34
        }
      },
      "auroc": 0.8062958333333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8183822916666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9127406250000001
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        },
        "0.01": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        }
      },
      "auroc": 0.8655614583333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9161625
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.7226916666666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        }
      },
      "auroc": 0.8194270833333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        }
      },
      "auroc": 0.8672723958333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 228,
          "fn": 172,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.8177161458333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 536,
          "fn": 264,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 485,
          "fn": 315,
          "accuracy": 0.60625
        }
      },
      "auroc": 0.8424942708333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.89815
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.698909375
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        },
        "0.01": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        }
      },
      "auroc": 0.7985296875000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9050697916666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.4250729166666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 219,
          "accuracy": 0.4525
        },
        "0.01": {
          "tp": 160,
          "fn": 240,
          "accuracy": 0.4
        }
      },
      "auroc": 0.6650713541666665
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        }
      },
      "auroc": 0.9016098958333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 63,
          "fn": 337,
          "accuracy": 0.1575
        },
        "0.01": {
          "tp": 39,
          "fn": 361,
          "accuracy": 0.0975
        }
      },
      "auroc": 0.5619911458333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 405,
          "accuracy": 0.49375
        },
        "0.01": {
          "tp": 344,
          "fn": 456,
          "accuracy": 0.43
        }
      },
      "auroc": 0.7318005208333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8763052083333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.7084447916666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 141,
          "fn": 259,
          "accuracy": 0.3525
        }
      },
      "auroc": 0.7923750000000002
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        }
      },
      "auroc": 0.8516739583333335
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.5700947916666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 255,
          "accuracy": 0.3625
        },
        "0.01": {
          "tp": 101,
          "fn": 299,
          "accuracy": 0.2525
        }
      },
      "auroc": 0.7108843749999999
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8639895833333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 66,
          "fn": 334,
          "accuracy": 0.165
        },
        "0.01": {
          "tp": 34,
          "fn": 366,
          "accuracy": 0.085
        }
      },
      "auroc": 0.6392697916666668
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 468,
          "accuracy": 0.415
        },
        "0.01": {
          "tp": 242,
          "fn": 558,
          "accuracy": 0.3025
        }
      },
      "auroc": 0.7516296875
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8427760416666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8427760416666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        }
      },
      "auroc": 0.8011760416666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        }
      },
      "auroc": 0.8011760416666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        }
      },
      "auroc": 0.8219760416666666
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        }
      },
      "auroc": 0.8219760416666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.884646875
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.884646875
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.8625739583333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.8625739583333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        }
      },
      "auroc": 0.8736104166666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        }
      },
      "auroc": 0.8736104166666666
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.8716624999999999
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.8716624999999999
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.8074979166666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.8074979166666667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        },
        "0.01": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8395802083333335
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        },
        "0.01": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8395802083333335
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9024593750000001
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.9024593750000001
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.8078625
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.8078625
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        }
      },
      "auroc": 0.8551609375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        }
      },
      "auroc": 0.8551609375
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.7966010416666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.7966010416666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.7360875
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.7360875
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 241,
          "accuracy": 0.3975
        },
        "0.01": {
          "tp": 107,
          "fn": 293,
          "accuracy": 0.2675
        }
      },
      "auroc": 0.7663442708333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 241,
          "accuracy": 0.3975
        },
        "0.01": {
          "tp": 107,
          "fn": 293,
          "accuracy": 0.2675
        }
      },
      "auroc": 0.7663442708333333
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1505,
          "fn": 695,
          "accuracy": 0.6840909090909091
        },
        "0.01": {
          "tp": 1278,
          "fn": 922,
          "accuracy": 0.5809090909090909
        }
      },
      "auroc": 0.8654971590909091
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1400,
          "fn": 800,
          "accuracy": 0.6363636363636364
        },
        "0.01": {
          "tp": 1177,
          "fn": 1023,
          "accuracy": 0.535
        }
      },
      "auroc": 0.8438867424242424
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2905,
          "fn": 1495,
          "accuracy": 0.6602272727272728
        },
        "0.01": {
          "tp": 2455,
          "fn": 1945,
          "accuracy": 0.5579545454545455
        }
      },
      "auroc": 0.8546919507575756
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9368270833333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.8638177083333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        }
      },
      "auroc": 0.9003223958333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9317708333333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.7228833333333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        },
        "0.01": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        }
      },
      "auroc": 0.8273270833333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        }
      },
      "auroc": 0.9342989583333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        }
      },
      "auroc": 0.7933505208333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 574,
          "fn": 226,
          "accuracy": 0.7175
        },
        "0.01": {
          "tp": 507,
          "fn": 293,
          "accuracy": 0.63375
        }
      },
      "auroc": 0.8638247395833334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9109666666666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.22537083333333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        },
        "0.01": {
          "tp": 163,
          "fn": 237,
          "accuracy": 0.4075
        }
      },
      "auroc": 0.56816875
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.852978125
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.06081666666666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        },
        "0.01": {
          "tp": 103,
          "fn": 297,
          "accuracy": 0.2575
        }
      },
      "auroc": 0.45689739583333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        }
      },
      "auroc": 0.8819723958333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.14309375
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 496,
          "accuracy": 0.38
        },
        "0.01": {
          "tp": 266,
          "fn": 534,
          "accuracy": 0.3325
        }
      },
      "auroc": 0.5125330729166667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.921046875
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.8413385416666668
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        }
      },
      "auroc": 0.8811927083333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.8984479166666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9407333333333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9195906250000001
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9063052083333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.320353125
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 229,
          "accuracy": 0.4275
        },
        "0.01": {
          "tp": 149,
          "fn": 251,
          "accuracy": 0.3725
        }
      },
      "auroc": 0.6133291666666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        }
      },
      "auroc": 0.9023765625
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        }
      },
      "auroc": 0.6305432291666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 523,
          "fn": 277,
          "accuracy": 0.65375
        },
        "0.01": {
          "tp": 477,
          "fn": 323,
          "accuracy": 0.59625
        }
      },
      "auroc": 0.7664598958333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.897940625
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.7542708333333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        },
        "0.01": {
          "tp": 228,
          "fn": 172,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8261057291666665
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        }
      },
      "auroc": 0.8219531250000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.06706875
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 294,
          "accuracy": 0.265
        },
        "0.01": {
          "tp": 74,
          "fn": 326,
          "accuracy": 0.185
        }
      },
      "auroc": 0.4445109375000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 267,
          "fn": 133,
          "accuracy": 0.6675
        },
        "0.01": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        }
      },
      "auroc": 0.8599468749999999
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 86,
          "fn": 314,
          "accuracy": 0.215
        }
      },
      "auroc": 0.41066979166666673
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 425,
          "accuracy": 0.46875
        },
        "0.01": {
          "tp": 302,
          "fn": 498,
          "accuracy": 0.3775
        }
      },
      "auroc": 0.6353083333333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9312791666666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        }
      },
      "auroc": 0.8313802083333335
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        },
        "0.01": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        }
      },
      "auroc": 0.8813296875000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.8793
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.48645312500000004
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 248,
          "accuracy": 0.38
        },
        "0.01": {
          "tp": 126,
          "fn": 274,
          "accuracy": 0.315
        }
      },
      "auroc": 0.6828765625
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9052895833333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 69,
          "fn": 331,
          "accuracy": 0.1725
        }
      },
      "auroc": 0.6589166666666668
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 439,
          "fn": 361,
          "accuracy": 0.54875
        },
        "0.01": {
          "tp": 369,
          "fn": 431,
          "accuracy": 0.46125
        }
      },
      "auroc": 0.782103125
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.8952770833333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.8952770833333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8531489583333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8531489583333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        },
        "0.01": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        }
      },
      "auroc": 0.8742130208333334
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        },
        "0.01": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        }
      },
      "auroc": 0.8742130208333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.8786750000000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.8786750000000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.8039843750000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.8039843750000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8413296875
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8413296875
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9207906250000001
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9207906250000001
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.833496875
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.833496875
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        }
      },
      "auroc": 0.8771437500000001
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 292,
          "fn": 108,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        }
      },
      "auroc": 0.8771437500000001
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.922596875
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.922596875
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.6464635416666666
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.6464635416666666
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        },
        "0.01": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        }
      },
      "auroc": 0.7845302083333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        },
        "0.01": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        }
      },
      "auroc": 0.7845302083333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8590093750000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8590093750000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.7564791666666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.7564791666666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 232,
          "fn": 168,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        }
      },
      "auroc": 0.8077442708333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 232,
          "fn": 168,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        }
      },
      "auroc": 0.8077442708333333
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1864,
          "fn": 336,
          "accuracy": 0.8472727272727273
        },
        "0.01": {
          "tp": 1708,
          "fn": 492,
          "accuracy": 0.7763636363636364
        }
      },
      "auroc": 0.906623390151515
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1319,
          "fn": 881,
          "accuracy": 0.5995454545454545
        },
        "0.01": {
          "tp": 1104,
          "fn": 1096,
          "accuracy": 0.5018181818181818
        }
      },
      "auroc": 0.8297471590909091
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3183,
          "fn": 1217,
          "accuracy": 0.7234090909090909
        },
        "0.01": {
          "tp": 2812,
          "fn": 1588,
          "accuracy": 0.639090909090909
        }
      },
      "auroc": 0.8681852746212122
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9441062499999999
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.8865729166666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        }
      },
      "auroc": 0.9153395833333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9394895833333334
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.75053125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        }
      },
      "auroc": 0.8450104166666668
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9417979166666667
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        },
        "0.01": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        }
      },
      "auroc": 0.8185520833333333
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 620,
          "fn": 180,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 576,
          "fn": 224,
          "accuracy": 0.72
        }
      },
      "auroc": 0.8801749999999999
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9273156250000001
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.20576770833333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 214,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        }
      },
      "auroc": 0.5665416666666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9036239583333334
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.059998958333333324
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        },
        "0.01": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        }
      },
      "auroc": 0.4818114583333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9154697916666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.13288333333333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 444,
          "accuracy": 0.445
        },
        "0.01": {
          "tp": 340,
          "fn": 460,
          "accuracy": 0.425
        }
      },
      "auroc": 0.5241765625
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9340979166666666
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.8794885416666667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9067932291666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.9088895833333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9414885416666666
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        }
      },
      "auroc": 0.9251890625000001
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9257500000000001
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.3775604166666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        }
      },
      "auroc": 0.6516552083333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9173197916666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        }
      },
      "auroc": 0.6595244791666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 545,
          "fn": 255,
          "accuracy": 0.68125
        },
        "0.01": {
          "tp": 518,
          "fn": 282,
          "accuracy": 0.6475
        }
      },
      "auroc": 0.7884221354166666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.917865625
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        }
      },
      "auroc": 0.7657552083333333
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        },
        "0.01": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        }
      },
      "auroc": 0.8418104166666668
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.8782479166666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07455833333333332
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 253,
          "accuracy": 0.3675
        },
        "0.01": {
          "tp": 123,
          "fn": 277,
          "accuracy": 0.3075
        }
      },
      "auroc": 0.476403125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        }
      },
      "auroc": 0.8980567708333334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 298,
          "accuracy": 0.255
        },
        "0.01": {
          "tp": 88,
          "fn": 312,
          "accuracy": 0.22
        }
      },
      "auroc": 0.42015677083333336
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 424,
          "fn": 376,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 373,
          "fn": 427,
          "accuracy": 0.46625
        }
      },
      "auroc": 0.6591067708333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.940453125
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        }
      },
      "auroc": 0.8599302083333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9001916666666666
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9097385416666667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.5138583333333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        },
        "0.01": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        }
      },
      "auroc": 0.7117984374999999
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9250958333333333
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 103,
          "fn": 297,
          "accuracy": 0.2575
        }
      },
      "auroc": 0.6868942708333334
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 504,
          "fn": 296,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 451,
          "fn": 349,
          "accuracy": 0.56375
        }
      },
      "auroc": 0.8059950520833332
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9116322916666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9116322916666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.8850197916666668
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.8850197916666668
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        }
      },
      "auroc": 0.8983260416666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        }
      },
      "auroc": 0.8983260416666667
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9071822916666668
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9071822916666668
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.8597625000000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.8597625000000001
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        }
      },
      "auroc": 0.8834723958333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        }
      },
      "auroc": 0.8834723958333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.934684375
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.934684375
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.869584375
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        }
      },
      "auroc": 0.869584375
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 291,
          "fn": 109,
          "accuracy": 0.7275
        }
      },
      "auroc": 0.902134375
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 291,
          "fn": 109,
          "accuracy": 0.7275
        }
      },
      "auroc": 0.902134375
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9398770833333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9398770833333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.6895645833333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.6895645833333334
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8147208333333333
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8147208333333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.8915114583333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.8915114583333333
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        }
      },
      "auroc": 0.8174218750000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        }
      },
      "auroc": 0.8174218750000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        },
        "0.01": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        }
      },
      "auroc": 0.8544666666666667
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        },
        "0.01": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        }
      },
      "auroc": 0.8544666666666667
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1979,
          "fn": 221,
          "accuracy": 0.8995454545454545
        },
        "0.01": {
          "tp": 1912,
          "fn": 288,
          "accuracy": 0.8690909090909091
        }
      },
      "auroc": 0.9234196022727273
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1577,
          "fn": 623,
          "accuracy": 0.7168181818181818
        },
        "0.01": {
          "tp": 1416,
          "fn": 784,
          "accuracy": 0.6436363636363637
        }
      },
      "auroc": 0.8688810606060606
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3556,
          "fn": 844,
          "accuracy": 0.8081818181818182
        },
        "0.01": {
          "tp": 3328,
          "fn": 1072,
          "accuracy": 0.7563636363636363
        }
      },
      "auroc": 0.8961503314393939
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.946875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.946875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.946875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.946875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.946875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.946875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.946875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.946875
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.946875
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9262416666666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9208114583333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9235265625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9455114583333333
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.917290625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9314010416666667
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9358765625
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9190510416666666
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 726,
          "fn": 74,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 703,
          "fn": 97,
          "accuracy": 0.87875
        }
      },
      "auroc": 0.9274638020833333
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9459124999999999
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9446197916666668
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9452661458333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.900653125
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9393645833333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9200088541666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9435552083333333
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9421635416666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.942859375
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        }
      },
      "auroc": 0.9221041666666667
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9407640625
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 738,
          "fn": 62,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 703,
          "fn": 97,
          "accuracy": 0.87875
        }
      },
      "auroc": 0.9314341145833334
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9227791666666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9257114583333335
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9242453125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9464947916666666
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.940553125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9435239583333332
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9346369791666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9331322916666667
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 758,
          "fn": 42,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 718,
          "fn": 82,
          "accuracy": 0.8975
        }
      },
      "auroc": 0.9338846354166667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.946340625
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.94474375
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9455421875000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.946875
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9401875000000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.94353125
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9466078125
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9424656250000001
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 792,
          "fn": 8,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 789,
          "fn": 11,
          "accuracy": 0.98625
        }
      },
      "auroc": 0.94453671875
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9257041666666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9257041666666667
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9278927083333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9278927083333333
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        }
      },
      "auroc": 0.9267984375
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        }
      },
      "auroc": 0.9267984375
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.94495
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.94495
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9460260416666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9460260416666666
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9454880208333334
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9454880208333334
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.946875
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.946875
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.946875
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.946875
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.946875
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.946875
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.946875
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.946875
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.946875
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.946875
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.946875
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.946875
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9430375
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9430375
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.942228125
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.942228125
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9426328125000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9426328125000001
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2081,
          "fn": 119,
          "accuracy": 0.9459090909090909
        },
        "0.01": {
          "tp": 2004,
          "fn": 196,
          "accuracy": 0.9109090909090909
        }
      },
      "auroc": 0.936022159090909
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2173,
          "fn": 27,
          "accuracy": 0.9877272727272727
        },
        "0.01": {
          "tp": 2159,
          "fn": 41,
          "accuracy": 0.9813636363636363
        }
      },
      "auroc": 0.943984375
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4254,
          "fn": 146,
          "accuracy": 0.9668181818181818
        },
        "0.01": {
          "tp": 4163,
          "fn": 237,
          "accuracy": 0.9461363636363637
        }
      },
      "auroc": 0.9400032670454546
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2221,
          "fn": 179,
          "accuracy": 0.9254166666666667
        },
        "0.01": {
          "tp": 2147,
          "fn": 253,
          "accuracy": 0.8945833333333333
        }
      },
      "auroc": 0.9316149305555557
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1553,
          "fn": 847,
          "accuracy": 0.6470833333333333
        },
        "0.01": {
          "tp": 1280,
          "fn": 1120,
          "accuracy": 0.5333333333333333
        }
      },
      "auroc": 0.8544657118055555
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3774,
          "fn": 1026,
          "accuracy": 0.78625
        },
        "0.01": {
          "tp": 3427,
          "fn": 1373,
          "accuracy": 0.7139583333333334
        }
      },
      "auroc": 0.8930403211805557
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2200,
          "fn": 200,
          "accuracy": 0.9166666666666666
        },
        "0.01": {
          "tp": 2109,
          "fn": 291,
          "accuracy": 0.87875
        }
      },
      "auroc": 0.9267902777777777
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 845,
          "fn": 1555,
          "accuracy": 0.35208333333333336
        },
        "0.01": {
          "tp": 635,
          "fn": 1765,
          "accuracy": 0.26458333333333334
        }
      },
      "auroc": 0.73003828125
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3045,
          "fn": 1755,
          "accuracy": 0.634375
        },
        "0.01": {
          "tp": 2744,
          "fn": 2056,
          "accuracy": 0.5716666666666667
        }
      },
      "auroc": 0.8284142795138889
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4421,
          "fn": 379,
          "accuracy": 0.9210416666666666
        },
        "0.01": {
          "tp": 4256,
          "fn": 544,
          "accuracy": 0.8866666666666667
        }
      },
      "auroc": 0.9292026041666668
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2398,
          "fn": 2402,
          "accuracy": 0.4995833333333333
        },
        "0.01": {
          "tp": 1915,
          "fn": 2885,
          "accuracy": 0.39895833333333336
        }
      },
      "auroc": 0.7922519965277777
    },
    {
      "domain": "poetry",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 6819,
          "fn": 2781,
          "accuracy": 0.7103125
        },
        "0.01": {
          "tp": 6171,
          "fn": 3429,
          "accuracy": 0.6428125
        }
      },
      "auroc": 0.8607273003472222
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2009,
          "fn": 391,
          "accuracy": 0.8370833333333333
        },
        "0.01": {
          "tp": 1839,
          "fn": 561,
          "accuracy": 0.76625
        }
      },
      "auroc": 0.9066609375
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 235,
          "fn": 2165,
          "accuracy": 0.09791666666666667
        },
        "0.01": {
          "tp": 199,
          "fn": 2201,
          "accuracy": 0.08291666666666667
        }
      },
      "auroc": 0.30055894097222224
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2244,
          "fn": 2556,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 2038,
          "fn": 2762,
          "accuracy": 0.4245833333333333
        }
      },
      "auroc": 0.6036099392361111
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1843,
          "fn": 557,
          "accuracy": 0.7679166666666667
        },
        "0.01": {
          "tp": 1698,
          "fn": 702,
          "accuracy": 0.7075
        }
      },
      "auroc": 0.8819318576388889
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 2193,
          "accuracy": 0.08625
        },
        "0.01": {
          "tp": 193,
          "fn": 2207,
          "accuracy": 0.08041666666666666
        }
      },
      "auroc": 0.18726961805555556
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2050,
          "fn": 2750,
          "accuracy": 0.4270833333333333
        },
        "0.01": {
          "tp": 1891,
          "fn": 2909,
          "accuracy": 0.39395833333333335
        }
      },
      "auroc": 0.5346007378472222
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3852,
          "fn": 948,
          "accuracy": 0.8025
        },
        "0.01": {
          "tp": 3537,
          "fn": 1263,
          "accuracy": 0.736875
        }
      },
      "auroc": 0.8942963975694445
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 442,
          "fn": 4358,
          "accuracy": 0.09208333333333334
        },
        "0.01": {
          "tp": 392,
          "fn": 4408,
          "accuracy": 0.08166666666666667
        }
      },
      "auroc": 0.2439142795138889
    },
    {
      "domain": "poetry",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4294,
          "fn": 5306,
          "accuracy": 0.44729166666666664
        },
        "0.01": {
          "tp": 3929,
          "fn": 5671,
          "accuracy": 0.4092708333333333
        }
      },
      "auroc": 0.5691053385416667
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2042,
          "fn": 358,
          "accuracy": 0.8508333333333333
        },
        "0.01": {
          "tp": 1918,
          "fn": 482,
          "accuracy": 0.7991666666666667
        }
      },
      "auroc": 0.9109181423611111
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1443,
          "fn": 957,
          "accuracy": 0.60125
        },
        "0.01": {
          "tp": 1224,
          "fn": 1176,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8438234375
    },
    {
      "domain": "poetry",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3485,
          "fn": 1315,
          "accuracy": 0.7260416666666667
        },
        "0.01": {
          "tp": 3142,
          "fn": 1658,
          "accuracy": 0.6545833333333333
        }
      },
      "auroc": 0.8773707899305555
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2281,
          "fn": 119,
          "accuracy": 0.9504166666666667
        },
        "0.01": {
          "tp": 2218,
          "fn": 182,
          "accuracy": 0.9241666666666667
        }
      },
      "auroc": 0.9349639756944444
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2066,
          "fn": 334,
          "accuracy": 0.8608333333333333
        },
        "0.01": {
          "tp": 1926,
          "fn": 474,
          "accuracy": 0.8025
        }
      },
      "auroc": 0.9124230902777779
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 344,
          "fn": 2056,
          "accuracy": 0.14333333333333334
        },
        "0.01": {
          "tp": 265,
          "fn": 2135,
          "accuracy": 0.11041666666666666
        }
      },
      "auroc": 0.46476692708333334
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2410,
          "fn": 2390,
          "accuracy": 0.5020833333333333
        },
        "0.01": {
          "tp": 2191,
          "fn": 2609,
          "accuracy": 0.45645833333333335
        }
      },
      "auroc": 0.6885950086805557
    },
    {
      "domain": "poetry",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2625,
          "fn": 2175,
          "accuracy": 0.546875
        },
        "0.01": {
          "tp": 2483,
          "fn": 2317,
          "accuracy": 0.5172916666666667
        }
      },
      "auroc": 0.6998654513888889
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1910,
          "fn": 490,
          "accuracy": 0.7958333333333333
        },
        "0.01": {
          "tp": 1694,
          "fn": 706,
          "accuracy": 0.7058333333333333
        }
      },
      "auroc": 0.8965765625000001
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1107,
          "fn": 1293,
          "accuracy": 0.46125
        },
        "0.01": {
          "tp": 901,
          "fn": 1499,
          "accuracy": 0.3754166666666667
        }
      },
      "auroc": 0.7388094618055556
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3017,
          "fn": 1783,
          "accuracy": 0.6285416666666667
        },
        "0.01": {
          "tp": 2595,
          "fn": 2205,
          "accuracy": 0.540625
        }
      },
      "auroc": 0.8176930121527777
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1610,
          "fn": 790,
          "accuracy": 0.6708333333333333
        },
        "0.01": {
          "tp": 1356,
          "fn": 1044,
          "accuracy": 0.565
        }
      },
      "auroc": 0.8572720486111111
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 213,
          "fn": 2187,
          "accuracy": 0.08875
        },
        "0.01": {
          "tp": 196,
          "fn": 2204,
          "accuracy": 0.08166666666666667
        }
      },
      "auroc": 0.20313116319444446
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1823,
          "fn": 2977,
          "accuracy": 0.3797916666666667
        },
        "0.01": {
          "tp": 1552,
          "fn": 3248,
          "accuracy": 0.3233333333333333
        }
      },
      "auroc": 0.5302016059027778
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3520,
          "fn": 1280,
          "accuracy": 0.7333333333333333
        },
        "0.01": {
          "tp": 3050,
          "fn": 1750,
          "accuracy": 0.6354166666666666
        }
      },
      "auroc": 0.8769243055555556
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1320,
          "fn": 3480,
          "accuracy": 0.275
        },
        "0.01": {
          "tp": 1097,
          "fn": 3703,
          "accuracy": 0.22854166666666667
        }
      },
      "auroc": 0.4709703125
    },
    {
      "domain": "poetry",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4840,
          "fn": 4760,
          "accuracy": 0.5041666666666667
        },
        "0.01": {
          "tp": 4147,
          "fn": 5453,
          "accuracy": 0.4319791666666667
        }
      },
      "auroc": 0.6739473090277778
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2147,
          "fn": 253,
          "accuracy": 0.8945833333333333
        },
        "0.01": {
          "tp": 2054,
          "fn": 346,
          "accuracy": 0.8558333333333333
        }
      },
      "auroc": 0.9234821180555556
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1187,
          "fn": 1213,
          "accuracy": 0.4945833333333333
        },
        "0.01": {
          "tp": 896,
          "fn": 1504,
          "accuracy": 0.37333333333333335
        }
      },
      "auroc": 0.808653732638889
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3334,
          "fn": 1466,
          "accuracy": 0.6945833333333333
        },
        "0.01": {
          "tp": 2950,
          "fn": 1850,
          "accuracy": 0.6145833333333334
        }
      },
      "auroc": 0.8660679253472223
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1782,
          "fn": 618,
          "accuracy": 0.7425
        },
        "0.01": {
          "tp": 1569,
          "fn": 831,
          "accuracy": 0.65375
        }
      },
      "auroc": 0.8831965277777779
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 2037,
          "accuracy": 0.15125
        },
        "0.01": {
          "tp": 258,
          "fn": 2142,
          "accuracy": 0.1075
        }
      },
      "auroc": 0.5295752604166667
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2145,
          "fn": 2655,
          "accuracy": 0.446875
        },
        "0.01": {
          "tp": 1827,
          "fn": 2973,
          "accuracy": 0.380625
        }
      },
      "auroc": 0.7063858940972222
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3929,
          "fn": 871,
          "accuracy": 0.8185416666666666
        },
        "0.01": {
          "tp": 3623,
          "fn": 1177,
          "accuracy": 0.7547916666666666
        }
      },
      "auroc": 0.9033393229166665
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1550,
          "fn": 3250,
          "accuracy": 0.3229166666666667
        },
        "0.01": {
          "tp": 1154,
          "fn": 3646,
          "accuracy": 0.24041666666666667
        }
      },
      "auroc": 0.6691144965277778
    },
    {
      "domain": "poetry",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5479,
          "fn": 4121,
          "accuracy": 0.5707291666666666
        },
        "0.01": {
          "tp": 4777,
          "fn": 4823,
          "accuracy": 0.4976041666666667
        }
      },
      "auroc": 0.7862269097222222
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1838,
          "fn": 562,
          "accuracy": 0.7658333333333334
        },
        "0.01": {
          "tp": 1676,
          "fn": 724,
          "accuracy": 0.6983333333333334
        }
      },
      "auroc": 0.885150607638889
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1838,
          "fn": 562,
          "accuracy": 0.7658333333333334
        },
        "0.01": {
          "tp": 1676,
          "fn": 724,
          "accuracy": 0.6983333333333334
        }
      },
      "auroc": 0.885150607638889
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1657,
          "fn": 743,
          "accuracy": 0.6904166666666667
        },
        "0.01": {
          "tp": 1472,
          "fn": 928,
          "accuracy": 0.6133333333333333
        }
      },
      "auroc": 0.8525796875000001
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1657,
          "fn": 743,
          "accuracy": 0.6904166666666667
        },
        "0.01": {
          "tp": 1472,
          "fn": 928,
          "accuracy": 0.6133333333333333
        }
      },
      "auroc": 0.8525796875000001
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3495,
          "fn": 1305,
          "accuracy": 0.728125
        },
        "0.01": {
          "tp": 3148,
          "fn": 1652,
          "accuracy": 0.6558333333333334
        }
      },
      "auroc": 0.8688651475694444
    },
    {
      "domain": "poetry",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3495,
          "fn": 1305,
          "accuracy": 0.728125
        },
        "0.01": {
          "tp": 3148,
          "fn": 1652,
          "accuracy": 0.6558333333333334
        }
      },
      "auroc": 0.8688651475694444
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1839,
          "fn": 561,
          "accuracy": 0.76625
        },
        "0.01": {
          "tp": 1664,
          "fn": 736,
          "accuracy": 0.6933333333333334
        }
      },
      "auroc": 0.8853918402777776
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1839,
          "fn": 561,
          "accuracy": 0.76625
        },
        "0.01": {
          "tp": 1664,
          "fn": 736,
          "accuracy": 0.6933333333333334
        }
      },
      "auroc": 0.8853918402777776
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1389,
          "fn": 1011,
          "accuracy": 0.57875
        },
        "0.01": {
          "tp": 1192,
          "fn": 1208,
          "accuracy": 0.49666666666666665
        }
      },
      "auroc": 0.8331319444444445
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1389,
          "fn": 1011,
          "accuracy": 0.57875
        },
        "0.01": {
          "tp": 1192,
          "fn": 1208,
          "accuracy": 0.49666666666666665
        }
      },
      "auroc": 0.8331319444444445
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3228,
          "fn": 1572,
          "accuracy": 0.6725
        },
        "0.01": {
          "tp": 2856,
          "fn": 1944,
          "accuracy": 0.595
        }
      },
      "auroc": 0.8592618923611112
    },
    {
      "domain": "poetry",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3228,
          "fn": 1572,
          "accuracy": 0.6725
        },
        "0.01": {
          "tp": 2856,
          "fn": 1944,
          "accuracy": 0.595
        }
      },
      "auroc": 0.8592618923611112
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2077,
          "fn": 323,
          "accuracy": 0.8654166666666666
        },
        "0.01": {
          "tp": 1958,
          "fn": 442,
          "accuracy": 0.8158333333333333
        }
      },
      "auroc": 0.9144721354166667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2077,
          "fn": 323,
          "accuracy": 0.8654166666666666
        },
        "0.01": {
          "tp": 1958,
          "fn": 442,
          "accuracy": 0.8158333333333333
        }
      },
      "auroc": 0.9144721354166667
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1375,
          "fn": 1025,
          "accuracy": 0.5729166666666666
        },
        "0.01": {
          "tp": 1064,
          "fn": 1336,
          "accuracy": 0.44333333333333336
        }
      },
      "auroc": 0.8430013888888889
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1375,
          "fn": 1025,
          "accuracy": 0.5729166666666666
        },
        "0.01": {
          "tp": 1064,
          "fn": 1336,
          "accuracy": 0.44333333333333336
        }
      },
      "auroc": 0.8430013888888889
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3452,
          "fn": 1348,
          "accuracy": 0.7191666666666666
        },
        "0.01": {
          "tp": 3022,
          "fn": 1778,
          "accuracy": 0.6295833333333334
        }
      },
      "auroc": 0.8787367621527777
    },
    {
      "domain": "poetry",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3452,
          "fn": 1348,
          "accuracy": 0.7191666666666666
        },
        "0.01": {
          "tp": 3022,
          "fn": 1778,
          "accuracy": 0.6295833333333334
        }
      },
      "auroc": 0.8787367621527777
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2193,
          "fn": 207,
          "accuracy": 0.91375
        },
        "0.01": {
          "tp": 2097,
          "fn": 303,
          "accuracy": 0.87375
        }
      },
      "auroc": 0.927540017361111
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2193,
          "fn": 207,
          "accuracy": 0.91375
        },
        "0.01": {
          "tp": 2097,
          "fn": 303,
          "accuracy": 0.87375
        }
      },
      "auroc": 0.927540017361111
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 641,
          "fn": 1759,
          "accuracy": 0.26708333333333334
        },
        "0.01": {
          "tp": 454,
          "fn": 1946,
          "accuracy": 0.18916666666666668
        }
      },
      "auroc": 0.7129361979166666
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 641,
          "fn": 1759,
          "accuracy": 0.26708333333333334
        },
        "0.01": {
          "tp": 454,
          "fn": 1946,
          "accuracy": 0.18916666666666668
        }
      },
      "auroc": 0.7129361979166666
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2834,
          "fn": 1966,
          "accuracy": 0.5904166666666667
        },
        "0.01": {
          "tp": 2551,
          "fn": 2249,
          "accuracy": 0.5314583333333334
        }
      },
      "auroc": 0.8202381076388889
    },
    {
      "domain": "poetry",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2834,
          "fn": 1966,
          "accuracy": 0.5904166666666667
        },
        "0.01": {
          "tp": 2551,
          "fn": 2249,
          "accuracy": 0.5314583333333334
        }
      },
      "auroc": 0.8202381076388889
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1652,
          "fn": 748,
          "accuracy": 0.6883333333333334
        },
        "0.01": {
          "tp": 1451,
          "fn": 949,
          "accuracy": 0.6045833333333334
        }
      },
      "auroc": 0.8653773437500001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1652,
          "fn": 748,
          "accuracy": 0.6883333333333334
        },
        "0.01": {
          "tp": 1451,
          "fn": 949,
          "accuracy": 0.6045833333333334
        }
      },
      "auroc": 0.8653773437500001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1292,
          "fn": 1108,
          "accuracy": 0.5383333333333333
        },
        "0.01": {
          "tp": 1115,
          "fn": 1285,
          "accuracy": 0.46458333333333335
        }
      },
      "auroc": 0.79048515625
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1292,
          "fn": 1108,
          "accuracy": 0.5383333333333333
        },
        "0.01": {
          "tp": 1115,
          "fn": 1285,
          "accuracy": 0.46458333333333335
        }
      },
      "auroc": 0.79048515625
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2944,
          "fn": 1856,
          "accuracy": 0.6133333333333333
        },
        "0.01": {
          "tp": 2566,
          "fn": 2234,
          "accuracy": 0.5345833333333333
        }
      },
      "auroc": 0.8279312500000001
    },
    {
      "domain": "poetry",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2944,
          "fn": 1856,
          "accuracy": 0.6133333333333333
        },
        "0.01": {
          "tp": 2566,
          "fn": 2234,
          "accuracy": 0.5345833333333333
        }
      },
      "auroc": 0.8279312500000001
    },
    {
      "domain": "poetry",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 17298,
          "fn": 9102,
          "accuracy": 0.6552272727272728
        },
        "0.01": {
          "tp": 15179,
          "fn": 11221,
          "accuracy": 0.5749621212121212
        }
      },
      "auroc": 0.848870146780303
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9455791666666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.94601875
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9458520833333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.8824510416666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        }
      },
      "auroc": 0.9141515625
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9461552083333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9140151041666665
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 778,
          "fn": 22,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 768,
          "fn": 32,
          "accuracy": 0.96
        }
      },
      "auroc": 0.93008515625
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.1776927083333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.5620755208333335
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        }
      },
      "auroc": 0.8437802083333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08311666666666669
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 255,
          "accuracy": 0.3625
        },
        "0.01": {
          "tp": 96,
          "fn": 304,
          "accuracy": 0.24
        }
      },
      "auroc": 0.4634484375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        }
      },
      "auroc": 0.8951192708333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.13040468750000003
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 449,
          "accuracy": 0.43875
        },
        "0.01": {
          "tp": 297,
          "fn": 503,
          "accuracy": 0.37125
        }
      },
      "auroc": 0.5127619791666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.94533125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.09703125000000001
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        }
      },
      "auroc": 0.5211812499999999
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9412322916666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08980625000000002
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        }
      },
      "auroc": 0.5155192708333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9432817708333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.09341875000000002
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 402,
          "fn": 398,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 394,
          "fn": 406,
          "accuracy": 0.4925
        }
      },
      "auroc": 0.5183502604166668
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9458614583333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9180968749999999
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.9319791666666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        }
      },
      "auroc": 0.8325572916666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.21883958333333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 261,
          "accuracy": 0.3475
        },
        "0.01": {
          "tp": 98,
          "fn": 302,
          "accuracy": 0.245
        }
      },
      "auroc": 0.5256984375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        }
      },
      "auroc": 0.889209375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        },
        "0.01": {
          "tp": 172,
          "fn": 228,
          "accuracy": 0.43
        }
      },
      "auroc": 0.5684682291666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 524,
          "fn": 276,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 469,
          "fn": 331,
          "accuracy": 0.58625
        }
      },
      "auroc": 0.7288388020833332
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.8198916666666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        }
      },
      "auroc": 0.883175
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.7599187500000001
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08753541666666668
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 83,
          "fn": 317,
          "accuracy": 0.2075
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.4237270833333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        },
        "0.01": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        }
      },
      "auroc": 0.8531885416666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 284,
          "accuracy": 0.29
        },
        "0.01": {
          "tp": 66,
          "fn": 334,
          "accuracy": 0.165
        }
      },
      "auroc": 0.45371354166666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 401,
          "accuracy": 0.49875
        },
        "0.01": {
          "tp": 320,
          "fn": 480,
          "accuracy": 0.4
        }
      },
      "auroc": 0.6534510416666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9430083333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.927546875
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9352776041666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9372541666666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.6317312500000001
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        }
      },
      "auroc": 0.7844927083333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.94013125
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 266,
          "fn": 134,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        }
      },
      "auroc": 0.7796390625
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 664,
          "fn": 136,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 607,
          "fn": 193,
          "accuracy": 0.75875
        }
      },
      "auroc": 0.85988515625
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.939346875
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.939346875
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9353458333333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9353458333333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9373463541666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9373463541666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9143427083333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9143427083333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.89619375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.89619375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9052682291666666
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9052682291666666
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9455895833333332
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9455895833333332
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9431114583333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9431114583333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9443505208333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9443505208333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9441395833333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9441395833333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9232958333333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9232958333333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.9337177083333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.9337177083333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9363302083333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9363302083333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9341677083333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9341677083333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.9352489583333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.9352489583333333
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2177,
          "fn": 23,
          "accuracy": 0.9895454545454545
        },
        "0.01": {
          "tp": 2140,
          "fn": 60,
          "accuracy": 0.9727272727272728
        }
      },
      "auroc": 0.9412113636363636
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 707,
          "fn": 493,
          "accuracy": 0.5891666666666666
        },
        "0.01": {
          "tp": 613,
          "fn": 587,
          "accuracy": 0.5108333333333334
        }
      },
      "auroc": 0.6476397569444444
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2884,
          "fn": 516,
          "accuracy": 0.8482352941176471
        },
        "0.01": {
          "tp": 2753,
          "fn": 647,
          "accuracy": 0.8097058823529412
        }
      },
      "auroc": 0.8375978553921569
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1924,
          "fn": 276,
          "accuracy": 0.8745454545454545
        },
        "0.01": {
          "tp": 1723,
          "fn": 477,
          "accuracy": 0.7831818181818182
        }
      },
      "auroc": 0.899337215909091
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 246,
          "fn": 954,
          "accuracy": 0.205
        },
        "0.01": {
          "tp": 211,
          "fn": 989,
          "accuracy": 0.17583333333333334
        }
      },
      "auroc": 0.3322467013888889
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2170,
          "fn": 1230,
          "accuracy": 0.638235294117647
        },
        "0.01": {
          "tp": 1934,
          "fn": 1466,
          "accuracy": 0.5688235294117647
        }
      },
      "auroc": 0.6991876225490196
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 4101,
          "fn": 299,
          "accuracy": 0.9320454545454545
        },
        "0.01": {
          "tp": 3863,
          "fn": 537,
          "accuracy": 0.8779545454545454
        }
      },
      "auroc": 0.9202742897727273
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 953,
          "fn": 1447,
          "accuracy": 0.39708333333333334
        },
        "0.01": {
          "tp": 824,
          "fn": 1576,
          "accuracy": 0.3433333333333333
        }
      },
      "auroc": 0.4899432291666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 5054,
          "fn": 1746,
          "accuracy": 0.7432352941176471
        },
        "0.01": {
          "tp": 4687,
          "fn": 2113,
          "accuracy": 0.6892647058823529
        }
      },
      "auroc": 0.7683927389705882
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9345395833333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9109760416666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9227578125000001
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9290041666666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.8143697916666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        }
      },
      "auroc": 0.8716869791666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        }
      },
      "auroc": 0.931771875
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        },
        "0.01": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        }
      },
      "auroc": 0.8626729166666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 736,
          "fn": 64,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 630,
          "fn": 170,
          "accuracy": 0.7875
        }
      },
      "auroc": 0.8972223958333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9410083333333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.135778125
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        }
      },
      "auroc": 0.5383932291666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        }
      },
      "auroc": 0.7836447916666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08344062500000002
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 298,
          "accuracy": 0.255
        },
        "0.01": {
          "tp": 61,
          "fn": 339,
          "accuracy": 0.1525
        }
      },
      "auroc": 0.43354270833333336
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 301,
          "fn": 99,
          "accuracy": 0.7525
        },
        "0.01": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        }
      },
      "auroc": 0.8623265625000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.10960937500000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 301,
          "fn": 499,
          "accuracy": 0.37625
        },
        "0.01": {
          "tp": 253,
          "fn": 547,
          "accuracy": 0.31625
        }
      },
      "auroc": 0.48596796875000003
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.8795583333333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08804270833333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        },
        "0.01": {
          "tp": 124,
          "fn": 276,
          "accuracy": 0.31
        }
      },
      "auroc": 0.48380052083333336
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        }
      },
      "auroc": 0.8646697916666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08599895833333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        },
        "0.01": {
          "tp": 110,
          "fn": 290,
          "accuracy": 0.275
        }
      },
      "auroc": 0.475334375
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        }
      },
      "auroc": 0.8721140625
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08702083333333335
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 469,
          "accuracy": 0.41375
        },
        "0.01": {
          "tp": 234,
          "fn": 566,
          "accuracy": 0.2925
        }
      },
      "auroc": 0.4795674479166666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9350020833333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.8144864583333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": {
          "tp": 273,
          "fn": 127,
          "accuracy": 0.6825
        }
      },
      "auroc": 0.8747442708333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.7674354166666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.22634895833333335
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 69,
          "fn": 331,
          "accuracy": 0.1725
        }
      },
      "auroc": 0.49689218749999997
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        }
      },
      "auroc": 0.85121875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 270,
          "accuracy": 0.325
        },
        "0.01": {
          "tp": 91,
          "fn": 309,
          "accuracy": 0.2275
        }
      },
      "auroc": 0.5204177083333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 435,
          "fn": 365,
          "accuracy": 0.54375
        },
        "0.01": {
          "tp": 342,
          "fn": 458,
          "accuracy": 0.4275
        }
      },
      "auroc": 0.6858182291666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9425218750000001
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.6598541666666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8011880208333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.6600427083333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08484687500000003
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 354,
          "accuracy": 0.115
        },
        "0.01": {
          "tp": 21,
          "fn": 379,
          "accuracy": 0.0525
        }
      },
      "auroc": 0.37244479166666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        }
      },
      "auroc": 0.8012822916666668
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 28,
          "fn": 372,
          "accuracy": 0.07
        },
        "0.01": {
          "tp": 14,
          "fn": 386,
          "accuracy": 0.035
        }
      },
      "auroc": 0.3723505208333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 272,
          "fn": 528,
          "accuracy": 0.34
        },
        "0.01": {
          "tp": 229,
          "fn": 571,
          "accuracy": 0.28625
        }
      },
      "auroc": 0.58681640625
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.871996875
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        }
      },
      "auroc": 0.8202875
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        },
        "0.01": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        }
      },
      "auroc": 0.8461421874999999
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        }
      },
      "auroc": 0.8667291666666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        },
        "0.01": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        }
      },
      "auroc": 0.48286666666666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        },
        "0.01": {
          "tp": 117,
          "fn": 283,
          "accuracy": 0.2925
        }
      },
      "auroc": 0.6747979166666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        }
      },
      "auroc": 0.8693630208333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 256,
          "accuracy": 0.36
        },
        "0.01": {
          "tp": 70,
          "fn": 330,
          "accuracy": 0.175
        }
      },
      "auroc": 0.6515770833333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 460,
          "fn": 340,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 291,
          "fn": 509,
          "accuracy": 0.36375
        }
      },
      "auroc": 0.7604700520833333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.8633395833333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.8633395833333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.8460427083333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.8460427083333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        }
      },
      "auroc": 0.8546911458333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        }
      },
      "auroc": 0.8546911458333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.8216531250000001
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.8216531250000001
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.7901489583333334
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.7901489583333334
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 144,
          "fn": 256,
          "accuracy": 0.36
        }
      },
      "auroc": 0.8059010416666668
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 238,
          "fn": 162,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 144,
          "fn": 256,
          "accuracy": 0.36
        }
      },
      "auroc": 0.8059010416666668
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.903328125
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.903328125
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.8741552083333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.8741552083333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        }
      },
      "auroc": 0.8887416666666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        }
      },
      "auroc": 0.8887416666666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9188708333333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9188708333333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.838634375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.838634375
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        }
      },
      "auroc": 0.8787526041666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        }
      },
      "auroc": 0.8787526041666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        }
      },
      "auroc": 0.87410625
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        }
      },
      "auroc": 0.87410625
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.8611708333333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.8611708333333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        }
      },
      "auroc": 0.8676385416666668
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        }
      },
      "auroc": 0.8676385416666668
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1936,
          "fn": 264,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 1609,
          "fn": 591,
          "accuracy": 0.7313636363636363
        }
      },
      "auroc": 0.8987204545454546
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 472,
          "fn": 728,
          "accuracy": 0.3933333333333333
        },
        "0.01": {
          "tp": 322,
          "fn": 878,
          "accuracy": 0.2683333333333333
        }
      },
      "auroc": 0.5715708333333334
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2408,
          "fn": 992,
          "accuracy": 0.7082352941176471
        },
        "0.01": {
          "tp": 1931,
          "fn": 1469,
          "accuracy": 0.5679411764705883
        }
      },
      "auroc": 0.7832558823529412
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1480,
          "fn": 720,
          "accuracy": 0.6727272727272727
        },
        "0.01": {
          "tp": 1011,
          "fn": 1189,
          "accuracy": 0.45954545454545453
        }
      },
      "auroc": 0.8256071022727273
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 1027,
          "accuracy": 0.14416666666666667
        },
        "0.01": {
          "tp": 110,
          "fn": 1090,
          "accuracy": 0.09166666666666666
        }
      },
      "auroc": 0.29631197916666674
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1653,
          "fn": 1747,
          "accuracy": 0.4861764705882353
        },
        "0.01": {
          "tp": 1121,
          "fn": 2279,
          "accuracy": 0.3297058823529412
        }
      },
      "auroc": 0.6387970588235294
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3416,
          "fn": 984,
          "accuracy": 0.7763636363636364
        },
        "0.01": {
          "tp": 2620,
          "fn": 1780,
          "accuracy": 0.5954545454545455
        }
      },
      "auroc": 0.8621637784090909
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 645,
          "fn": 1755,
          "accuracy": 0.26875
        },
        "0.01": {
          "tp": 432,
          "fn": 1968,
          "accuracy": 0.18
        }
      },
      "auroc": 0.43394140625000005
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 4061,
          "fn": 2739,
          "accuracy": 0.5972058823529411
        },
        "0.01": {
          "tp": 3052,
          "fn": 3748,
          "accuracy": 0.44882352941176473
        }
      },
      "auroc": 0.7110264705882353
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.924625
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.8941312499999999
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9093781250000001
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.919946875
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        }
      },
      "auroc": 0.7881541666666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 271,
          "fn": 129,
          "accuracy": 0.6775
        }
      },
      "auroc": 0.8540505208333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9222859375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        }
      },
      "auroc": 0.8411427083333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 722,
          "fn": 78,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 597,
          "fn": 203,
          "accuracy": 0.74625
        }
      },
      "auroc": 0.8817143229166666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.93935
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.17974062500000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        }
      },
      "auroc": 0.5595453125
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        },
        "0.01": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        }
      },
      "auroc": 0.7119520833333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08421041666666668
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        },
        "0.01": {
          "tp": 34,
          "fn": 366,
          "accuracy": 0.085
        }
      },
      "auroc": 0.39808125
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        }
      },
      "auroc": 0.8256510416666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.13197552083333336
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 266,
          "fn": 534,
          "accuracy": 0.3325
        },
        "0.01": {
          "tp": 228,
          "fn": 572,
          "accuracy": 0.285
        }
      },
      "auroc": 0.47881328125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.8898666666666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.09188541666666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 123,
          "fn": 277,
          "accuracy": 0.3075
        }
      },
      "auroc": 0.49087604166666665
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        }
      },
      "auroc": 0.8642583333333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08339375000000002
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 235,
          "accuracy": 0.4125
        },
        "0.01": {
          "tp": 98,
          "fn": 302,
          "accuracy": 0.245
        }
      },
      "auroc": 0.47382604166666664
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        }
      },
      "auroc": 0.8770625000000001
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08763958333333335
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 451,
          "accuracy": 0.43625
        },
        "0.01": {
          "tp": 221,
          "fn": 579,
          "accuracy": 0.27625
        }
      },
      "auroc": 0.48235104166666665
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9408135416666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.848115625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        }
      },
      "auroc": 0.8944645833333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.7391354166666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.173134375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 86,
          "fn": 314,
          "accuracy": 0.215
        },
        "0.01": {
          "tp": 49,
          "fn": 351,
          "accuracy": 0.1225
        }
      },
      "auroc": 0.45613489583333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        }
      },
      "auroc": 0.8399744791666668
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 255,
          "accuracy": 0.3625
        },
        "0.01": {
          "tp": 105,
          "fn": 295,
          "accuracy": 0.2625
        }
      },
      "auroc": 0.510625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 427,
          "fn": 373,
          "accuracy": 0.53375
        },
        "0.01": {
          "tp": 348,
          "fn": 452,
          "accuracy": 0.435
        }
      },
      "auroc": 0.6752997395833334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9434625
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 24,
          "fn": 176,
          "accuracy": 0.12
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.6252437500000001
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        },
        "0.01": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        }
      },
      "auroc": 0.784353125
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.6158916666666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08122083333333335
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 368,
          "accuracy": 0.08
        },
        "0.01": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        }
      },
      "auroc": 0.34855624999999996
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        },
        "0.01": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        }
      },
      "auroc": 0.7796770833333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 24,
          "fn": 376,
          "accuracy": 0.06
        },
        "0.01": {
          "tp": 11,
          "fn": 389,
          "accuracy": 0.0275
        }
      },
      "auroc": 0.35323229166666664
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 255,
          "fn": 545,
          "accuracy": 0.31875
        },
        "0.01": {
          "tp": 225,
          "fn": 575,
          "accuracy": 0.28125
        }
      },
      "auroc": 0.5664546875
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.87973125
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        },
        "0.01": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        }
      },
      "auroc": 0.8039864583333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        },
        "0.01": {
          "tp": 171,
          "fn": 229,
          "accuracy": 0.4275
        }
      },
      "auroc": 0.8418588541666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        }
      },
      "auroc": 0.8597354166666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.4645583333333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        },
        "0.01": {
          "tp": 110,
          "fn": 290,
          "accuracy": 0.275
        }
      },
      "auroc": 0.662146875
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        }
      },
      "auroc": 0.8697333333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 280,
          "accuracy": 0.3
        },
        "0.01": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        }
      },
      "auroc": 0.6342723958333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 436,
          "fn": 364,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 281,
          "fn": 519,
          "accuracy": 0.35125
        }
      },
      "auroc": 0.7520028645833332
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        }
      },
      "auroc": 0.8783052083333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        }
      },
      "auroc": 0.8783052083333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.86151875
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.86151875
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        }
      },
      "auroc": 0.8699119791666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        }
      },
      "auroc": 0.8699119791666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        }
      },
      "auroc": 0.8114187500000001
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        }
      },
      "auroc": 0.8114187500000001
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.78193125
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.78193125
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 127,
          "fn": 273,
          "accuracy": 0.3175
        }
      },
      "auroc": 0.7966749999999999
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 127,
          "fn": 273,
          "accuracy": 0.3175
        }
      },
      "auroc": 0.7966749999999999
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.8913302083333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.8913302083333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.85915
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.85915
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        }
      },
      "auroc": 0.8752401041666666
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        }
      },
      "auroc": 0.8752401041666666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9204416666666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9204416666666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.82145
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.82145
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        },
        "0.01": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        }
      },
      "auroc": 0.8709458333333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        },
        "0.01": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        }
      },
      "auroc": 0.8709458333333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        }
      },
      "auroc": 0.8610520833333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        }
      },
      "auroc": 0.8610520833333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.8509354166666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.8509354166666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        }
      },
      "auroc": 0.85599375
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        }
      },
      "auroc": 0.85599375
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1956,
          "fn": 244,
          "accuracy": 0.889090909090909
        },
        "0.01": {
          "tp": 1608,
          "fn": 592,
          "accuracy": 0.730909090909091
        }
      },
      "auroc": 0.8982178977272727
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 465,
          "fn": 735,
          "accuracy": 0.3875
        },
        "0.01": {
          "tp": 314,
          "fn": 886,
          "accuracy": 0.26166666666666666
        }
      },
      "auroc": 0.5738505208333333
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2421,
          "fn": 979,
          "accuracy": 0.7120588235294117
        },
        "0.01": {
          "tp": 1922,
          "fn": 1478,
          "accuracy": 0.5652941176470588
        }
      },
      "auroc": 0.7837352941176472
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1380,
          "fn": 820,
          "accuracy": 0.6272727272727273
        },
        "0.01": {
          "tp": 934,
          "fn": 1266,
          "accuracy": 0.42454545454545456
        }
      },
      "auroc": 0.8078095643939394
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 1040,
          "accuracy": 0.13333333333333333
        },
        "0.01": {
          "tp": 100,
          "fn": 1100,
          "accuracy": 0.08333333333333333
        }
      },
      "auroc": 0.27911197916666675
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1540,
          "fn": 1860,
          "accuracy": 0.45294117647058824
        },
        "0.01": {
          "tp": 1034,
          "fn": 2366,
          "accuracy": 0.30411764705882355
        }
      },
      "auroc": 0.6212104166666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3336,
          "fn": 1064,
          "accuracy": 0.7581818181818182
        },
        "0.01": {
          "tp": 2542,
          "fn": 1858,
          "accuracy": 0.5777272727272728
        }
      },
      "auroc": 0.8530137310606061
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 625,
          "fn": 1775,
          "accuracy": 0.2604166666666667
        },
        "0.01": {
          "tp": 414,
          "fn": 1986,
          "accuracy": 0.1725
        }
      },
      "auroc": 0.42648125000000003
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3961,
          "fn": 2839,
          "accuracy": 0.5825
        },
        "0.01": {
          "tp": 2956,
          "fn": 3844,
          "accuracy": 0.43470588235294116
        }
      },
      "auroc": 0.7024728553921568
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.5591635416666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.45967708333333335
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 384,
          "accuracy": 0.04
        },
        "0.01": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        }
      },
      "auroc": 0.5094203125000001
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.548278125
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.35361770833333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 387,
          "accuracy": 0.0325
        },
        "0.01": {
          "tp": 7,
          "fn": 393,
          "accuracy": 0.0175
        }
      },
      "auroc": 0.45094791666666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 374,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        }
      },
      "auroc": 0.5537208333333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.4066473958333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 771,
          "accuracy": 0.03625
        },
        "0.01": {
          "tp": 16,
          "fn": 784,
          "accuracy": 0.02
        }
      },
      "auroc": 0.4801841145833333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9064177083333335
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.10329895833333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        },
        "0.01": {
          "tp": 163,
          "fn": 237,
          "accuracy": 0.4075
        }
      },
      "auroc": 0.5048583333333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.39117604166666675
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08135208333333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.23626406249999998
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 165,
          "fn": 235,
          "accuracy": 0.4125
        }
      },
      "auroc": 0.648796875
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.09232552083333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 613,
          "accuracy": 0.23375
        },
        "0.01": {
          "tp": 165,
          "fn": 635,
          "accuracy": 0.20625
        }
      },
      "auroc": 0.3705611979166667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.5753458333333332
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08285416666666669
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.3291
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.503890625
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08230625000000001
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 388,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.29309843750000003
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 20,
          "fn": 380,
          "accuracy": 0.05
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.5396182291666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08258020833333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 20,
          "fn": 780,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 4,
          "fn": 796,
          "accuracy": 0.005
        }
      },
      "auroc": 0.31109921875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9047749999999999
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.49663854166666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        }
      },
      "auroc": 0.7007067708333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.4281697916666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.09128854166666668
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.2597291666666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        },
        "0.01": {
          "tp": 171,
          "fn": 229,
          "accuracy": 0.4275
        }
      },
      "auroc": 0.6664723958333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 374,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 21,
          "fn": 379,
          "accuracy": 0.0525
        }
      },
      "auroc": 0.2939635416666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 215,
          "fn": 585,
          "accuracy": 0.26875
        },
        "0.01": {
          "tp": 192,
          "fn": 608,
          "accuracy": 0.24
        }
      },
      "auroc": 0.48021796875000006
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.899103125
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.18948750000000003
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        },
        "0.01": {
          "tp": 158,
          "fn": 242,
          "accuracy": 0.395
        }
      },
      "auroc": 0.5442953125000001
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.28330937500000003
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07804583333333336
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.18067760416666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        },
        "0.01": {
          "tp": 157,
          "fn": 243,
          "accuracy": 0.3925
        }
      },
      "auroc": 0.59120625
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.1337666666666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 620,
          "accuracy": 0.225
        },
        "0.01": {
          "tp": 158,
          "fn": 642,
          "accuracy": 0.1975
        }
      },
      "auroc": 0.3624864583333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.49126562500000004
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.32300104166666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 383,
          "accuracy": 0.0425
        },
        "0.01": {
          "tp": 13,
          "fn": 387,
          "accuracy": 0.0325
        }
      },
      "auroc": 0.40713333333333335
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.4416489583333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.14933333333333335
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.29549114583333336
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 381,
          "accuracy": 0.0475
        },
        "0.01": {
          "tp": 14,
          "fn": 386,
          "accuracy": 0.035
        }
      },
      "auroc": 0.4664572916666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.23616718750000001
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 20,
          "fn": 780,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 14,
          "fn": 786,
          "accuracy": 0.0175
        }
      },
      "auroc": 0.35131223958333335
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.5005760416666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.5005760416666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.4353541666666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.4353541666666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 388,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.4679651041666666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 388,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.4679651041666666
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.386240625
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.386240625
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.35386354166666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.35386354166666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 393,
          "accuracy": 0.0175
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.3700520833333334
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 393,
          "accuracy": 0.0175
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.3700520833333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.4952239583333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.4952239583333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.4056458333333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.4056458333333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.4504348958333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.4504348958333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.602171875
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.602171875
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.42404791666666664
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.42404791666666664
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        }
      },
      "auroc": 0.5131098958333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        }
      },
      "auroc": 0.5131098958333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.4990447916666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.4990447916666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.4381552083333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.4381552083333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 377,
          "accuracy": 0.0575
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.4686
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 377,
          "accuracy": 0.0575
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.4686
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 623,
          "fn": 1577,
          "accuracy": 0.2831818181818182
        },
        "0.01": {
          "tp": 528,
          "fn": 1672,
          "accuracy": 0.24
        }
      },
      "auroc": 0.6199389204545455
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 1171,
          "accuracy": 0.024166666666666666
        },
        "0.01": {
          "tp": 23,
          "fn": 1177,
          "accuracy": 0.019166666666666665
        }
      },
      "auroc": 0.27582621527777784
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 652,
          "fn": 2748,
          "accuracy": 0.19176470588235295
        },
        "0.01": {
          "tp": 551,
          "fn": 2849,
          "accuracy": 0.16205882352941176
        }
      },
      "auroc": 0.4984873774509804
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 2141,
          "accuracy": 0.026818181818181817
        },
        "0.01": {
          "tp": 19,
          "fn": 2181,
          "accuracy": 0.008636363636363636
        }
      },
      "auroc": 0.423049053030303
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 1198,
          "accuracy": 0.0016666666666666668
        },
        "0.01": {
          "tp": 0,
          "fn": 1200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.13932395833333333
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 3339,
          "accuracy": 0.017941176470588235
        },
        "0.01": {
          "tp": 19,
          "fn": 3381,
          "accuracy": 0.005588235294117647
        }
      },
      "auroc": 0.3229107843137255
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 682,
          "fn": 3718,
          "accuracy": 0.155
        },
        "0.01": {
          "tp": 547,
          "fn": 3853,
          "accuracy": 0.12431818181818181
        }
      },
      "auroc": 0.5214939867424242
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 2369,
          "accuracy": 0.012916666666666667
        },
        "0.01": {
          "tp": 23,
          "fn": 2377,
          "accuracy": 0.009583333333333333
        }
      },
      "auroc": 0.20757508680555556
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 713,
          "fn": 6087,
          "accuracy": 0.1048529411764706
        },
        "0.01": {
          "tp": 570,
          "fn": 6230,
          "accuracy": 0.0838235294117647
        }
      },
      "auroc": 0.4106990808823529
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9451479166666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.94035625
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9427520833333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9440927083333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.8672812499999999
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        }
      },
      "auroc": 0.9056869791666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9446203125
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        }
      },
      "auroc": 0.90381875
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 772,
          "fn": 28,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 747,
          "fn": 53,
          "accuracy": 0.93375
        }
      },
      "auroc": 0.92421953125
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.17852083333333335
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        }
      },
      "auroc": 0.5624895833333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        }
      },
      "auroc": 0.8155875
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08345208333333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 278,
          "accuracy": 0.305
        },
        "0.01": {
          "tp": 69,
          "fn": 331,
          "accuracy": 0.1725
        }
      },
      "auroc": 0.4495197916666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        }
      },
      "auroc": 0.8810229166666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.13098645833333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 472,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 272,
          "fn": 528,
          "accuracy": 0.34
        }
      },
      "auroc": 0.5060046874999999
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9366083333333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.09637604166666669
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        }
      },
      "auroc": 0.5164921874999999
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9215062500000001
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08696666666666669
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.5042364583333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9290572916666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.09167135416666669
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 404,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 365,
          "fn": 435,
          "accuracy": 0.45625
        }
      },
      "auroc": 0.5103643229166667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9454302083333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.89085625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        }
      },
      "auroc": 0.9181432291666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.8072114583333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.2017666666666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 273,
          "accuracy": 0.3175
        },
        "0.01": {
          "tp": 89,
          "fn": 311,
          "accuracy": 0.2225
        }
      },
      "auroc": 0.5044890625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        }
      },
      "auroc": 0.8763208333333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        },
        "0.01": {
          "tp": 145,
          "fn": 255,
          "accuracy": 0.3625
        }
      },
      "auroc": 0.5463114583333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 497,
          "fn": 303,
          "accuracy": 0.62125
        },
        "0.01": {
          "tp": 432,
          "fn": 368,
          "accuracy": 0.54
        }
      },
      "auroc": 0.7113161458333335
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        },
        "0.01": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        }
      },
      "auroc": 0.7512593750000001
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 236,
          "fn": 164,
          "accuracy": 0.59
        }
      },
      "auroc": 0.8488588541666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.7231708333333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08356666666666669
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 70,
          "fn": 330,
          "accuracy": 0.175
        },
        "0.01": {
          "tp": 43,
          "fn": 357,
          "accuracy": 0.1075
        }
      },
      "auroc": 0.40336875
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        }
      },
      "auroc": 0.8348145833333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 68,
          "fn": 332,
          "accuracy": 0.17
        },
        "0.01": {
          "tp": 36,
          "fn": 364,
          "accuracy": 0.09
        }
      },
      "auroc": 0.41741302083333337
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 462,
          "accuracy": 0.4225
        },
        "0.01": {
          "tp": 279,
          "fn": 521,
          "accuracy": 0.34875
        }
      },
      "auroc": 0.6261138020833333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9285270833333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.8930885416666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9108078125000001
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9201979166666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.5814822916666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        },
        "0.01": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        }
      },
      "auroc": 0.7508401041666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9243625
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        },
        "0.01": {
          "tp": 158,
          "fn": 242,
          "accuracy": 0.395
        }
      },
      "auroc": 0.7372854166666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 606,
          "fn": 194,
          "accuracy": 0.7575
        },
        "0.01": {
          "tp": 508,
          "fn": 292,
          "accuracy": 0.635
        }
      },
      "auroc": 0.8308239583333332
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9184041666666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9184041666666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9064072916666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9064072916666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9124057291666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9124057291666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.8814416666666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.8814416666666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.8543479166666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.8543479166666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        },
        "0.01": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        }
      },
      "auroc": 0.8678947916666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        },
        "0.01": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        }
      },
      "auroc": 0.8678947916666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9408364583333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9408364583333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.92660625
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.92660625
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9337213541666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9337213541666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9423427083333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9423427083333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.9016593749999999
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.9016593749999999
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9220010416666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9220010416666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9216906249999999
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9216906249999999
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9115656249999999
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9115656249999999
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        }
      },
      "auroc": 0.916628125
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        }
      },
      "auroc": 0.916628125
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2137,
          "fn": 63,
          "accuracy": 0.9713636363636363
        },
        "0.01": {
          "tp": 2028,
          "fn": 172,
          "accuracy": 0.9218181818181819
        }
      },
      "auroc": 0.9321223484848485
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 619,
          "fn": 581,
          "accuracy": 0.5158333333333334
        },
        "0.01": {
          "tp": 523,
          "fn": 677,
          "accuracy": 0.43583333333333335
        }
      },
      "auroc": 0.6250762152777778
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2756,
          "fn": 644,
          "accuracy": 0.8105882352941176
        },
        "0.01": {
          "tp": 2551,
          "fn": 849,
          "accuracy": 0.7502941176470588
        }
      },
      "auroc": 0.823753125
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1813,
          "fn": 387,
          "accuracy": 0.8240909090909091
        },
        "0.01": {
          "tp": 1512,
          "fn": 688,
          "accuracy": 0.6872727272727273
        }
      },
      "auroc": 0.8756684659090909
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 217,
          "fn": 983,
          "accuracy": 0.18083333333333335
        },
        "0.01": {
          "tp": 172,
          "fn": 1028,
          "accuracy": 0.14333333333333334
        }
      },
      "auroc": 0.3174192708333333
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2030,
          "fn": 1370,
          "accuracy": 0.5970588235294118
        },
        "0.01": {
          "tp": 1684,
          "fn": 1716,
          "accuracy": 0.49529411764705883
        }
      },
      "auroc": 0.678639338235294
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3950,
          "fn": 450,
          "accuracy": 0.8977272727272727
        },
        "0.01": {
          "tp": 3540,
          "fn": 860,
          "accuracy": 0.8045454545454546
        }
      },
      "auroc": 0.9038954071969697
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 836,
          "fn": 1564,
          "accuracy": 0.34833333333333333
        },
        "0.01": {
          "tp": 695,
          "fn": 1705,
          "accuracy": 0.28958333333333336
        }
      },
      "auroc": 0.4712477430555555
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 4786,
          "fn": 2014,
          "accuracy": 0.7038235294117647
        },
        "0.01": {
          "tp": 4235,
          "fn": 2565,
          "accuracy": 0.6227941176470588
        }
      },
      "auroc": 0.751196231617647
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.912815625
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.9011104166666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        }
      },
      "auroc": 0.9069630208333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.8977229166666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8752906250000001
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        }
      },
      "auroc": 0.8865067708333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        }
      },
      "auroc": 0.9052692708333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        }
      },
      "auroc": 0.8882005208333332
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 719,
          "fn": 81,
          "accuracy": 0.89875
        },
        "0.01": {
          "tp": 589,
          "fn": 211,
          "accuracy": 0.73625
        }
      },
      "auroc": 0.8967348958333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8503489583333335
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.6157447916666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 136,
          "fn": 264,
          "accuracy": 0.34
        }
      },
      "auroc": 0.733046875
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        }
      },
      "auroc": 0.8740104166666668
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.5001375
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        },
        "0.01": {
          "tp": 137,
          "fn": 263,
          "accuracy": 0.3425
        }
      },
      "auroc": 0.6870739583333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        }
      },
      "auroc": 0.8621796875
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 65,
          "fn": 335,
          "accuracy": 0.1625
        },
        "0.01": {
          "tp": 33,
          "fn": 367,
          "accuracy": 0.0825
        }
      },
      "auroc": 0.5579411458333332
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 425,
          "accuracy": 0.46875
        },
        "0.01": {
          "tp": 273,
          "fn": 527,
          "accuracy": 0.34125
        }
      },
      "auroc": 0.7100604166666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        }
      },
      "auroc": 0.8519552083333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.6367499999999999
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        }
      },
      "auroc": 0.7443526041666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.8524510416666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.6092635416666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        },
        "0.01": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        }
      },
      "auroc": 0.7308572916666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        }
      },
      "auroc": 0.852203125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 329,
          "accuracy": 0.1775
        },
        "0.01": {
          "tp": 20,
          "fn": 380,
          "accuracy": 0.05
        }
      },
      "auroc": 0.6230067708333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 433,
          "accuracy": 0.45875
        },
        "0.01": {
          "tp": 216,
          "fn": 584,
          "accuracy": 0.27
        }
      },
      "auroc": 0.7376049479166669
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.7672427083333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.7956458333333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        },
        "0.01": {
          "tp": 139,
          "fn": 261,
          "accuracy": 0.3475
        }
      },
      "auroc": 0.7814442708333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        }
      },
      "auroc": 0.8294000000000001
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.638084375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 234,
          "accuracy": 0.415
        },
        "0.01": {
          "tp": 111,
          "fn": 289,
          "accuracy": 0.2775
        }
      },
      "auroc": 0.7337421875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        },
        "0.01": {
          "tp": 171,
          "fn": 229,
          "accuracy": 0.4275
        }
      },
      "auroc": 0.7983213541666665
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 260,
          "accuracy": 0.35
        },
        "0.01": {
          "tp": 79,
          "fn": 321,
          "accuracy": 0.1975
        }
      },
      "auroc": 0.7168651041666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 419,
          "accuracy": 0.47625
        },
        "0.01": {
          "tp": 250,
          "fn": 550,
          "accuracy": 0.3125
        }
      },
      "auroc": 0.7575932291666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.8353489583333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.7754208333333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        },
        "0.01": {
          "tp": 149,
          "fn": 251,
          "accuracy": 0.3725
        }
      },
      "auroc": 0.8053848958333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        }
      },
      "auroc": 0.8156874999999999
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.5437854166666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 265,
          "accuracy": 0.3375
        },
        "0.01": {
          "tp": 86,
          "fn": 314,
          "accuracy": 0.215
        }
      },
      "auroc": 0.6797364583333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        },
        "0.01": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        }
      },
      "auroc": 0.8255182291666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 291,
          "accuracy": 0.2725
        },
        "0.01": {
          "tp": 47,
          "fn": 353,
          "accuracy": 0.1175
        }
      },
      "auroc": 0.6596031250000001
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 430,
          "accuracy": 0.4625
        },
        "0.01": {
          "tp": 235,
          "fn": 565,
          "accuracy": 0.29375
        }
      },
      "auroc": 0.7425606770833333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        }
      },
      "auroc": 0.8668552083333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        }
      },
      "auroc": 0.8589104166666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        }
      },
      "auroc": 0.8628828125
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        }
      },
      "auroc": 0.8722864583333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        }
      },
      "auroc": 0.7924791666666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        }
      },
      "auroc": 0.8323828124999999
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        }
      },
      "auroc": 0.8695708333333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 256,
          "fn": 144,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 163,
          "fn": 237,
          "accuracy": 0.4075
        }
      },
      "auroc": 0.8256947916666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 582,
          "fn": 218,
          "accuracy": 0.7275
        },
        "0.01": {
          "tp": 403,
          "fn": 397,
          "accuracy": 0.50375
        }
      },
      "auroc": 0.8476328125
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.8086947916666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.8086947916666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.7856208333333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        }
      },
      "auroc": 0.7856208333333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        },
        "0.01": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        }
      },
      "auroc": 0.7971578125000001
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        },
        "0.01": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        }
      },
      "auroc": 0.7971578125000001
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.82794375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.82794375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.8198437500000001
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.8198437500000001
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        },
        "0.01": {
          "tp": 162,
          "fn": 238,
          "accuracy": 0.405
        }
      },
      "auroc": 0.8238937500000001
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        },
        "0.01": {
          "tp": 162,
          "fn": 238,
          "accuracy": 0.405
        }
      },
      "auroc": 0.8238937500000001
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.894275
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        }
      },
      "auroc": 0.894275
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.8904895833333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.8904895833333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        }
      },
      "auroc": 0.8923822916666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        }
      },
      "auroc": 0.8923822916666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        }
      },
      "auroc": 0.8923458333333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        }
      },
      "auroc": 0.8923458333333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.8839104166666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.8839104166666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": {
          "tp": 259,
          "fn": 141,
          "accuracy": 0.6475
        }
      },
      "auroc": 0.888128125
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": {
          "tp": 259,
          "fn": 141,
          "accuracy": 0.6475
        }
      },
      "auroc": 0.888128125
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        }
      },
      "auroc": 0.8575572916666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        }
      },
      "auroc": 0.8575572916666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.840015625
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.840015625
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8487864583333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8487864583333333
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1656,
          "fn": 544,
          "accuracy": 0.7527272727272727
        },
        "0.01": {
          "tp": 1199,
          "fn": 1001,
          "accuracy": 0.545
        }
      },
      "auroc": 0.8513984848484848
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 612,
          "fn": 588,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 386,
          "fn": 814,
          "accuracy": 0.32166666666666666
        }
      },
      "auroc": 0.7639303819444445
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2268,
          "fn": 1132,
          "accuracy": 0.6670588235294118
        },
        "0.01": {
          "tp": 1585,
          "fn": 1815,
          "accuracy": 0.4661764705882353
        }
      },
      "auroc": 0.8205273897058822
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1633,
          "fn": 567,
          "accuracy": 0.7422727272727273
        },
        "0.01": {
          "tp": 1180,
          "fn": 1020,
          "accuracy": 0.5363636363636364
        }
      },
      "auroc": 0.8510398674242424
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 821,
          "accuracy": 0.31583333333333335
        },
        "0.01": {
          "tp": 234,
          "fn": 966,
          "accuracy": 0.195
        }
      },
      "auroc": 0.6598401041666668
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2012,
          "fn": 1388,
          "accuracy": 0.591764705882353
        },
        "0.01": {
          "tp": 1414,
          "fn": 1986,
          "accuracy": 0.4158823529411765
        }
      },
      "auroc": 0.7835575980392155
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3289,
          "fn": 1111,
          "accuracy": 0.7475
        },
        "0.01": {
          "tp": 2379,
          "fn": 2021,
          "accuracy": 0.5406818181818182
        }
      },
      "auroc": 0.8512191761363636
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 991,
          "fn": 1409,
          "accuracy": 0.41291666666666665
        },
        "0.01": {
          "tp": 620,
          "fn": 1780,
          "accuracy": 0.25833333333333336
        }
      },
      "auroc": 0.7118852430555556
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 4280,
          "fn": 2520,
          "accuracy": 0.6294117647058823
        },
        "0.01": {
          "tp": 2999,
          "fn": 3801,
          "accuracy": 0.4410294117647059
        }
      },
      "auroc": 0.802042493872549
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8663802083333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.8539385416666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        }
      },
      "auroc": 0.8601593750000001
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.864025
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.7315552083333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        }
      },
      "auroc": 0.7977901041666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        }
      },
      "auroc": 0.8652026041666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 164,
          "fn": 236,
          "accuracy": 0.41
        }
      },
      "auroc": 0.792746875
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 552,
          "fn": 248,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 389,
          "fn": 411,
          "accuracy": 0.48625
        }
      },
      "auroc": 0.8289747395833332
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9336760416666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.16938750000000002
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        }
      },
      "auroc": 0.5515317708333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        },
        "0.01": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        }
      },
      "auroc": 0.73038125
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08265833333333335
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 319,
          "accuracy": 0.2025
        },
        "0.01": {
          "tp": 53,
          "fn": 347,
          "accuracy": 0.1325
        }
      },
      "auroc": 0.40651979166666674
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        }
      },
      "auroc": 0.8320286458333332
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.12602291666666665
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 281,
          "fn": 519,
          "accuracy": 0.35125
        },
        "0.01": {
          "tp": 243,
          "fn": 557,
          "accuracy": 0.30375
        }
      },
      "auroc": 0.47902578125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.841246875
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08954583333333335
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 256,
          "accuracy": 0.36
        },
        "0.01": {
          "tp": 89,
          "fn": 311,
          "accuracy": 0.2225
        }
      },
      "auroc": 0.46539635416666664
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        }
      },
      "auroc": 0.8020322916666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08498333333333336
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 291,
          "accuracy": 0.2725
        },
        "0.01": {
          "tp": 61,
          "fn": 339,
          "accuracy": 0.1525
        }
      },
      "auroc": 0.4435078125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        },
        "0.01": {
          "tp": 150,
          "fn": 250,
          "accuracy": 0.375
        }
      },
      "auroc": 0.8216395833333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08726458333333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 253,
          "fn": 547,
          "accuracy": 0.31625
        },
        "0.01": {
          "tp": 150,
          "fn": 650,
          "accuracy": 0.1875
        }
      },
      "auroc": 0.4544520833333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9313572916666668
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.7851739583333335
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        }
      },
      "auroc": 0.858265625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.7059947916666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.18158645833333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 321,
          "accuracy": 0.1975
        },
        "0.01": {
          "tp": 52,
          "fn": 348,
          "accuracy": 0.13
        }
      },
      "auroc": 0.443790625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        },
        "0.01": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        }
      },
      "auroc": 0.8186760416666665
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 85,
          "fn": 315,
          "accuracy": 0.2125
        }
      },
      "auroc": 0.4833802083333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 411,
          "accuracy": 0.48625
        },
        "0.01": {
          "tp": 320,
          "fn": 480,
          "accuracy": 0.4
        }
      },
      "auroc": 0.6510281250000001
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9269395833333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.5899833333333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 210,
          "fn": 190,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        }
      },
      "auroc": 0.7584614583333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.5657687499999999
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08269583333333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 366,
          "accuracy": 0.085
        },
        "0.01": {
          "tp": 20,
          "fn": 380,
          "accuracy": 0.05
        }
      },
      "auroc": 0.32423229166666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        }
      },
      "auroc": 0.7463541666666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 10,
          "fn": 390,
          "accuracy": 0.025
        }
      },
      "auroc": 0.3363395833333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 556,
          "accuracy": 0.305
        },
        "0.01": {
          "tp": 210,
          "fn": 590,
          "accuracy": 0.2625
        }
      },
      "auroc": 0.5413468749999999
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.8182333333333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.7716020833333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        },
        "0.01": {
          "tp": 121,
          "fn": 279,
          "accuracy": 0.3025
        }
      },
      "auroc": 0.7949177083333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        }
      },
      "auroc": 0.8034989583333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.4492708333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 279,
          "accuracy": 0.3025
        },
        "0.01": {
          "tp": 71,
          "fn": 329,
          "accuracy": 0.1775
        }
      },
      "auroc": 0.6263848958333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        },
        "0.01": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        }
      },
      "auroc": 0.8108661458333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 107,
          "fn": 293,
          "accuracy": 0.2675
        },
        "0.01": {
          "tp": 46,
          "fn": 354,
          "accuracy": 0.115
        }
      },
      "auroc": 0.6104364583333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 476,
          "accuracy": 0.405
        },
        "0.01": {
          "tp": 192,
          "fn": 608,
          "accuracy": 0.24
        }
      },
      "auroc": 0.7106513020833333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.7698979166666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.7698979166666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        }
      },
      "auroc": 0.7467166666666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        }
      },
      "auroc": 0.7467166666666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        },
        "0.01": {
          "tp": 83,
          "fn": 317,
          "accuracy": 0.2075
        }
      },
      "auroc": 0.7583072916666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        },
        "0.01": {
          "tp": 83,
          "fn": 317,
          "accuracy": 0.2075
        }
      },
      "auroc": 0.7583072916666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        },
        "0.01": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        }
      },
      "auroc": 0.7091541666666668
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        },
        "0.01": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        }
      },
      "auroc": 0.7091541666666668
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.6877104166666665
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.6877104166666665
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 283,
          "accuracy": 0.2925
        },
        "0.01": {
          "tp": 58,
          "fn": 342,
          "accuracy": 0.145
        }
      },
      "auroc": 0.6984322916666665
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 283,
          "accuracy": 0.2925
        },
        "0.01": {
          "tp": 58,
          "fn": 342,
          "accuracy": 0.145
        }
      },
      "auroc": 0.6984322916666665
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        }
      },
      "auroc": 0.8683479166666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        }
      },
      "auroc": 0.8683479166666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.8290104166666666
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        }
      },
      "auroc": 0.8290104166666666
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        },
        "0.01": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        }
      },
      "auroc": 0.8486791666666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        },
        "0.01": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        }
      },
      "auroc": 0.8486791666666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.8981156250000001
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.8981156250000001
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        },
        "0.01": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        }
      },
      "auroc": 0.7796208333333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        },
        "0.01": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        }
      },
      "auroc": 0.7796208333333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        }
      },
      "auroc": 0.8388682291666666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        }
      },
      "auroc": 0.8388682291666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        }
      },
      "auroc": 0.7876708333333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        }
      },
      "auroc": 0.7876708333333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.7716927083333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.7716927083333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        }
      },
      "auroc": 0.7796817708333332
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        }
      },
      "auroc": 0.7796817708333332
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1588,
          "fn": 612,
          "accuracy": 0.7218181818181818
        },
        "0.01": {
          "tp": 1239,
          "fn": 961,
          "accuracy": 0.5631818181818182
        }
      },
      "auroc": 0.8500927083333334
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 827,
          "accuracy": 0.31083333333333335
        },
        "0.01": {
          "tp": 246,
          "fn": 954,
          "accuracy": 0.205
        }
      },
      "auroc": 0.5432718750000001
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1961,
          "fn": 1439,
          "accuracy": 0.576764705882353
        },
        "0.01": {
          "tp": 1485,
          "fn": 1915,
          "accuracy": 0.43676470588235294
        }
      },
      "auroc": 0.7418030024509803
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 988,
          "fn": 1212,
          "accuracy": 0.4490909090909091
        },
        "0.01": {
          "tp": 582,
          "fn": 1618,
          "accuracy": 0.26454545454545453
        }
      },
      "auroc": 0.7533138257575758
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 1089,
          "accuracy": 0.0925
        },
        "0.01": {
          "tp": 62,
          "fn": 1138,
          "accuracy": 0.051666666666666666
        }
      },
      "auroc": 0.2687916666666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1099,
          "fn": 2301,
          "accuracy": 0.32323529411764707
        },
        "0.01": {
          "tp": 644,
          "fn": 2756,
          "accuracy": 0.18941176470588236
        }
      },
      "auroc": 0.5823060049019608
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2576,
          "fn": 1824,
          "accuracy": 0.5854545454545454
        },
        "0.01": {
          "tp": 1821,
          "fn": 2579,
          "accuracy": 0.4138636363636364
        }
      },
      "auroc": 0.8017032670454546
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 484,
          "fn": 1916,
          "accuracy": 0.20166666666666666
        },
        "0.01": {
          "tp": 308,
          "fn": 2092,
          "accuracy": 0.12833333333333333
        }
      },
      "auroc": 0.4060317708333333
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3060,
          "fn": 3740,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 2129,
          "fn": 4671,
          "accuracy": 0.31308823529411767
        }
      },
      "auroc": 0.6620545036764707
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9455791666666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9430520833333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.944315625
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9446552083333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.8829822916666668
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        }
      },
      "auroc": 0.91381875
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9451171875000001
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        }
      },
      "auroc": 0.9130171875
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 775,
          "fn": 25,
          "accuracy": 0.96875
        },
        "0.01": {
          "tp": 757,
          "fn": 43,
          "accuracy": 0.94625
        }
      },
      "auroc": 0.9290671875000002
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.17850625
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        },
        "0.01": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        }
      },
      "auroc": 0.5624822916666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        }
      },
      "auroc": 0.84955625
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08345208333333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 253,
          "accuracy": 0.3675
        },
        "0.01": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        }
      },
      "auroc": 0.46650416666666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        }
      },
      "auroc": 0.8980072916666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.1309791666666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 448,
          "accuracy": 0.44
        },
        "0.01": {
          "tp": 310,
          "fn": 490,
          "accuracy": 0.3875
        }
      },
      "auroc": 0.5144932291666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9359041666666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.09907708333333336
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        }
      },
      "auroc": 0.517490625
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9241958333333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08916354166666668
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        }
      },
      "auroc": 0.5066796874999999
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.93005
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.09412031250000003
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 403,
          "accuracy": 0.49625
        },
        "0.01": {
          "tp": 364,
          "fn": 436,
          "accuracy": 0.455
        }
      },
      "auroc": 0.51208515625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9458614583333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9089114583333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        }
      },
      "auroc": 0.9273864583333332
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        }
      },
      "auroc": 0.833353125
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.25275312499999997
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 263,
          "accuracy": 0.3425
        },
        "0.01": {
          "tp": 99,
          "fn": 301,
          "accuracy": 0.2475
        }
      },
      "auroc": 0.543053125
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        }
      },
      "auroc": 0.8896072916666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 219,
          "accuracy": 0.4525
        },
        "0.01": {
          "tp": 160,
          "fn": 240,
          "accuracy": 0.4
        }
      },
      "auroc": 0.5808322916666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 518,
          "fn": 282,
          "accuracy": 0.6475
        },
        "0.01": {
          "tp": 458,
          "fn": 342,
          "accuracy": 0.5725
        }
      },
      "auroc": 0.7352197916666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.945903125
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.8173927083333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        },
        "0.01": {
          "tp": 265,
          "fn": 135,
          "accuracy": 0.6625
        }
      },
      "auroc": 0.8816479166666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        }
      },
      "auroc": 0.7489114583333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.09432291666666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 322,
          "accuracy": 0.195
        },
        "0.01": {
          "tp": 51,
          "fn": 349,
          "accuracy": 0.1275
        }
      },
      "auroc": 0.4216171875
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        }
      },
      "auroc": 0.8474072916666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 111,
          "fn": 289,
          "accuracy": 0.2775
        },
        "0.01": {
          "tp": 66,
          "fn": 334,
          "accuracy": 0.165
        }
      },
      "auroc": 0.4558578125
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 411,
          "accuracy": 0.48625
        },
        "0.01": {
          "tp": 316,
          "fn": 484,
          "accuracy": 0.395
        }
      },
      "auroc": 0.6516325520833333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9335395833333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.9050135416666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9192765625
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9273093749999999
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.6108145833333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.7690619791666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9304244791666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        },
        "0.01": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        }
      },
      "auroc": 0.7579140624999999
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 637,
          "fn": 163,
          "accuracy": 0.79625
        },
        "0.01": {
          "tp": 548,
          "fn": 252,
          "accuracy": 0.685
        }
      },
      "auroc": 0.8441692708333335
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.91981875
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.91981875
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.913553125
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.913553125
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9166859375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9166859375
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.8956614583333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.8956614583333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.8782197916666666
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.8782197916666666
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        }
      },
      "auroc": 0.8869406249999999
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        }
      },
      "auroc": 0.8869406249999999
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9420052083333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9420052083333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9306020833333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9306020833333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9363036458333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9363036458333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9402479166666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9402479166666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9064166666666668
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.9064166666666668
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        }
      },
      "auroc": 0.9233322916666666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        }
      },
      "auroc": 0.9233322916666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.926571875
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.926571875
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9205041666666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9205041666666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9235380208333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9235380208333334
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2164,
          "fn": 36,
          "accuracy": 0.9836363636363636
        },
        "0.01": {
          "tp": 2056,
          "fn": 144,
          "accuracy": 0.9345454545454546
        }
      },
      "auroc": 0.934322821969697
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 687,
          "fn": 513,
          "accuracy": 0.5725
        },
        "0.01": {
          "tp": 577,
          "fn": 623,
          "accuracy": 0.48083333333333333
        }
      },
      "auroc": 0.6419921875
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2851,
          "fn": 549,
          "accuracy": 0.8385294117647059
        },
        "0.01": {
          "tp": 2633,
          "fn": 767,
          "accuracy": 0.7744117647058824
        }
      },
      "auroc": 0.8311473039215687
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1885,
          "fn": 315,
          "accuracy": 0.8568181818181818
        },
        "0.01": {
          "tp": 1603,
          "fn": 597,
          "accuracy": 0.7286363636363636
        }
      },
      "auroc": 0.8888433712121212
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 232,
          "fn": 968,
          "accuracy": 0.19333333333333333
        },
        "0.01": {
          "tp": 193,
          "fn": 1007,
          "accuracy": 0.16083333333333333
        }
      },
      "auroc": 0.3355814236111111
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2117,
          "fn": 1283,
          "accuracy": 0.6226470588235294
        },
        "0.01": {
          "tp": 1796,
          "fn": 1604,
          "accuracy": 0.528235294117647
        }
      },
      "auroc": 0.6935744485294117
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 4049,
          "fn": 351,
          "accuracy": 0.9202272727272728
        },
        "0.01": {
          "tp": 3659,
          "fn": 741,
          "accuracy": 0.831590909090909
        }
      },
      "auroc": 0.9115830965909092
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 919,
          "fn": 1481,
          "accuracy": 0.3829166666666667
        },
        "0.01": {
          "tp": 770,
          "fn": 1630,
          "accuracy": 0.32083333333333336
        }
      },
      "auroc": 0.48878680555555554
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 4968,
          "fn": 1832,
          "accuracy": 0.7305882352941176
        },
        "0.01": {
          "tp": 4429,
          "fn": 2371,
          "accuracy": 0.6513235294117647
        }
      },
      "auroc": 0.7623608762254903
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9206260416666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        }
      },
      "auroc": 0.9079395833333332
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        }
      },
      "auroc": 0.9142828124999999
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9158260416666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        }
      },
      "auroc": 0.88280625
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        }
      },
      "auroc": 0.8993161458333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        }
      },
      "auroc": 0.9182260416666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        }
      },
      "auroc": 0.8953729166666666
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 741,
          "fn": 59,
          "accuracy": 0.92625
        },
        "0.01": {
          "tp": 621,
          "fn": 179,
          "accuracy": 0.77625
        }
      },
      "auroc": 0.9067994791666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.942628125
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.41947499999999993
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.6810515625
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9194989583333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.45465625
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        }
      },
      "auroc": 0.6870776041666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.9310635416666668
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 52,
          "fn": 348,
          "accuracy": 0.13
        },
        "0.01": {
          "tp": 31,
          "fn": 369,
          "accuracy": 0.0775
        }
      },
      "auroc": 0.437065625
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 442,
          "fn": 358,
          "accuracy": 0.5525
        },
        "0.01": {
          "tp": 398,
          "fn": 402,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.6840645833333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.8606822916666665
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.28641354166666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 238,
          "accuracy": 0.405
        },
        "0.01": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        }
      },
      "auroc": 0.5735479166666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        }
      },
      "auroc": 0.8624072916666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.30824375000000004
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        },
        "0.01": {
          "tp": 110,
          "fn": 290,
          "accuracy": 0.275
        }
      },
      "auroc": 0.5853255208333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        }
      },
      "auroc": 0.8615447916666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 381,
          "accuracy": 0.0475
        },
        "0.01": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        }
      },
      "auroc": 0.2973286458333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 477,
          "accuracy": 0.40375
        },
        "0.01": {
          "tp": 222,
          "fn": 578,
          "accuracy": 0.2775
        }
      },
      "auroc": 0.5794367187499999
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9398125
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.79939375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 106,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        }
      },
      "auroc": 0.869603125
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.8844572916666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.6326833333333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": {
          "tp": 149,
          "fn": 251,
          "accuracy": 0.3725
        }
      },
      "auroc": 0.7585703125
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        }
      },
      "auroc": 0.9121348958333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 269,
          "accuracy": 0.3275
        },
        "0.01": {
          "tp": 69,
          "fn": 331,
          "accuracy": 0.1725
        }
      },
      "auroc": 0.7160385416666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 501,
          "fn": 299,
          "accuracy": 0.62625
        },
        "0.01": {
          "tp": 396,
          "fn": 404,
          "accuracy": 0.495
        }
      },
      "auroc": 0.8140867187499999
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9302968749999999
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.6726145833333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.8014557291666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        }
      },
      "auroc": 0.8644927083333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.5249375000000001
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        },
        "0.01": {
          "tp": 118,
          "fn": 282,
          "accuracy": 0.295
        }
      },
      "auroc": 0.6947151041666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        }
      },
      "auroc": 0.8973947916666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        },
        "0.01": {
          "tp": 19,
          "fn": 381,
          "accuracy": 0.0475
        }
      },
      "auroc": 0.5987760416666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 404,
          "fn": 396,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 309,
          "fn": 491,
          "accuracy": 0.38625
        }
      },
      "auroc": 0.7480854166666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.91101875
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.8672135416666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        }
      },
      "auroc": 0.8891161458333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.8987197916666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        }
      },
      "auroc": 0.7904677083333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        },
        "0.01": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        }
      },
      "auroc": 0.84459375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        }
      },
      "auroc": 0.9048692708333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        },
        "0.01": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        }
      },
      "auroc": 0.828840625
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 624,
          "fn": 176,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 476,
          "fn": 324,
          "accuracy": 0.595
        }
      },
      "auroc": 0.8668549479166666
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.822221875
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.822221875
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        }
      },
      "auroc": 0.8126770833333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        }
      },
      "auroc": 0.8126770833333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": {
          "tp": 147,
          "fn": 253,
          "accuracy": 0.3675
        }
      },
      "auroc": 0.8174494791666668
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": {
          "tp": 147,
          "fn": 253,
          "accuracy": 0.3675
        }
      },
      "auroc": 0.8174494791666668
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        }
      },
      "auroc": 0.8071520833333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        }
      },
      "auroc": 0.8071520833333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8135322916666666
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8135322916666666
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        }
      },
      "auroc": 0.8103421875000001
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        }
      },
      "auroc": 0.8103421875000001
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9193604166666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9193604166666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9209354166666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9209354166666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        }
      },
      "auroc": 0.9201479166666666
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        }
      },
      "auroc": 0.9201479166666666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.923678125
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.923678125
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9153260416666666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9153260416666666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9195020833333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9195020833333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.826646875
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.826646875
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.8233604166666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        }
      },
      "auroc": 0.8233604166666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 138,
          "fn": 262,
          "accuracy": 0.345
        }
      },
      "auroc": 0.8250036458333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 138,
          "fn": 262,
          "accuracy": 0.345
        }
      },
      "auroc": 0.8250036458333333
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1884,
          "fn": 316,
          "accuracy": 0.8563636363636363
        },
        "0.01": {
          "tp": 1576,
          "fn": 624,
          "accuracy": 0.7163636363636363
        }
      },
      "auroc": 0.8912839962121211
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 500,
          "fn": 700,
          "accuracy": 0.4166666666666667
        },
        "0.01": {
          "tp": 344,
          "fn": 856,
          "accuracy": 0.2866666666666667
        }
      },
      "auroc": 0.6588416666666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2384,
          "fn": 1016,
          "accuracy": 0.7011764705882353
        },
        "0.01": {
          "tp": 1920,
          "fn": 1480,
          "accuracy": 0.5647058823529412
        }
      },
      "auroc": 0.8092455269607841
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1784,
          "fn": 416,
          "accuracy": 0.8109090909090909
        },
        "0.01": {
          "tp": 1385,
          "fn": 815,
          "accuracy": 0.6295454545454545
        }
      },
      "auroc": 0.8755666666666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 835,
          "accuracy": 0.30416666666666664
        },
        "0.01": {
          "tp": 244,
          "fn": 956,
          "accuracy": 0.20333333333333334
        }
      },
      "auroc": 0.5989657986111111
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2149,
          "fn": 1251,
          "accuracy": 0.6320588235294118
        },
        "0.01": {
          "tp": 1629,
          "fn": 1771,
          "accuracy": 0.47911764705882354
        }
      },
      "auroc": 0.7779428308823528
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3668,
          "fn": 732,
          "accuracy": 0.8336363636363636
        },
        "0.01": {
          "tp": 2961,
          "fn": 1439,
          "accuracy": 0.6729545454545455
        }
      },
      "auroc": 0.883425331439394
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 865,
          "fn": 1535,
          "accuracy": 0.36041666666666666
        },
        "0.01": {
          "tp": 588,
          "fn": 1812,
          "accuracy": 0.245
        }
      },
      "auroc": 0.6289037326388889
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4533,
          "fn": 2267,
          "accuracy": 0.6666176470588235
        },
        "0.01": {
          "tp": 3549,
          "fn": 3251,
          "accuracy": 0.5219117647058824
        }
      },
      "auroc": 0.7935941789215686
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9440604166666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.93748125
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9407708333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.942784375
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.8645208333333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        }
      },
      "auroc": 0.9036526041666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9434223958333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        }
      },
      "auroc": 0.9010010416666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 768,
          "fn": 32,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 744,
          "fn": 56,
          "accuracy": 0.93
        }
      },
      "auroc": 0.92221171875
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.1776927083333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.5620755208333335
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.7452395833333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08311666666666669
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 73,
          "fn": 327,
          "accuracy": 0.1825
        },
        "0.01": {
          "tp": 35,
          "fn": 365,
          "accuracy": 0.0875
        }
      },
      "auroc": 0.414178125
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 273,
          "fn": 127,
          "accuracy": 0.6825
        },
        "0.01": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        }
      },
      "auroc": 0.8458489583333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.13040468750000003
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 521,
          "accuracy": 0.34875
        },
        "0.01": {
          "tp": 236,
          "fn": 564,
          "accuracy": 0.295
        }
      },
      "auroc": 0.48812682291666665
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9354947916666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.09673750000000002
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        }
      },
      "auroc": 0.5161161458333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9243041666666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08878750000000002
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        }
      },
      "auroc": 0.5065458333333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9298994791666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.09276250000000001
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 404,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 371,
          "fn": 429,
          "accuracy": 0.46375
        }
      },
      "auroc": 0.5113309895833333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9458614583333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9109625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.9284119791666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        }
      },
      "auroc": 0.775515625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.18839375000000003
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 300,
          "accuracy": 0.25
        },
        "0.01": {
          "tp": 61,
          "fn": 339,
          "accuracy": 0.1525
        }
      },
      "auroc": 0.4819546875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8606885416666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        }
      },
      "auroc": 0.549678125
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 480,
          "fn": 320,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 428,
          "fn": 372,
          "accuracy": 0.535
        }
      },
      "auroc": 0.7051833333333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        }
      },
      "auroc": 0.8062874999999999
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 301,
          "fn": 99,
          "accuracy": 0.7525
        },
        "0.01": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        }
      },
      "auroc": 0.8763729166666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.6743489583333333
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08631250000000003
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 351,
          "accuracy": 0.1225
        },
        "0.01": {
          "tp": 29,
          "fn": 371,
          "accuracy": 0.0725
        }
      },
      "auroc": 0.38033072916666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        }
      },
      "auroc": 0.8104036458333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 299,
          "accuracy": 0.2525
        },
        "0.01": {
          "tp": 61,
          "fn": 339,
          "accuracy": 0.1525
        }
      },
      "auroc": 0.44630000000000003
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 450,
          "accuracy": 0.4375
        },
        "0.01": {
          "tp": 290,
          "fn": 510,
          "accuracy": 0.3625
        }
      },
      "auroc": 0.6283518229166667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9303541666666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9001864583333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        }
      },
      "auroc": 0.9152703125
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9169645833333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.5915166666666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.754240625
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9236593750000001
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        },
        "0.01": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        }
      },
      "auroc": 0.7458515625000001
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 614,
          "fn": 186,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 528,
          "fn": 272,
          "accuracy": 0.66
        }
      },
      "auroc": 0.8347554687499998
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9352666666666668
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9352666666666668
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.93124375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.93124375
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9332552083333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9332552083333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.894103125
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.894103125
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.8702875
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.8702875
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8821953125
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8821953125
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9350802083333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9350802083333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.922103125
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.922103125
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9285916666666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9285916666666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9401677083333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9401677083333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.88315
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.88315
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9116588541666667
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9116588541666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.92515
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.92515
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9148354166666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9148354166666666
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9199927083333332
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9199927083333332
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2158,
          "fn": 42,
          "accuracy": 0.980909090909091
        },
        "0.01": {
          "tp": 2062,
          "fn": 138,
          "accuracy": 0.9372727272727273
        }
      },
      "auroc": 0.934405018939394
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 669,
          "fn": 531,
          "accuracy": 0.5575
        },
        "0.01": {
          "tp": 573,
          "fn": 627,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.6382246527777778
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2827,
          "fn": 573,
          "accuracy": 0.8314705882352941
        },
        "0.01": {
          "tp": 2635,
          "fn": 765,
          "accuracy": 0.775
        }
      },
      "auroc": 0.8298707720588234
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1714,
          "fn": 486,
          "accuracy": 0.7790909090909091
        },
        "0.01": {
          "tp": 1456,
          "fn": 744,
          "accuracy": 0.6618181818181819
        }
      },
      "auroc": 0.8637070075757576
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 988,
          "accuracy": 0.17666666666666667
        },
        "0.01": {
          "tp": 173,
          "fn": 1027,
          "accuracy": 0.14416666666666667
        }
      },
      "auroc": 0.31710798611111113
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1926,
          "fn": 1474,
          "accuracy": 0.5664705882352942
        },
        "0.01": {
          "tp": 1629,
          "fn": 1771,
          "accuracy": 0.47911764705882354
        }
      },
      "auroc": 0.6707897058823529
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3872,
          "fn": 528,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 3518,
          "fn": 882,
          "accuracy": 0.7995454545454546
        }
      },
      "auroc": 0.8990560132575757
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 881,
          "fn": 1519,
          "accuracy": 0.3670833333333333
        },
        "0.01": {
          "tp": 746,
          "fn": 1654,
          "accuracy": 0.31083333333333335
        }
      },
      "auroc": 0.47766631944444443
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 4753,
          "fn": 2047,
          "accuracy": 0.6989705882352941
        },
        "0.01": {
          "tp": 4264,
          "fn": 2536,
          "accuracy": 0.6270588235294118
        }
      },
      "auroc": 0.7503302389705881
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9453541666666667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9459062500000001
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9456614583333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.8818645833333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.9137630208333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9460598958333333
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.9136093750000001
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 776,
          "fn": 24,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 766,
          "fn": 34,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9298346354166667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.16794270833333336
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        }
      },
      "auroc": 0.5572005208333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.8337458333333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08311666666666669
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 259,
          "accuracy": 0.3525
        },
        "0.01": {
          "tp": 86,
          "fn": 314,
          "accuracy": 0.215
        }
      },
      "auroc": 0.45843124999999996
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        }
      },
      "auroc": 0.8901020833333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.12552968750000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 455,
          "accuracy": 0.43125
        },
        "0.01": {
          "tp": 288,
          "fn": 512,
          "accuracy": 0.36
        }
      },
      "auroc": 0.5078158854166667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.944553125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.09551562500000002
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.5200343749999999
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.93918125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08901250000000002
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.5140968749999999
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9418671875
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.09226406250000002
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 402,
          "fn": 398,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 390,
          "fn": 410,
          "accuracy": 0.4875
        }
      },
      "auroc": 0.517065625
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9458614583333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9140979166666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9299796875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        }
      },
      "auroc": 0.8232947916666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.20997395833333335
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 267,
          "accuracy": 0.3325
        },
        "0.01": {
          "tp": 94,
          "fn": 306,
          "accuracy": 0.235
        }
      },
      "auroc": 0.516634375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        }
      },
      "auroc": 0.884578125
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.5620359375
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 517,
          "fn": 283,
          "accuracy": 0.64625
        },
        "0.01": {
          "tp": 462,
          "fn": 338,
          "accuracy": 0.5775
        }
      },
      "auroc": 0.7233070312500001
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.8097520833333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 259,
          "fn": 141,
          "accuracy": 0.6475
        }
      },
      "auroc": 0.8781052083333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        },
        "0.01": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        }
      },
      "auroc": 0.746634375
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08720000000000003
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 77,
          "fn": 323,
          "accuracy": 0.1925
        },
        "0.01": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        }
      },
      "auroc": 0.4169171875
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        },
        "0.01": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        }
      },
      "auroc": 0.8465463541666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 292,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 59,
          "fn": 341,
          "accuracy": 0.1475
        }
      },
      "auroc": 0.4484760416666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 415,
          "accuracy": 0.48125
        },
        "0.01": {
          "tp": 309,
          "fn": 491,
          "accuracy": 0.38625
        }
      },
      "auroc": 0.6475111979166668
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.942434375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9233520833333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.9328932291666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9356979166666667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.6227302083333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        }
      },
      "auroc": 0.7792140624999999
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9390661458333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        }
      },
      "auroc": 0.7730411458333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 658,
          "fn": 142,
          "accuracy": 0.8225
        },
        "0.01": {
          "tp": 591,
          "fn": 209,
          "accuracy": 0.73875
        }
      },
      "auroc": 0.8560536458333332
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9389802083333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9389802083333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9343052083333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9343052083333334
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9366427083333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9366427083333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9113864583333334
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9113864583333334
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.8913156250000001
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.8913156250000001
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9013510416666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        }
      },
      "auroc": 0.9013510416666667
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9448208333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9448208333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9402135416666666
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9402135416666666
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9425171874999999
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9425171874999999
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9431541666666666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9431541666666666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9161520833333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9161520833333333
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        }
      },
      "auroc": 0.9296531250000001
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        }
      },
      "auroc": 0.9296531250000001
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.93445
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.93445
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9323270833333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9323270833333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9333885416666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9333885416666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2176,
          "fn": 24,
          "accuracy": 0.9890909090909091
        },
        "0.01": {
          "tp": 2128,
          "fn": 72,
          "accuracy": 0.9672727272727273
        }
      },
      "auroc": 0.940455965909091
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 694,
          "fn": 506,
          "accuracy": 0.5783333333333334
        },
        "0.01": {
          "tp": 600,
          "fn": 600,
          "accuracy": 0.5
        }
      },
      "auroc": 0.6426690972222222
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2870,
          "fn": 530,
          "accuracy": 0.8441176470588235
        },
        "0.01": {
          "tp": 2728,
          "fn": 672,
          "accuracy": 0.8023529411764706
        }
      },
      "auroc": 0.8353547181372549
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1899,
          "fn": 301,
          "accuracy": 0.8631818181818182
        },
        "0.01": {
          "tp": 1679,
          "fn": 521,
          "accuracy": 0.7631818181818182
        }
      },
      "auroc": 0.8944117424242424
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 960,
          "accuracy": 0.2
        },
        "0.01": {
          "tp": 203,
          "fn": 997,
          "accuracy": 0.16916666666666666
        }
      },
      "auroc": 0.3289829861111111
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2139,
          "fn": 1261,
          "accuracy": 0.6291176470588236
        },
        "0.01": {
          "tp": 1882,
          "fn": 1518,
          "accuracy": 0.5535294117647059
        }
      },
      "auroc": 0.6948486519607844
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 4075,
          "fn": 325,
          "accuracy": 0.9261363636363636
        },
        "0.01": {
          "tp": 3807,
          "fn": 593,
          "accuracy": 0.8652272727272727
        }
      },
      "auroc": 0.9174338541666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 934,
          "fn": 1466,
          "accuracy": 0.38916666666666666
        },
        "0.01": {
          "tp": 803,
          "fn": 1597,
          "accuracy": 0.33458333333333334
        }
      },
      "auroc": 0.4858260416666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 5009,
          "fn": 1791,
          "accuracy": 0.7366176470588235
        },
        "0.01": {
          "tp": 4610,
          "fn": 2190,
          "accuracy": 0.6779411764705883
        }
      },
      "auroc": 0.7651016850490195
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9461322916666667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9292874999999999
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9377098958333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9252447916666666
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9358515625
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9462953125000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        }
      },
      "auroc": 0.9272661458333333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        },
        "0.01": {
          "tp": 759,
          "fn": 41,
          "accuracy": 0.94875
        }
      },
      "auroc": 0.9367807291666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9344322916666666
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9404453125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.925578125
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9360182291666667
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        }
      },
      "auroc": 0.9300052083333333
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        },
        "0.01": {
          "tp": 761,
          "fn": 39,
          "accuracy": 0.95125
        }
      },
      "auroc": 0.9382317708333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9461624999999999
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9415958333333333
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9438791666666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.94486875
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9340541666666666
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9394614583333334
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9455156250000001
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.937825
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 783,
          "fn": 17,
          "accuracy": 0.97875
        }
      },
      "auroc": 0.9416703125
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9388114583333332
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9426348958333334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9461322916666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9386197916666666
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9423760416666667
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9462953125000001
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.938715625
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 786,
          "fn": 14,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.94250546875
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.945903125
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9443270833333334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9451151041666668
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9461807291666666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9453927083333333
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        }
      },
      "auroc": 0.94578671875
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.94370625
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.94370625
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9406333333333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9406333333333333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9421697916666667
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9421697916666667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9456614583333334
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9456614583333334
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9450239583333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9450239583333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9453427083333333
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9453427083333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9464583333333334
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9449229166666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9449229166666667
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.94563125
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.94563125
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9452770833333333
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9452770833333333
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2199,
          "fn": 1,
          "accuracy": 0.9995454545454545
        },
        "0.01": {
          "tp": 2195,
          "fn": 5,
          "accuracy": 0.9977272727272727
        }
      },
      "auroc": 0.9459395833333334
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1194,
          "fn": 6,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 1154,
          "fn": 46,
          "accuracy": 0.9616666666666667
        }
      },
      "auroc": 0.9395072916666667
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3393,
          "fn": 7,
          "accuracy": 0.9979411764705882
        },
        "0.01": {
          "tp": 3349,
          "fn": 51,
          "accuracy": 0.985
        }
      },
      "auroc": 0.943669362745098
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2198,
          "fn": 2,
          "accuracy": 0.9990909090909091
        },
        "0.01": {
          "tp": 2190,
          "fn": 10,
          "accuracy": 0.9954545454545455
        }
      },
      "auroc": 0.9454985795454545
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1172,
          "fn": 28,
          "accuracy": 0.9766666666666667
        },
        "0.01": {
          "tp": 1133,
          "fn": 67,
          "accuracy": 0.9441666666666667
        }
      },
      "auroc": 0.9357137152777778
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3370,
          "fn": 30,
          "accuracy": 0.9911764705882353
        },
        "0.01": {
          "tp": 3323,
          "fn": 77,
          "accuracy": 0.9773529411764705
        }
      },
      "auroc": 0.9420450980392157
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4397,
          "fn": 3,
          "accuracy": 0.9993181818181818
        },
        "0.01": {
          "tp": 4385,
          "fn": 15,
          "accuracy": 0.9965909090909091
        }
      },
      "auroc": 0.945719081439394
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2366,
          "fn": 34,
          "accuracy": 0.9858333333333333
        },
        "0.01": {
          "tp": 2287,
          "fn": 113,
          "accuracy": 0.9529166666666666
        }
      },
      "auroc": 0.9376105034722222
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 6763,
          "fn": 37,
          "accuracy": 0.9945588235294117
        },
        "0.01": {
          "tp": 6672,
          "fn": 128,
          "accuracy": 0.9811764705882353
        }
      },
      "auroc": 0.9428572303921569
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2152,
          "fn": 248,
          "accuracy": 0.8966666666666666
        },
        "0.01": {
          "tp": 2017,
          "fn": 383,
          "accuracy": 0.8404166666666667
        }
      },
      "auroc": 0.8993593750000001
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2082,
          "fn": 318,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 1889,
          "fn": 511,
          "accuracy": 0.7870833333333334
        }
      },
      "auroc": 0.8821711805555554
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4234,
          "fn": 566,
          "accuracy": 0.8820833333333333
        },
        "0.01": {
          "tp": 3906,
          "fn": 894,
          "accuracy": 0.81375
        }
      },
      "auroc": 0.8907652777777778
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2127,
          "fn": 273,
          "accuracy": 0.88625
        },
        "0.01": {
          "tp": 1982,
          "fn": 418,
          "accuracy": 0.8258333333333333
        }
      },
      "auroc": 0.8953589409722222
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1807,
          "fn": 593,
          "accuracy": 0.7529166666666667
        },
        "0.01": {
          "tp": 1536,
          "fn": 864,
          "accuracy": 0.64
        }
      },
      "auroc": 0.8142793402777778
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3934,
          "fn": 866,
          "accuracy": 0.8195833333333333
        },
        "0.01": {
          "tp": 3518,
          "fn": 1282,
          "accuracy": 0.7329166666666667
        }
      },
      "auroc": 0.8548191406250001
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4279,
          "fn": 521,
          "accuracy": 0.8914583333333334
        },
        "0.01": {
          "tp": 3999,
          "fn": 801,
          "accuracy": 0.833125
        }
      },
      "auroc": 0.897359157986111
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3889,
          "fn": 911,
          "accuracy": 0.8102083333333333
        },
        "0.01": {
          "tp": 3425,
          "fn": 1375,
          "accuracy": 0.7135416666666666
        }
      },
      "auroc": 0.8482252604166667
    },
    {
      "domain": "recipes",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8168,
          "fn": 1432,
          "accuracy": 0.8508333333333333
        },
        "0.01": {
          "tp": 7424,
          "fn": 2176,
          "accuracy": 0.7733333333333333
        }
      },
      "auroc": 0.872792209201389
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2323,
          "fn": 77,
          "accuracy": 0.9679166666666666
        },
        "0.01": {
          "tp": 2243,
          "fn": 157,
          "accuracy": 0.9345833333333333
        }
      },
      "auroc": 0.9326544270833333
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 298,
          "fn": 2102,
          "accuracy": 0.12416666666666666
        },
        "0.01": {
          "tp": 231,
          "fn": 2169,
          "accuracy": 0.09625
        }
      },
      "auroc": 0.28608897569444447
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2621,
          "fn": 2179,
          "accuracy": 0.5460416666666666
        },
        "0.01": {
          "tp": 2474,
          "fn": 2326,
          "accuracy": 0.5154166666666666
        }
      },
      "auroc": 0.6093717013888889
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1431,
          "fn": 969,
          "accuracy": 0.59625
        },
        "0.01": {
          "tp": 1042,
          "fn": 1358,
          "accuracy": 0.43416666666666665
        }
      },
      "auroc": 0.7870859375000001
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 242,
          "fn": 2158,
          "accuracy": 0.10083333333333333
        },
        "0.01": {
          "tp": 206,
          "fn": 2194,
          "accuracy": 0.08583333333333333
        }
      },
      "auroc": 0.21899618055555556
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1673,
          "fn": 3127,
          "accuracy": 0.3485416666666667
        },
        "0.01": {
          "tp": 1248,
          "fn": 3552,
          "accuracy": 0.26
        }
      },
      "auroc": 0.5030410590277777
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3754,
          "fn": 1046,
          "accuracy": 0.7820833333333334
        },
        "0.01": {
          "tp": 3285,
          "fn": 1515,
          "accuracy": 0.684375
        }
      },
      "auroc": 0.8598701822916667
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 540,
          "fn": 4260,
          "accuracy": 0.1125
        },
        "0.01": {
          "tp": 437,
          "fn": 4363,
          "accuracy": 0.09104166666666667
        }
      },
      "auroc": 0.25254257812500003
    },
    {
      "domain": "recipes",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4294,
          "fn": 5306,
          "accuracy": 0.44729166666666664
        },
        "0.01": {
          "tp": 3722,
          "fn": 5878,
          "accuracy": 0.3877083333333333
        }
      },
      "auroc": 0.5562063802083334
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2005,
          "fn": 395,
          "accuracy": 0.8354166666666667
        },
        "0.01": {
          "tp": 1714,
          "fn": 686,
          "accuracy": 0.7141666666666666
        }
      },
      "auroc": 0.878583767361111
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 2150,
          "accuracy": 0.10416666666666667
        },
        "0.01": {
          "tp": 207,
          "fn": 2193,
          "accuracy": 0.08625
        }
      },
      "auroc": 0.2245551215277778
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2255,
          "fn": 2545,
          "accuracy": 0.46979166666666666
        },
        "0.01": {
          "tp": 1921,
          "fn": 2879,
          "accuracy": 0.40020833333333333
        }
      },
      "auroc": 0.5515694444444444
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1933,
          "fn": 467,
          "accuracy": 0.8054166666666667
        },
        "0.01": {
          "tp": 1585,
          "fn": 815,
          "accuracy": 0.6604166666666667
        }
      },
      "auroc": 0.862215625
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 235,
          "fn": 2165,
          "accuracy": 0.09791666666666667
        },
        "0.01": {
          "tp": 186,
          "fn": 2214,
          "accuracy": 0.0775
        }
      },
      "auroc": 0.21862534722222224
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2168,
          "fn": 2632,
          "accuracy": 0.45166666666666666
        },
        "0.01": {
          "tp": 1771,
          "fn": 3029,
          "accuracy": 0.36895833333333333
        }
      },
      "auroc": 0.5404204861111112
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3938,
          "fn": 862,
          "accuracy": 0.8204166666666667
        },
        "0.01": {
          "tp": 3299,
          "fn": 1501,
          "accuracy": 0.6872916666666666
        }
      },
      "auroc": 0.8703996961805556
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 485,
          "fn": 4315,
          "accuracy": 0.10104166666666667
        },
        "0.01": {
          "tp": 393,
          "fn": 4407,
          "accuracy": 0.081875
        }
      },
      "auroc": 0.22159023437499997
    },
    {
      "domain": "recipes",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4423,
          "fn": 5177,
          "accuracy": 0.4607291666666667
        },
        "0.01": {
          "tp": 3692,
          "fn": 5908,
          "accuracy": 0.38458333333333333
        }
      },
      "auroc": 0.5459949652777778
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2277,
          "fn": 123,
          "accuracy": 0.94875
        },
        "0.01": {
          "tp": 2191,
          "fn": 209,
          "accuracy": 0.9129166666666667
        }
      },
      "auroc": 0.9245034722222221
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1716,
          "fn": 684,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 1425,
          "fn": 975,
          "accuracy": 0.59375
        }
      },
      "auroc": 0.83533125
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3993,
          "fn": 807,
          "accuracy": 0.831875
        },
        "0.01": {
          "tp": 3616,
          "fn": 1184,
          "accuracy": 0.7533333333333333
        }
      },
      "auroc": 0.8799173611111111
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1421,
          "fn": 979,
          "accuracy": 0.5920833333333333
        },
        "0.01": {
          "tp": 1041,
          "fn": 1359,
          "accuracy": 0.43375
        }
      },
      "auroc": 0.7809494791666667
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 264,
          "fn": 2136,
          "accuracy": 0.11
        },
        "0.01": {
          "tp": 223,
          "fn": 2177,
          "accuracy": 0.09291666666666666
        }
      },
      "auroc": 0.32907560763888893
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1685,
          "fn": 3115,
          "accuracy": 0.35104166666666664
        },
        "0.01": {
          "tp": 1264,
          "fn": 3536,
          "accuracy": 0.2633333333333333
        }
      },
      "auroc": 0.5550125434027777
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3698,
          "fn": 1102,
          "accuracy": 0.7704166666666666
        },
        "0.01": {
          "tp": 3232,
          "fn": 1568,
          "accuracy": 0.6733333333333333
        }
      },
      "auroc": 0.8527264756944444
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1980,
          "fn": 2820,
          "accuracy": 0.4125
        },
        "0.01": {
          "tp": 1648,
          "fn": 3152,
          "accuracy": 0.3433333333333333
        }
      },
      "auroc": 0.5822034288194444
    },
    {
      "domain": "recipes",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5678,
          "fn": 3922,
          "accuracy": 0.5914583333333333
        },
        "0.01": {
          "tp": 4880,
          "fn": 4720,
          "accuracy": 0.5083333333333333
        }
      },
      "auroc": 0.7174649522569444
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2304,
          "fn": 96,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 2212,
          "fn": 188,
          "accuracy": 0.9216666666666666
        }
      },
      "auroc": 0.9296556423611109
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 907,
          "fn": 1493,
          "accuracy": 0.3779166666666667
        },
        "0.01": {
          "tp": 571,
          "fn": 1829,
          "accuracy": 0.23791666666666667
        }
      },
      "auroc": 0.7046665798611111
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3211,
          "fn": 1589,
          "accuracy": 0.6689583333333333
        },
        "0.01": {
          "tp": 2783,
          "fn": 2017,
          "accuracy": 0.5797916666666667
        }
      },
      "auroc": 0.817161111111111
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 944,
          "fn": 1456,
          "accuracy": 0.3933333333333333
        },
        "0.01": {
          "tp": 678,
          "fn": 1722,
          "accuracy": 0.2825
        }
      },
      "auroc": 0.7003591145833334
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 230,
          "fn": 2170,
          "accuracy": 0.09583333333333334
        },
        "0.01": {
          "tp": 205,
          "fn": 2195,
          "accuracy": 0.08541666666666667
        }
      },
      "auroc": 0.23109079861111115
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1174,
          "fn": 3626,
          "accuracy": 0.24458333333333335
        },
        "0.01": {
          "tp": 883,
          "fn": 3917,
          "accuracy": 0.18395833333333333
        }
      },
      "auroc": 0.46572495659722224
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3248,
          "fn": 1552,
          "accuracy": 0.6766666666666666
        },
        "0.01": {
          "tp": 2890,
          "fn": 1910,
          "accuracy": 0.6020833333333333
        }
      },
      "auroc": 0.8150073784722223
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1137,
          "fn": 3663,
          "accuracy": 0.236875
        },
        "0.01": {
          "tp": 776,
          "fn": 4024,
          "accuracy": 0.16166666666666665
        }
      },
      "auroc": 0.46787868923611114
    },
    {
      "domain": "recipes",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4385,
          "fn": 5215,
          "accuracy": 0.45677083333333335
        },
        "0.01": {
          "tp": 3666,
          "fn": 5934,
          "accuracy": 0.381875
        }
      },
      "auroc": 0.6414430338541667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1995,
          "fn": 405,
          "accuracy": 0.83125
        },
        "0.01": {
          "tp": 1745,
          "fn": 655,
          "accuracy": 0.7270833333333333
        }
      },
      "auroc": 0.8719519097222223
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1768,
          "fn": 632,
          "accuracy": 0.7366666666666667
        },
        "0.01": {
          "tp": 1352,
          "fn": 1048,
          "accuracy": 0.5633333333333334
        }
      },
      "auroc": 0.8283872395833334
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3763,
          "fn": 1037,
          "accuracy": 0.7839583333333333
        },
        "0.01": {
          "tp": 3097,
          "fn": 1703,
          "accuracy": 0.6452083333333334
        }
      },
      "auroc": 0.8501695746527778
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1930,
          "fn": 470,
          "accuracy": 0.8041666666666667
        },
        "0.01": {
          "tp": 1659,
          "fn": 741,
          "accuracy": 0.69125
        }
      },
      "auroc": 0.860495486111111
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 731,
          "fn": 1669,
          "accuracy": 0.3045833333333333
        },
        "0.01": {
          "tp": 479,
          "fn": 1921,
          "accuracy": 0.19958333333333333
        }
      },
      "auroc": 0.5926315104166667
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2661,
          "fn": 2139,
          "accuracy": 0.554375
        },
        "0.01": {
          "tp": 2138,
          "fn": 2662,
          "accuracy": 0.4454166666666667
        }
      },
      "auroc": 0.7265634982638889
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3925,
          "fn": 875,
          "accuracy": 0.8177083333333334
        },
        "0.01": {
          "tp": 3404,
          "fn": 1396,
          "accuracy": 0.7091666666666666
        }
      },
      "auroc": 0.8662236979166666
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2499,
          "fn": 2301,
          "accuracy": 0.520625
        },
        "0.01": {
          "tp": 1831,
          "fn": 2969,
          "accuracy": 0.38145833333333334
        }
      },
      "auroc": 0.710509375
    },
    {
      "domain": "recipes",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 6424,
          "fn": 3176,
          "accuracy": 0.6691666666666667
        },
        "0.01": {
          "tp": 5235,
          "fn": 4365,
          "accuracy": 0.5453125
        }
      },
      "auroc": 0.7883665364583333
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1836,
          "fn": 564,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 1550,
          "fn": 850,
          "accuracy": 0.6458333333333334
        }
      },
      "auroc": 0.8532131944444444
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1836,
          "fn": 564,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 1550,
          "fn": 850,
          "accuracy": 0.6458333333333334
        }
      },
      "auroc": 0.8532131944444444
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1765,
          "fn": 635,
          "accuracy": 0.7354166666666667
        },
        "0.01": {
          "tp": 1424,
          "fn": 976,
          "accuracy": 0.5933333333333334
        }
      },
      "auroc": 0.8374515625
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1765,
          "fn": 635,
          "accuracy": 0.7354166666666667
        },
        "0.01": {
          "tp": 1424,
          "fn": 976,
          "accuracy": 0.5933333333333334
        }
      },
      "auroc": 0.8374515625
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3601,
          "fn": 1199,
          "accuracy": 0.7502083333333334
        },
        "0.01": {
          "tp": 2974,
          "fn": 1826,
          "accuracy": 0.6195833333333334
        }
      },
      "auroc": 0.8453323784722222
    },
    {
      "domain": "recipes",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3601,
          "fn": 1199,
          "accuracy": 0.7502083333333334
        },
        "0.01": {
          "tp": 2974,
          "fn": 1826,
          "accuracy": 0.6195833333333334
        }
      },
      "auroc": 0.8453323784722222
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1643,
          "fn": 757,
          "accuracy": 0.6845833333333333
        },
        "0.01": {
          "tp": 1266,
          "fn": 1134,
          "accuracy": 0.5275
        }
      },
      "auroc": 0.8171799479166667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1643,
          "fn": 757,
          "accuracy": 0.6845833333333333
        },
        "0.01": {
          "tp": 1266,
          "fn": 1134,
          "accuracy": 0.5275
        }
      },
      "auroc": 0.8171799479166667
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1550,
          "fn": 850,
          "accuracy": 0.6458333333333334
        },
        "0.01": {
          "tp": 1192,
          "fn": 1208,
          "accuracy": 0.49666666666666665
        }
      },
      "auroc": 0.7985348958333334
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1550,
          "fn": 850,
          "accuracy": 0.6458333333333334
        },
        "0.01": {
          "tp": 1192,
          "fn": 1208,
          "accuracy": 0.49666666666666665
        }
      },
      "auroc": 0.7985348958333334
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3193,
          "fn": 1607,
          "accuracy": 0.6652083333333333
        },
        "0.01": {
          "tp": 2458,
          "fn": 2342,
          "accuracy": 0.5120833333333333
        }
      },
      "auroc": 0.8078574218749999
    },
    {
      "domain": "recipes",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3193,
          "fn": 1607,
          "accuracy": 0.6652083333333333
        },
        "0.01": {
          "tp": 2458,
          "fn": 2342,
          "accuracy": 0.5120833333333333
        }
      },
      "auroc": 0.8078574218749999
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2101,
          "fn": 299,
          "accuracy": 0.8754166666666666
        },
        "0.01": {
          "tp": 1873,
          "fn": 527,
          "accuracy": 0.7804166666666666
        }
      },
      "auroc": 0.8855546874999999
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2101,
          "fn": 299,
          "accuracy": 0.8754166666666666
        },
        "0.01": {
          "tp": 1873,
          "fn": 527,
          "accuracy": 0.7804166666666666
        }
      },
      "auroc": 0.8855546874999999
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2006,
          "fn": 394,
          "accuracy": 0.8358333333333333
        },
        "0.01": {
          "tp": 1716,
          "fn": 684,
          "accuracy": 0.715
        }
      },
      "auroc": 0.8657067708333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2006,
          "fn": 394,
          "accuracy": 0.8358333333333333
        },
        "0.01": {
          "tp": 1716,
          "fn": 684,
          "accuracy": 0.715
        }
      },
      "auroc": 0.8657067708333333
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4107,
          "fn": 693,
          "accuracy": 0.855625
        },
        "0.01": {
          "tp": 3589,
          "fn": 1211,
          "accuracy": 0.7477083333333333
        }
      },
      "auroc": 0.8756307291666666
    },
    {
      "domain": "recipes",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4107,
          "fn": 693,
          "accuracy": 0.855625
        },
        "0.01": {
          "tp": 3589,
          "fn": 1211,
          "accuracy": 0.7477083333333333
        }
      },
      "auroc": 0.8756307291666666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2140,
          "fn": 260,
          "accuracy": 0.8916666666666667
        },
        "0.01": {
          "tp": 1982,
          "fn": 418,
          "accuracy": 0.8258333333333333
        }
      },
      "auroc": 0.9010111979166666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2140,
          "fn": 260,
          "accuracy": 0.8916666666666667
        },
        "0.01": {
          "tp": 1982,
          "fn": 418,
          "accuracy": 0.8258333333333333
        }
      },
      "auroc": 0.9010111979166666
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1835,
          "fn": 565,
          "accuracy": 0.7645833333333333
        },
        "0.01": {
          "tp": 1456,
          "fn": 944,
          "accuracy": 0.6066666666666667
        }
      },
      "auroc": 0.8450101562500001
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1835,
          "fn": 565,
          "accuracy": 0.7645833333333333
        },
        "0.01": {
          "tp": 1456,
          "fn": 944,
          "accuracy": 0.6066666666666667
        }
      },
      "auroc": 0.8450101562500001
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3975,
          "fn": 825,
          "accuracy": 0.828125
        },
        "0.01": {
          "tp": 3438,
          "fn": 1362,
          "accuracy": 0.71625
        }
      },
      "auroc": 0.8730106770833335
    },
    {
      "domain": "recipes",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3975,
          "fn": 825,
          "accuracy": 0.828125
        },
        "0.01": {
          "tp": 3438,
          "fn": 1362,
          "accuracy": 0.71625
        }
      },
      "auroc": 0.8730106770833335
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1878,
          "fn": 522,
          "accuracy": 0.7825
        },
        "0.01": {
          "tp": 1575,
          "fn": 825,
          "accuracy": 0.65625
        }
      },
      "auroc": 0.8579328125000001
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1878,
          "fn": 522,
          "accuracy": 0.7825
        },
        "0.01": {
          "tp": 1575,
          "fn": 825,
          "accuracy": 0.65625
        }
      },
      "auroc": 0.8579328125000001
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1815,
          "fn": 585,
          "accuracy": 0.75625
        },
        "0.01": {
          "tp": 1499,
          "fn": 901,
          "accuracy": 0.6245833333333334
        }
      },
      "auroc": 0.8453634548611111
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1815,
          "fn": 585,
          "accuracy": 0.75625
        },
        "0.01": {
          "tp": 1499,
          "fn": 901,
          "accuracy": 0.6245833333333334
        }
      },
      "auroc": 0.8453634548611111
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3693,
          "fn": 1107,
          "accuracy": 0.769375
        },
        "0.01": {
          "tp": 3074,
          "fn": 1726,
          "accuracy": 0.6404166666666666
        }
      },
      "auroc": 0.8516481336805557
    },
    {
      "domain": "recipes",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3693,
          "fn": 1107,
          "accuracy": 0.769375
        },
        "0.01": {
          "tp": 3074,
          "fn": 1726,
          "accuracy": 0.6404166666666666
        }
      },
      "auroc": 0.8516481336805557
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 22654,
          "fn": 3746,
          "accuracy": 0.8581060606060606
        },
        "0.01": {
          "tp": 20368,
          "fn": 6032,
          "accuracy": 0.7715151515151515
        }
      },
      "auroc": 0.8865091303661615
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7021,
          "fn": 7379,
          "accuracy": 0.48756944444444444
        },
        "0.01": {
          "tp": 5675,
          "fn": 8725,
          "accuracy": 0.3940972222222222
        }
      },
      "auroc": 0.6268667245370371
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 29675,
          "fn": 11125,
          "accuracy": 0.727328431372549
        },
        "0.01": {
          "tp": 26043,
          "fn": 14757,
          "accuracy": 0.6383088235294118
        }
      },
      "auroc": 0.7948706341911764
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18757,
          "fn": 7643,
          "accuracy": 0.7104924242424242
        },
        "0.01": {
          "tp": 15274,
          "fn": 11126,
          "accuracy": 0.5785606060606061
        }
      },
      "auroc": 0.8253210385101011
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3509,
          "fn": 10891,
          "accuracy": 0.24368055555555557
        },
        "0.01": {
          "tp": 2835,
          "fn": 11565,
          "accuracy": 0.196875
        }
      },
      "auroc": 0.40078313078703703
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 22266,
          "fn": 18534,
          "accuracy": 0.5457352941176471
        },
        "0.01": {
          "tp": 18109,
          "fn": 22691,
          "accuracy": 0.4438480392156863
        }
      },
      "auroc": 0.6754841299019607
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 41411,
          "fn": 11389,
          "accuracy": 0.7842992424242424
        },
        "0.01": {
          "tp": 35642,
          "fn": 17158,
          "accuracy": 0.6750378787878788
        }
      },
      "auroc": 0.8559150844381314
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 10530,
          "fn": 18270,
          "accuracy": 0.365625
        },
        "0.01": {
          "tp": 8510,
          "fn": 20290,
          "accuracy": 0.2954861111111111
        }
      },
      "auroc": 0.513824927662037
    },
    {
      "domain": "recipes",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 51941,
          "fn": 29659,
          "accuracy": 0.636531862745098
        },
        "0.01": {
          "tp": 44152,
          "fn": 37448,
          "accuracy": 0.5410784313725491
        }
      },
      "auroc": 0.7351773820465687
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9466822916666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9413572916666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9440197916666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9479604166666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.8060958333333332
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.8770281249999999
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9473213541666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        }
      },
      "auroc": 0.8737265625
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 756,
          "fn": 44,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 730,
          "fn": 70,
          "accuracy": 0.9125
        }
      },
      "auroc": 0.9105239583333332
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9445791666666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.131946875
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.5382630208333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9026406250000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.053438541666666665
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 156,
          "fn": 244,
          "accuracy": 0.39
        }
      },
      "auroc": 0.47803958333333324
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9236098958333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.09269270833333332
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 412,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 357,
          "fn": 443,
          "accuracy": 0.44625
        }
      },
      "auroc": 0.5081513020833335
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.8765468750000001
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.7498885416666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        },
        "0.01": {
          "tp": 166,
          "fn": 234,
          "accuracy": 0.415
        }
      },
      "auroc": 0.8132177083333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        }
      },
      "auroc": 0.862459375
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.5098364583333332
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        },
        "0.01": {
          "tp": 128,
          "fn": 272,
          "accuracy": 0.32
        }
      },
      "auroc": 0.6861479166666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        }
      },
      "auroc": 0.8695031249999999
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        },
        "0.01": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        }
      },
      "auroc": 0.6298625000000001
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 460,
          "fn": 340,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 294,
          "fn": 506,
          "accuracy": 0.3675
        }
      },
      "auroc": 0.7496828125
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9428874999999999
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9480458333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9454666666666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9372895833333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.377625
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        }
      },
      "auroc": 0.6574572916666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9400885416666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        }
      },
      "auroc": 0.6628354166666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 599,
          "fn": 201,
          "accuracy": 0.74875
        },
        "0.01": {
          "tp": 580,
          "fn": 220,
          "accuracy": 0.725
        }
      },
      "auroc": 0.8014619791666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.94235
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        }
      },
      "auroc": 0.6346885416666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        }
      },
      "auroc": 0.7885192708333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.875096875
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.05482187499999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        },
        "0.01": {
          "tp": 123,
          "fn": 277,
          "accuracy": 0.3075
        }
      },
      "auroc": 0.46495937499999995
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        }
      },
      "auroc": 0.9087234375
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 301,
          "accuracy": 0.2475
        },
        "0.01": {
          "tp": 64,
          "fn": 336,
          "accuracy": 0.16
        }
      },
      "auroc": 0.34475520833333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 463,
          "fn": 337,
          "accuracy": 0.57875
        },
        "0.01": {
          "tp": 383,
          "fn": 417,
          "accuracy": 0.47875
        }
      },
      "auroc": 0.6267393229166667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.931853125
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9145135416666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9231833333333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9052958333333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.6397822916666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 181,
          "fn": 219,
          "accuracy": 0.4525
        }
      },
      "auroc": 0.7725390625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9185744791666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.7771479166666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 627,
          "fn": 173,
          "accuracy": 0.78375
        },
        "0.01": {
          "tp": 537,
          "fn": 263,
          "accuracy": 0.67125
        }
      },
      "auroc": 0.8478611979166667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9036989583333332
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9036989583333332
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9055072916666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9055072916666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        }
      },
      "auroc": 0.904603125
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        }
      },
      "auroc": 0.904603125
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        }
      },
      "auroc": 0.8548927083333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        }
      },
      "auroc": 0.8548927083333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.8076541666666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.8076541666666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8312734374999999
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 281,
          "fn": 119,
          "accuracy": 0.7025
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8312734374999999
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9188166666666666
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9188166666666666
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.8934625
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.8934625
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        }
      },
      "auroc": 0.9061395833333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 303,
          "fn": 97,
          "accuracy": 0.7575
        }
      },
      "auroc": 0.9061395833333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9439229166666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9439229166666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        },
        "0.01": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        }
      },
      "auroc": 0.7359010416666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        },
        "0.01": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        }
      },
      "auroc": 0.7359010416666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        },
        "0.01": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        }
      },
      "auroc": 0.8399119791666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        },
        "0.01": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        }
      },
      "auroc": 0.8399119791666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        }
      },
      "auroc": 0.8357145833333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        }
      },
      "auroc": 0.8357145833333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.7432489583333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 81,
          "fn": 119,
          "accuracy": 0.405
        }
      },
      "auroc": 0.7432489583333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        },
        "0.01": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        }
      },
      "auroc": 0.7894817708333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        },
        "0.01": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        }
      },
      "auroc": 0.7894817708333333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2038,
          "fn": 162,
          "accuracy": 0.9263636363636364
        },
        "0.01": {
          "tp": 1856,
          "fn": 344,
          "accuracy": 0.8436363636363636
        }
      },
      "auroc": 0.9129040719696969
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 784,
          "fn": 416,
          "accuracy": 0.6533333333333333
        },
        "0.01": {
          "tp": 667,
          "fn": 533,
          "accuracy": 0.5558333333333333
        }
      },
      "auroc": 0.7200734374999999
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2822,
          "fn": 578,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 2523,
          "fn": 877,
          "accuracy": 0.7420588235294118
        }
      },
      "auroc": 0.8448462009803923
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1766,
          "fn": 434,
          "accuracy": 0.8027272727272727
        },
        "0.01": {
          "tp": 1453,
          "fn": 747,
          "accuracy": 0.6604545454545454
        }
      },
      "auroc": 0.8651378787878787
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 253,
          "fn": 947,
          "accuracy": 0.21083333333333334
        },
        "0.01": {
          "tp": 173,
          "fn": 1027,
          "accuracy": 0.14416666666666667
        }
      },
      "auroc": 0.4069333333333333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2019,
          "fn": 1381,
          "accuracy": 0.5938235294117648
        },
        "0.01": {
          "tp": 1626,
          "fn": 1774,
          "accuracy": 0.47823529411764704
        }
      },
      "auroc": 0.7034186274509804
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3804,
          "fn": 596,
          "accuracy": 0.8645454545454545
        },
        "0.01": {
          "tp": 3309,
          "fn": 1091,
          "accuracy": 0.7520454545454546
        }
      },
      "auroc": 0.8890209753787879
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1037,
          "fn": 1363,
          "accuracy": 0.4320833333333333
        },
        "0.01": {
          "tp": 840,
          "fn": 1560,
          "accuracy": 0.35
        }
      },
      "auroc": 0.5635033854166667
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 4841,
          "fn": 1959,
          "accuracy": 0.7119117647058824
        },
        "0.01": {
          "tp": 4149,
          "fn": 2651,
          "accuracy": 0.6101470588235294
        }
      },
      "auroc": 0.7741324142156862
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9400510416666668
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9083197916666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        }
      },
      "auroc": 0.9241854166666668
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9418385416666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.7409291666666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        }
      },
      "auroc": 0.8413838541666668
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9409447916666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        }
      },
      "auroc": 0.8246244791666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 710,
          "fn": 90,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 632,
          "fn": 168,
          "accuracy": 0.79
        }
      },
      "auroc": 0.8827846354166666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9258760416666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.096753125
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        }
      },
      "auroc": 0.5113145833333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        }
      },
      "auroc": 0.8594614583333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.051569791666666656
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 237,
          "accuracy": 0.4075
        },
        "0.01": {
          "tp": 109,
          "fn": 291,
          "accuracy": 0.2725
        }
      },
      "auroc": 0.4555156250000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        }
      },
      "auroc": 0.89266875
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.07416145833333332
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 441,
          "accuracy": 0.44875
        },
        "0.01": {
          "tp": 293,
          "fn": 507,
          "accuracy": 0.36625
        }
      },
      "auroc": 0.4834151041666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        }
      },
      "auroc": 0.7726479166666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        },
        "0.01": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        }
      },
      "auroc": 0.5844427083333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 256,
          "accuracy": 0.36
        },
        "0.01": {
          "tp": 66,
          "fn": 334,
          "accuracy": 0.165
        }
      },
      "auroc": 0.6785453124999999
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        }
      },
      "auroc": 0.7419781249999999
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.35890625000000004
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 295,
          "accuracy": 0.2625
        },
        "0.01": {
          "tp": 53,
          "fn": 347,
          "accuracy": 0.1325
        }
      },
      "auroc": 0.5504421875
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        },
        "0.01": {
          "tp": 111,
          "fn": 289,
          "accuracy": 0.2775
        }
      },
      "auroc": 0.7573130208333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 362,
          "accuracy": 0.095
        },
        "0.01": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        }
      },
      "auroc": 0.4716744791666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 551,
          "accuracy": 0.31125
        },
        "0.01": {
          "tp": 119,
          "fn": 681,
          "accuracy": 0.14875
        }
      },
      "auroc": 0.61449375
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.8913208333333335
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9457208333333332
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        }
      },
      "auroc": 0.9185208333333335
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9205281249999999
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.36062812499999997
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        }
      },
      "auroc": 0.6405781249999999
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9059244791666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        }
      },
      "auroc": 0.6531744791666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 570,
          "fn": 230,
          "accuracy": 0.7125
        },
        "0.01": {
          "tp": 514,
          "fn": 286,
          "accuracy": 0.6425
        }
      },
      "auroc": 0.7795494791666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9286208333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        },
        "0.01": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        }
      },
      "auroc": 0.5484520833333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        }
      },
      "auroc": 0.7385364583333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        }
      },
      "auroc": 0.8256749999999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.055698958333333326
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 261,
          "accuracy": 0.3475
        },
        "0.01": {
          "tp": 90,
          "fn": 310,
          "accuracy": 0.225
        }
      },
      "auroc": 0.44068697916666655
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": {
          "tp": 273,
          "fn": 127,
          "accuracy": 0.6825
        }
      },
      "auroc": 0.8771479166666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 66,
          "fn": 334,
          "accuracy": 0.165
        },
        "0.01": {
          "tp": 41,
          "fn": 359,
          "accuracy": 0.1025
        }
      },
      "auroc": 0.30207552083333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 401,
          "accuracy": 0.49875
        },
        "0.01": {
          "tp": 314,
          "fn": 486,
          "accuracy": 0.3925
        }
      },
      "auroc": 0.5896117187500001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9024052083333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.8525895833333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        },
        "0.01": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        }
      },
      "auroc": 0.8774973958333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.8546041666666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.5126625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        },
        "0.01": {
          "tp": 126,
          "fn": 274,
          "accuracy": 0.315
        }
      },
      "auroc": 0.6836333333333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        }
      },
      "auroc": 0.8785046875
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 236,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 122,
          "fn": 278,
          "accuracy": 0.305
        }
      },
      "auroc": 0.6826260416666665
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 502,
          "fn": 298,
          "accuracy": 0.6275
        },
        "0.01": {
          "tp": 406,
          "fn": 394,
          "accuracy": 0.5075
        }
      },
      "auroc": 0.7805653645833333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.86051875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.86051875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.8545333333333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        }
      },
      "auroc": 0.8545333333333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        }
      },
      "auroc": 0.8575260416666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        }
      },
      "auroc": 0.8575260416666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        }
      },
      "auroc": 0.7836645833333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        }
      },
      "auroc": 0.7836645833333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.7393854166666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.7393854166666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 132,
          "fn": 268,
          "accuracy": 0.33
        }
      },
      "auroc": 0.761525
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 132,
          "fn": 268,
          "accuracy": 0.33
        }
      },
      "auroc": 0.761525
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.8509052083333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.8509052083333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.805021875
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.805021875
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        },
        "0.01": {
          "tp": 178,
          "fn": 222,
          "accuracy": 0.445
        }
      },
      "auroc": 0.8279635416666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        },
        "0.01": {
          "tp": 178,
          "fn": 222,
          "accuracy": 0.445
        }
      },
      "auroc": 0.8279635416666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9337666666666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9337666666666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.6622364583333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.6622364583333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.7980015625
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.7980015625
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        }
      },
      "auroc": 0.7588708333333332
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        }
      },
      "auroc": 0.7588708333333332
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        }
      },
      "auroc": 0.6673895833333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        }
      },
      "auroc": 0.6673895833333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 120,
          "fn": 280,
          "accuracy": 0.3
        }
      },
      "auroc": 0.7131302083333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 120,
          "fn": 280,
          "accuracy": 0.3
        }
      },
      "auroc": 0.7131302083333333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1780,
          "fn": 420,
          "accuracy": 0.8090909090909091
        },
        "0.01": {
          "tp": 1481,
          "fn": 719,
          "accuracy": 0.6731818181818182
        }
      },
      "auroc": 0.8680589015151514
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 635,
          "fn": 565,
          "accuracy": 0.5291666666666667
        },
        "0.01": {
          "tp": 520,
          "fn": 680,
          "accuracy": 0.43333333333333335
        }
      },
      "auroc": 0.6560463541666665
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2415,
          "fn": 985,
          "accuracy": 0.7102941176470589
        },
        "0.01": {
          "tp": 2001,
          "fn": 1399,
          "accuracy": 0.5885294117647059
        }
      },
      "auroc": 0.793230943627451
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1469,
          "fn": 731,
          "accuracy": 0.6677272727272727
        },
        "0.01": {
          "tp": 1069,
          "fn": 1131,
          "accuracy": 0.4859090909090909
        }
      },
      "auroc": 0.8066047348484848
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 1045,
          "accuracy": 0.12916666666666668
        },
        "0.01": {
          "tp": 99,
          "fn": 1101,
          "accuracy": 0.0825
        }
      },
      "auroc": 0.3467324652777778
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1624,
          "fn": 1776,
          "accuracy": 0.4776470588235294
        },
        "0.01": {
          "tp": 1168,
          "fn": 2232,
          "accuracy": 0.34352941176470586
        }
      },
      "auroc": 0.644296875
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3249,
          "fn": 1151,
          "accuracy": 0.7384090909090909
        },
        "0.01": {
          "tp": 2550,
          "fn": 1850,
          "accuracy": 0.5795454545454546
        }
      },
      "auroc": 0.8373318181818181
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 790,
          "fn": 1610,
          "accuracy": 0.32916666666666666
        },
        "0.01": {
          "tp": 619,
          "fn": 1781,
          "accuracy": 0.2579166666666667
        }
      },
      "auroc": 0.5013894097222222
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 4039,
          "fn": 2761,
          "accuracy": 0.5939705882352941
        },
        "0.01": {
          "tp": 3169,
          "fn": 3631,
          "accuracy": 0.46602941176470586
        }
      },
      "auroc": 0.7187639093137255
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.93875
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        }
      },
      "auroc": 0.8931666666666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        }
      },
      "auroc": 0.9159583333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.94045
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        }
      },
      "auroc": 0.7147302083333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        },
        "0.01": {
          "tp": 271,
          "fn": 129,
          "accuracy": 0.6775
        }
      },
      "auroc": 0.8275901041666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9396
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 291,
          "fn": 109,
          "accuracy": 0.7275
        },
        "0.01": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        }
      },
      "auroc": 0.8039484375
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 686,
          "fn": 114,
          "accuracy": 0.8575
        },
        "0.01": {
          "tp": 608,
          "fn": 192,
          "accuracy": 0.76
        }
      },
      "auroc": 0.8717742187499999
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9267135416666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.10177499999999999
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        }
      },
      "auroc": 0.5142442708333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        }
      },
      "auroc": 0.8293125
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.05311354166666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 259,
          "accuracy": 0.3525
        },
        "0.01": {
          "tp": 84,
          "fn": 316,
          "accuracy": 0.21
        }
      },
      "auroc": 0.44121302083333336
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8780130208333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.07744427083333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 465,
          "accuracy": 0.41875
        },
        "0.01": {
          "tp": 263,
          "fn": 537,
          "accuracy": 0.32875
        }
      },
      "auroc": 0.4777286458333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.7808166666666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.5765375
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 262,
          "accuracy": 0.345
        },
        "0.01": {
          "tp": 66,
          "fn": 334,
          "accuracy": 0.165
        }
      },
      "auroc": 0.6786770833333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.764171875
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.353753125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 60,
          "fn": 340,
          "accuracy": 0.15
        }
      },
      "auroc": 0.5589624999999999
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 119,
          "fn": 281,
          "accuracy": 0.2975
        }
      },
      "auroc": 0.7724942708333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 366,
          "accuracy": 0.085
        },
        "0.01": {
          "tp": 7,
          "fn": 393,
          "accuracy": 0.0175
        }
      },
      "auroc": 0.4651453125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 252,
          "fn": 548,
          "accuracy": 0.315
        },
        "0.01": {
          "tp": 126,
          "fn": 674,
          "accuracy": 0.1575
        }
      },
      "auroc": 0.6188197916666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9105593750000001
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9469854166666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9287723958333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9191541666666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.288778125
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        },
        "0.01": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        }
      },
      "auroc": 0.6039661458333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9148567708333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.6178817708333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 574,
          "fn": 226,
          "accuracy": 0.7175
        },
        "0.01": {
          "tp": 531,
          "fn": 269,
          "accuracy": 0.66375
        }
      },
      "auroc": 0.7663692708333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9245656250000001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.4953010416666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.7099333333333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        }
      },
      "auroc": 0.7789656250000001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.04844270833333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 60,
          "fn": 340,
          "accuracy": 0.15
        }
      },
      "auroc": 0.41370416666666665
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        }
      },
      "auroc": 0.851765625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 352,
          "accuracy": 0.12
        },
        "0.01": {
          "tp": 28,
          "fn": 372,
          "accuracy": 0.07
        }
      },
      "auroc": 0.271871875
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 447,
          "accuracy": 0.44125
        },
        "0.01": {
          "tp": 268,
          "fn": 532,
          "accuracy": 0.335
        }
      },
      "auroc": 0.5618187499999999
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9052760416666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.8364177083333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        }
      },
      "auroc": 0.8708468750000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8487052083333332
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.439825
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 244,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 118,
          "fn": 282,
          "accuracy": 0.295
        }
      },
      "auroc": 0.6442651041666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        },
        "0.01": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        }
      },
      "auroc": 0.876990625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 250,
          "accuracy": 0.375
        },
        "0.01": {
          "tp": 95,
          "fn": 305,
          "accuracy": 0.2375
        }
      },
      "auroc": 0.6381213541666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 479,
          "fn": 321,
          "accuracy": 0.59875
        },
        "0.01": {
          "tp": 375,
          "fn": 425,
          "accuracy": 0.46875
        }
      },
      "auroc": 0.7575559895833335
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.8742510416666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.8742510416666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.8725822916666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.8725822916666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        },
        "0.01": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        }
      },
      "auroc": 0.8734166666666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        },
        "0.01": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        }
      },
      "auroc": 0.8734166666666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.762115625
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        }
      },
      "auroc": 0.762115625
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        },
        "0.01": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        }
      },
      "auroc": 0.6986552083333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        },
        "0.01": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        }
      },
      "auroc": 0.6986552083333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        },
        "0.01": {
          "tp": 110,
          "fn": 290,
          "accuracy": 0.275
        }
      },
      "auroc": 0.7303854166666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        },
        "0.01": {
          "tp": 110,
          "fn": 290,
          "accuracy": 0.275
        }
      },
      "auroc": 0.7303854166666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        }
      },
      "auroc": 0.8391041666666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        }
      },
      "auroc": 0.8391041666666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.7805239583333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.7805239583333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 144,
          "fn": 256,
          "accuracy": 0.36
        }
      },
      "auroc": 0.8098140625
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 144,
          "fn": 256,
          "accuracy": 0.36
        }
      },
      "auroc": 0.8098140625
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9203958333333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9203958333333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.5648427083333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.5648427083333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 181,
          "fn": 219,
          "accuracy": 0.4525
        }
      },
      "auroc": 0.7426192708333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 181,
          "fn": 219,
          "accuracy": 0.4525
        }
      },
      "auroc": 0.7426192708333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.7248979166666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.7248979166666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        }
      },
      "auroc": 0.6319770833333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        }
      },
      "auroc": 0.6319770833333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        },
        "0.01": {
          "tp": 98,
          "fn": 302,
          "accuracy": 0.245
        }
      },
      "auroc": 0.6784375
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        },
        "0.01": {
          "tp": 98,
          "fn": 302,
          "accuracy": 0.245
        }
      },
      "auroc": 0.6784375
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1765,
          "fn": 435,
          "accuracy": 0.8022727272727272
        },
        "0.01": {
          "tp": 1450,
          "fn": 750,
          "accuracy": 0.6590909090909091
        }
      },
      "auroc": 0.8643132575757577
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 588,
          "fn": 612,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 471,
          "fn": 729,
          "accuracy": 0.3925
        }
      },
      "auroc": 0.6416972222222223
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2353,
          "fn": 1047,
          "accuracy": 0.6920588235294117
        },
        "0.01": {
          "tp": 1921,
          "fn": 1479,
          "accuracy": 0.565
        }
      },
      "auroc": 0.7857428921568628
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1356,
          "fn": 844,
          "accuracy": 0.6163636363636363
        },
        "0.01": {
          "tp": 973,
          "fn": 1227,
          "accuracy": 0.44227272727272726
        }
      },
      "auroc": 0.7844855113636364
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 1061,
          "accuracy": 0.11583333333333333
        },
        "0.01": {
          "tp": 90,
          "fn": 1110,
          "accuracy": 0.075
        }
      },
      "auroc": 0.3164404513888889
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1495,
          "fn": 1905,
          "accuracy": 0.43970588235294117
        },
        "0.01": {
          "tp": 1063,
          "fn": 2337,
          "accuracy": 0.3126470588235294
        }
      },
      "auroc": 0.6192931372549019
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3121,
          "fn": 1279,
          "accuracy": 0.7093181818181818
        },
        "0.01": {
          "tp": 2423,
          "fn": 1977,
          "accuracy": 0.5506818181818182
        }
      },
      "auroc": 0.8243993844696968
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 727,
          "fn": 1673,
          "accuracy": 0.30291666666666667
        },
        "0.01": {
          "tp": 561,
          "fn": 1839,
          "accuracy": 0.23375
        }
      },
      "auroc": 0.4790688368055555
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3848,
          "fn": 2952,
          "accuracy": 0.5658823529411765
        },
        "0.01": {
          "tp": 2984,
          "fn": 3816,
          "accuracy": 0.4388235294117647
        }
      },
      "auroc": 0.7025180147058823
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        }
      },
      "auroc": 0.867346875
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        },
        "0.01": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        }
      },
      "auroc": 0.67143125
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        },
        "0.01": {
          "tp": 145,
          "fn": 255,
          "accuracy": 0.3625
        }
      },
      "auroc": 0.7693890625
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8576250000000001
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.46308541666666664
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 120,
          "fn": 280,
          "accuracy": 0.3
        }
      },
      "auroc": 0.6603552083333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 319,
          "fn": 81,
          "accuracy": 0.7975
        },
        "0.01": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        }
      },
      "auroc": 0.8624859375
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 302,
          "accuracy": 0.245
        },
        "0.01": {
          "tp": 41,
          "fn": 359,
          "accuracy": 0.1025
        }
      },
      "auroc": 0.5672583333333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 417,
          "fn": 383,
          "accuracy": 0.52125
        },
        "0.01": {
          "tp": 265,
          "fn": 535,
          "accuracy": 0.33125
        }
      },
      "auroc": 0.7148721354166665
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9403250000000001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.06987916666666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": {
          "tp": 192,
          "fn": 208,
          "accuracy": 0.48
        }
      },
      "auroc": 0.5051020833333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.6506927083333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.04751666666666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 343,
          "accuracy": 0.1425
        },
        "0.01": {
          "tp": 23,
          "fn": 377,
          "accuracy": 0.0575
        }
      },
      "auroc": 0.3491046875
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        }
      },
      "auroc": 0.7955088541666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.058697916666666655
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 256,
          "fn": 544,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 215,
          "fn": 585,
          "accuracy": 0.26875
        }
      },
      "auroc": 0.4271033854166667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.6248614583333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.35661458333333335
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        },
        "0.01": {
          "tp": 16,
          "fn": 384,
          "accuracy": 0.04
        }
      },
      "auroc": 0.49073802083333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.6074260416666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.23547604166666664
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 41,
          "fn": 359,
          "accuracy": 0.1025
        },
        "0.01": {
          "tp": 12,
          "fn": 388,
          "accuracy": 0.03
        }
      },
      "auroc": 0.42145104166666664
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 83,
          "fn": 317,
          "accuracy": 0.2075
        },
        "0.01": {
          "tp": 25,
          "fn": 375,
          "accuracy": 0.0625
        }
      },
      "auroc": 0.61614375
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.2960453125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 91,
          "fn": 709,
          "accuracy": 0.11375
        },
        "0.01": {
          "tp": 28,
          "fn": 772,
          "accuracy": 0.035
        }
      },
      "auroc": 0.45609453125000005
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.938290625
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9164052083333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9273479166666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        }
      },
      "auroc": 0.8055291666666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.10564791666666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 267,
          "accuracy": 0.3325
        },
        "0.01": {
          "tp": 74,
          "fn": 326,
          "accuracy": 0.185
        }
      },
      "auroc": 0.4555885416666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        }
      },
      "auroc": 0.8719098958333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        },
        "0.01": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        }
      },
      "auroc": 0.5110265625
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 520,
          "fn": 280,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 432,
          "fn": 368,
          "accuracy": 0.54
        }
      },
      "auroc": 0.6914682291666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.939915625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.23080729166666664
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.5853614583333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        },
        "0.01": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        }
      },
      "auroc": 0.6182531250000001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.042273958333333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        },
        "0.01": {
          "tp": 20,
          "fn": 380,
          "accuracy": 0.05
        }
      },
      "auroc": 0.3302635416666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.7790843749999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 388,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.136540625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 542,
          "accuracy": 0.3225
        },
        "0.01": {
          "tp": 211,
          "fn": 589,
          "accuracy": 0.26375
        }
      },
      "auroc": 0.4578125
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        }
      },
      "auroc": 0.8297249999999999
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.5664802083333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 235,
          "accuracy": 0.4125
        },
        "0.01": {
          "tp": 102,
          "fn": 298,
          "accuracy": 0.255
        }
      },
      "auroc": 0.6981026041666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        }
      },
      "auroc": 0.7224885416666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.23002499999999998
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 322,
          "accuracy": 0.195
        },
        "0.01": {
          "tp": 37,
          "fn": 363,
          "accuracy": 0.0925
        }
      },
      "auroc": 0.4762567708333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 133,
          "fn": 267,
          "accuracy": 0.3325
        }
      },
      "auroc": 0.7761067708333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 21,
          "fn": 379,
          "accuracy": 0.0525
        },
        "0.01": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        }
      },
      "auroc": 0.39825260416666663
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 243,
          "fn": 557,
          "accuracy": 0.30375
        },
        "0.01": {
          "tp": 139,
          "fn": 661,
          "accuracy": 0.17375
        }
      },
      "auroc": 0.5871796874999999
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.7721312499999999
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.7721312499999999
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.7591458333333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.7591458333333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        },
        "0.01": {
          "tp": 137,
          "fn": 263,
          "accuracy": 0.3425
        }
      },
      "auroc": 0.7656385416666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        },
        "0.01": {
          "tp": 137,
          "fn": 263,
          "accuracy": 0.3425
        }
      },
      "auroc": 0.7656385416666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.5585375
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.5585375
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.4702854166666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.4702854166666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 65,
          "fn": 335,
          "accuracy": 0.1625
        },
        "0.01": {
          "tp": 23,
          "fn": 377,
          "accuracy": 0.0575
        }
      },
      "auroc": 0.5144114583333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 65,
          "fn": 335,
          "accuracy": 0.1625
        },
        "0.01": {
          "tp": 23,
          "fn": 377,
          "accuracy": 0.0575
        }
      },
      "auroc": 0.5144114583333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.6351947916666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.6351947916666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.560446875
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.560446875
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 319,
          "accuracy": 0.2025
        },
        "0.01": {
          "tp": 22,
          "fn": 378,
          "accuracy": 0.055
        }
      },
      "auroc": 0.5978208333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 319,
          "accuracy": 0.2025
        },
        "0.01": {
          "tp": 22,
          "fn": 378,
          "accuracy": 0.055
        }
      },
      "auroc": 0.5978208333333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        }
      },
      "auroc": 0.7822510416666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        }
      },
      "auroc": 0.7822510416666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.3047895833333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.3047895833333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 274,
          "accuracy": 0.315
        },
        "0.01": {
          "tp": 66,
          "fn": 334,
          "accuracy": 0.165
        }
      },
      "auroc": 0.5435203125
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 274,
          "accuracy": 0.315
        },
        "0.01": {
          "tp": 66,
          "fn": 334,
          "accuracy": 0.165
        }
      },
      "auroc": 0.5435203125
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.56935625
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.56935625
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.4888739583333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        },
        "0.01": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        }
      },
      "auroc": 0.4888739583333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 319,
          "accuracy": 0.2025
        },
        "0.01": {
          "tp": 35,
          "fn": 365,
          "accuracy": 0.0875
        }
      },
      "auroc": 0.5291151041666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 81,
          "fn": 319,
          "accuracy": 0.2025
        },
        "0.01": {
          "tp": 35,
          "fn": 365,
          "accuracy": 0.0875
        }
      },
      "auroc": 0.5291151041666666
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1325,
          "fn": 875,
          "accuracy": 0.6022727272727273
        },
        "0.01": {
          "tp": 979,
          "fn": 1221,
          "accuracy": 0.445
        }
      },
      "auroc": 0.7689032196969696
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 907,
          "accuracy": 0.24416666666666667
        },
        "0.01": {
          "tp": 217,
          "fn": 983,
          "accuracy": 0.18083333333333335
        }
      },
      "auroc": 0.46860295138888886
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1618,
          "fn": 1782,
          "accuracy": 0.4758823529411765
        },
        "0.01": {
          "tp": 1196,
          "fn": 2204,
          "accuracy": 0.3517647058823529
        }
      },
      "auroc": 0.6629148897058824
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 715,
          "fn": 1485,
          "accuracy": 0.325
        },
        "0.01": {
          "tp": 370,
          "fn": 1830,
          "accuracy": 0.16818181818181818
        }
      },
      "auroc": 0.6223232954545455
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 1164,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 7,
          "fn": 1193,
          "accuracy": 0.005833333333333334
        }
      },
      "auroc": 0.18733750000000002
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 751,
          "fn": 2649,
          "accuracy": 0.22088235294117647
        },
        "0.01": {
          "tp": 377,
          "fn": 3023,
          "accuracy": 0.11088235294117647
        }
      },
      "auroc": 0.4687988970588235
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2040,
          "fn": 2360,
          "accuracy": 0.4636363636363636
        },
        "0.01": {
          "tp": 1349,
          "fn": 3051,
          "accuracy": 0.3065909090909091
        }
      },
      "auroc": 0.6956132575757576
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 2071,
          "accuracy": 0.13708333333333333
        },
        "0.01": {
          "tp": 224,
          "fn": 2176,
          "accuracy": 0.09333333333333334
        }
      },
      "auroc": 0.32797022569444445
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2369,
          "fn": 4431,
          "accuracy": 0.3483823529411765
        },
        "0.01": {
          "tp": 1573,
          "fn": 5227,
          "accuracy": 0.2313235294117647
        }
      },
      "auroc": 0.565856893382353
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9459145833333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9306041666666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.938259375
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9474427083333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.7787250000000001
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.8630838541666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9466786458333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        }
      },
      "auroc": 0.8546645833333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 746,
          "fn": 54,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 695,
          "fn": 105,
          "accuracy": 0.86875
        }
      },
      "auroc": 0.9006716145833333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9442885416666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.12136354166666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.5328260416666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.884859375
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.052603125
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        },
        "0.01": {
          "tp": 131,
          "fn": 269,
          "accuracy": 0.3275
        }
      },
      "auroc": 0.46873125000000004
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        }
      },
      "auroc": 0.9145739583333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.08698333333333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 426,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 332,
          "fn": 468,
          "accuracy": 0.415
        }
      },
      "auroc": 0.5007786458333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8534499999999999
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 69,
          "fn": 131,
          "accuracy": 0.345
        },
        "0.01": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        }
      },
      "auroc": 0.6858239583333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        },
        "0.01": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        }
      },
      "auroc": 0.7696369791666668
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        }
      },
      "auroc": 0.8393479166666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.46250625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 224,
          "accuracy": 0.44
        },
        "0.01": {
          "tp": 106,
          "fn": 294,
          "accuracy": 0.265
        }
      },
      "auroc": 0.6509270833333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.8463989583333335
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 308,
          "accuracy": 0.23
        },
        "0.01": {
          "tp": 30,
          "fn": 370,
          "accuracy": 0.075
        }
      },
      "auroc": 0.5741651041666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 402,
          "accuracy": 0.4975
        },
        "0.01": {
          "tp": 231,
          "fn": 569,
          "accuracy": 0.28875
        }
      },
      "auroc": 0.71028203125
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9419458333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.94755
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9447479166666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9341083333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.326309375
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        }
      },
      "auroc": 0.6302088541666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9380270833333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        },
        "0.01": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        }
      },
      "auroc": 0.6369296874999999
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 600,
          "fn": 200,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 573,
          "fn": 227,
          "accuracy": 0.71625
        }
      },
      "auroc": 0.7874783854166667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9423729166666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.5855333333333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        },
        "0.01": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        }
      },
      "auroc": 0.763953125
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8586374999999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.05602916666666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 239,
          "accuracy": 0.4025
        },
        "0.01": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        }
      },
      "auroc": 0.45733333333333337
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9005052083333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 322,
          "accuracy": 0.195
        },
        "0.01": {
          "tp": 49,
          "fn": 351,
          "accuracy": 0.1225
        }
      },
      "auroc": 0.32078124999999996
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 436,
          "fn": 364,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 359,
          "fn": 441,
          "accuracy": 0.44875
        }
      },
      "auroc": 0.6106432291666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9266635416666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.8933604166666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        }
      },
      "auroc": 0.9100119791666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.8884062500000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.5626166666666668
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 156,
          "fn": 244,
          "accuracy": 0.39
        }
      },
      "auroc": 0.7255114583333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9075348958333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 155,
          "fn": 245,
          "accuracy": 0.3875
        }
      },
      "auroc": 0.7279885416666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 580,
          "fn": 220,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 485,
          "fn": 315,
          "accuracy": 0.60625
        }
      },
      "auroc": 0.81776171875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.888075
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.888075
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.8915093749999999
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.8915093749999999
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.8897921875000001
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.8897921875000001
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.8275583333333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.8275583333333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        }
      },
      "auroc": 0.7659395833333335
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        }
      },
      "auroc": 0.7659395833333335
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        }
      },
      "auroc": 0.7967489583333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        }
      },
      "auroc": 0.7967489583333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.8938708333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.8938708333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8540572916666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8540572916666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        }
      },
      "auroc": 0.8739640625
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        }
      },
      "auroc": 0.8739640625
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9412072916666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9412072916666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.6644041666666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.6644041666666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        }
      },
      "auroc": 0.8028057291666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        }
      },
      "auroc": 0.8028057291666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.8069583333333332
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.8069583333333332
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.7125739583333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.7125739583333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": {
          "tp": 163,
          "fn": 237,
          "accuracy": 0.4075
        }
      },
      "auroc": 0.7597661458333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": {
          "tp": 163,
          "fn": 237,
          "accuracy": 0.4075
        }
      },
      "auroc": 0.7597661458333334
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1966,
          "fn": 234,
          "accuracy": 0.8936363636363637
        },
        "0.01": {
          "tp": 1742,
          "fn": 458,
          "accuracy": 0.7918181818181819
        }
      },
      "auroc": 0.9011186553030303
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 722,
          "fn": 478,
          "accuracy": 0.6016666666666667
        },
        "0.01": {
          "tp": 602,
          "fn": 598,
          "accuracy": 0.5016666666666667
        }
      },
      "auroc": 0.694039236111111
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2688,
          "fn": 712,
          "accuracy": 0.7905882352941176
        },
        "0.01": {
          "tp": 2344,
          "fn": 1056,
          "accuracy": 0.6894117647058824
        }
      },
      "auroc": 0.8280318014705883
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1652,
          "fn": 548,
          "accuracy": 0.7509090909090909
        },
        "0.01": {
          "tp": 1317,
          "fn": 883,
          "accuracy": 0.5986363636363636
        }
      },
      "auroc": 0.8401169507575758
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 215,
          "fn": 985,
          "accuracy": 0.17916666666666667
        },
        "0.01": {
          "tp": 133,
          "fn": 1067,
          "accuracy": 0.11083333333333334
        }
      },
      "auroc": 0.3731315972222222
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1867,
          "fn": 1533,
          "accuracy": 0.5491176470588235
        },
        "0.01": {
          "tp": 1450,
          "fn": 1950,
          "accuracy": 0.4264705882352941
        }
      },
      "auroc": 0.6752985906862745
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3618,
          "fn": 782,
          "accuracy": 0.8222727272727273
        },
        "0.01": {
          "tp": 3059,
          "fn": 1341,
          "accuracy": 0.6952272727272727
        }
      },
      "auroc": 0.870617803030303
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 937,
          "fn": 1463,
          "accuracy": 0.3904166666666667
        },
        "0.01": {
          "tp": 735,
          "fn": 1665,
          "accuracy": 0.30625
        }
      },
      "auroc": 0.5335854166666667
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 4555,
          "fn": 2245,
          "accuracy": 0.6698529411764705
        },
        "0.01": {
          "tp": 3794,
          "fn": 3006,
          "accuracy": 0.5579411764705883
        }
      },
      "auroc": 0.7516651960784314
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.934696875
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        }
      },
      "auroc": 0.9061322916666668
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        }
      },
      "auroc": 0.9204145833333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9337354166666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.8701947916666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        }
      },
      "auroc": 0.9019651041666668
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9342161458333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 273,
          "fn": 127,
          "accuracy": 0.6825
        }
      },
      "auroc": 0.8881635416666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 747,
          "fn": 53,
          "accuracy": 0.93375
        },
        "0.01": {
          "tp": 639,
          "fn": 161,
          "accuracy": 0.79875
        }
      },
      "auroc": 0.9111898437500001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9359041666666668
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.5921260416666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        },
        "0.01": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        }
      },
      "auroc": 0.7640151041666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.8955645833333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9157343750000001
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8272864583333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.759959375
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": {
          "tp": 130,
          "fn": 270,
          "accuracy": 0.325
        }
      },
      "auroc": 0.7936229166666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 132,
          "fn": 68,
          "accuracy": 0.66
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.8069854166666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.7103302083333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        },
        "0.01": {
          "tp": 91,
          "fn": 309,
          "accuracy": 0.2275
        }
      },
      "auroc": 0.7586578125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        },
        "0.01": {
          "tp": 155,
          "fn": 245,
          "accuracy": 0.3875
        }
      },
      "auroc": 0.8171359375
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        },
        "0.01": {
          "tp": 66,
          "fn": 334,
          "accuracy": 0.165
        }
      },
      "auroc": 0.7351447916666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 460,
          "fn": 340,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 221,
          "fn": 579,
          "accuracy": 0.27625
        }
      },
      "auroc": 0.7761403645833332
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9285458333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9405458333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9345458333333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9251895833333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.7528802083333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        }
      },
      "auroc": 0.8390348958333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9268677083333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        },
        "0.01": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        }
      },
      "auroc": 0.8467130208333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 680,
          "fn": 120,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 600,
          "fn": 200,
          "accuracy": 0.75
        }
      },
      "auroc": 0.8867903645833334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9382458333333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8114593749999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        }
      },
      "auroc": 0.8748526041666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.8842833333333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.47391666666666665
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        },
        "0.01": {
          "tp": 140,
          "fn": 260,
          "accuracy": 0.35
        }
      },
      "auroc": 0.6790999999999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        }
      },
      "auroc": 0.9112645833333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 255,
          "accuracy": 0.3625
        },
        "0.01": {
          "tp": 87,
          "fn": 313,
          "accuracy": 0.2175
        }
      },
      "auroc": 0.6426880208333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 513,
          "fn": 287,
          "accuracy": 0.64125
        },
        "0.01": {
          "tp": 416,
          "fn": 384,
          "accuracy": 0.52
        }
      },
      "auroc": 0.7769763020833333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9146124999999999
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.8785572916666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        }
      },
      "auroc": 0.8965848958333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.8804135416666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.7768260416666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 271,
          "fn": 129,
          "accuracy": 0.6775
        },
        "0.01": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        }
      },
      "auroc": 0.8286197916666668
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        }
      },
      "auroc": 0.8975130208333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 274,
          "fn": 126,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 165,
          "fn": 235,
          "accuracy": 0.4125
        }
      },
      "auroc": 0.8276916666666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 627,
          "fn": 173,
          "accuracy": 0.78375
        },
        "0.01": {
          "tp": 464,
          "fn": 336,
          "accuracy": 0.58
        }
      },
      "auroc": 0.8626023437500001
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.8593583333333332
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.8593583333333332
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8643177083333332
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8643177083333332
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        }
      },
      "auroc": 0.8618380208333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        }
      },
      "auroc": 0.8618380208333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8558656249999999
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8558656249999999
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.8399395833333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.8399395833333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        }
      },
      "auroc": 0.8479026041666666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        }
      },
      "auroc": 0.8479026041666666
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        }
      },
      "auroc": 0.8808239583333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        }
      },
      "auroc": 0.8808239583333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.8672375
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.8672375
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        }
      },
      "auroc": 0.8740307291666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        }
      },
      "auroc": 0.8740307291666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.928803125
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.928803125
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        }
      },
      "auroc": 0.86230625
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        }
      },
      "auroc": 0.86230625
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        }
      },
      "auroc": 0.8955546875
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        }
      },
      "auroc": 0.8955546875
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        }
      },
      "auroc": 0.8329520833333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        }
      },
      "auroc": 0.8329520833333334
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        }
      },
      "auroc": 0.8108833333333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 90,
          "fn": 110,
          "accuracy": 0.45
        }
      },
      "auroc": 0.8108833333333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        }
      },
      "auroc": 0.8219177083333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        }
      },
      "auroc": 0.8219177083333333
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1950,
          "fn": 250,
          "accuracy": 0.8863636363636364
        },
        "0.01": {
          "tp": 1624,
          "fn": 576,
          "accuracy": 0.7381818181818182
        }
      },
      "auroc": 0.8942813446969696
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 824,
          "fn": 376,
          "accuracy": 0.6866666666666666
        },
        "0.01": {
          "tp": 604,
          "fn": 596,
          "accuracy": 0.5033333333333333
        }
      },
      "auroc": 0.8147967013888888
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2774,
          "fn": 626,
          "accuracy": 0.8158823529411765
        },
        "0.01": {
          "tp": 2228,
          "fn": 1172,
          "accuracy": 0.6552941176470588
        }
      },
      "auroc": 0.8662279411764706
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1828,
          "fn": 372,
          "accuracy": 0.8309090909090909
        },
        "0.01": {
          "tp": 1394,
          "fn": 806,
          "accuracy": 0.6336363636363637
        }
      },
      "auroc": 0.8700778409090909
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3778,
          "fn": 622,
          "accuracy": 0.8586363636363636
        },
        "0.01": {
          "tp": 3018,
          "fn": 1382,
          "accuracy": 0.6859090909090909
        }
      },
      "auroc": 0.8821795928030302
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9466822916666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.939528125
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9431052083333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9479604166666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.8050458333333335
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        }
      },
      "auroc": 0.876503125
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9473213541666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        }
      },
      "auroc": 0.8722869791666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 754,
          "fn": 46,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 727,
          "fn": 73,
          "accuracy": 0.90875
        }
      },
      "auroc": 0.9098041666666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9437770833333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.12461145833333331
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        }
      },
      "auroc": 0.5341942708333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 137,
          "fn": 63,
          "accuracy": 0.685
        }
      },
      "auroc": 0.8831833333333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.05276458333333332
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        },
        "0.01": {
          "tp": 137,
          "fn": 263,
          "accuracy": 0.3425
        }
      },
      "auroc": 0.46797395833333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9134802083333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.08868802083333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 422,
          "accuracy": 0.4725
        },
        "0.01": {
          "tp": 335,
          "fn": 465,
          "accuracy": 0.41875
        }
      },
      "auroc": 0.5010841145833334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        }
      },
      "auroc": 0.87568125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.7486666666666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 166,
          "fn": 234,
          "accuracy": 0.415
        }
      },
      "auroc": 0.8121739583333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        }
      },
      "auroc": 0.86114375
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.507690625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        },
        "0.01": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        }
      },
      "auroc": 0.6844171875000001
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.8684125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 276,
          "accuracy": 0.31
        },
        "0.01": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        }
      },
      "auroc": 0.6281786458333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 458,
          "fn": 342,
          "accuracy": 0.5725
        },
        "0.01": {
          "tp": 291,
          "fn": 509,
          "accuracy": 0.36375
        }
      },
      "auroc": 0.7482955729166666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9415114583333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9480458333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9447786458333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.935159375
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.35048541666666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        },
        "0.01": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        }
      },
      "auroc": 0.6428223958333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9383354166666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.6492656249999998
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 595,
          "fn": 205,
          "accuracy": 0.74375
        },
        "0.01": {
          "tp": 575,
          "fn": 225,
          "accuracy": 0.71875
        }
      },
      "auroc": 0.7938005208333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9419697916666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.6149
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 289,
          "fn": 111,
          "accuracy": 0.7225
        },
        "0.01": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        }
      },
      "auroc": 0.7784348958333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        }
      },
      "auroc": 0.8653927083333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.05534062499999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 241,
          "accuracy": 0.3975
        },
        "0.01": {
          "tp": 125,
          "fn": 275,
          "accuracy": 0.3125
        }
      },
      "auroc": 0.46036666666666665
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 320,
          "fn": 80,
          "accuracy": 0.8
        }
      },
      "auroc": 0.90368125
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 308,
          "accuracy": 0.23
        },
        "0.01": {
          "tp": 59,
          "fn": 341,
          "accuracy": 0.1475
        }
      },
      "auroc": 0.33512031249999996
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 448,
          "fn": 352,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 379,
          "fn": 421,
          "accuracy": 0.47375
        }
      },
      "auroc": 0.61940078125
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.931540625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.91191875
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9217296875000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9049104166666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.6317187500000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        }
      },
      "auroc": 0.7683145833333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9182255208333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        },
        "0.01": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        }
      },
      "auroc": 0.77181875
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 622,
          "fn": 178,
          "accuracy": 0.7775
        },
        "0.01": {
          "tp": 531,
          "fn": 269,
          "accuracy": 0.66375
        }
      },
      "auroc": 0.8450221354166666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9031260416666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9031260416666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9054614583333332
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9054614583333332
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        }
      },
      "auroc": 0.9042937499999999
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        }
      },
      "auroc": 0.9042937499999999
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.8489833333333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.8489833333333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.7942520833333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.7942520833333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.8216177083333334
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.8216177083333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9162333333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9162333333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.8918385416666665
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.8918385416666665
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        }
      },
      "auroc": 0.9040359375
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        }
      },
      "auroc": 0.9040359375
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9439229166666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9439229166666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.7284770833333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        },
        "0.01": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        }
      },
      "auroc": 0.7284770833333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        }
      },
      "auroc": 0.8361999999999999
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 226,
          "fn": 174,
          "accuracy": 0.565
        }
      },
      "auroc": 0.8361999999999999
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.8289666666666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 57,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        }
      },
      "auroc": 0.8289666666666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.7307989583333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.7307989583333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        },
        "0.01": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        }
      },
      "auroc": 0.7798828124999999
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        },
        "0.01": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        }
      },
      "auroc": 0.7798828124999999
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2029,
          "fn": 171,
          "accuracy": 0.9222727272727272
        },
        "0.01": {
          "tp": 1840,
          "fn": 360,
          "accuracy": 0.8363636363636363
        }
      },
      "auroc": 0.9111267992424243
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 774,
          "fn": 426,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 655,
          "fn": 545,
          "accuracy": 0.5458333333333333
        }
      },
      "auroc": 0.7146118055555555
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2803,
          "fn": 597,
          "accuracy": 0.8244117647058824
        },
        "0.01": {
          "tp": 2495,
          "fn": 905,
          "accuracy": 0.7338235294117647
        }
      },
      "auroc": 0.8417685661764706
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1737,
          "fn": 463,
          "accuracy": 0.7895454545454546
        },
        "0.01": {
          "tp": 1422,
          "fn": 778,
          "accuracy": 0.6463636363636364
        }
      },
      "auroc": 0.8589616477272727
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 953,
          "accuracy": 0.20583333333333334
        },
        "0.01": {
          "tp": 168,
          "fn": 1032,
          "accuracy": 0.14
        }
      },
      "auroc": 0.40050763888888885
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1984,
          "fn": 1416,
          "accuracy": 0.5835294117647059
        },
        "0.01": {
          "tp": 1590,
          "fn": 1810,
          "accuracy": 0.4676470588235294
        }
      },
      "auroc": 0.697154350490196
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3766,
          "fn": 634,
          "accuracy": 0.855909090909091
        },
        "0.01": {
          "tp": 3262,
          "fn": 1138,
          "accuracy": 0.7413636363636363
        }
      },
      "auroc": 0.8850442234848486
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1021,
          "fn": 1379,
          "accuracy": 0.42541666666666667
        },
        "0.01": {
          "tp": 823,
          "fn": 1577,
          "accuracy": 0.34291666666666665
        }
      },
      "auroc": 0.5575597222222222
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 4787,
          "fn": 2013,
          "accuracy": 0.7039705882352941
        },
        "0.01": {
          "tp": 4085,
          "fn": 2715,
          "accuracy": 0.600735294117647
        }
      },
      "auroc": 0.7694614583333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.946434375
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9362052083333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9413197916666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.947590625
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.8062593750000001
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.876925
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9470125
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 350,
          "fn": 50,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        }
      },
      "auroc": 0.8712322916666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 750,
          "fn": 50,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 715,
          "fn": 85,
          "accuracy": 0.89375
        }
      },
      "auroc": 0.9091223958333332
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.945153125
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.12367499999999998
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        }
      },
      "auroc": 0.5344140625
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9030635416666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.051882291666666656
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        },
        "0.01": {
          "tp": 149,
          "fn": 251,
          "accuracy": 0.3725
        }
      },
      "auroc": 0.47747291666666664
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        }
      },
      "auroc": 0.9241083333333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.08777864583333332
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 411,
          "accuracy": 0.48625
        },
        "0.01": {
          "tp": 347,
          "fn": 453,
          "accuracy": 0.43375
        }
      },
      "auroc": 0.5059434895833332
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        }
      },
      "auroc": 0.8724770833333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        }
      },
      "auroc": 0.7507177083333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 269,
          "fn": 131,
          "accuracy": 0.6725
        },
        "0.01": {
          "tp": 160,
          "fn": 240,
          "accuracy": 0.4
        }
      },
      "auroc": 0.8115973958333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        }
      },
      "auroc": 0.8584083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        },
        "0.01": {
          "tp": 14,
          "fn": 186,
          "accuracy": 0.07
        }
      },
      "auroc": 0.5132010416666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 131,
          "fn": 269,
          "accuracy": 0.3275
        }
      },
      "auroc": 0.6858046874999999
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        }
      },
      "auroc": 0.8654427083333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 265,
          "accuracy": 0.3375
        },
        "0.01": {
          "tp": 48,
          "fn": 352,
          "accuracy": 0.12
        }
      },
      "auroc": 0.631959375
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 467,
          "fn": 333,
          "accuracy": 0.58375
        },
        "0.01": {
          "tp": 291,
          "fn": 509,
          "accuracy": 0.36375
        }
      },
      "auroc": 0.7487010416666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.941940625
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9472583333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9445994791666668
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9338979166666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.41280833333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        }
      },
      "auroc": 0.673353125
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9379192708333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.6800333333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 598,
          "fn": 202,
          "accuracy": 0.7475
        },
        "0.01": {
          "tp": 577,
          "fn": 223,
          "accuracy": 0.72125
        }
      },
      "auroc": 0.8089763020833334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9430416666666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.6469697916666668
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 301,
          "fn": 99,
          "accuracy": 0.7525
        },
        "0.01": {
          "tp": 263,
          "fn": 137,
          "accuracy": 0.6575
        }
      },
      "auroc": 0.7950057291666668
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8751270833333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.06285833333333332
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 235,
          "accuracy": 0.4125
        },
        "0.01": {
          "tp": 130,
          "fn": 270,
          "accuracy": 0.325
        }
      },
      "auroc": 0.46899270833333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        }
      },
      "auroc": 0.9090843750000001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 296,
          "accuracy": 0.26
        },
        "0.01": {
          "tp": 70,
          "fn": 330,
          "accuracy": 0.175
        }
      },
      "auroc": 0.3549140625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 466,
          "fn": 334,
          "accuracy": 0.5825
        },
        "0.01": {
          "tp": 393,
          "fn": 407,
          "accuracy": 0.49125
        }
      },
      "auroc": 0.63199921875
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9313104166666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9104114583333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9208609375
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9034208333333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.6501479166666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        },
        "0.01": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        }
      },
      "auroc": 0.776784375
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.917365625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        },
        "0.01": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        }
      },
      "auroc": 0.7802796874999999
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 628,
          "fn": 172,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 527,
          "fn": 273,
          "accuracy": 0.65875
        }
      },
      "auroc": 0.8488226562500001
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9005677083333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9005677083333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.8980249999999999
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.8980249999999999
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        }
      },
      "auroc": 0.8992963541666668
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        },
        "0.01": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        }
      },
      "auroc": 0.8992963541666668
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.8518333333333332
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.8518333333333332
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        }
      },
      "auroc": 0.79564375
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        }
      },
      "auroc": 0.79564375
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        }
      },
      "auroc": 0.8237385416666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 278,
          "fn": 122,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        }
      },
      "auroc": 0.8237385416666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.914525
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.914525
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        }
      },
      "auroc": 0.8920750000000001
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        }
      },
      "auroc": 0.8920750000000001
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        }
      },
      "auroc": 0.9033
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        }
      },
      "auroc": 0.9033
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9432770833333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9432770833333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.7384802083333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 83,
          "fn": 117,
          "accuracy": 0.415
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.7384802083333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        },
        "0.01": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        }
      },
      "auroc": 0.8408786458333335
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        },
        "0.01": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        }
      },
      "auroc": 0.8408786458333335
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.8379531250000001
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        }
      },
      "auroc": 0.8379531250000001
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.7458833333333335
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.7458833333333335
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.7919182291666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.7919182291666667
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2029,
          "fn": 171,
          "accuracy": 0.9222727272727272
        },
        "0.01": {
          "tp": 1838,
          "fn": 362,
          "accuracy": 0.8354545454545454
        }
      },
      "auroc": 0.9116830492424242
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 785,
          "fn": 415,
          "accuracy": 0.6541666666666667
        },
        "0.01": {
          "tp": 649,
          "fn": 551,
          "accuracy": 0.5408333333333334
        }
      },
      "auroc": 0.71920625
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2814,
          "fn": 586,
          "accuracy": 0.8276470588235294
        },
        "0.01": {
          "tp": 2487,
          "fn": 913,
          "accuracy": 0.7314705882352941
        }
      },
      "auroc": 0.8437500612745099
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1767,
          "fn": 433,
          "accuracy": 0.8031818181818182
        },
        "0.01": {
          "tp": 1438,
          "fn": 762,
          "accuracy": 0.6536363636363637
        }
      },
      "auroc": 0.8628741477272727
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 263,
          "fn": 937,
          "accuracy": 0.21916666666666668
        },
        "0.01": {
          "tp": 171,
          "fn": 1029,
          "accuracy": 0.1425
        }
      },
      "auroc": 0.4161928819444445
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2030,
          "fn": 1370,
          "accuracy": 0.5970588235294118
        },
        "0.01": {
          "tp": 1609,
          "fn": 1791,
          "accuracy": 0.47323529411764703
        }
      },
      "auroc": 0.7052219362745098
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3796,
          "fn": 604,
          "accuracy": 0.8627272727272727
        },
        "0.01": {
          "tp": 3276,
          "fn": 1124,
          "accuracy": 0.7445454545454545
        }
      },
      "auroc": 0.8872785984848484
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1048,
          "fn": 1352,
          "accuracy": 0.43666666666666665
        },
        "0.01": {
          "tp": 820,
          "fn": 1580,
          "accuracy": 0.3416666666666667
        }
      },
      "auroc": 0.5676995659722223
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 4844,
          "fn": 1956,
          "accuracy": 0.7123529411764706
        },
        "0.01": {
          "tp": 4096,
          "fn": 2704,
          "accuracy": 0.6023529411764705
        }
      },
      "auroc": 0.7744859987745097
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9295458333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8761104166666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        }
      },
      "auroc": 0.902828125
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9267208333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.8069510416666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        }
      },
      "auroc": 0.8668359375
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        }
      },
      "auroc": 0.9281333333333333
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        }
      },
      "auroc": 0.8415307291666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 695,
          "fn": 105,
          "accuracy": 0.86875
        },
        "0.01": {
          "tp": 570,
          "fn": 230,
          "accuracy": 0.7125
        }
      },
      "auroc": 0.8848320312500001
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9324625
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.40727395833333335
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        },
        "0.01": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        }
      },
      "auroc": 0.6698682291666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9281291666666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.362021875
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 216,
          "fn": 184,
          "accuracy": 0.54
        },
        "0.01": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        }
      },
      "auroc": 0.6450755208333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9302958333333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 40,
          "fn": 360,
          "accuracy": 0.1
        },
        "0.01": {
          "tp": 26,
          "fn": 374,
          "accuracy": 0.065
        }
      },
      "auroc": 0.38464791666666664
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 427,
          "fn": 373,
          "accuracy": 0.53375
        },
        "0.01": {
          "tp": 388,
          "fn": 412,
          "accuracy": 0.485
        }
      },
      "auroc": 0.657471875
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.7503854166666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.6221677083333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 257,
          "accuracy": 0.3575
        },
        "0.01": {
          "tp": 57,
          "fn": 343,
          "accuracy": 0.1425
        }
      },
      "auroc": 0.6862765625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        }
      },
      "auroc": 0.7358708333333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.5152135416666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 277,
          "accuracy": 0.3075
        },
        "0.01": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        }
      },
      "auroc": 0.6255421875
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        },
        "0.01": {
          "tp": 84,
          "fn": 316,
          "accuracy": 0.21
        }
      },
      "auroc": 0.743128125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 329,
          "accuracy": 0.1775
        },
        "0.01": {
          "tp": 23,
          "fn": 377,
          "accuracy": 0.0575
        }
      },
      "auroc": 0.568690625
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 266,
          "fn": 534,
          "accuracy": 0.3325
        },
        "0.01": {
          "tp": 107,
          "fn": 693,
          "accuracy": 0.13375
        }
      },
      "auroc": 0.655909375
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9197739583333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9140416666666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9169078125000001
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9190979166666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.7280604166666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8235791666666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        }
      },
      "auroc": 0.9194359375000001
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        }
      },
      "auroc": 0.8210510416666665
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 658,
          "fn": 142,
          "accuracy": 0.8225
        },
        "0.01": {
          "tp": 534,
          "fn": 266,
          "accuracy": 0.6675
        }
      },
      "auroc": 0.8702434895833334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.929340625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 60,
          "fn": 140,
          "accuracy": 0.3
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.6468895833333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        }
      },
      "auroc": 0.7881151041666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9057687500000001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.4750114583333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        },
        "0.01": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        }
      },
      "auroc": 0.6903901041666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9175546875
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 318,
          "accuracy": 0.205
        },
        "0.01": {
          "tp": 36,
          "fn": 364,
          "accuracy": 0.09
        }
      },
      "auroc": 0.5609505208333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 457,
          "fn": 343,
          "accuracy": 0.57125
        },
        "0.01": {
          "tp": 382,
          "fn": 418,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.7392526041666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        }
      },
      "auroc": 0.8630385416666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        }
      },
      "auroc": 0.7535260416666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 167,
          "fn": 233,
          "accuracy": 0.4175
        }
      },
      "auroc": 0.8082822916666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        }
      },
      "auroc": 0.8436052083333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        }
      },
      "auroc": 0.6411697916666668
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        },
        "0.01": {
          "tp": 116,
          "fn": 284,
          "accuracy": 0.29
        }
      },
      "auroc": 0.7423875
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 227,
          "fn": 173,
          "accuracy": 0.5675
        }
      },
      "auroc": 0.853321875
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 249,
          "accuracy": 0.3775
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.6973479166666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 465,
          "fn": 335,
          "accuracy": 0.58125
        },
        "0.01": {
          "tp": 283,
          "fn": 517,
          "accuracy": 0.35375
        }
      },
      "auroc": 0.7753348958333335
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8319583333333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8319583333333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.8300020833333331
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.8300020833333331
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8309802083333333
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        },
        "0.01": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8309802083333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.8796916666666668
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.8796916666666668
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        }
      },
      "auroc": 0.8633302083333332
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        }
      },
      "auroc": 0.8633302083333332
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        }
      },
      "auroc": 0.8715109375
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        }
      },
      "auroc": 0.8715109375
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.8132958333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.8132958333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.8062135416666668
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.8062135416666668
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        },
        "0.01": {
          "tp": 160,
          "fn": 240,
          "accuracy": 0.4
        }
      },
      "auroc": 0.8097546875
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        },
        "0.01": {
          "tp": 160,
          "fn": 240,
          "accuracy": 0.4
        }
      },
      "auroc": 0.8097546875
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.8996208333333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        }
      },
      "auroc": 0.8996208333333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.840271875
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.840271875
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        }
      },
      "auroc": 0.8699463541666665
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 253,
          "fn": 147,
          "accuracy": 0.6325
        }
      },
      "auroc": 0.8699463541666665
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8145062499999999
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8145062499999999
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.7622885416666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.7622885416666667
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": {
          "tp": 157,
          "fn": 243,
          "accuracy": 0.3925
        }
      },
      "auroc": 0.7883973958333332
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": {
          "tp": 157,
          "fn": 243,
          "accuracy": 0.3925
        }
      },
      "auroc": 0.7883973958333332
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1811,
          "fn": 389,
          "accuracy": 0.8231818181818182
        },
        "0.01": {
          "tp": 1430,
          "fn": 770,
          "accuracy": 0.65
        }
      },
      "auroc": 0.869419981060606
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 585,
          "fn": 615,
          "accuracy": 0.4875
        },
        "0.01": {
          "tp": 386,
          "fn": 814,
          "accuracy": 0.32166666666666666
        }
      },
      "auroc": 0.7033348958333332
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2396,
          "fn": 1004,
          "accuracy": 0.7047058823529412
        },
        "0.01": {
          "tp": 1816,
          "fn": 1584,
          "accuracy": 0.5341176470588235
        }
      },
      "auroc": 0.8108017156862743
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1693,
          "fn": 507,
          "accuracy": 0.7695454545454545
        },
        "0.01": {
          "tp": 1303,
          "fn": 897,
          "accuracy": 0.5922727272727273
        }
      },
      "auroc": 0.851027178030303
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 343,
          "fn": 857,
          "accuracy": 0.28583333333333333
        },
        "0.01": {
          "tp": 173,
          "fn": 1027,
          "accuracy": 0.14416666666666667
        }
      },
      "auroc": 0.5880713541666668
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2036,
          "fn": 1364,
          "accuracy": 0.5988235294117648
        },
        "0.01": {
          "tp": 1476,
          "fn": 1924,
          "accuracy": 0.43411764705882355
        }
      },
      "auroc": 0.7582192401960784
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3504,
          "fn": 896,
          "accuracy": 0.7963636363636364
        },
        "0.01": {
          "tp": 2733,
          "fn": 1667,
          "accuracy": 0.6211363636363636
        }
      },
      "auroc": 0.8602235795454545
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 928,
          "fn": 1472,
          "accuracy": 0.38666666666666666
        },
        "0.01": {
          "tp": 559,
          "fn": 1841,
          "accuracy": 0.23291666666666666
        }
      },
      "auroc": 0.645703125
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4432,
          "fn": 2368,
          "accuracy": 0.6517647058823529
        },
        "0.01": {
          "tp": 3292,
          "fn": 3508,
          "accuracy": 0.48411764705882354
        }
      },
      "auroc": 0.7845104779411766
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9449479166666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9245604166666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9347541666666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9457447916666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.7688697916666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        },
        "0.01": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        }
      },
      "auroc": 0.8573072916666665
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9453463541666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        }
      },
      "auroc": 0.8467151041666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 737,
          "fn": 63,
          "accuracy": 0.92125
        },
        "0.01": {
          "tp": 675,
          "fn": 125,
          "accuracy": 0.84375
        }
      },
      "auroc": 0.8960307291666668
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9404625
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.131946875
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.5362046875
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        }
      },
      "auroc": 0.8338416666666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.053438541666666665
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 257,
          "accuracy": 0.3575
        },
        "0.01": {
          "tp": 76,
          "fn": 324,
          "accuracy": 0.19
        }
      },
      "auroc": 0.44364010416666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": {
          "tp": 264,
          "fn": 136,
          "accuracy": 0.66
        }
      },
      "auroc": 0.8871520833333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.09269270833333332
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 344,
          "fn": 456,
          "accuracy": 0.43
        },
        "0.01": {
          "tp": 267,
          "fn": 533,
          "accuracy": 0.33375
        }
      },
      "auroc": 0.4899223958333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 86,
          "fn": 114,
          "accuracy": 0.43
        }
      },
      "auroc": 0.8304989583333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.6957333333333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        }
      },
      "auroc": 0.7631161458333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.8087177083333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.46015833333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 249,
          "accuracy": 0.3775
        },
        "0.01": {
          "tp": 84,
          "fn": 316,
          "accuracy": 0.21
        }
      },
      "auroc": 0.6344380208333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        },
        "0.01": {
          "tp": 163,
          "fn": 237,
          "accuracy": 0.4075
        }
      },
      "auroc": 0.8196083333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 312,
          "accuracy": 0.22
        },
        "0.01": {
          "tp": 33,
          "fn": 367,
          "accuracy": 0.0825
        }
      },
      "auroc": 0.5779458333333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 429,
          "accuracy": 0.46375
        },
        "0.01": {
          "tp": 196,
          "fn": 604,
          "accuracy": 0.245
        }
      },
      "auroc": 0.6987770833333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9318447916666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9480458333333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9399453125000001
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.916815625
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.29606770833333335
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        },
        "0.01": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        }
      },
      "auroc": 0.6064416666666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9243302083333332
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        }
      },
      "auroc": 0.6220567708333332
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 585,
          "fn": 215,
          "accuracy": 0.73125
        },
        "0.01": {
          "tp": 556,
          "fn": 244,
          "accuracy": 0.695
        }
      },
      "auroc": 0.7731934895833333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9372854166666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.6050739583333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        }
      },
      "auroc": 0.7711796875000001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        }
      },
      "auroc": 0.809365625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.055257291666666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 129,
          "fn": 271,
          "accuracy": 0.3225
        },
        "0.01": {
          "tp": 78,
          "fn": 322,
          "accuracy": 0.195
        }
      },
      "auroc": 0.43231145833333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        }
      },
      "auroc": 0.8733255208333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 89,
          "fn": 311,
          "accuracy": 0.2225
        },
        "0.01": {
          "tp": 49,
          "fn": 351,
          "accuracy": 0.1225
        }
      },
      "auroc": 0.33016562499999996
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 413,
          "fn": 387,
          "accuracy": 0.51625
        },
        "0.01": {
          "tp": 317,
          "fn": 483,
          "accuracy": 0.39625
        }
      },
      "auroc": 0.6017455729166666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9217979166666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.89820625
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        }
      },
      "auroc": 0.9100020833333335
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.8796479166666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 30,
          "fn": 170,
          "accuracy": 0.15
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.55981875
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 149,
          "fn": 251,
          "accuracy": 0.3725
        }
      },
      "auroc": 0.7197333333333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 317,
          "fn": 83,
          "accuracy": 0.7925
        }
      },
      "auroc": 0.9007229166666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        },
        "0.01": {
          "tp": 155,
          "fn": 245,
          "accuracy": 0.3875
        }
      },
      "auroc": 0.7290125000000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 568,
          "fn": 232,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 472,
          "fn": 328,
          "accuracy": 0.59
        }
      },
      "auroc": 0.8148677083333334
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.8927572916666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.8927572916666666
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.8921531250000001
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.8921531250000001
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.8924552083333335
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.8924552083333335
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.7901208333333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.7901208333333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.7254645833333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.7254645833333333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        },
        "0.01": {
          "tp": 136,
          "fn": 264,
          "accuracy": 0.34
        }
      },
      "auroc": 0.7577927083333332
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        },
        "0.01": {
          "tp": 136,
          "fn": 264,
          "accuracy": 0.34
        }
      },
      "auroc": 0.7577927083333332
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        }
      },
      "auroc": 0.8813833333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        }
      },
      "auroc": 0.8813833333333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.84358125
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        }
      },
      "auroc": 0.84358125
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        }
      },
      "auroc": 0.8624822916666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        }
      },
      "auroc": 0.8624822916666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9351239583333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9351239583333334
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.6128572916666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 39,
          "fn": 161,
          "accuracy": 0.195
        },
        "0.01": {
          "tp": 12,
          "fn": 188,
          "accuracy": 0.06
        }
      },
      "auroc": 0.6128572916666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 236,
          "fn": 164,
          "accuracy": 0.59
        },
        "0.01": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        }
      },
      "auroc": 0.7739906249999999
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 236,
          "fn": 164,
          "accuracy": 0.59
        },
        "0.01": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        }
      },
      "auroc": 0.7739906249999999
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.7644812499999999
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        }
      },
      "auroc": 0.7644812499999999
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.6714249999999999
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.6714249999999999
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        },
        "0.01": {
          "tp": 127,
          "fn": 273,
          "accuracy": 0.3175
        }
      },
      "auroc": 0.717953125
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        },
        "0.01": {
          "tp": 127,
          "fn": 273,
          "accuracy": 0.3175
        }
      },
      "auroc": 0.717953125
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1903,
          "fn": 297,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 1640,
          "fn": 560,
          "accuracy": 0.7454545454545455
        }
      },
      "auroc": 0.8882458333333332
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 736,
          "fn": 464,
          "accuracy": 0.6133333333333333
        },
        "0.01": {
          "tp": 594,
          "fn": 606,
          "accuracy": 0.495
        }
      },
      "auroc": 0.7005944444444444
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2639,
          "fn": 761,
          "accuracy": 0.7761764705882352
        },
        "0.01": {
          "tp": 2234,
          "fn": 1166,
          "accuracy": 0.6570588235294118
        }
      },
      "auroc": 0.822015931372549
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1513,
          "fn": 687,
          "accuracy": 0.6877272727272727
        },
        "0.01": {
          "tp": 1108,
          "fn": 1092,
          "accuracy": 0.5036363636363637
        }
      },
      "auroc": 0.8126922348484849
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 1002,
          "accuracy": 0.165
        },
        "0.01": {
          "tp": 128,
          "fn": 1072,
          "accuracy": 0.10666666666666667
        }
      },
      "auroc": 0.3656017361111111
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1711,
          "fn": 1689,
          "accuracy": 0.5032352941176471
        },
        "0.01": {
          "tp": 1236,
          "fn": 2164,
          "accuracy": 0.3635294117647059
        }
      },
      "auroc": 0.654895588235294
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3416,
          "fn": 984,
          "accuracy": 0.7763636363636364
        },
        "0.01": {
          "tp": 2748,
          "fn": 1652,
          "accuracy": 0.6245454545454545
        }
      },
      "auroc": 0.850469034090909
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 934,
          "fn": 1466,
          "accuracy": 0.38916666666666666
        },
        "0.01": {
          "tp": 722,
          "fn": 1678,
          "accuracy": 0.30083333333333334
        }
      },
      "auroc": 0.5330980902777778
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 4350,
          "fn": 2450,
          "accuracy": 0.6397058823529411
        },
        "0.01": {
          "tp": 3470,
          "fn": 3330,
          "accuracy": 0.5102941176470588
        }
      },
      "auroc": 0.7384557598039215
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9456885416666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9317479166666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9387182291666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477854166666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.7877739583333332
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        }
      },
      "auroc": 0.8677796875000001
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9467369791666667
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 343,
          "fn": 57,
          "accuracy": 0.8575
        },
        "0.01": {
          "tp": 311,
          "fn": 89,
          "accuracy": 0.7775
        }
      },
      "auroc": 0.8597609375000002
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 742,
          "fn": 58,
          "accuracy": 0.9275
        },
        "0.01": {
          "tp": 708,
          "fn": 92,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9032489583333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9438427083333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.12352083333333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        }
      },
      "auroc": 0.5336817708333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.8914677083333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.05388854166666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        },
        "0.01": {
          "tp": 143,
          "fn": 257,
          "accuracy": 0.3575
        }
      },
      "auroc": 0.472678125
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        }
      },
      "auroc": 0.9176552083333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        }
      },
      "auroc": 0.08870468749999999
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 418,
          "accuracy": 0.4775
        },
        "0.01": {
          "tp": 341,
          "fn": 459,
          "accuracy": 0.42625
        }
      },
      "auroc": 0.5031799479166666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        }
      },
      "auroc": 0.8687010416666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        }
      },
      "auroc": 0.71033125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        }
      },
      "auroc": 0.7895161458333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        }
      },
      "auroc": 0.843121875
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 23,
          "fn": 177,
          "accuracy": 0.115
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.48270208333333336
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 113,
          "fn": 287,
          "accuracy": 0.2825
        }
      },
      "auroc": 0.6629119791666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        }
      },
      "auroc": 0.8559114583333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 297,
          "accuracy": 0.2575
        },
        "0.01": {
          "tp": 35,
          "fn": 365,
          "accuracy": 0.0875
        }
      },
      "auroc": 0.5965166666666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 424,
          "fn": 376,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 259,
          "fn": 541,
          "accuracy": 0.32375
        }
      },
      "auroc": 0.7262140625000001
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9428874999999999
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9476677083333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9452776041666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9359104166666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.3355729166666668
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        }
      },
      "auroc": 0.6357416666666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.9393989583333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        }
      },
      "auroc": 0.6416203125000001
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 598,
          "fn": 202,
          "accuracy": 0.7475
        },
        "0.01": {
          "tp": 583,
          "fn": 217,
          "accuracy": 0.72875
        }
      },
      "auroc": 0.7905096354166667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.941446875
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 47,
          "fn": 153,
          "accuracy": 0.235
        }
      },
      "auroc": 0.591709375
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 284,
          "fn": 116,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        }
      },
      "auroc": 0.7665781249999999
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        }
      },
      "auroc": 0.85920625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.054324999999999984
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 244,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        }
      },
      "auroc": 0.45676562499999995
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        }
      },
      "auroc": 0.9003265625
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 313,
          "accuracy": 0.2175
        },
        "0.01": {
          "tp": 47,
          "fn": 353,
          "accuracy": 0.1175
        }
      },
      "auroc": 0.32301718749999997
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 440,
          "fn": 360,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 352,
          "fn": 448,
          "accuracy": 0.44
        }
      },
      "auroc": 0.6116718750000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9280666666666666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9020552083333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9150609375000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9010635416666668
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.5955177083333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        }
      },
      "auroc": 0.7482906250000001
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        }
      },
      "auroc": 0.9145651041666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": {
          "tp": 174,
          "fn": 226,
          "accuracy": 0.435
        }
      },
      "auroc": 0.7487864583333336
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 609,
          "fn": 191,
          "accuracy": 0.76125
        },
        "0.01": {
          "tp": 515,
          "fn": 285,
          "accuracy": 0.64375
        }
      },
      "auroc": 0.8316757812500001
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.8985541666666668
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.8985541666666668
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9023208333333332
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9023208333333332
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9004375
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 330,
          "fn": 70,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9004375
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.8374593749999998
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        },
        "0.01": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        }
      },
      "auroc": 0.8374593749999998
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        }
      },
      "auroc": 0.784521875
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        }
      },
      "auroc": 0.784521875
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        },
        "0.01": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        }
      },
      "auroc": 0.8109906250000001
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        },
        "0.01": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        }
      },
      "auroc": 0.8109906250000001
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.906646875
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.906646875
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.8739177083333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.8739177083333334
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        }
      },
      "auroc": 0.8902822916666666
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 275,
          "fn": 125,
          "accuracy": 0.6875
        }
      },
      "auroc": 0.8902822916666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9414135416666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9414135416666667
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.6891239583333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 21,
          "fn": 179,
          "accuracy": 0.105
        }
      },
      "auroc": 0.6891239583333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        },
        "0.01": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        }
      },
      "auroc": 0.8152687500000001
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        },
        "0.01": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        }
      },
      "auroc": 0.8152687500000001
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.8174250000000001
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        },
        "0.01": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        }
      },
      "auroc": 0.8174250000000001
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.7224489583333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.7224489583333333
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        }
      },
      "auroc": 0.7699369791666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        }
      },
      "auroc": 0.7699369791666666
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2007,
          "fn": 193,
          "accuracy": 0.9122727272727272
        },
        "0.01": {
          "tp": 1796,
          "fn": 404,
          "accuracy": 0.8163636363636364
        }
      },
      "auroc": 0.9065574810606061
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 747,
          "fn": 453,
          "accuracy": 0.6225
        },
        "0.01": {
          "tp": 613,
          "fn": 587,
          "accuracy": 0.5108333333333334
        }
      },
      "auroc": 0.7011720486111112
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2754,
          "fn": 646,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 2409,
          "fn": 991,
          "accuracy": 0.7085294117647059
        }
      },
      "auroc": 0.8340685049019607
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1697,
          "fn": 503,
          "accuracy": 0.7713636363636364
        },
        "0.01": {
          "tp": 1367,
          "fn": 833,
          "accuracy": 0.6213636363636363
        }
      },
      "auroc": 0.8500807765151515
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 974,
          "accuracy": 0.18833333333333332
        },
        "0.01": {
          "tp": 156,
          "fn": 1044,
          "accuracy": 0.13
        }
      },
      "auroc": 0.3849633680555555
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1923,
          "fn": 1477,
          "accuracy": 0.5655882352941176
        },
        "0.01": {
          "tp": 1523,
          "fn": 1877,
          "accuracy": 0.44794117647058823
        }
      },
      "auroc": 0.6859216911764705
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3704,
          "fn": 696,
          "accuracy": 0.8418181818181818
        },
        "0.01": {
          "tp": 3163,
          "fn": 1237,
          "accuracy": 0.7188636363636364
        }
      },
      "auroc": 0.8783191287878787
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 973,
          "fn": 1427,
          "accuracy": 0.40541666666666665
        },
        "0.01": {
          "tp": 769,
          "fn": 1631,
          "accuracy": 0.3204166666666667
        }
      },
      "auroc": 0.5430677083333334
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 4677,
          "fn": 2123,
          "accuracy": 0.6877941176470588
        },
        "0.01": {
          "tp": 3932,
          "fn": 2868,
          "accuracy": 0.5782352941176471
        }
      },
      "auroc": 0.7599950980392156
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9485416666666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9485416666666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9485416666666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9485416666666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9476677083333334
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9481046875
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9485416666666666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9481046875
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9483231770833334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9464604166666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.936546875
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9415036458333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9482666666666666
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.928034375
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.9381505208333334
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9473635416666667
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.932290625
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 789,
          "fn": 11,
          "accuracy": 0.98625
        },
        "0.01": {
          "tp": 760,
          "fn": 40,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9398270833333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9394239583333334
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9336229166666666
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 372,
          "fn": 28,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9365234375
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9343343749999999
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9305833333333333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        }
      },
      "auroc": 0.9324588541666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9368791666666667
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        }
      },
      "auroc": 0.932103125
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 731,
          "fn": 69,
          "accuracy": 0.91375
        }
      },
      "auroc": 0.9344911458333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9454614583333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9469135416666666
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9461875
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9472270833333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9467218749999999
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9469744791666667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9463442708333334
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9468177083333333
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 789,
          "fn": 11,
          "accuracy": 0.98625
        }
      },
      "auroc": 0.9465809895833334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9478500000000001
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9441375
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.94599375
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9477322916666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9442760416666667
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9460041666666666
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9477911458333333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9442067708333334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 791,
          "fn": 9,
          "accuracy": 0.98875
        }
      },
      "auroc": 0.9459989583333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9432375
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9465270833333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9448822916666667
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9439958333333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9420927083333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 385,
          "fn": 15,
          "accuracy": 0.9625
        }
      },
      "auroc": 0.9430442708333333
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        }
      },
      "auroc": 0.9436166666666668
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9443098958333334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        }
      },
      "auroc": 0.9439632812500001
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.916971875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.916971875
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9215645833333335
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9215645833333335
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9192682291666667
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 344,
          "fn": 56,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9192682291666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9476239583333332
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9476239583333332
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9471125
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9471125
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9473682291666667
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9473682291666667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9468239583333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9468239583333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9465833333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9465833333333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9467036458333333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9467036458333333
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9485416666666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9485416666666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9485416666666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9485416666666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9485416666666666
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9485416666666666
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.94355
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.94355
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9356187499999999
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9356187499999999
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9395843749999999
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9395843749999999
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2190,
          "fn": 10,
          "accuracy": 0.9954545454545455
        },
        "0.01": {
          "tp": 2141,
          "fn": 59,
          "accuracy": 0.9731818181818181
        }
      },
      "auroc": 0.9431351325757575
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1195,
          "fn": 5,
          "accuracy": 0.9958333333333333
        },
        "0.01": {
          "tp": 1160,
          "fn": 40,
          "accuracy": 0.9666666666666667
        }
      },
      "auroc": 0.9427149305555556
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3385,
          "fn": 15,
          "accuracy": 0.9955882352941177
        },
        "0.01": {
          "tp": 3301,
          "fn": 99,
          "accuracy": 0.9708823529411764
        }
      },
      "auroc": 0.9429868259803922
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2180,
          "fn": 20,
          "accuracy": 0.990909090909091
        },
        "0.01": {
          "tp": 2132,
          "fn": 68,
          "accuracy": 0.9690909090909091
        }
      },
      "auroc": 0.9426835227272727
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1186,
          "fn": 14,
          "accuracy": 0.9883333333333333
        },
        "0.01": {
          "tp": 1136,
          "fn": 64,
          "accuracy": 0.9466666666666667
        }
      },
      "auroc": 0.9398960069444444
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3366,
          "fn": 34,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 3268,
          "fn": 132,
          "accuracy": 0.9611764705882353
        }
      },
      "auroc": 0.941699693627451
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4370,
          "fn": 30,
          "accuracy": 0.9931818181818182
        },
        "0.01": {
          "tp": 4273,
          "fn": 127,
          "accuracy": 0.9711363636363637
        }
      },
      "auroc": 0.942909327651515
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2381,
          "fn": 19,
          "accuracy": 0.9920833333333333
        },
        "0.01": {
          "tp": 2296,
          "fn": 104,
          "accuracy": 0.9566666666666667
        }
      },
      "auroc": 0.94130546875
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 6751,
          "fn": 49,
          "accuracy": 0.9927941176470588
        },
        "0.01": {
          "tp": 6569,
          "fn": 231,
          "accuracy": 0.9660294117647059
        }
      },
      "auroc": 0.9423432598039216
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2340,
          "fn": 60,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 2232,
          "fn": 168,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9362735243055555
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2153,
          "fn": 247,
          "accuracy": 0.8970833333333333
        },
        "0.01": {
          "tp": 1922,
          "fn": 478,
          "accuracy": 0.8008333333333333
        }
      },
      "auroc": 0.9006421006944444
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4493,
          "fn": 307,
          "accuracy": 0.9360416666666667
        },
        "0.01": {
          "tp": 4154,
          "fn": 646,
          "accuracy": 0.8654166666666666
        }
      },
      "auroc": 0.9184578125
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2344,
          "fn": 56,
          "accuracy": 0.9766666666666667
        },
        "0.01": {
          "tp": 2248,
          "fn": 152,
          "accuracy": 0.9366666666666666
        }
      },
      "auroc": 0.9361163194444444
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1703,
          "fn": 697,
          "accuracy": 0.7095833333333333
        },
        "0.01": {
          "tp": 1361,
          "fn": 1039,
          "accuracy": 0.5670833333333334
        }
      },
      "auroc": 0.7746940104166666
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4047,
          "fn": 753,
          "accuracy": 0.843125
        },
        "0.01": {
          "tp": 3609,
          "fn": 1191,
          "accuracy": 0.751875
        }
      },
      "auroc": 0.8554051649305555
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4684,
          "fn": 116,
          "accuracy": 0.9758333333333333
        },
        "0.01": {
          "tp": 4480,
          "fn": 320,
          "accuracy": 0.9333333333333333
        }
      },
      "auroc": 0.9361949218749999
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3856,
          "fn": 944,
          "accuracy": 0.8033333333333333
        },
        "0.01": {
          "tp": 3283,
          "fn": 1517,
          "accuracy": 0.6839583333333333
        }
      },
      "auroc": 0.8376680555555555
    },
    {
      "domain": "reddit",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8540,
          "fn": 1060,
          "accuracy": 0.8895833333333333
        },
        "0.01": {
          "tp": 7763,
          "fn": 1837,
          "accuracy": 0.8086458333333333
        }
      },
      "auroc": 0.8869314887152777
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2367,
          "fn": 33,
          "accuracy": 0.98625
        },
        "0.01": {
          "tp": 2297,
          "fn": 103,
          "accuracy": 0.9570833333333333
        }
      },
      "auroc": 0.9391537326388888
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 2130,
          "accuracy": 0.1125
        },
        "0.01": {
          "tp": 224,
          "fn": 2176,
          "accuracy": 0.09333333333333334
        }
      },
      "auroc": 0.2467848958333333
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2637,
          "fn": 2163,
          "accuracy": 0.549375
        },
        "0.01": {
          "tp": 2521,
          "fn": 2279,
          "accuracy": 0.5252083333333334
        }
      },
      "auroc": 0.592969314236111
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1979,
          "fn": 421,
          "accuracy": 0.8245833333333333
        },
        "0.01": {
          "tp": 1537,
          "fn": 863,
          "accuracy": 0.6404166666666666
        }
      },
      "auroc": 0.8675402777777778
    },
    {
      "domain": "reddit",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4346,
          "fn": 454,
          "accuracy": 0.9054166666666666
        },
        "0.01": {
          "tp": 3834,
          "fn": 966,
          "accuracy": 0.79875
        }
      },
      "auroc": 0.9033470052083333
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1705,
          "fn": 695,
          "accuracy": 0.7104166666666667
        },
        "0.01": {
          "tp": 1137,
          "fn": 1263,
          "accuracy": 0.47375
        }
      },
      "auroc": 0.8227314236111112
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 914,
          "fn": 1486,
          "accuracy": 0.38083333333333336
        },
        "0.01": {
          "tp": 445,
          "fn": 1955,
          "accuracy": 0.18541666666666667
        }
      },
      "auroc": 0.6812088541666665
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2619,
          "fn": 2181,
          "accuracy": 0.545625
        },
        "0.01": {
          "tp": 1582,
          "fn": 3218,
          "accuracy": 0.32958333333333334
        }
      },
      "auroc": 0.7519701388888889
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1586,
          "fn": 814,
          "accuracy": 0.6608333333333334
        },
        "0.01": {
          "tp": 1047,
          "fn": 1353,
          "accuracy": 0.43625
        }
      },
      "auroc": 0.8053304687499999
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 481,
          "fn": 1919,
          "accuracy": 0.20041666666666666
        },
        "0.01": {
          "tp": 265,
          "fn": 2135,
          "accuracy": 0.11041666666666666
        }
      },
      "auroc": 0.5033631076388889
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2067,
          "fn": 2733,
          "accuracy": 0.430625
        },
        "0.01": {
          "tp": 1312,
          "fn": 3488,
          "accuracy": 0.2733333333333333
        }
      },
      "auroc": 0.6543467881944444
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3291,
          "fn": 1509,
          "accuracy": 0.685625
        },
        "0.01": {
          "tp": 2184,
          "fn": 2616,
          "accuracy": 0.455
        }
      },
      "auroc": 0.8140309461805556
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1395,
          "fn": 3405,
          "accuracy": 0.290625
        },
        "0.01": {
          "tp": 710,
          "fn": 4090,
          "accuracy": 0.14791666666666667
        }
      },
      "auroc": 0.5922859809027778
    },
    {
      "domain": "reddit",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4686,
          "fn": 4914,
          "accuracy": 0.488125
        },
        "0.01": {
          "tp": 2894,
          "fn": 6706,
          "accuracy": 0.30145833333333333
        }
      },
      "auroc": 0.7031584635416667
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2331,
          "fn": 69,
          "accuracy": 0.97125
        },
        "0.01": {
          "tp": 2180,
          "fn": 220,
          "accuracy": 0.9083333333333333
        }
      },
      "auroc": 0.9314141493055555
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2374,
          "fn": 26,
          "accuracy": 0.9891666666666666
        },
        "0.01": {
          "tp": 2305,
          "fn": 95,
          "accuracy": 0.9604166666666667
        }
      },
      "auroc": 0.9414355034722222
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4705,
          "fn": 95,
          "accuracy": 0.9802083333333333
        },
        "0.01": {
          "tp": 4485,
          "fn": 315,
          "accuracy": 0.934375
        }
      },
      "auroc": 0.9364248263888889
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2255,
          "fn": 145,
          "accuracy": 0.9395833333333333
        },
        "0.01": {
          "tp": 2073,
          "fn": 327,
          "accuracy": 0.86375
        }
      },
      "auroc": 0.9191589409722223
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 416,
          "fn": 1984,
          "accuracy": 0.17333333333333334
        },
        "0.01": {
          "tp": 286,
          "fn": 2114,
          "accuracy": 0.11916666666666667
        }
      },
      "auroc": 0.4401321180555556
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2671,
          "fn": 2129,
          "accuracy": 0.5564583333333334
        },
        "0.01": {
          "tp": 2359,
          "fn": 2441,
          "accuracy": 0.49145833333333333
        }
      },
      "auroc": 0.6796455295138888
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4586,
          "fn": 214,
          "accuracy": 0.9554166666666667
        },
        "0.01": {
          "tp": 4253,
          "fn": 547,
          "accuracy": 0.8860416666666666
        }
      },
      "auroc": 0.925286545138889
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2790,
          "fn": 2010,
          "accuracy": 0.58125
        },
        "0.01": {
          "tp": 2591,
          "fn": 2209,
          "accuracy": 0.5397916666666667
        }
      },
      "auroc": 0.690783810763889
    },
    {
      "domain": "reddit",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7376,
          "fn": 2224,
          "accuracy": 0.7683333333333333
        },
        "0.01": {
          "tp": 6844,
          "fn": 2756,
          "accuracy": 0.7129166666666666
        }
      },
      "auroc": 0.8080351779513889
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2352,
          "fn": 48,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 2290,
          "fn": 110,
          "accuracy": 0.9541666666666667
        }
      },
      "auroc": 0.938083767361111
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1062,
          "fn": 1338,
          "accuracy": 0.4425
        },
        "0.01": {
          "tp": 719,
          "fn": 1681,
          "accuracy": 0.2995833333333333
        }
      },
      "auroc": 0.6129934895833333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3414,
          "fn": 1386,
          "accuracy": 0.71125
        },
        "0.01": {
          "tp": 3009,
          "fn": 1791,
          "accuracy": 0.626875
        }
      },
      "auroc": 0.7755386284722222
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1792,
          "fn": 608,
          "accuracy": 0.7466666666666667
        },
        "0.01": {
          "tp": 1350,
          "fn": 1050,
          "accuracy": 0.5625
        }
      },
      "auroc": 0.8419586805555554
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 2160,
          "accuracy": 0.1
        },
        "0.01": {
          "tp": 206,
          "fn": 2194,
          "accuracy": 0.08583333333333333
        }
      },
      "auroc": 0.19818767361111111
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2032,
          "fn": 2768,
          "accuracy": 0.42333333333333334
        },
        "0.01": {
          "tp": 1556,
          "fn": 3244,
          "accuracy": 0.32416666666666666
        }
      },
      "auroc": 0.5200731770833333
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4144,
          "fn": 656,
          "accuracy": 0.8633333333333333
        },
        "0.01": {
          "tp": 3640,
          "fn": 1160,
          "accuracy": 0.7583333333333333
        }
      },
      "auroc": 0.8900212239583334
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1302,
          "fn": 3498,
          "accuracy": 0.27125
        },
        "0.01": {
          "tp": 925,
          "fn": 3875,
          "accuracy": 0.19270833333333334
        }
      },
      "auroc": 0.4055905815972223
    },
    {
      "domain": "reddit",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5446,
          "fn": 4154,
          "accuracy": 0.5672916666666666
        },
        "0.01": {
          "tp": 4565,
          "fn": 5035,
          "accuracy": 0.47552083333333334
        }
      },
      "auroc": 0.6478059027777778
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2220,
          "fn": 180,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 2008,
          "fn": 392,
          "accuracy": 0.8366666666666667
        }
      },
      "auroc": 0.9107939236111111
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1895,
          "fn": 505,
          "accuracy": 0.7895833333333333
        },
        "0.01": {
          "tp": 1523,
          "fn": 877,
          "accuracy": 0.6345833333333334
        }
      },
      "auroc": 0.8553802951388888
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4115,
          "fn": 685,
          "accuracy": 0.8572916666666667
        },
        "0.01": {
          "tp": 3531,
          "fn": 1269,
          "accuracy": 0.735625
        }
      },
      "auroc": 0.883087109375
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1968,
          "fn": 432,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 1626,
          "fn": 774,
          "accuracy": 0.6775
        }
      },
      "auroc": 0.8730464409722224
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 664,
          "fn": 1736,
          "accuracy": 0.27666666666666667
        },
        "0.01": {
          "tp": 354,
          "fn": 2046,
          "accuracy": 0.1475
        }
      },
      "auroc": 0.5985169270833334
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2632,
          "fn": 2168,
          "accuracy": 0.5483333333333333
        },
        "0.01": {
          "tp": 1980,
          "fn": 2820,
          "accuracy": 0.4125
        }
      },
      "auroc": 0.7357816840277778
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4188,
          "fn": 612,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 3634,
          "fn": 1166,
          "accuracy": 0.7570833333333333
        }
      },
      "auroc": 0.8919201822916666
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2559,
          "fn": 2241,
          "accuracy": 0.533125
        },
        "0.01": {
          "tp": 1877,
          "fn": 2923,
          "accuracy": 0.3910416666666667
        }
      },
      "auroc": 0.7269486111111112
    },
    {
      "domain": "reddit",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 6747,
          "fn": 2853,
          "accuracy": 0.7028125
        },
        "0.01": {
          "tp": 5511,
          "fn": 4089,
          "accuracy": 0.5740625
        }
      },
      "auroc": 0.809434396701389
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2040,
          "fn": 360,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 1735,
          "fn": 665,
          "accuracy": 0.7229166666666667
        }
      },
      "auroc": 0.8751640625
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2040,
          "fn": 360,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 1735,
          "fn": 665,
          "accuracy": 0.7229166666666667
        }
      },
      "auroc": 0.8751640625
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2023,
          "fn": 377,
          "accuracy": 0.8429166666666666
        },
        "0.01": {
          "tp": 1714,
          "fn": 686,
          "accuracy": 0.7141666666666666
        }
      },
      "auroc": 0.8747602430555556
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2023,
          "fn": 377,
          "accuracy": 0.8429166666666666
        },
        "0.01": {
          "tp": 1714,
          "fn": 686,
          "accuracy": 0.7141666666666666
        }
      },
      "auroc": 0.8747602430555556
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4063,
          "fn": 737,
          "accuracy": 0.8464583333333333
        },
        "0.01": {
          "tp": 3449,
          "fn": 1351,
          "accuracy": 0.7185416666666666
        }
      },
      "auroc": 0.8749621527777778
    },
    {
      "domain": "reddit",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4063,
          "fn": 737,
          "accuracy": 0.8464583333333333
        },
        "0.01": {
          "tp": 3449,
          "fn": 1351,
          "accuracy": 0.7185416666666666
        }
      },
      "auroc": 0.8749621527777778
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1642,
          "fn": 758,
          "accuracy": 0.6841666666666667
        },
        "0.01": {
          "tp": 1241,
          "fn": 1159,
          "accuracy": 0.5170833333333333
        }
      },
      "auroc": 0.81652890625
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1642,
          "fn": 758,
          "accuracy": 0.6841666666666667
        },
        "0.01": {
          "tp": 1241,
          "fn": 1159,
          "accuracy": 0.5170833333333333
        }
      },
      "auroc": 0.81652890625
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1431,
          "fn": 969,
          "accuracy": 0.59625
        },
        "0.01": {
          "tp": 1008,
          "fn": 1392,
          "accuracy": 0.42
        }
      },
      "auroc": 0.7693486979166666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1431,
          "fn": 969,
          "accuracy": 0.59625
        },
        "0.01": {
          "tp": 1008,
          "fn": 1392,
          "accuracy": 0.42
        }
      },
      "auroc": 0.7693486979166666
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3073,
          "fn": 1727,
          "accuracy": 0.6402083333333334
        },
        "0.01": {
          "tp": 2249,
          "fn": 2551,
          "accuracy": 0.4685416666666667
        }
      },
      "auroc": 0.7929388020833333
    },
    {
      "domain": "reddit",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3073,
          "fn": 1727,
          "accuracy": 0.6402083333333334
        },
        "0.01": {
          "tp": 2249,
          "fn": 2551,
          "accuracy": 0.4685416666666667
        }
      },
      "auroc": 0.7929388020833333
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1966,
          "fn": 434,
          "accuracy": 0.8191666666666667
        },
        "0.01": {
          "tp": 1517,
          "fn": 883,
          "accuracy": 0.6320833333333333
        }
      },
      "auroc": 0.8664686631944446
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1966,
          "fn": 434,
          "accuracy": 0.8191666666666667
        },
        "0.01": {
          "tp": 1517,
          "fn": 883,
          "accuracy": 0.6320833333333333
        }
      },
      "auroc": 0.8664686631944446
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1780,
          "fn": 620,
          "accuracy": 0.7416666666666667
        },
        "0.01": {
          "tp": 1261,
          "fn": 1139,
          "accuracy": 0.5254166666666666
        }
      },
      "auroc": 0.8345799479166667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1780,
          "fn": 620,
          "accuracy": 0.7416666666666667
        },
        "0.01": {
          "tp": 1261,
          "fn": 1139,
          "accuracy": 0.5254166666666666
        }
      },
      "auroc": 0.8345799479166667
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3746,
          "fn": 1054,
          "accuracy": 0.7804166666666666
        },
        "0.01": {
          "tp": 2778,
          "fn": 2022,
          "accuracy": 0.57875
        }
      },
      "auroc": 0.8505243055555556
    },
    {
      "domain": "reddit",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3746,
          "fn": 1054,
          "accuracy": 0.7804166666666666
        },
        "0.01": {
          "tp": 2778,
          "fn": 2022,
          "accuracy": 0.57875
        }
      },
      "auroc": 0.8505243055555556
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2273,
          "fn": 127,
          "accuracy": 0.9470833333333334
        },
        "0.01": {
          "tp": 2103,
          "fn": 297,
          "accuracy": 0.87625
        }
      },
      "auroc": 0.9218539062499999
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2273,
          "fn": 127,
          "accuracy": 0.9470833333333334
        },
        "0.01": {
          "tp": 2103,
          "fn": 297,
          "accuracy": 0.87625
        }
      },
      "auroc": 0.9218539062499999
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 970,
          "fn": 1430,
          "accuracy": 0.4041666666666667
        },
        "0.01": {
          "tp": 569,
          "fn": 1831,
          "accuracy": 0.23708333333333334
        }
      },
      "auroc": 0.6960193576388888
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 970,
          "fn": 1430,
          "accuracy": 0.4041666666666667
        },
        "0.01": {
          "tp": 569,
          "fn": 1831,
          "accuracy": 0.23708333333333334
        }
      },
      "auroc": 0.6960193576388888
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3243,
          "fn": 1557,
          "accuracy": 0.675625
        },
        "0.01": {
          "tp": 2672,
          "fn": 2128,
          "accuracy": 0.5566666666666666
        }
      },
      "auroc": 0.8089366319444445
    },
    {
      "domain": "reddit",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3243,
          "fn": 1557,
          "accuracy": 0.675625
        },
        "0.01": {
          "tp": 2672,
          "fn": 2128,
          "accuracy": 0.5566666666666666
        }
      },
      "auroc": 0.8089366319444445
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1557,
          "fn": 843,
          "accuracy": 0.64875
        },
        "0.01": {
          "tp": 1077,
          "fn": 1323,
          "accuracy": 0.44875
        }
      },
      "auroc": 0.7946360243055555
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1557,
          "fn": 843,
          "accuracy": 0.64875
        },
        "0.01": {
          "tp": 1077,
          "fn": 1323,
          "accuracy": 0.44875
        }
      },
      "auroc": 0.7946360243055555
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1245,
          "fn": 1155,
          "accuracy": 0.51875
        },
        "0.01": {
          "tp": 913,
          "fn": 1487,
          "accuracy": 0.3804166666666667
        }
      },
      "auroc": 0.7186175347222222
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1245,
          "fn": 1155,
          "accuracy": 0.51875
        },
        "0.01": {
          "tp": 913,
          "fn": 1487,
          "accuracy": 0.3804166666666667
        }
      },
      "auroc": 0.7186175347222222
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2802,
          "fn": 1998,
          "accuracy": 0.58375
        },
        "0.01": {
          "tp": 1990,
          "fn": 2810,
          "accuracy": 0.41458333333333336
        }
      },
      "auroc": 0.7566267795138889
    },
    {
      "domain": "reddit",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2802,
          "fn": 1998,
          "accuracy": 0.58375
        },
        "0.01": {
          "tp": 1990,
          "fn": 2810,
          "accuracy": 0.41458333333333336
        }
      },
      "auroc": 0.7566267795138889
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 22793,
          "fn": 3607,
          "accuracy": 0.8633712121212122
        },
        "0.01": {
          "tp": 19817,
          "fn": 6583,
          "accuracy": 0.7506439393939394
        }
      },
      "auroc": 0.886645643939394
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8668,
          "fn": 5732,
          "accuracy": 0.6019444444444444
        },
        "0.01": {
          "tp": 7138,
          "fn": 7262,
          "accuracy": 0.49569444444444444
        }
      },
      "auroc": 0.7064075231481481
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 31461,
          "fn": 9339,
          "accuracy": 0.7711029411764706
        },
        "0.01": {
          "tp": 26955,
          "fn": 13845,
          "accuracy": 0.6606617647058823
        }
      },
      "auroc": 0.8230321895424837
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 19373,
          "fn": 7027,
          "accuracy": 0.7338257575757576
        },
        "0.01": {
          "tp": 15346,
          "fn": 11054,
          "accuracy": 0.5812878787878788
        }
      },
      "auroc": 0.8305888099747476
    },
    {
      "domain": "reddit",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 42166,
          "fn": 10634,
          "accuracy": 0.7985984848484848
        },
        "0.01": {
          "tp": 35163,
          "fn": 17637,
          "accuracy": 0.6659659090909091
        }
      },
      "auroc": 0.8586172269570707
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.932928125
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9331307291666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9329552083333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.8022458333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": null
      },
      "auroc": 0.8676005208333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9331442708333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": null
      },
      "auroc": 0.8675869791666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 763,
          "fn": 37,
          "accuracy": 0.95375
        },
        "0.01": null
      },
      "auroc": 0.900365625
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9325229166666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.04839166666666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.49045729166666663
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.8949125000000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.03839270833333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        },
        "0.01": null
      },
      "auroc": 0.46665260416666665
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9137177083333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.0433921875
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 418,
          "accuracy": 0.4775
        },
        "0.01": null
      },
      "auroc": 0.4785549479166666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9306104166666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.774065625
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.8523380208333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9169135416666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        },
        "0.01": null
      },
      "auroc": 0.35286249999999997
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        },
        "0.01": null
      },
      "auroc": 0.6348880208333332
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9237619791666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 257,
          "accuracy": 0.3575
        },
        "0.01": null
      },
      "auroc": 0.5634640625000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 539,
          "fn": 261,
          "accuracy": 0.67375
        },
        "0.01": null
      },
      "auroc": 0.7436130208333332
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9193208333333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        },
        "0.01": null
      },
      "auroc": 0.47984062499999997
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.6995807291666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9263270833333335
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 210,
          "fn": 190,
          "accuracy": 0.525
        },
        "0.01": null
      },
      "auroc": 0.7065869791666668
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 604,
          "fn": 196,
          "accuracy": 0.755
        },
        "0.01": null
      },
      "auroc": 0.81645703125
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": null
      },
      "auroc": 0.5958562499999999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        },
        "0.01": null
      },
      "auroc": 0.7645947916666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.87068125
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.042630208333333336
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 229,
          "accuracy": 0.4275
        },
        "0.01": null
      },
      "auroc": 0.4566557291666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.9020072916666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 307,
          "accuracy": 0.2325
        },
        "0.01": null
      },
      "auroc": 0.31924322916666664
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 464,
          "fn": 336,
          "accuracy": 0.58
        },
        "0.01": null
      },
      "auroc": 0.6106252604166665
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9325500000000001
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9329416666666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.930625
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": null
      },
      "auroc": 0.7244979166666666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": null
      },
      "auroc": 0.8275614583333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9319791666666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": null
      },
      "auroc": 0.8285239583333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 705,
          "fn": 95,
          "accuracy": 0.88125
        },
        "0.01": null
      },
      "auroc": 0.8802515625
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9228822916666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9228822916666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9245958333333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9245958333333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9237390625
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9237390625
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.8960010416666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.8960010416666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.8615541666666668
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.8615541666666668
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.8787776041666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.8787776041666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9329552083333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9329552083333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9331442708333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9331442708333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": null
      },
      "auroc": 0.8019645833333335
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": null
      },
      "auroc": 0.8019645833333335
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": null
      },
      "auroc": 0.8676489583333334
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": null
      },
      "auroc": 0.8676489583333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9149989583333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9149989583333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.8565708333333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": null
      },
      "auroc": 0.8565708333333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.8857848958333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.8857848958333334
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2168,
          "fn": 32,
          "accuracy": 0.9854545454545455
        },
        "0.01": null
      },
      "auroc": 0.9270014204545455
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 821,
          "fn": 379,
          "accuracy": 0.6841666666666667
        },
        "0.01": null
      },
      "auroc": 0.7028541666666667
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2989,
          "fn": 411,
          "accuracy": 0.8791176470588236
        },
        "0.01": null
      },
      "auroc": 0.847890625
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1987,
          "fn": 213,
          "accuracy": 0.9031818181818182
        },
        "0.01": null
      },
      "auroc": 0.8948226325757574
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 907,
          "accuracy": 0.24416666666666667
        },
        "0.01": null
      },
      "auroc": 0.4067449652777778
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2280,
          "fn": 1120,
          "accuracy": 0.6705882352941176
        },
        "0.01": null
      },
      "auroc": 0.7225599264705882
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 4155,
          "fn": 245,
          "accuracy": 0.9443181818181818
        },
        "0.01": null
      },
      "auroc": 0.9109120265151515
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1114,
          "fn": 1286,
          "accuracy": 0.46416666666666667
        },
        "0.01": null
      },
      "auroc": 0.5547995659722222
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 5269,
          "fn": 1531,
          "accuracy": 0.7748529411764706
        },
        "0.01": null
      },
      "auroc": 0.785225275735294
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9327541666666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9279354166666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9303447916666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9328614583333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.7734114583333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": null
      },
      "auroc": 0.8531364583333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9328078125
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.8506734374999999
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 754,
          "fn": 46,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.891740625
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9203874999999999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.04263854166666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        },
        "0.01": null
      },
      "auroc": 0.4815130208333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": null
      },
      "auroc": 0.8671197916666668
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.0323125
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 234,
          "accuracy": 0.415
        },
        "0.01": null
      },
      "auroc": 0.4497161458333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": null
      },
      "auroc": 0.8937536458333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.03747552083333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 436,
          "accuracy": 0.455
        },
        "0.01": null
      },
      "auroc": 0.46561458333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.8944583333333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": null
      },
      "auroc": 0.6551072916666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        },
        "0.01": null
      },
      "auroc": 0.7747828124999999
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.8728135416666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": null
      },
      "auroc": 0.2711989583333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 224,
          "accuracy": 0.44
        },
        "0.01": null
      },
      "auroc": 0.5720062499999999
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.8836359375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 336,
          "accuracy": 0.16
        },
        "0.01": null
      },
      "auroc": 0.46315312499999994
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 422,
          "fn": 378,
          "accuracy": 0.5275
        },
        "0.01": null
      },
      "auroc": 0.6733945312499999
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.904190625
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9187619791666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9105729166666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": null
      },
      "auroc": 0.46227916666666663
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        },
        "0.01": null
      },
      "auroc": 0.6864260416666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9073817708333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        },
        "0.01": null
      },
      "auroc": 0.6978062500000001
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 586,
          "fn": 214,
          "accuracy": 0.7325
        },
        "0.01": null
      },
      "auroc": 0.8025940104166668
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9213635416666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": null
      },
      "auroc": 0.5118458333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 256,
          "fn": 144,
          "accuracy": 0.64
        },
        "0.01": null
      },
      "auroc": 0.7166046875
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.833025
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.04078645833333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 248,
          "accuracy": 0.38
        },
        "0.01": null
      },
      "auroc": 0.43690572916666665
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": null
      },
      "auroc": 0.8771942708333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 341,
          "accuracy": 0.1475
        },
        "0.01": null
      },
      "auroc": 0.2763161458333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 408,
          "fn": 392,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.5767552083333332
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9298770833333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9197812500000001
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9248291666666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9185729166666666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        },
        "0.01": null
      },
      "auroc": 0.6608999999999999
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": null
      },
      "auroc": 0.7897364583333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.924225
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        },
        "0.01": null
      },
      "auroc": 0.7903406250000001
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 673,
          "fn": 127,
          "accuracy": 0.84125
        },
        "0.01": null
      },
      "auroc": 0.8572828125
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.8784041666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.8784041666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.8823760416666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.8823760416666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": null
      },
      "auroc": 0.8803901041666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": null
      },
      "auroc": 0.8803901041666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.8679718750000001
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.8679718750000001
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": null
      },
      "auroc": 0.8309958333333335
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": null
      },
      "auroc": 0.8309958333333335
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": null
      },
      "auroc": 0.8494838541666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": null
      },
      "auroc": 0.8494838541666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9310447916666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9310447916666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9321890625000001
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9321890625000001
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": null
      },
      "auroc": 0.7569072916666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 96,
          "fn": 104,
          "accuracy": 0.48
        },
        "0.01": null
      },
      "auroc": 0.7569072916666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        },
        "0.01": null
      },
      "auroc": 0.8451203125
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        },
        "0.01": null
      },
      "auroc": 0.8451203125
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.8924406249999999
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.8924406249999999
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.8257395833333332
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.8257395833333332
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": null
      },
      "auroc": 0.8590901041666668
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": null
      },
      "auroc": 0.8590901041666668
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2097,
          "fn": 103,
          "accuracy": 0.9531818181818181
        },
        "0.01": null
      },
      "auroc": 0.909864962121212
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 713,
          "fn": 487,
          "accuracy": 0.5941666666666666
        },
        "0.01": null
      },
      "auroc": 0.6651069444444444
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2810,
          "fn": 590,
          "accuracy": 0.8264705882352941
        },
        "0.01": null
      },
      "auroc": 0.8234797794117646
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1839,
          "fn": 361,
          "accuracy": 0.8359090909090909
        },
        "0.01": null
      },
      "auroc": 0.8692753787878789
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 950,
          "accuracy": 0.20833333333333334
        },
        "0.01": null
      },
      "auroc": 0.37348142361111114
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2089,
          "fn": 1311,
          "accuracy": 0.6144117647058823
        },
        "0.01": null
      },
      "auroc": 0.6942892769607842
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3936,
          "fn": 464,
          "accuracy": 0.8945454545454545
        },
        "0.01": null
      },
      "auroc": 0.8895701704545456
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 963,
          "fn": 1437,
          "accuracy": 0.40125
        },
        "0.01": null
      },
      "auroc": 0.5192941840277777
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 4899,
          "fn": 1901,
          "accuracy": 0.7204411764705883
        },
        "0.01": null
      },
      "auroc": 0.7588845281862745
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9327
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9166333333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9246666666666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9315895833333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": null
      },
      "auroc": 0.7301104166666665
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": null
      },
      "auroc": 0.8308500000000001
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9321447916666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        },
        "0.01": null
      },
      "auroc": 0.8233718750000001
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 733,
          "fn": 67,
          "accuracy": 0.91625
        },
        "0.01": null
      },
      "auroc": 0.8777583333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9172020833333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.0406625
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        },
        "0.01": null
      },
      "auroc": 0.4789322916666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": null
      },
      "auroc": 0.8361291666666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.036331249999999995
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 249,
          "accuracy": 0.3775
        },
        "0.01": null
      },
      "auroc": 0.43623020833333337
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": null
      },
      "auroc": 0.8766656250000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.038496875
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 455,
          "accuracy": 0.43125
        },
        "0.01": null
      },
      "auroc": 0.4575812500000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.8797041666666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        },
        "0.01": null
      },
      "auroc": 0.5989864583333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 221,
          "fn": 179,
          "accuracy": 0.5525
        },
        "0.01": null
      },
      "auroc": 0.7393453124999999
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.84708125
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.22291458333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 238,
          "accuracy": 0.405
        },
        "0.01": null
      },
      "auroc": 0.5349979166666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": null
      },
      "auroc": 0.8633927083333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 44,
          "fn": 356,
          "accuracy": 0.11
        },
        "0.01": null
      },
      "auroc": 0.41095052083333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 417,
          "accuracy": 0.47875
        },
        "0.01": null
      },
      "auroc": 0.6371716145833334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.8983895833333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9158614583333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.8986979166666668
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.3404833333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        },
        "0.01": null
      },
      "auroc": 0.619590625
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.89854375
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        },
        "0.01": null
      },
      "auroc": 0.6369083333333332
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 581,
          "fn": 219,
          "accuracy": 0.72625
        },
        "0.01": null
      },
      "auroc": 0.7677260416666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.909059375
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        },
        "0.01": null
      },
      "auroc": 0.44946249999999993
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        },
        "0.01": null
      },
      "auroc": 0.6792609375000002
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 107,
          "fn": 93,
          "accuracy": 0.535
        },
        "0.01": null
      },
      "auroc": 0.7865635416666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.032653125
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 107,
          "fn": 293,
          "accuracy": 0.2675
        },
        "0.01": null
      },
      "auroc": 0.40960833333333335
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": null
      },
      "auroc": 0.8478114583333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 362,
          "accuracy": 0.095
        },
        "0.01": null
      },
      "auroc": 0.24105781250000005
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 462,
          "accuracy": 0.4225
        },
        "0.01": null
      },
      "auroc": 0.5444346354166667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9247375
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9046458333333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9146916666666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9115156249999999
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 51,
          "fn": 149,
          "accuracy": 0.255
        },
        "0.01": null
      },
      "auroc": 0.5509927083333332
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.7312541666666668
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9181265625
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": null
      },
      "auroc": 0.7278192708333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 631,
          "fn": 169,
          "accuracy": 0.78875
        },
        "0.01": null
      },
      "auroc": 0.8229729166666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.8815395833333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.8815395833333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.8791229166666668
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.8791229166666668
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.88033125
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": null
      },
      "auroc": 0.88033125
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.8455229166666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": null
      },
      "auroc": 0.8455229166666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.7775739583333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.7775739583333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": null
      },
      "auroc": 0.8115484375
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": null
      },
      "auroc": 0.8115484375
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9314322916666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9314322916666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9206385416666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9206385416666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9260354166666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9260354166666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9327
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9327
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": null
      },
      "auroc": 0.6498739583333334
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        },
        "0.01": null
      },
      "auroc": 0.6498739583333334
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.7912869791666668
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.7912869791666668
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.8761958333333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.8761958333333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": null
      },
      "auroc": 0.7869458333333335
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        },
        "0.01": null
      },
      "auroc": 0.7869458333333335
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        },
        "0.01": null
      },
      "auroc": 0.8315708333333335
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        },
        "0.01": null
      },
      "auroc": 0.8315708333333335
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2061,
          "fn": 139,
          "accuracy": 0.9368181818181818
        },
        "0.01": null
      },
      "auroc": 0.9026530303030302
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 664,
          "fn": 536,
          "accuracy": 0.5533333333333333
        },
        "0.01": null
      },
      "auroc": 0.6406206597222222
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2725,
          "fn": 675,
          "accuracy": 0.8014705882352942
        },
        "0.01": null
      },
      "auroc": 0.8101710171568628
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1653,
          "fn": 547,
          "accuracy": 0.7513636363636363
        },
        "0.01": null
      },
      "auroc": 0.8387029356060606
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 1005,
          "accuracy": 0.1625
        },
        "0.01": null
      },
      "auroc": 0.31891423611111114
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1848,
          "fn": 1552,
          "accuracy": 0.5435294117647059
        },
        "0.01": null
      },
      "auroc": 0.655248100490196
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3714,
          "fn": 686,
          "accuracy": 0.8440909090909091
        },
        "0.01": null
      },
      "auroc": 0.8706779829545455
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 859,
          "fn": 1541,
          "accuracy": 0.35791666666666666
        },
        "0.01": null
      },
      "auroc": 0.47976744791666665
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 4573,
          "fn": 2227,
          "accuracy": 0.6725
        },
        "0.01": null
      },
      "auroc": 0.7327095588235294
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.8804708333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 73,
          "fn": 127,
          "accuracy": 0.365
        },
        "0.01": null
      },
      "auroc": 0.7066395833333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        },
        "0.01": null
      },
      "auroc": 0.7935552083333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.8791354166666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        },
        "0.01": null
      },
      "auroc": 0.47194583333333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.675540625
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.879803125
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 298,
          "accuracy": 0.255
        },
        "0.01": null
      },
      "auroc": 0.5892927083333332
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 455,
          "fn": 345,
          "accuracy": 0.56875
        },
        "0.01": null
      },
      "auroc": 0.7345479166666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.8961364583333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.030998958333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        },
        "0.01": null
      },
      "auroc": 0.46356770833333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": null
      },
      "auroc": 0.6459416666666665
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.035648958333333335
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 341,
          "accuracy": 0.1475
        },
        "0.01": null
      },
      "auroc": 0.34079531250000006
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        },
        "0.01": null
      },
      "auroc": 0.7710390625000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.033323958333333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 556,
          "accuracy": 0.305
        },
        "0.01": null
      },
      "auroc": 0.4021815104166666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": null
      },
      "auroc": 0.7313531249999999
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.36378125
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 305,
          "accuracy": 0.2375
        },
        "0.01": null
      },
      "auroc": 0.5475671875
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        },
        "0.01": null
      },
      "auroc": 0.69254375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.12428020833333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 67,
          "fn": 333,
          "accuracy": 0.1675
        },
        "0.01": null
      },
      "auroc": 0.4084119791666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 240,
          "accuracy": 0.4
        },
        "0.01": null
      },
      "auroc": 0.7119484375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.24403072916666663
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 638,
          "accuracy": 0.2025
        },
        "0.01": null
      },
      "auroc": 0.4779895833333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9225916666666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.8756447916666665
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.8991182291666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        },
        "0.01": null
      },
      "auroc": 0.7945322916666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.101925
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 126,
          "fn": 274,
          "accuracy": 0.315
        },
        "0.01": null
      },
      "auroc": 0.44822864583333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 321,
          "fn": 79,
          "accuracy": 0.8025
        },
        "0.01": null
      },
      "auroc": 0.8585619791666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 229,
          "accuracy": 0.4275
        },
        "0.01": null
      },
      "auroc": 0.48878489583333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 492,
          "fn": 308,
          "accuracy": 0.615
        },
        "0.01": null
      },
      "auroc": 0.6736734375
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.8972645833333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.18946666666666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        },
        "0.01": null
      },
      "auroc": 0.5433656250000001
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        },
        "0.01": null
      },
      "auroc": 0.5896
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.02946666666666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 38,
          "fn": 362,
          "accuracy": 0.095
        },
        "0.01": null
      },
      "auroc": 0.3095333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        },
        "0.01": null
      },
      "auroc": 0.7434322916666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 398,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.10946666666666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 220,
          "fn": 580,
          "accuracy": 0.275
        },
        "0.01": null
      },
      "auroc": 0.4264494791666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.8622416666666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        },
        "0.01": null
      },
      "auroc": 0.7138135416666668
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 243,
          "fn": 157,
          "accuracy": 0.6075
        },
        "0.01": null
      },
      "auroc": 0.7880276041666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": null
      },
      "auroc": 0.8143708333333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.2670645833333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 261,
          "accuracy": 0.3475
        },
        "0.01": null
      },
      "auroc": 0.5407177083333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.83830625
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 322,
          "accuracy": 0.195
        },
        "0.01": null
      },
      "auroc": 0.49043906249999997
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 418,
          "accuracy": 0.4775
        },
        "0.01": null
      },
      "auroc": 0.66437265625
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": null
      },
      "auroc": 0.7380791666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": null
      },
      "auroc": 0.7380791666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.7355624999999999
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": null
      },
      "auroc": 0.7355624999999999
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        },
        "0.01": null
      },
      "auroc": 0.7368208333333335
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        },
        "0.01": null
      },
      "auroc": 0.7368208333333335
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        },
        "0.01": null
      },
      "auroc": 0.7108354166666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        },
        "0.01": null
      },
      "auroc": 0.7108354166666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        },
        "0.01": null
      },
      "auroc": 0.6064552083333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        },
        "0.01": null
      },
      "auroc": 0.6064552083333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 267,
          "accuracy": 0.3325
        },
        "0.01": null
      },
      "auroc": 0.6586453125
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 267,
          "accuracy": 0.3325
        },
        "0.01": null
      },
      "auroc": 0.6586453125
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": null
      },
      "auroc": 0.8027239583333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": null
      },
      "auroc": 0.8027239583333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        },
        "0.01": null
      },
      "auroc": 0.7138510416666668
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        },
        "0.01": null
      },
      "auroc": 0.7138510416666668
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        },
        "0.01": null
      },
      "auroc": 0.7582875
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        },
        "0.01": null
      },
      "auroc": 0.7582875
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.8515489583333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.8515489583333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": null
      },
      "auroc": 0.4201197916666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": null
      },
      "auroc": 0.4201197916666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 235,
          "accuracy": 0.4125
        },
        "0.01": null
      },
      "auroc": 0.635834375
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 235,
          "accuracy": 0.4125
        },
        "0.01": null
      },
      "auroc": 0.635834375
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.7736635416666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        },
        "0.01": null
      },
      "auroc": 0.7736635416666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        },
        "0.01": null
      },
      "auroc": 0.6445239583333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 78,
          "fn": 122,
          "accuracy": 0.39
        },
        "0.01": null
      },
      "auroc": 0.6445239583333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        },
        "0.01": null
      },
      "auroc": 0.70909375
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        },
        "0.01": null
      },
      "auroc": 0.70909375
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1616,
          "fn": 584,
          "accuracy": 0.7345454545454545
        },
        "0.01": null
      },
      "auroc": 0.8242644886363636
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 878,
          "accuracy": 0.2683333333333333
        },
        "0.01": null
      },
      "auroc": 0.48005746527777776
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1938,
          "fn": 1462,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.7027796568627451
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 898,
          "fn": 1302,
          "accuracy": 0.4081818181818182
        },
        "0.01": null
      },
      "auroc": 0.6851487689393938
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 1167,
          "accuracy": 0.0275
        },
        "0.01": null
      },
      "auroc": 0.171721875
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 931,
          "fn": 2469,
          "accuracy": 0.2738235294117647
        },
        "0.01": null
      },
      "auroc": 0.5039392769607843
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2514,
          "fn": 1886,
          "accuracy": 0.5713636363636364
        },
        "0.01": null
      },
      "auroc": 0.7547066287878788
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 2045,
          "accuracy": 0.14791666666666667
        },
        "0.01": null
      },
      "auroc": 0.32588967013888887
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2869,
          "fn": 3931,
          "accuracy": 0.4219117647058824
        },
        "0.01": null
      },
      "auroc": 0.6033594669117648
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9318385416666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9325859375000001
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9327
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": null
      },
      "auroc": 0.7856020833333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.8591510416666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9330166666666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": null
      },
      "auroc": 0.8587203125
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 757,
          "fn": 43,
          "accuracy": 0.94625
        },
        "0.01": null
      },
      "auroc": 0.8958684895833333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.93213125
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.043207291666666675
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.4876692708333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.875121875
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.03709479166666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 227,
          "accuracy": 0.4325
        },
        "0.01": null
      },
      "auroc": 0.4561083333333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": null
      },
      "auroc": 0.9036265625000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.040151041666666665
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 427,
          "accuracy": 0.46625
        },
        "0.01": null
      },
      "auroc": 0.4718888020833334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9205447916666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": null
      },
      "auroc": 0.7254218750000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        },
        "0.01": null
      },
      "auroc": 0.8229833333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.905415625
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        },
        "0.01": null
      },
      "auroc": 0.30817916666666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": null
      },
      "auroc": 0.6067973958333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9129802083333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        },
        "0.01": null
      },
      "auroc": 0.5168005208333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 503,
          "fn": 297,
          "accuracy": 0.62875
        },
        "0.01": null
      },
      "auroc": 0.7148903645833332
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.922934375
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9281338541666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9153791666666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.41712916666666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        },
        "0.01": null
      },
      "auroc": 0.6662541666666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9191567708333335
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.6752312500000001
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 597,
          "fn": 203,
          "accuracy": 0.74625
        },
        "0.01": null
      },
      "auroc": 0.7971940104166667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.932928125
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        },
        "0.01": null
      },
      "auroc": 0.5201541666666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        },
        "0.01": null
      },
      "auroc": 0.7265411458333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.8444458333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.03802291666666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 244,
          "accuracy": 0.39
        },
        "0.01": null
      },
      "auroc": 0.44123437499999996
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.8886869791666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        },
        "0.01": null
      },
      "auroc": 0.27908854166666663
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 418,
          "fn": 382,
          "accuracy": 0.5225
        },
        "0.01": null
      },
      "auroc": 0.5838877604166667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.93185
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9293989583333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9306244791666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9297697916666666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": null
      },
      "auroc": 0.67063125
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        },
        "0.01": null
      },
      "auroc": 0.8002005208333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9308098958333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        },
        "0.01": null
      },
      "auroc": 0.8000151041666668
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 682,
          "fn": 118,
          "accuracy": 0.8525
        },
        "0.01": null
      },
      "auroc": 0.8654124999999999
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.8973166666666668
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.8973166666666668
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.8919104166666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.8919104166666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.8946135416666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.8946135416666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": null
      },
      "auroc": 0.8703520833333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": null
      },
      "auroc": 0.8703520833333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": null
      },
      "auroc": 0.8278197916666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": null
      },
      "auroc": 0.8278197916666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.8490859375000002
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.8490859375000002
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9327
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9327
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9330166666666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9330166666666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.7651656250000001
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.7651656250000001
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        },
        "0.01": null
      },
      "auroc": 0.8492494791666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        },
        "0.01": null
      },
      "auroc": 0.8492494791666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9074760416666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9074760416666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.8378614583333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        },
        "0.01": null
      },
      "auroc": 0.8378614583333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": null
      },
      "auroc": 0.8726687500000001
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": null
      },
      "auroc": 0.8726687500000001
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2143,
          "fn": 57,
          "accuracy": 0.9740909090909091
        },
        "0.01": null
      },
      "auroc": 0.9195939393939393
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 760,
          "fn": 440,
          "accuracy": 0.6333333333333333
        },
        "0.01": null
      },
      "auroc": 0.6805590277777779
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2903,
          "fn": 497,
          "accuracy": 0.8538235294117648
        },
        "0.01": null
      },
      "auroc": 0.8352286764705883
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1892,
          "fn": 308,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.8780263257575759
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 259,
          "fn": 941,
          "accuracy": 0.21583333333333332
        },
        "0.01": null
      },
      "auroc": 0.3761098958333332
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2151,
          "fn": 1249,
          "accuracy": 0.6326470588235295
        },
        "0.01": null
      },
      "auroc": 0.700879350490196
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 4035,
          "fn": 365,
          "accuracy": 0.9170454545454545
        },
        "0.01": null
      },
      "auroc": 0.8988101325757577
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1019,
          "fn": 1381,
          "accuracy": 0.4245833333333333
        },
        "0.01": null
      },
      "auroc": 0.5283344618055557
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 5054,
          "fn": 1746,
          "accuracy": 0.7432352941176471
        },
        "0.01": null
      },
      "auroc": 0.7680540134803923
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9308489583333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9219458333333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9263973958333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9262385416666666
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.87278125
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.8995098958333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9285437500000001
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.8973635416666668
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 763,
          "fn": 37,
          "accuracy": 0.95375
        },
        "0.01": null
      },
      "auroc": 0.9129536458333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.929284375
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        },
        "0.01": null
      },
      "auroc": 0.57509375
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 228,
          "fn": 172,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.7521890625000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9055375
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        },
        "0.01": null
      },
      "auroc": 0.4365510416666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 210,
          "fn": 190,
          "accuracy": 0.525
        },
        "0.01": null
      },
      "auroc": 0.6710442708333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9174109375
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 352,
          "accuracy": 0.12
        },
        "0.01": null
      },
      "auroc": 0.5058223958333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 438,
          "fn": 362,
          "accuracy": 0.5475
        },
        "0.01": null
      },
      "auroc": 0.7116166666666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.8847958333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.7690260416666665
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.8269109375000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.879128125
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        },
        "0.01": null
      },
      "auroc": 0.6996520833333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": null
      },
      "auroc": 0.7893901041666668
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.8819619791666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 234,
          "accuracy": 0.415
        },
        "0.01": null
      },
      "auroc": 0.7343390624999999
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 535,
          "fn": 265,
          "accuracy": 0.66875
        },
        "0.01": null
      },
      "auroc": 0.8081505208333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.8892489583333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.929921875
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9095854166666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9122979166666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        },
        "0.01": null
      },
      "auroc": 0.7790979166666665
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.8456979166666668
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9007734375
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        },
        "0.01": null
      },
      "auroc": 0.8545098958333336
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 688,
          "fn": 112,
          "accuracy": 0.86
        },
        "0.01": null
      },
      "auroc": 0.8776416666666665
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9295458333333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": null
      },
      "auroc": 0.8159958333333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": null
      },
      "auroc": 0.8727708333333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": null
      },
      "auroc": 0.8930197916666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        },
        "0.01": null
      },
      "auroc": 0.564921875
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": null
      },
      "auroc": 0.7289708333333332
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9112828125
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 234,
          "accuracy": 0.415
        },
        "0.01": null
      },
      "auroc": 0.6904588541666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 548,
          "fn": 252,
          "accuracy": 0.685
        },
        "0.01": null
      },
      "auroc": 0.8008708333333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9198375
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9035052083333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9116713541666668
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9081635416666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": null
      },
      "auroc": 0.8399364583333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": null
      },
      "auroc": 0.87405
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9140005208333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        },
        "0.01": null
      },
      "auroc": 0.8717208333333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 735,
          "fn": 65,
          "accuracy": 0.91875
        },
        "0.01": null
      },
      "auroc": 0.8928606770833333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.8597166666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.8597166666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.8689083333333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.8689083333333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": null
      },
      "auroc": 0.8643125
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": null
      },
      "auroc": 0.8643125
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.8927197916666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.8927197916666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.8835249999999999
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.8835249999999999
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.8881223958333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.8881223958333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9301604166666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9301604166666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9255020833333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9255020833333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.92783125
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.92783125
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9313781250000001
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9313781250000001
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.8987083333333334
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.8987083333333334
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9150432291666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9150432291666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9023791666666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9023791666666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.8663500000000001
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.8663500000000001
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": null
      },
      "auroc": 0.8843645833333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": null
      },
      "auroc": 0.8843645833333333
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2098,
          "fn": 102,
          "accuracy": 0.9536363636363636
        },
        "0.01": null
      },
      "auroc": 0.9090832386363636
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 856,
          "fn": 344,
          "accuracy": 0.7133333333333334
        },
        "0.01": null
      },
      "auroc": 0.8192480902777778
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2954,
          "fn": 446,
          "accuracy": 0.8688235294117647
        },
        "0.01": null
      },
      "auroc": 0.8773767156862745
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2034,
          "fn": 166,
          "accuracy": 0.9245454545454546
        },
        "0.01": null
      },
      "auroc": 0.8970344696969696
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 545,
          "fn": 655,
          "accuracy": 0.45416666666666666
        },
        "0.01": null
      },
      "auroc": 0.6988234375
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2579,
          "fn": 821,
          "accuracy": 0.7585294117647059
        },
        "0.01": null
      },
      "auroc": 0.8270776348039215
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 4132,
          "fn": 268,
          "accuracy": 0.9390909090909091
        },
        "0.01": null
      },
      "auroc": 0.9030588541666666
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1401,
          "fn": 999,
          "accuracy": 0.58375
        },
        "0.01": null
      },
      "auroc": 0.7590357638888888
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 5533,
          "fn": 1267,
          "accuracy": 0.8136764705882353
        },
        "0.01": null
      },
      "auroc": 0.852227175245098
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.932928125
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9331307291666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9327
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.800628125
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": null
      },
      "auroc": 0.8666640625
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9330166666666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": null
      },
      "auroc": 0.866778125
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 763,
          "fn": 37,
          "accuracy": 0.95375
        },
        "0.01": null
      },
      "auroc": 0.8998973958333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9291802083333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.04750416666666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": null
      },
      "auroc": 0.48834218749999997
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.8829541666666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.03584895833333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        },
        "0.01": null
      },
      "auroc": 0.4594015625
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9060671875000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.0416765625
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 426,
          "accuracy": 0.4675
        },
        "0.01": null
      },
      "auroc": 0.47387187499999994
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9306104166666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": null
      },
      "auroc": 0.7719739583333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 327,
          "fn": 73,
          "accuracy": 0.8175
        },
        "0.01": null
      },
      "auroc": 0.8512921875000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9145208333333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        },
        "0.01": null
      },
      "auroc": 0.34920104166666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 210,
          "fn": 190,
          "accuracy": 0.525
        },
        "0.01": null
      },
      "auroc": 0.6318609375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.922565625
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 142,
          "fn": 258,
          "accuracy": 0.355
        },
        "0.01": null
      },
      "auroc": 0.5605875000000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 537,
          "fn": 263,
          "accuracy": 0.67125
        },
        "0.01": null
      },
      "auroc": 0.7415765624999998
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9308625
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9320979166666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9156447916666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": null
      },
      "auroc": 0.4610197916666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": null
      },
      "auroc": 0.6883322916666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9232536458333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        },
        "0.01": null
      },
      "auroc": 0.6971765625
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 600,
          "fn": 200,
          "accuracy": 0.75
        },
        "0.01": null
      },
      "auroc": 0.8102151041666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9328614583333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        },
        "0.01": null
      },
      "auroc": 0.59115
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 291,
          "fn": 109,
          "accuracy": 0.7275
        },
        "0.01": null
      },
      "auroc": 0.7620057291666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.856803125
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.04150104166666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 237,
          "accuracy": 0.4075
        },
        "0.01": null
      },
      "auroc": 0.44915208333333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": null
      },
      "auroc": 0.8948322916666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 91,
          "fn": 309,
          "accuracy": 0.2275
        },
        "0.01": null
      },
      "auroc": 0.3163255208333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 454,
          "fn": 346,
          "accuracy": 0.5675
        },
        "0.01": null
      },
      "auroc": 0.60557890625
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9325500000000001
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9329416666666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.92974375
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": null
      },
      "auroc": 0.7205604166666666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.8251520833333335
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9315385416666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        },
        "0.01": null
      },
      "auroc": 0.8265552083333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 704,
          "fn": 96,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.879046875
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9032916666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9032916666666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9175937500000001
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9175937500000001
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9104427083333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9104427083333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.8931875
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.8931875
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": null
      },
      "auroc": 0.8566239583333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": null
      },
      "auroc": 0.8566239583333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.8749057291666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.8749057291666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9329416666666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9329416666666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9331375
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9331375
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": null
      },
      "auroc": 0.798015625
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 124,
          "fn": 76,
          "accuracy": 0.62
        },
        "0.01": null
      },
      "auroc": 0.798015625
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": null
      },
      "auroc": 0.8656744791666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        },
        "0.01": null
      },
      "auroc": 0.8656744791666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.91431875
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.91431875
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.8546802083333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.8546802083333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": null
      },
      "auroc": 0.8844994791666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 345,
          "fn": 55,
          "accuracy": 0.8625
        },
        "0.01": null
      },
      "auroc": 0.8844994791666667
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2156,
          "fn": 44,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9243314393939394
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 818,
          "fn": 382,
          "accuracy": 0.6816666666666666
        },
        "0.01": null
      },
      "auroc": 0.701573263888889
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2974,
          "fn": 426,
          "accuracy": 0.8747058823529412
        },
        "0.01": null
      },
      "auroc": 0.8457109068627451
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1964,
          "fn": 236,
          "accuracy": 0.8927272727272727
        },
        "0.01": null
      },
      "auroc": 0.8902019886363636
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 291,
          "fn": 909,
          "accuracy": 0.2425
        },
        "0.01": null
      },
      "auroc": 0.4014598958333333
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2255,
          "fn": 1145,
          "accuracy": 0.663235294117647
        },
        "0.01": null
      },
      "auroc": 0.7177047794117648
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 4120,
          "fn": 280,
          "accuracy": 0.9363636363636364
        },
        "0.01": null
      },
      "auroc": 0.9072667140151515
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1109,
          "fn": 1291,
          "accuracy": 0.46208333333333335
        },
        "0.01": null
      },
      "auroc": 0.551516579861111
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 5229,
          "fn": 1571,
          "accuracy": 0.7689705882352941
        },
        "0.01": null
      },
      "auroc": 0.7817078431372549
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9328614583333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9330973958333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9329833333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": null
      },
      "auroc": 0.8065927083333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.8697880208333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9331583333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.8697270833333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 762,
          "fn": 38,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9014427083333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.04807395833333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.49070364583333337
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.8991989583333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.03839270833333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        },
        "0.01": null
      },
      "auroc": 0.4687958333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9162661458333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.04323333333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 416,
          "accuracy": 0.48
        },
        "0.01": null
      },
      "auroc": 0.47974973958333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9308135416666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": null
      },
      "auroc": 0.7780135416666668
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": null
      },
      "auroc": 0.8544135416666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9198604166666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        },
        "0.01": null
      },
      "auroc": 0.3727697916666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 214,
          "fn": 186,
          "accuracy": 0.535
        },
        "0.01": null
      },
      "auroc": 0.6463151041666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9253369791666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        },
        "0.01": null
      },
      "auroc": 0.5753916666666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 545,
          "fn": 255,
          "accuracy": 0.68125
        },
        "0.01": null
      },
      "auroc": 0.7503643229166667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9189375000000001
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": null
      },
      "auroc": 0.5083833333333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        },
        "0.01": null
      },
      "auroc": 0.7136604166666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9261354166666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        },
        "0.01": null
      },
      "auroc": 0.7208583333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 605,
          "fn": 195,
          "accuracy": 0.75625
        },
        "0.01": null
      },
      "auroc": 0.823496875
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        },
        "0.01": null
      },
      "auroc": 0.6059375
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        },
        "0.01": null
      },
      "auroc": 0.7696354166666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.8785416666666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.04593958333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        },
        "0.01": null
      },
      "auroc": 0.46224062499999996
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 369,
          "fn": 31,
          "accuracy": 0.9225
        },
        "0.01": null
      },
      "auroc": 0.9059375000000001
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 307,
          "accuracy": 0.2325
        },
        "0.01": null
      },
      "auroc": 0.32593854166666664
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 462,
          "fn": 338,
          "accuracy": 0.5775
        },
        "0.01": null
      },
      "auroc": 0.6159380208333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9319989583333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9326661458333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9317697916666666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": null
      },
      "auroc": 0.710834375
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.8213020833333332
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9325515624999999
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 96,
          "accuracy": 0.76
        },
        "0.01": null
      },
      "auroc": 0.8214166666666665
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 704,
          "fn": 96,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.8769841145833333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.920384375
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.920384375
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9208729166666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9208729166666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9206286458333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9206286458333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9060854166666668
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": null
      },
      "auroc": 0.9060854166666668
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.8699197916666668
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.8699197916666668
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": null
      },
      "auroc": 0.8880026041666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": null
      },
      "auroc": 0.8880026041666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": null
      },
      "auroc": 0.7963625
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        },
        "0.01": null
      },
      "auroc": 0.7963625
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": null
      },
      "auroc": 0.8648479166666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        },
        "0.01": null
      },
      "auroc": 0.8648479166666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9174427083333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9174427083333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.8660708333333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.8660708333333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": null
      },
      "auroc": 0.8917567708333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": null
      },
      "auroc": 0.8917567708333334
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2174,
          "fn": 26,
          "accuracy": 0.9881818181818182
        },
        "0.01": null
      },
      "auroc": 0.9280053977272729
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 824,
          "fn": 376,
          "accuracy": 0.6866666666666666
        },
        "0.01": null
      },
      "auroc": 0.7050364583333333
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2998,
          "fn": 402,
          "accuracy": 0.8817647058823529
        },
        "0.01": null
      },
      "auroc": 0.8493104779411765
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1996,
          "fn": 204,
          "accuracy": 0.9072727272727272
        },
        "0.01": null
      },
      "auroc": 0.8970773674242425
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 294,
          "fn": 906,
          "accuracy": 0.245
        },
        "0.01": null
      },
      "auroc": 0.41381875
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2290,
          "fn": 1110,
          "accuracy": 0.6735294117647059
        },
        "0.01": null
      },
      "auroc": 0.7265155024509804
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 4170,
          "fn": 230,
          "accuracy": 0.9477272727272728
        },
        "0.01": null
      },
      "auroc": 0.9125413825757576
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1118,
          "fn": 1282,
          "accuracy": 0.4658333333333333
        },
        "0.01": null
      },
      "auroc": 0.5594276041666666
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 5288,
          "fn": 1512,
          "accuracy": 0.7776470588235294
        },
        "0.01": null
      },
      "auroc": 0.7879129901960784
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.759178125
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": null
      },
      "auroc": 0.6430072916666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 240,
          "accuracy": 0.4
        },
        "0.01": null
      },
      "auroc": 0.7010927083333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        },
        "0.01": null
      },
      "auroc": 0.7270427083333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        },
        "0.01": null
      },
      "auroc": 0.5396583333333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 121,
          "fn": 279,
          "accuracy": 0.3025
        },
        "0.01": null
      },
      "auroc": 0.6333505208333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        },
        "0.01": null
      },
      "auroc": 0.7431104166666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 86,
          "fn": 314,
          "accuracy": 0.215
        },
        "0.01": null
      },
      "auroc": 0.5913328124999999
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 281,
          "fn": 519,
          "accuracy": 0.35125
        },
        "0.01": null
      },
      "auroc": 0.6672216145833334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9163833333333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": null
      },
      "auroc": 0.27236875
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.5943760416666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": null
      },
      "auroc": 0.8927885416666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        },
        "0.01": null
      },
      "auroc": 0.32200104166666665
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": null
      },
      "auroc": 0.6073947916666668
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9045859374999999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 20,
          "fn": 380,
          "accuracy": 0.05
        },
        "0.01": null
      },
      "auroc": 0.2971848958333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 401,
          "accuracy": 0.49875
        },
        "0.01": null
      },
      "auroc": 0.6008854166666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": null
      },
      "auroc": 0.5185645833333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": null
      },
      "auroc": 0.4153239583333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 24,
          "fn": 376,
          "accuracy": 0.06
        },
        "0.01": null
      },
      "auroc": 0.4669442708333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        },
        "0.01": null
      },
      "auroc": 0.5197302083333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.25867291666666664
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 21,
          "fn": 379,
          "accuracy": 0.0525
        },
        "0.01": null
      },
      "auroc": 0.3892015625
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 364,
          "accuracy": 0.09
        },
        "0.01": null
      },
      "auroc": 0.5191473958333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        },
        "0.01": null
      },
      "auroc": 0.3369984375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 755,
          "accuracy": 0.05625
        },
        "0.01": null
      },
      "auroc": 0.42807291666666664
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.909428125
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.8323989583333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": null
      },
      "auroc": 0.8709135416666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.8669135416666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        },
        "0.01": null
      },
      "auroc": 0.6329927083333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        },
        "0.01": null
      },
      "auroc": 0.749953125
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.8881708333333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        },
        "0.01": null
      },
      "auroc": 0.7326958333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 546,
          "fn": 254,
          "accuracy": 0.6825
        },
        "0.01": null
      },
      "auroc": 0.8104333333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9118177083333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        },
        "0.01": null
      },
      "auroc": 0.5596104166666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        },
        "0.01": null
      },
      "auroc": 0.7357140625
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": null
      },
      "auroc": 0.8471312500000001
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": null
      },
      "auroc": 0.4107864583333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 231,
          "accuracy": 0.4225
        },
        "0.01": null
      },
      "auroc": 0.6289588541666666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.8794744791666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 355,
          "accuracy": 0.1125
        },
        "0.01": null
      },
      "auroc": 0.48519843749999997
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 402,
          "accuracy": 0.4975
        },
        "0.01": null
      },
      "auroc": 0.6823364583333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        },
        "0.01": null
      },
      "auroc": 0.609275
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.4508979166666666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 43,
          "fn": 357,
          "accuracy": 0.1075
        },
        "0.01": null
      },
      "auroc": 0.5300864583333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        },
        "0.01": null
      },
      "auroc": 0.587928125
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.31073958333333335
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 369,
          "accuracy": 0.0775
        },
        "0.01": null
      },
      "auroc": 0.4493338541666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 69,
          "fn": 331,
          "accuracy": 0.1725
        },
        "0.01": null
      },
      "auroc": 0.5986015625000001
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 395,
          "accuracy": 0.0125
        },
        "0.01": null
      },
      "auroc": 0.38081875000000004
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 74,
          "fn": 726,
          "accuracy": 0.0925
        },
        "0.01": null
      },
      "auroc": 0.48971015625000003
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        },
        "0.01": null
      },
      "auroc": 0.6894739583333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        },
        "0.01": null
      },
      "auroc": 0.6894739583333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        },
        "0.01": null
      },
      "auroc": 0.6672812499999999
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 66,
          "fn": 134,
          "accuracy": 0.33
        },
        "0.01": null
      },
      "auroc": 0.6672812499999999
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 269,
          "accuracy": 0.3275
        },
        "0.01": null
      },
      "auroc": 0.6783776041666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 269,
          "accuracy": 0.3275
        },
        "0.01": null
      },
      "auroc": 0.6783776041666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": null
      },
      "auroc": 0.8016604166666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 131,
          "fn": 69,
          "accuracy": 0.655
        },
        "0.01": null
      },
      "auroc": 0.8016604166666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.7530322916666669
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        },
        "0.01": null
      },
      "auroc": 0.7530322916666669
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": null
      },
      "auroc": 0.7773463541666668
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        },
        "0.01": null
      },
      "auroc": 0.7773463541666668
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        },
        "0.01": null
      },
      "auroc": 0.6484427083333334
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        },
        "0.01": null
      },
      "auroc": 0.6484427083333334
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        },
        "0.01": null
      },
      "auroc": 0.6199624999999999
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 41,
          "fn": 159,
          "accuracy": 0.205
        },
        "0.01": null
      },
      "auroc": 0.6199624999999999
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 315,
          "accuracy": 0.2125
        },
        "0.01": null
      },
      "auroc": 0.6342026041666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 85,
          "fn": 315,
          "accuracy": 0.2125
        },
        "0.01": null
      },
      "auroc": 0.6342026041666666
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        },
        "0.01": null
      },
      "auroc": 0.6898052083333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        },
        "0.01": null
      },
      "auroc": 0.6898052083333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": null
      },
      "auroc": 0.5405885416666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": null
      },
      "auroc": 0.5405885416666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 83,
          "fn": 317,
          "accuracy": 0.2075
        },
        "0.01": null
      },
      "auroc": 0.6151968750000001
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 83,
          "fn": 317,
          "accuracy": 0.2075
        },
        "0.01": null
      },
      "auroc": 0.6151968750000001
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": null
      },
      "auroc": 0.6857468749999998
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 136,
          "accuracy": 0.32
        },
        "0.01": null
      },
      "auroc": 0.6857468749999998
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": null
      },
      "auroc": 0.6418052083333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": null
      },
      "auroc": 0.6418052083333334
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 281,
          "accuracy": 0.2975
        },
        "0.01": null
      },
      "auroc": 0.6637760416666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 281,
          "accuracy": 0.2975
        },
        "0.01": null
      },
      "auroc": 0.6637760416666667
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1108,
          "fn": 1092,
          "accuracy": 0.5036363636363637
        },
        "0.01": null
      },
      "auroc": 0.7399796401515153
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 950,
          "accuracy": 0.20833333333333334
        },
        "0.01": null
      },
      "auroc": 0.5289345486111112
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1358,
          "fn": 2042,
          "accuracy": 0.39941176470588236
        },
        "0.01": null
      },
      "auroc": 0.665493137254902
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 933,
          "fn": 1267,
          "accuracy": 0.4240909090909091
        },
        "0.01": null
      },
      "auroc": 0.6967458333333334
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 1097,
          "accuracy": 0.08583333333333333
        },
        "0.01": null
      },
      "auroc": 0.41247517361111113
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1036,
          "fn": 2364,
          "accuracy": 0.30470588235294116
        },
        "0.01": null
      },
      "auroc": 0.5964150122549019
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2041,
          "fn": 2359,
          "accuracy": 0.46386363636363637
        },
        "0.01": null
      },
      "auroc": 0.7183627367424242
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 2047,
          "accuracy": 0.14708333333333334
        },
        "0.01": null
      },
      "auroc": 0.47070486111111115
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2394,
          "fn": 4406,
          "accuracy": 0.35205882352941176
        },
        "0.01": null
      },
      "auroc": 0.6309540747549021
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9328614583333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9211458333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9270036458333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9307875
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.7415135416666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": null
      },
      "auroc": 0.8361505208333334
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9318244791666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.8313296874999999
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 737,
          "fn": 63,
          "accuracy": 0.92125
        },
        "0.01": null
      },
      "auroc": 0.8815770833333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9250979166666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.04839166666666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        },
        "0.01": null
      },
      "auroc": 0.4867447916666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        },
        "0.01": null
      },
      "auroc": 0.8174635416666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.03839270833333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 140,
          "fn": 260,
          "accuracy": 0.35
        },
        "0.01": null
      },
      "auroc": 0.427928125
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.8712807291666669
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.0433921875
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 462,
          "accuracy": 0.4225
        },
        "0.01": null
      },
      "auroc": 0.4573364583333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.8692520833333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        },
        "0.01": null
      },
      "auroc": 0.6382270833333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        },
        "0.01": null
      },
      "auroc": 0.7537395833333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 147,
          "fn": 53,
          "accuracy": 0.735
        },
        "0.01": null
      },
      "auroc": 0.8310625
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": null
      },
      "auroc": 0.26001145833333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 250,
          "accuracy": 0.375
        },
        "0.01": null
      },
      "auroc": 0.5455369791666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 318,
          "fn": 82,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.8501572916666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 49,
          "fn": 351,
          "accuracy": 0.1225
        },
        "0.01": null
      },
      "auroc": 0.44911927083333336
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 367,
          "fn": 433,
          "accuracy": 0.45875
        },
        "0.01": null
      },
      "auroc": 0.64963828125
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9299083333333333
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9316208333333332
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.8866833333333335
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": null
      },
      "auroc": 0.329471875
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 219,
          "accuracy": 0.4525
        },
        "0.01": null
      },
      "auroc": 0.6080776041666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.9082958333333334
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.6314026041666666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 581,
          "fn": 219,
          "accuracy": 0.72625
        },
        "0.01": null
      },
      "auroc": 0.7698492187499999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9233395833333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 138,
          "accuracy": 0.31
        },
        "0.01": null
      },
      "auroc": 0.5313604166666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        },
        "0.01": null
      },
      "auroc": 0.7273499999999999
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 105,
          "accuracy": 0.475
        },
        "0.01": null
      },
      "auroc": 0.7609739583333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.03506458333333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 305,
          "accuracy": 0.2375
        },
        "0.01": null
      },
      "auroc": 0.3980192708333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 295,
          "fn": 105,
          "accuracy": 0.7375
        },
        "0.01": null
      },
      "auroc": 0.8421567708333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        },
        "0.01": null
      },
      "auroc": 0.2832125
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 443,
          "accuracy": 0.44625
        },
        "0.01": null
      },
      "auroc": 0.5626846354166666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9241520833333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9091479166666666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.91665
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9045000000000001
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        },
        "0.01": null
      },
      "auroc": 0.5892385416666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        },
        "0.01": null
      },
      "auroc": 0.7468692708333332
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9143260416666666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": null
      },
      "auroc": 0.7491932291666666
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 638,
          "fn": 162,
          "accuracy": 0.7975
        },
        "0.01": null
      },
      "auroc": 0.8317596354166665
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.8841875
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.8841875
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.8753958333333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.8753958333333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": null
      },
      "auroc": 0.8797916666666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": null
      },
      "auroc": 0.8797916666666666
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": null
      },
      "auroc": 0.8241916666666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": null
      },
      "auroc": 0.8241916666666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.736721875
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 98,
          "fn": 102,
          "accuracy": 0.49
        },
        "0.01": null
      },
      "auroc": 0.736721875
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": null
      },
      "auroc": 0.7804567708333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 239,
          "fn": 161,
          "accuracy": 0.5975
        },
        "0.01": null
      },
      "auroc": 0.7804567708333334
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.93054375
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.93054375
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9213770833333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9213770833333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9259604166666666
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9259604166666666
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9327
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9327
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        },
        "0.01": null
      },
      "auroc": 0.6500020833333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        },
        "0.01": null
      },
      "auroc": 0.6500020833333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        },
        "0.01": null
      },
      "auroc": 0.7913510416666667
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        },
        "0.01": null
      },
      "auroc": 0.7913510416666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.8581010416666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.8581010416666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.7471927083333332
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 113,
          "fn": 87,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.7471927083333332
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        },
        "0.01": null
      },
      "auroc": 0.8026468750000002
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 277,
          "fn": 123,
          "accuracy": 0.6925
        },
        "0.01": null
      },
      "auroc": 0.8026468750000002
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2048,
          "fn": 152,
          "accuracy": 0.9309090909090909
        },
        "0.01": null
      },
      "auroc": 0.9031214015151515
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 698,
          "fn": 502,
          "accuracy": 0.5816666666666667
        },
        "0.01": null
      },
      "auroc": 0.6636010416666667
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2746,
          "fn": 654,
          "accuracy": 0.8076470588235294
        },
        "0.01": null
      },
      "auroc": 0.8185848039215686
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1567,
          "fn": 633,
          "accuracy": 0.7122727272727273
        },
        "0.01": null
      },
      "auroc": 0.8238327651515152
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 998,
          "accuracy": 0.16833333333333333
        },
        "0.01": null
      },
      "auroc": 0.3322821180555555
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1769,
          "fn": 1631,
          "accuracy": 0.5202941176470588
        },
        "0.01": null
      },
      "auroc": 0.6503443014705883
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3615,
          "fn": 785,
          "accuracy": 0.821590909090909
        },
        "0.01": null
      },
      "auroc": 0.8634770833333334
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 900,
          "fn": 1500,
          "accuracy": 0.375
        },
        "0.01": null
      },
      "auroc": 0.4979415798611111
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 4515,
          "fn": 2285,
          "accuracy": 0.6639705882352941
        },
        "0.01": null
      },
      "auroc": 0.7344645526960785
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9311958333333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9322645833333333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.932928125
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": null
      },
      "auroc": 0.7868187500000001
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 359,
          "fn": 41,
          "accuracy": 0.8975
        },
        "0.01": null
      },
      "auroc": 0.8598734374999999
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9331307291666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.8590072916666667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 758,
          "fn": 42,
          "accuracy": 0.9475
        },
        "0.01": null
      },
      "auroc": 0.8960690104166666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9325229166666666
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.04408854166666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.48830572916666665
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.8779614583333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.03839270833333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        },
        "0.01": null
      },
      "auroc": 0.45817708333333335
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": null
      },
      "auroc": 0.9052421875000001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.041240625
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 425,
          "accuracy": 0.46875
        },
        "0.01": null
      },
      "auroc": 0.47324140625
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9195302083333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": null
      },
      "auroc": 0.7355802083333335
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        },
        "0.01": null
      },
      "auroc": 0.8275552083333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9088645833333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": null
      },
      "auroc": 0.320971875
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": null
      },
      "auroc": 0.6149182291666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9141973958333334
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 283,
          "accuracy": 0.2925
        },
        "0.01": null
      },
      "auroc": 0.5282760416666668
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 503,
          "fn": 297,
          "accuracy": 0.62875
        },
        "0.01": null
      },
      "auroc": 0.72123671875
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9167500000000001
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        },
        "0.01": null
      },
      "auroc": 0.4230312500000001
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": null
      },
      "auroc": 0.669890625
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9250416666666668
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": null
      },
      "auroc": 0.6781822916666668
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 601,
          "fn": 199,
          "accuracy": 0.75125
        },
        "0.01": null
      },
      "auroc": 0.8016119791666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9328041666666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": null
      },
      "auroc": 0.5552135416666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        },
        "0.01": null
      },
      "auroc": 0.7440088541666668
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.8551895833333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.03749166666666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        },
        "0.01": null
      },
      "auroc": 0.446340625
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": null
      },
      "auroc": 0.893996875
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 318,
          "accuracy": 0.205
        },
        "0.01": null
      },
      "auroc": 0.2963526041666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 450,
          "fn": 350,
          "accuracy": 0.5625
        },
        "0.01": null
      },
      "auroc": 0.5951747395833333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9327541666666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9286833333333334
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.93071875
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.92824375
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 88,
          "fn": 112,
          "accuracy": 0.44
        },
        "0.01": null
      },
      "auroc": 0.6646479166666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 286,
          "fn": 114,
          "accuracy": 0.715
        },
        "0.01": null
      },
      "auroc": 0.7964458333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9304989583333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        },
        "0.01": null
      },
      "auroc": 0.7966656249999999
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 686,
          "fn": 114,
          "accuracy": 0.8575
        },
        "0.01": null
      },
      "auroc": 0.8635822916666666
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9193479166666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9193479166666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9211177083333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9211177083333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9202328125
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9202328125
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.88770625
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": null
      },
      "auroc": 0.88770625
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.8462833333333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": null
      },
      "auroc": 0.8462833333333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": null
      },
      "auroc": 0.8669947916666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 335,
          "fn": 65,
          "accuracy": 0.8375
        },
        "0.01": null
      },
      "auroc": 0.8669947916666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9316458333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9316458333333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9324895833333333
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9324895833333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        },
        "0.01": null
      },
      "auroc": 0.7366291666666666
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        },
        "0.01": null
      },
      "auroc": 0.7366291666666666
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        },
        "0.01": null
      },
      "auroc": 0.8349812500000001
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 287,
          "fn": 113,
          "accuracy": 0.7175
        },
        "0.01": null
      },
      "auroc": 0.8349812500000001
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.90876875
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.90876875
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": null
      },
      "auroc": 0.8412416666666668
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": null
      },
      "auroc": 0.8412416666666668
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.8750052083333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.8750052083333333
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2157,
          "fn": 43,
          "accuracy": 0.9804545454545455
        },
        "0.01": null
      },
      "auroc": 0.9242516098484849
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 787,
          "fn": 413,
          "accuracy": 0.6558333333333334
        },
        "0.01": null
      },
      "auroc": 0.6880157986111111
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2944,
          "fn": 456,
          "accuracy": 0.8658823529411764
        },
        "0.01": null
      },
      "auroc": 0.8408742647058822
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1913,
          "fn": 287,
          "accuracy": 0.8695454545454545
        },
        "0.01": null
      },
      "auroc": 0.8815322916666668
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 265,
          "fn": 935,
          "accuracy": 0.22083333333333333
        },
        "0.01": null
      },
      "auroc": 0.37855902777777783
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2178,
          "fn": 1222,
          "accuracy": 0.6405882352941177
        },
        "0.01": null
      },
      "auroc": 0.7040123161764706
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 4070,
          "fn": 330,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9028919507575757
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1052,
          "fn": 1348,
          "accuracy": 0.43833333333333335
        },
        "0.01": null
      },
      "auroc": 0.5332874131944445
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 5122,
          "fn": 1678,
          "accuracy": 0.7532352941176471
        },
        "0.01": null
      },
      "auroc": 0.7724432904411764
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9279447916666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9306390625
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9316927083333334
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9115385416666667
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.921615625
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9325130208333333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": null
      },
      "auroc": 0.9197416666666668
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 785,
          "fn": 15,
          "accuracy": 0.98125
        },
        "0.01": null
      },
      "auroc": 0.92612734375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9283750000000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9169125
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.92264375
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9295281249999999
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": null
      },
      "auroc": 0.9032729166666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9164005208333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9289515625000001
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9100927083333333
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 793,
          "fn": 7,
          "accuracy": 0.99125
        },
        "0.01": null
      },
      "auroc": 0.9195221354166666
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.932928125
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.932928125
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.932928125
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9229000000000002
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9281166666666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9331307291666667
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9279140625
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 6,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9305223958333334
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9322250000000001
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9327791666666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9315947916666667
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9324640625
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9319098958333333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9326216145833333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.932928125
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9329833333333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9329557291666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9329552083333333
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9324041666666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9326796875
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9329416666666667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.93269375
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9328177083333333
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9262843749999999
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9262843749999999
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9240760416666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9240760416666667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9251802083333334
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9251802083333334
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9325500000000001
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9325500000000001
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9316489583333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9316489583333333
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9320994791666667
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9320994791666667
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9333333333333332
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9308239583333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9308239583333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9320786458333333
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9320786458333333
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9322135416666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9322135416666667
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9323229166666668
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9323229166666668
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9322682291666666
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9322682291666666
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2199,
          "fn": 1,
          "accuracy": 0.9995454545454545
        },
        "0.01": null
      },
      "auroc": 0.9319950757575758
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1198,
          "fn": 2,
          "accuracy": 0.9983333333333333
        },
        "0.01": null
      },
      "auroc": 0.9293878472222222
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3397,
          "fn": 3,
          "accuracy": 0.9991176470588236
        },
        "0.01": null
      },
      "auroc": 0.9310748774509804
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2196,
          "fn": 4,
          "accuracy": 0.9981818181818182
        },
        "0.01": null
      },
      "auroc": 0.9314892045454545
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1177,
          "fn": 23,
          "accuracy": 0.9808333333333333
        },
        "0.01": null
      },
      "auroc": 0.9225072916666668
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3373,
          "fn": 27,
          "accuracy": 0.9920588235294118
        },
        "0.01": null
      },
      "auroc": 0.9283191176470589
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4395,
          "fn": 5,
          "accuracy": 0.9988636363636364
        },
        "0.01": null
      },
      "auroc": 0.9317421401515151
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2375,
          "fn": 25,
          "accuracy": 0.9895833333333334
        },
        "0.01": null
      },
      "auroc": 0.9259475694444443
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 6770,
          "fn": 30,
          "accuracy": 0.9955882352941177
        },
        "0.01": null
      },
      "auroc": 0.9296969975490197
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2279,
          "fn": 121,
          "accuracy": 0.9495833333333333
        },
        "0.01": null
      },
      "auroc": 0.9140677951388888
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2118,
          "fn": 282,
          "accuracy": 0.8825
        },
        "0.01": null
      },
      "auroc": 0.8860327256944445
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4397,
          "fn": 403,
          "accuracy": 0.9160416666666666
        },
        "0.01": null
      },
      "auroc": 0.9000502604166667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2264,
          "fn": 136,
          "accuracy": 0.9433333333333334
        },
        "0.01": null
      },
      "auroc": 0.9104379340277778
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1665,
          "fn": 735,
          "accuracy": 0.69375
        },
        "0.01": null
      },
      "auroc": 0.7537201388888889
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3929,
          "fn": 871,
          "accuracy": 0.8185416666666666
        },
        "0.01": null
      },
      "auroc": 0.8320790364583333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4543,
          "fn": 257,
          "accuracy": 0.9464583333333333
        },
        "0.01": null
      },
      "auroc": 0.9122528645833333
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3783,
          "fn": 1017,
          "accuracy": 0.788125
        },
        "0.01": null
      },
      "auroc": 0.8198764322916667
    },
    {
      "domain": "reviews",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8326,
          "fn": 1274,
          "accuracy": 0.8672916666666667
        },
        "0.01": null
      },
      "auroc": 0.8660646484375001
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2368,
          "fn": 32,
          "accuracy": 0.9866666666666667
        },
        "0.01": null
      },
      "auroc": 0.9247929687499999
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 232,
          "fn": 2168,
          "accuracy": 0.09666666666666666
        },
        "0.01": null
      },
      "auroc": 0.18078038194444446
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2600,
          "fn": 2200,
          "accuracy": 0.5416666666666666
        },
        "0.01": null
      },
      "auroc": 0.5527866753472223
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1978,
          "fn": 422,
          "accuracy": 0.8241666666666667
        },
        "0.01": null
      },
      "auroc": 0.8605684895833333
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 223,
          "fn": 2177,
          "accuracy": 0.09291666666666666
        },
        "0.01": null
      },
      "auroc": 0.16674149305555552
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2201,
          "fn": 2599,
          "accuracy": 0.4585416666666667
        },
        "0.01": null
      },
      "auroc": 0.5136549913194444
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4346,
          "fn": 454,
          "accuracy": 0.9054166666666666
        },
        "0.01": null
      },
      "auroc": 0.8926807291666665
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 455,
          "fn": 4345,
          "accuracy": 0.09479166666666666
        },
        "0.01": null
      },
      "auroc": 0.1737609375
    },
    {
      "domain": "reviews",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4801,
          "fn": 4799,
          "accuracy": 0.5001041666666667
        },
        "0.01": null
      },
      "auroc": 0.5332208333333335
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2030,
          "fn": 370,
          "accuracy": 0.8458333333333333
        },
        "0.01": null
      },
      "auroc": 0.8615510416666665
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1044,
          "fn": 1356,
          "accuracy": 0.435
        },
        "0.01": null
      },
      "auroc": 0.6785349826388889
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3074,
          "fn": 1726,
          "accuracy": 0.6404166666666666
        },
        "0.01": null
      },
      "auroc": 0.7700430121527777
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1911,
          "fn": 489,
          "accuracy": 0.79625
        },
        "0.01": null
      },
      "auroc": 0.8447885416666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 2051,
          "accuracy": 0.14541666666666667
        },
        "0.01": null
      },
      "auroc": 0.3703322916666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2260,
          "fn": 2540,
          "accuracy": 0.4708333333333333
        },
        "0.01": null
      },
      "auroc": 0.6075604166666666
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3941,
          "fn": 859,
          "accuracy": 0.8210416666666667
        },
        "0.01": null
      },
      "auroc": 0.8531697916666667
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1393,
          "fn": 3407,
          "accuracy": 0.29020833333333335
        },
        "0.01": null
      },
      "auroc": 0.5244336371527778
    },
    {
      "domain": "reviews",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5334,
          "fn": 4266,
          "accuracy": 0.555625
        },
        "0.01": null
      },
      "auroc": 0.6888017144097223
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2350,
          "fn": 50,
          "accuracy": 0.9791666666666666
        },
        "0.01": null
      },
      "auroc": 0.9200401909722222
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2315,
          "fn": 85,
          "accuracy": 0.9645833333333333
        },
        "0.01": null
      },
      "auroc": 0.919796701388889
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4665,
          "fn": 135,
          "accuracy": 0.971875
        },
        "0.01": null
      },
      "auroc": 0.9199184461805556
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2205,
          "fn": 195,
          "accuracy": 0.91875
        },
        "0.01": null
      },
      "auroc": 0.8990886284722222
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 405,
          "fn": 1995,
          "accuracy": 0.16875
        },
        "0.01": null
      },
      "auroc": 0.48821284722222225
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2610,
          "fn": 2190,
          "accuracy": 0.54375
        },
        "0.01": null
      },
      "auroc": 0.6936507378472223
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4555,
          "fn": 245,
          "accuracy": 0.9489583333333333
        },
        "0.01": null
      },
      "auroc": 0.9095644097222223
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2720,
          "fn": 2080,
          "accuracy": 0.5666666666666667
        },
        "0.01": null
      },
      "auroc": 0.7040047743055556
    },
    {
      "domain": "reviews",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7275,
          "fn": 2325,
          "accuracy": 0.7578125
        },
        "0.01": null
      },
      "auroc": 0.8067845920138889
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2365,
          "fn": 35,
          "accuracy": 0.9854166666666667
        },
        "0.01": null
      },
      "auroc": 0.9242486979166666
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 957,
          "fn": 1443,
          "accuracy": 0.39875
        },
        "0.01": null
      },
      "auroc": 0.5715231770833333
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3322,
          "fn": 1478,
          "accuracy": 0.6920833333333334
        },
        "0.01": null
      },
      "auroc": 0.7478859375
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1759,
          "fn": 641,
          "accuracy": 0.7329166666666667
        },
        "0.01": null
      },
      "auroc": 0.8291090277777777
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 236,
          "fn": 2164,
          "accuracy": 0.09833333333333333
        },
        "0.01": null
      },
      "auroc": 0.18757161458333332
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1995,
          "fn": 2805,
          "accuracy": 0.415625
        },
        "0.01": null
      },
      "auroc": 0.5083403211805556
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4124,
          "fn": 676,
          "accuracy": 0.8591666666666666
        },
        "0.01": null
      },
      "auroc": 0.8766788628472222
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1193,
          "fn": 3607,
          "accuracy": 0.24854166666666666
        },
        "0.01": null
      },
      "auroc": 0.37954739583333336
    },
    {
      "domain": "reviews",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5317,
          "fn": 4283,
          "accuracy": 0.5538541666666666
        },
        "0.01": null
      },
      "auroc": 0.6281131293402777
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2204,
          "fn": 196,
          "accuracy": 0.9183333333333333
        },
        "0.01": null
      },
      "auroc": 0.8973044270833332
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2045,
          "fn": 355,
          "accuracy": 0.8520833333333333
        },
        "0.01": null
      },
      "auroc": 0.8658296875
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4249,
          "fn": 551,
          "accuracy": 0.8852083333333334
        },
        "0.01": null
      },
      "auroc": 0.8815670572916667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2136,
          "fn": 264,
          "accuracy": 0.89
        },
        "0.01": null
      },
      "auroc": 0.8856798611111111
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1029,
          "fn": 1371,
          "accuracy": 0.42875
        },
        "0.01": null
      },
      "auroc": 0.6368706597222222
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3165,
          "fn": 1635,
          "accuracy": 0.659375
        },
        "0.01": null
      },
      "auroc": 0.7612752604166667
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4340,
          "fn": 460,
          "accuracy": 0.9041666666666667
        },
        "0.01": null
      },
      "auroc": 0.8914921440972221
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3074,
          "fn": 1726,
          "accuracy": 0.6404166666666666
        },
        "0.01": null
      },
      "auroc": 0.7513501736111112
    },
    {
      "domain": "reviews",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7414,
          "fn": 2186,
          "accuracy": 0.7722916666666667
        },
        "0.01": null
      },
      "auroc": 0.8214211588541667
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2029,
          "fn": 371,
          "accuracy": 0.8454166666666667
        },
        "0.01": null
      },
      "auroc": 0.8684090277777778
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2029,
          "fn": 371,
          "accuracy": 0.8454166666666667
        },
        "0.01": null
      },
      "auroc": 0.8684090277777778
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2033,
          "fn": 367,
          "accuracy": 0.8470833333333333
        },
        "0.01": null
      },
      "auroc": 0.8674011284722222
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2033,
          "fn": 367,
          "accuracy": 0.8470833333333333
        },
        "0.01": null
      },
      "auroc": 0.8674011284722222
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4062,
          "fn": 738,
          "accuracy": 0.84625
        },
        "0.01": null
      },
      "auroc": 0.8679050781250001
    },
    {
      "domain": "reviews",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4062,
          "fn": 738,
          "accuracy": 0.84625
        },
        "0.01": null
      },
      "auroc": 0.8679050781250001
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1962,
          "fn": 438,
          "accuracy": 0.8175
        },
        "0.01": null
      },
      "auroc": 0.8607320312500002
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1962,
          "fn": 438,
          "accuracy": 0.8175
        },
        "0.01": null
      },
      "auroc": 0.8607320312500002
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1704,
          "fn": 696,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.8151795138888888
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1704,
          "fn": 696,
          "accuracy": 0.71
        },
        "0.01": null
      },
      "auroc": 0.8151795138888888
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3666,
          "fn": 1134,
          "accuracy": 0.76375
        },
        "0.01": null
      },
      "auroc": 0.8379557725694444
    },
    {
      "domain": "reviews",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3666,
          "fn": 1134,
          "accuracy": 0.76375
        },
        "0.01": null
      },
      "auroc": 0.8379557725694444
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2166,
          "fn": 234,
          "accuracy": 0.9025
        },
        "0.01": null
      },
      "auroc": 0.8980530381944443
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2166,
          "fn": 234,
          "accuracy": 0.9025
        },
        "0.01": null
      },
      "auroc": 0.8980530381944443
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2100,
          "fn": 300,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.8857737847222222
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2100,
          "fn": 300,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.8857737847222222
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4266,
          "fn": 534,
          "accuracy": 0.88875
        },
        "0.01": null
      },
      "auroc": 0.8919134114583332
    },
    {
      "domain": "reviews",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4266,
          "fn": 534,
          "accuracy": 0.88875
        },
        "0.01": null
      },
      "auroc": 0.8919134114583332
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2220,
          "fn": 180,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9059554687500002
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2220,
          "fn": 180,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9059554687500002
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1144,
          "fn": 1256,
          "accuracy": 0.4766666666666667
        },
        "0.01": null
      },
      "auroc": 0.728763454861111
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1144,
          "fn": 1256,
          "accuracy": 0.4766666666666667
        },
        "0.01": null
      },
      "auroc": 0.728763454861111
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3364,
          "fn": 1436,
          "accuracy": 0.7008333333333333
        },
        "0.01": null
      },
      "auroc": 0.8173594618055556
    },
    {
      "domain": "reviews",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3364,
          "fn": 1436,
          "accuracy": 0.7008333333333333
        },
        "0.01": null
      },
      "auroc": 0.8173594618055556
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2052,
          "fn": 348,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.8736454861111109
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2052,
          "fn": 348,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.8736454861111109
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1638,
          "fn": 762,
          "accuracy": 0.6825
        },
        "0.01": null
      },
      "auroc": 0.8084421006944444
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1638,
          "fn": 762,
          "accuracy": 0.6825
        },
        "0.01": null
      },
      "auroc": 0.8084421006944444
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3690,
          "fn": 1110,
          "accuracy": 0.76875
        },
        "0.01": null
      },
      "auroc": 0.8410437934027777
    },
    {
      "domain": "reviews",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3690,
          "fn": 1110,
          "accuracy": 0.76875
        },
        "0.01": null
      },
      "auroc": 0.8410437934027777
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 24025,
          "fn": 2375,
          "accuracy": 0.9100378787878788
        },
        "0.01": null
      },
      "auroc": 0.8953454703282828
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8711,
          "fn": 5689,
          "accuracy": 0.6049305555555555
        },
        "0.01": null
      },
      "auroc": 0.683749609375
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 32736,
          "fn": 8064,
          "accuracy": 0.8023529411764706
        },
        "0.01": null
      },
      "auroc": 0.8206645782271241
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 20872,
          "fn": 5528,
          "accuracy": 0.7906060606060606
        },
        "0.01": null
      },
      "auroc": 0.8486574968434344
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3907,
          "fn": 10493,
          "accuracy": 0.27131944444444445
        },
        "0.01": null
      },
      "auroc": 0.43390817418981475
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 24779,
          "fn": 16021,
          "accuracy": 0.607328431372549
        },
        "0.01": null
      },
      "auroc": 0.7022753829656864
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 44897,
          "fn": 7903,
          "accuracy": 0.8503219696969697
        },
        "0.01": null
      },
      "auroc": 0.8720014835858587
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 12618,
          "fn": 16182,
          "accuracy": 0.438125
        },
        "0.01": null
      },
      "auroc": 0.5588288917824074
    },
    {
      "domain": "reviews",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 57515,
          "fn": 24085,
          "accuracy": 0.7048406862745098
        },
        "0.01": null
      },
      "auroc": 0.7614699805964052
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.8650697916666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.9063890625000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.9063890625000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 777,
          "fn": 23,
          "accuracy": 0.97125
        },
        "0.01": {
          "tp": 771,
          "fn": 29,
          "accuracy": 0.96375
        }
      },
      "auroc": 0.9270486979166666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9456083333333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08432291666666669
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        }
      },
      "auroc": 0.514965625
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.8581822916666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.08150520833333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 244,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 101,
          "fn": 299,
          "accuracy": 0.2525
        }
      },
      "auroc": 0.46984375000000006
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        }
      },
      "auroc": 0.9018953125
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.08291406250000001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 445,
          "accuracy": 0.44375
        },
        "0.01": {
          "tp": 298,
          "fn": 502,
          "accuracy": 0.3725
        }
      },
      "auroc": 0.4924046875
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9472260416666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.440271875
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 259,
          "fn": 141,
          "accuracy": 0.6475
        },
        "0.01": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        }
      },
      "auroc": 0.6937489583333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9470104166666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 195,
          "accuracy": 0.025
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.12945937500000002
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.5382348958333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9471182291666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 64,
          "fn": 336,
          "accuracy": 0.16
        },
        "0.01": {
          "tp": 47,
          "fn": 353,
          "accuracy": 0.1175
        }
      },
      "auroc": 0.284865625
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 464,
          "fn": 336,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 444,
          "fn": 356,
          "accuracy": 0.555
        }
      },
      "auroc": 0.6159919270833334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9464854166666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.947359375
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9469223958333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9241125
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.407165625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 211,
          "accuracy": 0.4725
        },
        "0.01": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        }
      },
      "auroc": 0.6656390625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9352989583333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.6772625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 588,
          "fn": 212,
          "accuracy": 0.735
        },
        "0.01": {
          "tp": 577,
          "fn": 223,
          "accuracy": 0.72125
        }
      },
      "auroc": 0.8062807291666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9451604166666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.845809375
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.8954848958333332
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 85,
          "accuracy": 0.575
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.8146375000000001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07554687500000001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 115,
          "fn": 285,
          "accuracy": 0.2875
        },
        "0.01": {
          "tp": 56,
          "fn": 344,
          "accuracy": 0.14
        }
      },
      "auroc": 0.44509218750000007
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        },
        "0.01": {
          "tp": 254,
          "fn": 146,
          "accuracy": 0.635
        }
      },
      "auroc": 0.8798989583333332
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 240,
          "accuracy": 0.4
        },
        "0.01": {
          "tp": 136,
          "fn": 264,
          "accuracy": 0.34
        }
      },
      "auroc": 0.46067812500000005
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 473,
          "fn": 327,
          "accuracy": 0.59125
        },
        "0.01": {
          "tp": 390,
          "fn": 410,
          "accuracy": 0.4875
        }
      },
      "auroc": 0.6702885416666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.943821875
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9457651041666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9403770833333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        },
        "0.01": {
          "tp": 32,
          "fn": 168,
          "accuracy": 0.16
        }
      },
      "auroc": 0.5380208333333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        },
        "0.01": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        }
      },
      "auroc": 0.7391989583333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9440427083333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        },
        "0.01": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        }
      },
      "auroc": 0.7409213541666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 650,
          "fn": 150,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 620,
          "fn": 180,
          "accuracy": 0.775
        }
      },
      "auroc": 0.8424820312500001
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.945
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.945
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.924615625
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.924615625
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.9348078125
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.9348078125
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9310833333333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9310833333333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9149677083333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9149677083333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9230255208333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9230255208333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9456739583333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9456739583333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8792864583333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8792864583333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        }
      },
      "auroc": 0.9124802083333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        }
      },
      "auroc": 0.9124802083333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9328895833333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9328895833333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9041958333333332
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9041958333333332
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9185427083333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9185427083333333
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2179,
          "fn": 21,
          "accuracy": 0.9904545454545455
        },
        "0.01": {
          "tp": 2151,
          "fn": 49,
          "accuracy": 0.9777272727272728
        }
      },
      "auroc": 0.9438410984848487
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 818,
          "fn": 382,
          "accuracy": 0.6816666666666666
        },
        "0.01": {
          "tp": 778,
          "fn": 422,
          "accuracy": 0.6483333333333333
        }
      },
      "auroc": 0.7015489583333333
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2997,
          "fn": 403,
          "accuracy": 0.8814705882352941
        },
        "0.01": {
          "tp": 2929,
          "fn": 471,
          "accuracy": 0.8614705882352941
        }
      },
      "auroc": 0.8583262254901961
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1957,
          "fn": 243,
          "accuracy": 0.8895454545454545
        },
        "0.01": {
          "tp": 1727,
          "fn": 473,
          "accuracy": 0.785
        }
      },
      "auroc": 0.909345643939394
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 245,
          "fn": 955,
          "accuracy": 0.20416666666666666
        },
        "0.01": {
          "tp": 205,
          "fn": 995,
          "accuracy": 0.17083333333333334
        }
      },
      "auroc": 0.34946128472222227
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2202,
          "fn": 1198,
          "accuracy": 0.6476470588235295
        },
        "0.01": {
          "tp": 1932,
          "fn": 1468,
          "accuracy": 0.5682352941176471
        }
      },
      "auroc": 0.7117393995098039
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 4136,
          "fn": 264,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 3878,
          "fn": 522,
          "accuracy": 0.8813636363636363
        }
      },
      "auroc": 0.9265933712121213
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1063,
          "fn": 1337,
          "accuracy": 0.4429166666666667
        },
        "0.01": {
          "tp": 983,
          "fn": 1417,
          "accuracy": 0.40958333333333335
        }
      },
      "auroc": 0.5255051215277777
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 5199,
          "fn": 1601,
          "accuracy": 0.7645588235294117
        },
        "0.01": {
          "tp": 4861,
          "fn": 1939,
          "accuracy": 0.7148529411764706
        }
      },
      "auroc": 0.7850328124999998
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9424322916666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9450703125000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9472729166666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        }
      },
      "auroc": 0.8359708333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.891621875
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9474906249999999
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        },
        "0.01": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        }
      },
      "auroc": 0.8892015624999999
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 763,
          "fn": 37,
          "accuracy": 0.95375
        },
        "0.01": {
          "tp": 730,
          "fn": 70,
          "accuracy": 0.9125
        }
      },
      "auroc": 0.9183460937500001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9292041666666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.0771510416666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 186,
          "fn": 214,
          "accuracy": 0.465
        }
      },
      "auroc": 0.5031776041666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.7867958333333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07348541666666669
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 283,
          "accuracy": 0.2925
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.43014062499999994
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        }
      },
      "auroc": 0.8580000000000001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.0753182291666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 311,
          "fn": 489,
          "accuracy": 0.38875
        },
        "0.01": {
          "tp": 240,
          "fn": 560,
          "accuracy": 0.3
        }
      },
      "auroc": 0.4666591145833333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9377062500000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.3827770833333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 231,
          "fn": 169,
          "accuracy": 0.5775
        },
        "0.01": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        }
      },
      "auroc": 0.6602416666666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        }
      },
      "auroc": 0.9305541666666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.11226770833333335
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        },
        "0.01": {
          "tp": 176,
          "fn": 224,
          "accuracy": 0.44
        }
      },
      "auroc": 0.5214109375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9341302083333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 365,
          "accuracy": 0.0875
        },
        "0.01": {
          "tp": 20,
          "fn": 380,
          "accuracy": 0.05
        }
      },
      "auroc": 0.24752239583333335
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 424,
          "fn": 376,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 383,
          "fn": 417,
          "accuracy": 0.47875
        }
      },
      "auroc": 0.5908263020833333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.942521875
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9458291666666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9441755208333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.9097916666666668
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.4191895833333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 215,
          "accuracy": 0.4625
        },
        "0.01": {
          "tp": 171,
          "fn": 229,
          "accuracy": 0.4275
        }
      },
      "auroc": 0.664490625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9261567708333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.682509375
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 583,
          "fn": 217,
          "accuracy": 0.72875
        },
        "0.01": {
          "tp": 563,
          "fn": 237,
          "accuracy": 0.70375
        }
      },
      "auroc": 0.8043330729166667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9304260416666665
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        }
      },
      "auroc": 0.7811000000000001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        }
      },
      "auroc": 0.8557630208333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.754253125
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07288333333333336
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 318,
          "accuracy": 0.205
        },
        "0.01": {
          "tp": 33,
          "fn": 367,
          "accuracy": 0.0825
        }
      },
      "auroc": 0.41356822916666663
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 276,
          "fn": 124,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        }
      },
      "auroc": 0.8423395833333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 128,
          "fn": 272,
          "accuracy": 0.32
        },
        "0.01": {
          "tp": 79,
          "fn": 321,
          "accuracy": 0.1975
        }
      },
      "auroc": 0.4269916666666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 404,
          "fn": 396,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 301,
          "fn": 499,
          "accuracy": 0.37625
        }
      },
      "auroc": 0.6346656250000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9455208333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9252291666666665
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.935375
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9196802083333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        },
        "0.01": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        }
      },
      "auroc": 0.4620708333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        },
        "0.01": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        }
      },
      "auroc": 0.6908755208333335
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 367,
          "fn": 33,
          "accuracy": 0.9175
        }
      },
      "auroc": 0.9326005208333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 223,
          "fn": 177,
          "accuracy": 0.5575
        },
        "0.01": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        }
      },
      "auroc": 0.6936500000000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 612,
          "fn": 188,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 554,
          "fn": 246,
          "accuracy": 0.6925
        }
      },
      "auroc": 0.8131252604166667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9166979166666666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9166979166666666
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.8659552083333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.8659552083333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        }
      },
      "auroc": 0.8913265625
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        }
      },
      "auroc": 0.8913265625
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.8952677083333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 144,
          "fn": 56,
          "accuracy": 0.72
        }
      },
      "auroc": 0.8952677083333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        }
      },
      "auroc": 0.86711875
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        }
      },
      "auroc": 0.86711875
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        },
        "0.01": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        }
      },
      "auroc": 0.8811932291666665
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        },
        "0.01": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        }
      },
      "auroc": 0.8811932291666665
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.941784375
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.941784375
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9447463541666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9447463541666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9408312500000001
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9408312500000001
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.8240125
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 80,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 56,
          "fn": 144,
          "accuracy": 0.28
        }
      },
      "auroc": 0.8240125
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        }
      },
      "auroc": 0.882421875
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        }
      },
      "auroc": 0.882421875
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.907290625
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.907290625
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.8695875
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.8695875
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        }
      },
      "auroc": 0.8884390625
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        }
      },
      "auroc": 0.8884390625
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2124,
          "fn": 76,
          "accuracy": 0.9654545454545455
        },
        "0.01": {
          "tp": 2025,
          "fn": 175,
          "accuracy": 0.9204545454545454
        }
      },
      "auroc": 0.9309893939393938
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 750,
          "fn": 450,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 661,
          "fn": 539,
          "accuracy": 0.5508333333333333
        }
      },
      "auroc": 0.675753125
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2874,
          "fn": 526,
          "accuracy": 0.8452941176470589
        },
        "0.01": {
          "tp": 2686,
          "fn": 714,
          "accuracy": 0.79
        }
      },
      "auroc": 0.8409060049019607
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1768,
          "fn": 432,
          "accuracy": 0.8036363636363636
        },
        "0.01": {
          "tp": 1434,
          "fn": 766,
          "accuracy": 0.6518181818181819
        }
      },
      "auroc": 0.8742551136363634
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 203,
          "fn": 997,
          "accuracy": 0.16916666666666666
        },
        "0.01": {
          "tp": 155,
          "fn": 1045,
          "accuracy": 0.12916666666666668
        }
      },
      "auroc": 0.32931128472222226
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1971,
          "fn": 1429,
          "accuracy": 0.5797058823529412
        },
        "0.01": {
          "tp": 1589,
          "fn": 1811,
          "accuracy": 0.4673529411764706
        }
      },
      "auroc": 0.6819219975490196
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3892,
          "fn": 508,
          "accuracy": 0.8845454545454545
        },
        "0.01": {
          "tp": 3459,
          "fn": 941,
          "accuracy": 0.7861363636363636
        }
      },
      "auroc": 0.9026222537878787
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 953,
          "fn": 1447,
          "accuracy": 0.39708333333333334
        },
        "0.01": {
          "tp": 816,
          "fn": 1584,
          "accuracy": 0.34
        }
      },
      "auroc": 0.502532204861111
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 4845,
          "fn": 1955,
          "accuracy": 0.7125
        },
        "0.01": {
          "tp": 4275,
          "fn": 2525,
          "accuracy": 0.6286764705882353
        }
      },
      "auroc": 0.7614140012254902
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9353520833333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9415302083333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9468322916666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        },
        "0.01": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        }
      },
      "auroc": 0.8121979166666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.8795151041666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9472703124999999
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        }
      },
      "auroc": 0.873775
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 748,
          "fn": 52,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 698,
          "fn": 102,
          "accuracy": 0.8725
        }
      },
      "auroc": 0.9105226562500001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9320291666666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07856770833333335
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        },
        "0.01": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        }
      },
      "auroc": 0.5052984374999999
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        },
        "0.01": {
          "tp": 26,
          "fn": 174,
          "accuracy": 0.13
        }
      },
      "auroc": 0.7336531250000001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.07823750000000002
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 77,
          "fn": 323,
          "accuracy": 0.1925
        },
        "0.01": {
          "tp": 27,
          "fn": 373,
          "accuracy": 0.0675
        }
      },
      "auroc": 0.4059453125
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8328411458333332
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.0784026041666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 271,
          "fn": 529,
          "accuracy": 0.33875
        },
        "0.01": {
          "tp": 209,
          "fn": 591,
          "accuracy": 0.26125
        }
      },
      "auroc": 0.455621875
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9405354166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.35191041666666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 229,
          "fn": 171,
          "accuracy": 0.5725
        },
        "0.01": {
          "tp": 205,
          "fn": 195,
          "accuracy": 0.5125
        }
      },
      "auroc": 0.6462229166666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9322708333333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.111440625
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        },
        "0.01": {
          "tp": 181,
          "fn": 219,
          "accuracy": 0.4525
        }
      },
      "auroc": 0.5218557291666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        }
      },
      "auroc": 0.936403125
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 32,
          "fn": 368,
          "accuracy": 0.08
        },
        "0.01": {
          "tp": 16,
          "fn": 384,
          "accuracy": 0.04
        }
      },
      "auroc": 0.23167552083333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 420,
          "fn": 380,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 386,
          "fn": 414,
          "accuracy": 0.4825
        }
      },
      "auroc": 0.5840393229166667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.93955625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.94635
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9429531250000001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        }
      },
      "auroc": 0.90504375
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.31760104166666664
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 222,
          "accuracy": 0.445
        },
        "0.01": {
          "tp": 158,
          "fn": 242,
          "accuracy": 0.395
        }
      },
      "auroc": 0.6113223958333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9223
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        }
      },
      "auroc": 0.6319755208333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 574,
          "fn": 226,
          "accuracy": 0.7175
        },
        "0.01": {
          "tp": 546,
          "fn": 254,
          "accuracy": 0.6825
        }
      },
      "auroc": 0.7771377604166667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9358354166666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 113,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        }
      },
      "auroc": 0.7064
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        },
        "0.01": {
          "tp": 234,
          "fn": 166,
          "accuracy": 0.585
        }
      },
      "auroc": 0.8211177083333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 147,
          "accuracy": 0.265
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.694403125
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.06765625000000001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 53,
          "fn": 347,
          "accuracy": 0.1325
        },
        "0.01": {
          "tp": 18,
          "fn": 382,
          "accuracy": 0.045
        }
      },
      "auroc": 0.38102968750000005
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 246,
          "fn": 154,
          "accuracy": 0.615
        },
        "0.01": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        }
      },
      "auroc": 0.8151192708333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 87,
          "fn": 313,
          "accuracy": 0.2175
        },
        "0.01": {
          "tp": 45,
          "fn": 355,
          "accuracy": 0.1125
        }
      },
      "auroc": 0.38702812499999995
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 467,
          "accuracy": 0.41625
        },
        "0.01": {
          "tp": 252,
          "fn": 548,
          "accuracy": 0.315
        }
      },
      "auroc": 0.6010736979166668
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9456427083333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9150697916666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9303562499999999
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.9145479166666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 20,
          "fn": 180,
          "accuracy": 0.1
        },
        "0.01": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        }
      },
      "auroc": 0.3866052083333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        },
        "0.01": {
          "tp": 175,
          "fn": 225,
          "accuracy": 0.4375
        }
      },
      "auroc": 0.6505765625
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9300953124999999
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 172,
          "fn": 228,
          "accuracy": 0.43
        }
      },
      "auroc": 0.6508375000000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 585,
          "fn": 215,
          "accuracy": 0.73125
        },
        "0.01": {
          "tp": 535,
          "fn": 265,
          "accuracy": 0.66875
        }
      },
      "auroc": 0.79046640625
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9180302083333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.9180302083333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8675614583333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8675614583333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        }
      },
      "auroc": 0.8927958333333335
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 62,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        }
      },
      "auroc": 0.8927958333333335
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        }
      },
      "auroc": 0.8790281249999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        }
      },
      "auroc": 0.8790281249999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.8308333333333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.8308333333333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        }
      },
      "auroc": 0.8549307291666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 300,
          "fn": 100,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 209,
          "fn": 191,
          "accuracy": 0.5225
        }
      },
      "auroc": 0.8549307291666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9473145833333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9473145833333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9328729166666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        }
      },
      "auroc": 0.9328729166666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.94009375
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 378,
          "fn": 22,
          "accuracy": 0.945
        }
      },
      "auroc": 0.94009375
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9365406249999999
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        }
      },
      "auroc": 0.9365406249999999
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.7541260416666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.7541260416666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        }
      },
      "auroc": 0.8453333333333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 270,
          "fn": 130,
          "accuracy": 0.675
        },
        "0.01": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        }
      },
      "auroc": 0.8453333333333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.8966447916666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.8966447916666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8541322916666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8541322916666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        }
      },
      "auroc": 0.8753885416666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 333,
          "fn": 67,
          "accuracy": 0.8325
        },
        "0.01": {
          "tp": 282,
          "fn": 118,
          "accuracy": 0.705
        }
      },
      "auroc": 0.8753885416666667
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2101,
          "fn": 99,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 1976,
          "fn": 224,
          "accuracy": 0.8981818181818182
        }
      },
      "auroc": 0.9289877840909091
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 695,
          "fn": 505,
          "accuracy": 0.5791666666666667
        },
        "0.01": {
          "tp": 603,
          "fn": 597,
          "accuracy": 0.5025
        }
      },
      "auroc": 0.6556083333333333
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2796,
          "fn": 604,
          "accuracy": 0.8223529411764706
        },
        "0.01": {
          "tp": 2579,
          "fn": 821,
          "accuracy": 0.7585294117647059
        }
      },
      "auroc": 0.8325009191176471
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 605,
          "accuracy": 0.725
        },
        "0.01": {
          "tp": 1278,
          "fn": 922,
          "accuracy": 0.5809090909090909
        }
      },
      "auroc": 0.8514797348484849
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 1024,
          "accuracy": 0.14666666666666667
        },
        "0.01": {
          "tp": 129,
          "fn": 1071,
          "accuracy": 0.1075
        }
      },
      "auroc": 0.2956230902777778
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1771,
          "fn": 1629,
          "accuracy": 0.5208823529411765
        },
        "0.01": {
          "tp": 1407,
          "fn": 1993,
          "accuracy": 0.4138235294117647
        }
      },
      "auroc": 0.6552950367647059
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3696,
          "fn": 704,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 3254,
          "fn": 1146,
          "accuracy": 0.7395454545454545
        }
      },
      "auroc": 0.8902337594696969
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 871,
          "fn": 1529,
          "accuracy": 0.36291666666666667
        },
        "0.01": {
          "tp": 732,
          "fn": 1668,
          "accuracy": 0.305
        }
      },
      "auroc": 0.4756157118055555
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 4567,
          "fn": 2233,
          "accuracy": 0.6716176470588235
        },
        "0.01": {
          "tp": 3986,
          "fn": 2814,
          "accuracy": 0.5861764705882353
        }
      },
      "auroc": 0.7438979779411764
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8945166666666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.746428125
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 259,
          "fn": 141,
          "accuracy": 0.6475
        },
        "0.01": {
          "tp": 168,
          "fn": 232,
          "accuracy": 0.42
        }
      },
      "auroc": 0.8204723958333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8717958333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        },
        "0.01": {
          "tp": 10,
          "fn": 190,
          "accuracy": 0.05
        }
      },
      "auroc": 0.5380479166666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        },
        "0.01": {
          "tp": 124,
          "fn": 276,
          "accuracy": 0.31
        }
      },
      "auroc": 0.7049218749999999
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 336,
          "fn": 64,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        }
      },
      "auroc": 0.8831562500000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 282,
          "accuracy": 0.295
        },
        "0.01": {
          "tp": 48,
          "fn": 352,
          "accuracy": 0.12
        }
      },
      "auroc": 0.6422380208333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 454,
          "fn": 346,
          "accuracy": 0.5675
        },
        "0.01": {
          "tp": 292,
          "fn": 508,
          "accuracy": 0.365
        }
      },
      "auroc": 0.7626971354166667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9186447916666669
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07037500000000002
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 212,
          "accuracy": 0.47
        },
        "0.01": {
          "tp": 178,
          "fn": 222,
          "accuracy": 0.445
        }
      },
      "auroc": 0.49450989583333343
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.44010625
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.07691666666666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 388,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.25851145833333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": {
          "tp": 181,
          "fn": 219,
          "accuracy": 0.4525
        }
      },
      "auroc": 0.6793755208333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.07364583333333335
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 600,
          "accuracy": 0.25
        },
        "0.01": {
          "tp": 182,
          "fn": 618,
          "accuracy": 0.2275
        }
      },
      "auroc": 0.3765106770833333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 117,
          "fn": 83,
          "accuracy": 0.585
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.8173968749999999
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.16719687500000002
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 120,
          "fn": 280,
          "accuracy": 0.3
        },
        "0.01": {
          "tp": 63,
          "fn": 337,
          "accuracy": 0.1575
        }
      },
      "auroc": 0.49229687499999997
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.772090625
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07300520833333335
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 297,
          "accuracy": 0.2575
        },
        "0.01": {
          "tp": 38,
          "fn": 362,
          "accuracy": 0.095
        }
      },
      "auroc": 0.4225479166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 101,
          "fn": 299,
          "accuracy": 0.2525
        }
      },
      "auroc": 0.79474375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.12010104166666669
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 223,
          "fn": 577,
          "accuracy": 0.27875
        },
        "0.01": {
          "tp": 101,
          "fn": 699,
          "accuracy": 0.12625
        }
      },
      "auroc": 0.45742239583333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9436666666666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9092770833333335
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 379,
          "fn": 21,
          "accuracy": 0.9475
        },
        "0.01": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.926471875
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 97,
          "accuracy": 0.515
        },
        "0.01": {
          "tp": 57,
          "fn": 143,
          "accuracy": 0.285
        }
      },
      "auroc": 0.7694645833333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.11183958333333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 103,
          "fn": 297,
          "accuracy": 0.2575
        },
        "0.01": {
          "tp": 57,
          "fn": 343,
          "accuracy": 0.1425
        }
      },
      "auroc": 0.4406520833333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 252,
          "fn": 148,
          "accuracy": 0.63
        }
      },
      "auroc": 0.8565656250000001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        },
        "0.01": {
          "tp": 156,
          "fn": 244,
          "accuracy": 0.39
        }
      },
      "auroc": 0.5105583333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 482,
          "fn": 318,
          "accuracy": 0.6025
        },
        "0.01": {
          "tp": 408,
          "fn": 392,
          "accuracy": 0.51
        }
      },
      "auroc": 0.6835619791666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.916259375
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        }
      },
      "auroc": 0.3604333333333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 181,
          "fn": 219,
          "accuracy": 0.4525
        }
      },
      "auroc": 0.6383463541666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 191,
          "accuracy": 0.045
        },
        "0.01": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        }
      },
      "auroc": 0.4072864583333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.06263541666666668
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 391,
          "accuracy": 0.0225
        },
        "0.01": {
          "tp": 4,
          "fn": 396,
          "accuracy": 0.01
        }
      },
      "auroc": 0.23496093750000002
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        },
        "0.01": {
          "tp": 182,
          "fn": 218,
          "accuracy": 0.455
        }
      },
      "auroc": 0.6617729166666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 392,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 3,
          "fn": 397,
          "accuracy": 0.0075
        }
      },
      "auroc": 0.21153437500000002
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 205,
          "fn": 595,
          "accuracy": 0.25625
        },
        "0.01": {
          "tp": 185,
          "fn": 615,
          "accuracy": 0.23125
        }
      },
      "auroc": 0.43665364583333344
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        },
        "0.01": {
          "tp": 130,
          "fn": 70,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8858302083333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 15,
          "fn": 185,
          "accuracy": 0.075
        }
      },
      "auroc": 0.6806479166666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 219,
          "fn": 181,
          "accuracy": 0.5475
        },
        "0.01": {
          "tp": 145,
          "fn": 255,
          "accuracy": 0.3625
        }
      },
      "auroc": 0.7832390625000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 92,
          "fn": 108,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        }
      },
      "auroc": 0.7451114583333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.18272083333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 93,
          "fn": 307,
          "accuracy": 0.2325
        },
        "0.01": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        }
      },
      "auroc": 0.46391614583333335
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        },
        "0.01": {
          "tp": 180,
          "fn": 220,
          "accuracy": 0.45
        }
      },
      "auroc": 0.8154708333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 345,
          "accuracy": 0.1375
        },
        "0.01": {
          "tp": 15,
          "fn": 385,
          "accuracy": 0.0375
        }
      },
      "auroc": 0.431684375
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 488,
          "accuracy": 0.39
        },
        "0.01": {
          "tp": 195,
          "fn": 605,
          "accuracy": 0.24375
        }
      },
      "auroc": 0.6235776041666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.7839833333333335
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 76,
          "fn": 124,
          "accuracy": 0.38
        }
      },
      "auroc": 0.7839833333333335
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        },
        "0.01": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        }
      },
      "auroc": 0.6833708333333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 67,
          "fn": 133,
          "accuracy": 0.335
        },
        "0.01": {
          "tp": 31,
          "fn": 169,
          "accuracy": 0.155
        }
      },
      "auroc": 0.6833708333333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        },
        "0.01": {
          "tp": 107,
          "fn": 293,
          "accuracy": 0.2675
        }
      },
      "auroc": 0.7336770833333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 221,
          "accuracy": 0.4475
        },
        "0.01": {
          "tp": 107,
          "fn": 293,
          "accuracy": 0.2675
        }
      },
      "auroc": 0.7336770833333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.702578125
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 89,
          "fn": 111,
          "accuracy": 0.445
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.702578125
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.6039625
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 25,
          "fn": 175,
          "accuracy": 0.125
        }
      },
      "auroc": 0.6039625
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 257,
          "accuracy": 0.3575
        },
        "0.01": {
          "tp": 71,
          "fn": 329,
          "accuracy": 0.1775
        }
      },
      "auroc": 0.6532703125
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 143,
          "fn": 257,
          "accuracy": 0.3575
        },
        "0.01": {
          "tp": 71,
          "fn": 329,
          "accuracy": 0.1775
        }
      },
      "auroc": 0.6532703125
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        }
      },
      "auroc": 0.8219812500000001
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 125,
          "fn": 75,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 68,
          "fn": 132,
          "accuracy": 0.34
        }
      },
      "auroc": 0.8219812500000001
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.7423802083333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.7423802083333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 86,
          "fn": 314,
          "accuracy": 0.215
        }
      },
      "auroc": 0.7821807291666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        },
        "0.01": {
          "tp": 86,
          "fn": 314,
          "accuracy": 0.215
        }
      },
      "auroc": 0.7821807291666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.7848916666666668
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 63,
          "fn": 137,
          "accuracy": 0.315
        }
      },
      "auroc": 0.7848916666666668
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.4167041666666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.4167041666666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 63,
          "fn": 337,
          "accuracy": 0.1575
        }
      },
      "auroc": 0.6007979166666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 114,
          "fn": 286,
          "accuracy": 0.285
        },
        "0.01": {
          "tp": 63,
          "fn": 337,
          "accuracy": 0.1575
        }
      },
      "auroc": 0.6007979166666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.7847499999999998
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        },
        "0.01": {
          "tp": 72,
          "fn": 128,
          "accuracy": 0.36
        }
      },
      "auroc": 0.7847499999999998
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        },
        "0.01": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        }
      },
      "auroc": 0.7210416666666666
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        },
        "0.01": {
          "tp": 50,
          "fn": 150,
          "accuracy": 0.25
        }
      },
      "auroc": 0.7210416666666666
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        },
        "0.01": {
          "tp": 122,
          "fn": 278,
          "accuracy": 0.305
        }
      },
      "auroc": 0.7528958333333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        },
        "0.01": {
          "tp": 122,
          "fn": 278,
          "accuracy": 0.305
        }
      },
      "auroc": 0.7528958333333334
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1587,
          "fn": 613,
          "accuracy": 0.7213636363636363
        },
        "0.01": {
          "tp": 1199,
          "fn": 1001,
          "accuracy": 0.545
        }
      },
      "auroc": 0.841318087121212
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 871,
          "accuracy": 0.27416666666666667
        },
        "0.01": {
          "tp": 212,
          "fn": 988,
          "accuracy": 0.17666666666666667
        }
      },
      "auroc": 0.48905972222222216
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1916,
          "fn": 1484,
          "accuracy": 0.5635294117647058
        },
        "0.01": {
          "tp": 1411,
          "fn": 1989,
          "accuracy": 0.415
        }
      },
      "auroc": 0.7169916053921569
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 769,
          "fn": 1431,
          "accuracy": 0.34954545454545455
        },
        "0.01": {
          "tp": 390,
          "fn": 1810,
          "accuracy": 0.17727272727272728
        }
      },
      "auroc": 0.6521195075757577
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 1164,
          "accuracy": 0.03
        },
        "0.01": {
          "tp": 11,
          "fn": 1189,
          "accuracy": 0.009166666666666667
        }
      },
      "auroc": 0.17419427083333333
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 805,
          "fn": 2595,
          "accuracy": 0.23676470588235293
        },
        "0.01": {
          "tp": 401,
          "fn": 2999,
          "accuracy": 0.11794117647058823
        }
      },
      "auroc": 0.48344001225490196
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2356,
          "fn": 2044,
          "accuracy": 0.5354545454545454
        },
        "0.01": {
          "tp": 1589,
          "fn": 2811,
          "accuracy": 0.36113636363636364
        }
      },
      "auroc": 0.7467187973484848
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 2035,
          "accuracy": 0.15208333333333332
        },
        "0.01": {
          "tp": 223,
          "fn": 2177,
          "accuracy": 0.09291666666666666
        }
      },
      "auroc": 0.33162699652777783
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2721,
          "fn": 4079,
          "accuracy": 0.4001470588235294
        },
        "0.01": {
          "tp": 1812,
          "fn": 4988,
          "accuracy": 0.2664705882352941
        }
      },
      "auroc": 0.6002158088235294
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9466187500000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9471635416666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 175,
          "fn": 25,
          "accuracy": 0.875
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.8564947916666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9021015625
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 361,
          "fn": 39,
          "accuracy": 0.9025
        }
      },
      "auroc": 0.9015567708333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 775,
          "fn": 25,
          "accuracy": 0.96875
        },
        "0.01": {
          "tp": 761,
          "fn": 39,
          "accuracy": 0.95125
        }
      },
      "auroc": 0.9246325520833334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.944971875
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08195416666666669
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        }
      },
      "auroc": 0.5134630208333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        },
        "0.01": {
          "tp": 84,
          "fn": 116,
          "accuracy": 0.42
        }
      },
      "auroc": 0.8330197916666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.08112812500000001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 139,
          "fn": 261,
          "accuracy": 0.3475
        },
        "0.01": {
          "tp": 85,
          "fn": 315,
          "accuracy": 0.2125
        }
      },
      "auroc": 0.4570739583333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 280,
          "fn": 120,
          "accuracy": 0.7
        }
      },
      "auroc": 0.8889958333333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.08154114583333336
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 338,
          "fn": 462,
          "accuracy": 0.4225
        },
        "0.01": {
          "tp": 281,
          "fn": 519,
          "accuracy": 0.35125
        }
      },
      "auroc": 0.48526848958333335
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9464625
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        },
        "0.01": {
          "tp": 38,
          "fn": 162,
          "accuracy": 0.19
        }
      },
      "auroc": 0.41066250000000004
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 248,
          "fn": 152,
          "accuracy": 0.62
        },
        "0.01": {
          "tp": 236,
          "fn": 164,
          "accuracy": 0.59
        }
      },
      "auroc": 0.6785624999999998
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9455895833333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.12459375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        }
      },
      "auroc": 0.5350916666666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9460260416666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 51,
          "fn": 349,
          "accuracy": 0.1275
        },
        "0.01": {
          "tp": 39,
          "fn": 361,
          "accuracy": 0.0975
        }
      },
      "auroc": 0.267628125
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 450,
          "fn": 350,
          "accuracy": 0.5625
        },
        "0.01": {
          "tp": 434,
          "fn": 366,
          "accuracy": 0.5425
        }
      },
      "auroc": 0.6068270833333335
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9464854166666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.947359375
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9469223958333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9200270833333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.36558541666666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        },
        "0.01": {
          "tp": 176,
          "fn": 224,
          "accuracy": 0.44
        }
      },
      "auroc": 0.64280625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        },
        "0.01": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        }
      },
      "auroc": 0.93325625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.6564723958333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 582,
          "fn": 218,
          "accuracy": 0.7275
        },
        "0.01": {
          "tp": 574,
          "fn": 226,
          "accuracy": 0.7175
        }
      },
      "auroc": 0.7948643229166666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9443083333333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        }
      },
      "auroc": 0.8207489583333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 310,
          "fn": 90,
          "accuracy": 0.775
        }
      },
      "auroc": 0.8825286458333332
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        }
      },
      "auroc": 0.7847739583333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07131250000000003
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 101,
          "fn": 299,
          "accuracy": 0.2525
        },
        "0.01": {
          "tp": 46,
          "fn": 354,
          "accuracy": 0.115
        }
      },
      "auroc": 0.42804322916666665
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        },
        "0.01": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        }
      },
      "auroc": 0.8645411458333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 148,
          "fn": 252,
          "accuracy": 0.37
        },
        "0.01": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        }
      },
      "auroc": 0.4460307291666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 447,
          "fn": 353,
          "accuracy": 0.55875
        },
        "0.01": {
          "tp": 356,
          "fn": 444,
          "accuracy": 0.445
        }
      },
      "auroc": 0.6552859375
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9417197916666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9447140625
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.9372187500000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.49235625
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        },
        "0.01": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        }
      },
      "auroc": 0.7147875000000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9424635416666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        }
      },
      "auroc": 0.7170380208333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 639,
          "fn": 161,
          "accuracy": 0.79875
        },
        "0.01": {
          "tp": 601,
          "fn": 199,
          "accuracy": 0.75125
        }
      },
      "auroc": 0.82975078125
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9418927083333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9418927083333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9166000000000001
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9166000000000001
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9292463541666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 356,
          "fn": 44,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9292463541666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9282343750000001
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9282343750000001
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.9035114583333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.9035114583333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9158729166666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 324,
          "fn": 76,
          "accuracy": 0.81
        }
      },
      "auroc": 0.9158729166666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9467208333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9467208333333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9472145833333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9472145833333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.944203125
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.944203125
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        }
      },
      "auroc": 0.8548479166666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 141,
          "fn": 59,
          "accuracy": 0.705
        },
        "0.01": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        }
      },
      "auroc": 0.8548479166666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        }
      },
      "auroc": 0.8995255208333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 339,
          "fn": 61,
          "accuracy": 0.8475
        },
        "0.01": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        }
      },
      "auroc": 0.8995255208333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9286343750000001
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9286343750000001
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.8993635416666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 165,
          "fn": 35,
          "accuracy": 0.825
        }
      },
      "auroc": 0.8993635416666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9139989583333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        }
      },
      "auroc": 0.9139989583333334
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2170,
          "fn": 30,
          "accuracy": 0.9863636363636363
        },
        "0.01": {
          "tp": 2139,
          "fn": 61,
          "accuracy": 0.9722727272727273
        }
      },
      "auroc": 0.9425743371212121
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 794,
          "fn": 406,
          "accuracy": 0.6616666666666666
        },
        "0.01": {
          "tp": 741,
          "fn": 459,
          "accuracy": 0.6175
        }
      },
      "auroc": 0.6915105902777777
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2964,
          "fn": 436,
          "accuracy": 0.8717647058823529
        },
        "0.01": {
          "tp": 2880,
          "fn": 520,
          "accuracy": 0.8470588235294118
        }
      },
      "auroc": 0.8539636029411765
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1890,
          "fn": 310,
          "accuracy": 0.8590909090909091
        },
        "0.01": {
          "tp": 1645,
          "fn": 555,
          "accuracy": 0.7477272727272727
        }
      },
      "auroc": 0.8990346590909091
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 974,
          "accuracy": 0.18833333333333332
        },
        "0.01": {
          "tp": 188,
          "fn": 1012,
          "accuracy": 0.15666666666666668
        }
      },
      "auroc": 0.33191180555555555
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2116,
          "fn": 1284,
          "accuracy": 0.6223529411764706
        },
        "0.01": {
          "tp": 1833,
          "fn": 1567,
          "accuracy": 0.5391176470588235
        }
      },
      "auroc": 0.6988736519607843
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 4060,
          "fn": 340,
          "accuracy": 0.9227272727272727
        },
        "0.01": {
          "tp": 3784,
          "fn": 616,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9208044981060606
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1020,
          "fn": 1380,
          "accuracy": 0.425
        },
        "0.01": {
          "tp": 929,
          "fn": 1471,
          "accuracy": 0.38708333333333333
        }
      },
      "auroc": 0.5117111979166666
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 5080,
          "fn": 1720,
          "accuracy": 0.7470588235294118
        },
        "0.01": {
          "tp": 4713,
          "fn": 2087,
          "accuracy": 0.6930882352941177
        }
      },
      "auroc": 0.7764186274509803
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9472729166666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9434010416666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9453369791666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.945034375
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.923090625
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        }
      },
      "auroc": 0.9340625
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9461536458333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9332458333333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 787,
          "fn": 13,
          "accuracy": 0.98375
        },
        "0.01": {
          "tp": 758,
          "fn": 42,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9396997395833334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9340062499999999
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.6371916666666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 250,
          "fn": 150,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        }
      },
      "auroc": 0.7855989583333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 128,
          "fn": 72,
          "accuracy": 0.64
        }
      },
      "auroc": 0.8854635416666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 355,
          "fn": 45,
          "accuracy": 0.8875
        },
        "0.01": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        }
      },
      "auroc": 0.9097348958333332
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        }
      },
      "auroc": 0.937825
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 118,
          "fn": 82,
          "accuracy": 0.59
        },
        "0.01": {
          "tp": 71,
          "fn": 129,
          "accuracy": 0.355
        }
      },
      "auroc": 0.8108739583333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 316,
          "fn": 84,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        }
      },
      "auroc": 0.8743494791666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9235885416666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 40,
          "fn": 160,
          "accuracy": 0.2
        }
      },
      "auroc": 0.7180031250000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 271,
          "fn": 129,
          "accuracy": 0.6775
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8207958333333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 387,
          "fn": 13,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9307067708333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 200,
          "accuracy": 0.5
        },
        "0.01": {
          "tp": 111,
          "fn": 289,
          "accuracy": 0.2775
        }
      },
      "auroc": 0.7644385416666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 587,
          "fn": 213,
          "accuracy": 0.73375
        },
        "0.01": {
          "tp": 463,
          "fn": 337,
          "accuracy": 0.57875
        }
      },
      "auroc": 0.84757265625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9432520833333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9089552083333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 61,
          "fn": 139,
          "accuracy": 0.305
        }
      },
      "auroc": 0.8218083333333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 309,
          "fn": 91,
          "accuracy": 0.7725
        },
        "0.01": {
          "tp": 230,
          "fn": 170,
          "accuracy": 0.575
        }
      },
      "auroc": 0.8653817708333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 380,
          "fn": 20,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9261036458333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9344250000000001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        }
      },
      "auroc": 0.8684520833333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": {
          "tp": 295,
          "fn": 105,
          "accuracy": 0.7375
        }
      },
      "auroc": 0.9014385416666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 102,
          "fn": 98,
          "accuracy": 0.51
        }
      },
      "auroc": 0.8704125
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 29,
          "fn": 171,
          "accuracy": 0.145
        },
        "0.01": {
          "tp": 7,
          "fn": 193,
          "accuracy": 0.035
        }
      },
      "auroc": 0.5943093749999999
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        },
        "0.01": {
          "tp": 109,
          "fn": 291,
          "accuracy": 0.2725
        }
      },
      "auroc": 0.7323609375
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 288,
          "fn": 112,
          "accuracy": 0.72
        }
      },
      "auroc": 0.90241875
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 116,
          "fn": 284,
          "accuracy": 0.29
        }
      },
      "auroc": 0.7313807291666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 544,
          "fn": 256,
          "accuracy": 0.68
        },
        "0.01": {
          "tp": 404,
          "fn": 396,
          "accuracy": 0.505
        }
      },
      "auroc": 0.8168997395833333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9408083333333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        }
      },
      "auroc": 0.9300208333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9354145833333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 172,
          "fn": 28,
          "accuracy": 0.86
        }
      },
      "auroc": 0.9239895833333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 65,
          "fn": 135,
          "accuracy": 0.325
        }
      },
      "auroc": 0.7892302083333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        },
        "0.01": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        }
      },
      "auroc": 0.8566098958333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9323989583333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        }
      },
      "auroc": 0.8596255208333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 690,
          "fn": 110,
          "accuracy": 0.8625
        },
        "0.01": {
          "tp": 599,
          "fn": 201,
          "accuracy": 0.74875
        }
      },
      "auroc": 0.8960122395833333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9079354166666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9079354166666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.87755625
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 123,
          "fn": 77,
          "accuracy": 0.615
        }
      },
      "auroc": 0.87755625
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        }
      },
      "auroc": 0.8927458333333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 268,
          "fn": 132,
          "accuracy": 0.67
        }
      },
      "auroc": 0.8927458333333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9160843750000001
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.9160843750000001
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        }
      },
      "auroc": 0.8972541666666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        }
      },
      "auroc": 0.8972541666666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9066692708333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 353,
          "fn": 47,
          "accuracy": 0.8825
        },
        "0.01": {
          "tp": 290,
          "fn": 110,
          "accuracy": 0.725
        }
      },
      "auroc": 0.9066692708333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.946984375
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.946984375
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.94483125
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.94483125
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9459078125000001
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9459078125000001
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9409
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9409
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9290770833333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9290770833333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9349885416666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        },
        "0.01": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9349885416666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9141833333333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9141833333333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.893896875
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        }
      },
      "auroc": 0.893896875
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9040401041666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 358,
          "fn": 42,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 312,
          "fn": 88,
          "accuracy": 0.78
        }
      },
      "auroc": 0.9040401041666667
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2131,
          "fn": 69,
          "accuracy": 0.9686363636363636
        },
        "0.01": {
          "tp": 1998,
          "fn": 202,
          "accuracy": 0.9081818181818182
        }
      },
      "auroc": 0.933061553030303
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1974,
          "fn": 226,
          "accuracy": 0.8972727272727272
        },
        "0.01": {
          "tp": 1710,
          "fn": 490,
          "accuracy": 0.7772727272727272
        }
      },
      "auroc": 0.9090963068181819
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 4105,
          "fn": 295,
          "accuracy": 0.9329545454545455
        },
        "0.01": {
          "tp": 3708,
          "fn": 692,
          "accuracy": 0.8427272727272728
        }
      },
      "auroc": 0.9210789299242425
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9472260416666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.943271875
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9452489583333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 168,
          "fn": 32,
          "accuracy": 0.84
        },
        "0.01": {
          "tp": 154,
          "fn": 46,
          "accuracy": 0.77
        }
      },
      "auroc": 0.8428020833333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        }
      },
      "auroc": 0.8952552083333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9474671875
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        }
      },
      "auroc": 0.8930369791666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 766,
          "fn": 34,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 747,
          "fn": 53,
          "accuracy": 0.93375
        }
      },
      "auroc": 0.9202520833333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9264010416666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07704375000000001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        },
        "0.01": {
          "tp": 183,
          "fn": 217,
          "accuracy": 0.4575
        }
      },
      "auroc": 0.5017223958333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 95,
          "accuracy": 0.525
        },
        "0.01": {
          "tp": 52,
          "fn": 148,
          "accuracy": 0.26
        }
      },
      "auroc": 0.7574093749999999
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07786458333333335
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 105,
          "fn": 295,
          "accuracy": 0.2625
        },
        "0.01": {
          "tp": 52,
          "fn": 348,
          "accuracy": 0.13
        }
      },
      "auroc": 0.41763697916666664
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 104,
          "accuracy": 0.74
        },
        "0.01": {
          "tp": 235,
          "fn": 165,
          "accuracy": 0.5875
        }
      },
      "auroc": 0.8419052083333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 400,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07745416666666669
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 296,
          "fn": 504,
          "accuracy": 0.37
        },
        "0.01": {
          "tp": 235,
          "fn": 565,
          "accuracy": 0.29375
        }
      },
      "auroc": 0.4596796875
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9436854166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": {
          "tp": 33,
          "fn": 167,
          "accuracy": 0.165
        }
      },
      "auroc": 0.39741458333333335
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 228,
          "fn": 172,
          "accuracy": 0.57
        }
      },
      "auroc": 0.67055
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9376895833333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.11600729166666668
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        },
        "0.01": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        }
      },
      "auroc": 0.5268484375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 381,
          "fn": 19,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.9406875
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 352,
          "accuracy": 0.12
        },
        "0.01": {
          "tp": 34,
          "fn": 366,
          "accuracy": 0.085
        }
      },
      "auroc": 0.2567109375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 441,
          "fn": 359,
          "accuracy": 0.55125
        },
        "0.01": {
          "tp": 415,
          "fn": 385,
          "accuracy": 0.51875
        }
      },
      "auroc": 0.59869921875
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9422197916666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9460135416666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9441166666666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        }
      },
      "auroc": 0.9164802083333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.3359364583333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        }
      },
      "auroc": 0.6262083333333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.92935
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        }
      },
      "auroc": 0.6409750000000001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 579,
          "fn": 221,
          "accuracy": 0.72375
        },
        "0.01": {
          "tp": 560,
          "fn": 240,
          "accuracy": 0.7
        }
      },
      "auroc": 0.7851625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9247729166666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 88,
          "accuracy": 0.56
        },
        "0.01": {
          "tp": 74,
          "fn": 126,
          "accuracy": 0.37
        }
      },
      "auroc": 0.7556947916666668
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 302,
          "fn": 98,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 257,
          "fn": 143,
          "accuracy": 0.6425
        }
      },
      "auroc": 0.8402338541666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 70,
          "fn": 130,
          "accuracy": 0.35
        },
        "0.01": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        }
      },
      "auroc": 0.7106645833333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07123645833333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 70,
          "fn": 330,
          "accuracy": 0.175
        },
        "0.01": {
          "tp": 34,
          "fn": 366,
          "accuracy": 0.085
        }
      },
      "auroc": 0.39095052083333337
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        },
        "0.01": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        }
      },
      "auroc": 0.8177187499999999
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 112,
          "fn": 288,
          "accuracy": 0.28
        },
        "0.01": {
          "tp": 74,
          "fn": 326,
          "accuracy": 0.185
        }
      },
      "auroc": 0.41346562499999995
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 372,
          "fn": 428,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 291,
          "fn": 509,
          "accuracy": 0.36375
        }
      },
      "auroc": 0.6155921875
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9430375
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.9365885416666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        },
        "0.01": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        }
      },
      "auroc": 0.9398130208333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 13,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.921921875
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 45,
          "fn": 155,
          "accuracy": 0.225
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.47479791666666665
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 232,
          "fn": 168,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 193,
          "fn": 207,
          "accuracy": 0.4825
        }
      },
      "auroc": 0.6983598958333332
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9324796875
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        },
        "0.01": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        }
      },
      "auroc": 0.7056932291666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 627,
          "fn": 173,
          "accuracy": 0.78375
        },
        "0.01": {
          "tp": 577,
          "fn": 223,
          "accuracy": 0.72125
        }
      },
      "auroc": 0.8190864583333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9124104166666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9124104166666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.875334375
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        },
        "0.01": {
          "tp": 129,
          "fn": 71,
          "accuracy": 0.645
        }
      },
      "auroc": 0.875334375
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        }
      },
      "auroc": 0.8938723958333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 293,
          "fn": 107,
          "accuracy": 0.7325
        }
      },
      "auroc": 0.8938723958333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.8756249999999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 140,
          "fn": 60,
          "accuracy": 0.7
        }
      },
      "auroc": 0.8756249999999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.8558395833333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 111,
          "fn": 89,
          "accuracy": 0.555
        }
      },
      "auroc": 0.8558395833333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        },
        "0.01": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        }
      },
      "auroc": 0.8657322916666668
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 315,
          "fn": 85,
          "accuracy": 0.7875
        },
        "0.01": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        }
      },
      "auroc": 0.8657322916666668
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9470885416666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9470885416666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.946015625
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.946015625
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9465520833333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9465520833333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9417135416666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9417135416666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.8381552083333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 127,
          "fn": 73,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 77,
          "fn": 123,
          "accuracy": 0.385
        }
      },
      "auroc": 0.8381552083333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        }
      },
      "auroc": 0.889934375
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        }
      },
      "auroc": 0.889934375
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9047333333333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 179,
          "fn": 21,
          "accuracy": 0.895
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9047333333333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.8602260416666666
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.8602260416666666
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        }
      },
      "auroc": 0.8824796875
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 63,
          "accuracy": 0.8425
        },
        "0.01": {
          "tp": 305,
          "fn": 95,
          "accuracy": 0.7625
        }
      },
      "auroc": 0.8824796875
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2099,
          "fn": 101,
          "accuracy": 0.9540909090909091
        },
        "0.01": {
          "tp": 2006,
          "fn": 194,
          "accuracy": 0.9118181818181819
        }
      },
      "auroc": 0.9280830492424242
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 750,
          "fn": 450,
          "accuracy": 0.625
        },
        "0.01": {
          "tp": 687,
          "fn": 513,
          "accuracy": 0.5725
        }
      },
      "auroc": 0.6760045138888888
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2849,
          "fn": 551,
          "accuracy": 0.8379411764705882
        },
        "0.01": {
          "tp": 2693,
          "fn": 707,
          "accuracy": 0.7920588235294118
        }
      },
      "auroc": 0.8391141544117646
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1733,
          "fn": 467,
          "accuracy": 0.7877272727272727
        },
        "0.01": {
          "tp": 1472,
          "fn": 728,
          "accuracy": 0.6690909090909091
        }
      },
      "auroc": 0.8697677083333333
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 219,
          "fn": 981,
          "accuracy": 0.1825
        },
        "0.01": {
          "tp": 177,
          "fn": 1023,
          "accuracy": 0.1475
        }
      },
      "auroc": 0.3197741319444445
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1952,
          "fn": 1448,
          "accuracy": 0.5741176470588235
        },
        "0.01": {
          "tp": 1649,
          "fn": 1751,
          "accuracy": 0.485
        }
      },
      "auroc": 0.6756523284313727
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3832,
          "fn": 568,
          "accuracy": 0.8709090909090909
        },
        "0.01": {
          "tp": 3478,
          "fn": 922,
          "accuracy": 0.7904545454545454
        }
      },
      "auroc": 0.8989253787878788
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 969,
          "fn": 1431,
          "accuracy": 0.40375
        },
        "0.01": {
          "tp": 864,
          "fn": 1536,
          "accuracy": 0.36
        }
      },
      "auroc": 0.49788932291666665
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 4801,
          "fn": 1999,
          "accuracy": 0.7060294117647059
        },
        "0.01": {
          "tp": 4342,
          "fn": 2458,
          "accuracy": 0.6385294117647059
        }
      },
      "auroc": 0.7573832414215687
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9471385416666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9474234375
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        }
      },
      "auroc": 0.8634541666666665
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.90558125
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9052963541666668
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 774,
          "fn": 26,
          "accuracy": 0.9675
        },
        "0.01": {
          "tp": 766,
          "fn": 34,
          "accuracy": 0.9575
        }
      },
      "auroc": 0.92650234375
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9441125
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08522604166666668
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        }
      },
      "auroc": 0.5146692708333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 153,
          "fn": 47,
          "accuracy": 0.765
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.8555864583333332
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.08133229166666668
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 154,
          "fn": 246,
          "accuracy": 0.385
        },
        "0.01": {
          "tp": 102,
          "fn": 298,
          "accuracy": 0.255
        }
      },
      "auroc": 0.46845937499999996
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 351,
          "fn": 49,
          "accuracy": 0.8775
        },
        "0.01": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        }
      },
      "auroc": 0.8998494791666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.08327916666666668
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 352,
          "fn": 448,
          "accuracy": 0.44
        },
        "0.01": {
          "tp": 298,
          "fn": 502,
          "accuracy": 0.3725
        }
      },
      "auroc": 0.49156432291666663
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9467135416666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 58,
          "fn": 142,
          "accuracy": 0.29
        },
        "0.01": {
          "tp": 42,
          "fn": 158,
          "accuracy": 0.21
        }
      },
      "auroc": 0.4488989583333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        },
        "0.01": {
          "tp": 241,
          "fn": 159,
          "accuracy": 0.6025
        }
      },
      "auroc": 0.6978062500000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9459
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 196,
          "accuracy": 0.02
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.13260729166666668
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 204,
          "fn": 196,
          "accuracy": 0.51
        },
        "0.01": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        }
      },
      "auroc": 0.5392536458333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9463067708333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 62,
          "fn": 338,
          "accuracy": 0.155
        },
        "0.01": {
          "tp": 43,
          "fn": 357,
          "accuracy": 0.1075
        }
      },
      "auroc": 0.290753125
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 462,
          "fn": 338,
          "accuracy": 0.5775
        },
        "0.01": {
          "tp": 439,
          "fn": 361,
          "accuracy": 0.54875
        }
      },
      "auroc": 0.6185299479166666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9464854166666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9470125
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9467489583333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9220354166666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.44458229166666663
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 191,
          "fn": 209,
          "accuracy": 0.4775
        },
        "0.01": {
          "tp": 178,
          "fn": 222,
          "accuracy": 0.445
        }
      },
      "auroc": 0.6833088541666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9342604166666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 207,
          "fn": 193,
          "accuracy": 0.5175
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.6957973958333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 589,
          "fn": 211,
          "accuracy": 0.73625
        },
        "0.01": {
          "tp": 576,
          "fn": 224,
          "accuracy": 0.72
        }
      },
      "auroc": 0.81502890625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.942375
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 41,
          "accuracy": 0.795
        },
        "0.01": {
          "tp": 126,
          "fn": 74,
          "accuracy": 0.63
        }
      },
      "auroc": 0.8425406249999999
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        }
      },
      "auroc": 0.8924578125
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 91,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.8048458333333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07917812500000002
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 109,
          "fn": 291,
          "accuracy": 0.2725
        },
        "0.01": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        }
      },
      "auroc": 0.4420119791666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        },
        "0.01": {
          "tp": 251,
          "fn": 149,
          "accuracy": 0.6275
        }
      },
      "auroc": 0.8736104166666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 159,
          "fn": 241,
          "accuracy": 0.3975
        },
        "0.01": {
          "tp": 126,
          "fn": 274,
          "accuracy": 0.315
        }
      },
      "auroc": 0.460859375
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 466,
          "fn": 334,
          "accuracy": 0.5825
        },
        "0.01": {
          "tp": 377,
          "fn": 423,
          "accuracy": 0.47125
        }
      },
      "auroc": 0.6672348958333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9432833333333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9454958333333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9397229166666666
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 48,
          "fn": 152,
          "accuracy": 0.24
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.524053125
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 244,
          "fn": 156,
          "accuracy": 0.61
        },
        "0.01": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        }
      },
      "auroc": 0.7318880208333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 388,
          "fn": 12,
          "accuracy": 0.97
        }
      },
      "auroc": 0.943715625
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        },
        "0.01": {
          "tp": 224,
          "fn": 176,
          "accuracy": 0.56
        }
      },
      "auroc": 0.7336682291666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 643,
          "fn": 157,
          "accuracy": 0.80375
        },
        "0.01": {
          "tp": 612,
          "fn": 188,
          "accuracy": 0.765
        }
      },
      "auroc": 0.8386919270833333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9420020833333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.9420020833333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.922178125
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.922178125
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9320901041666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 360,
          "fn": 40,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9320901041666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.93040625
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        }
      },
      "auroc": 0.93040625
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9133822916666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.9133822916666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        }
      },
      "auroc": 0.9218942708333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 341,
          "fn": 59,
          "accuracy": 0.8525
        }
      },
      "auroc": 0.9218942708333333
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9474375
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9474375
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9475729166666667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9475729166666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9451864583333335
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9451864583333335
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8632427083333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 85,
          "fn": 115,
          "accuracy": 0.425
        }
      },
      "auroc": 0.8632427083333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        }
      },
      "auroc": 0.9042145833333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        },
        "0.01": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        }
      },
      "auroc": 0.9042145833333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9339833333333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        }
      },
      "auroc": 0.9339833333333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9027895833333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 182,
          "fn": 18,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 161,
          "fn": 39,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9027895833333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9183864583333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 375,
          "fn": 25,
          "accuracy": 0.9375
        },
        "0.01": {
          "tp": 346,
          "fn": 54,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9183864583333334
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2175,
          "fn": 25,
          "accuracy": 0.9886363636363636
        },
        "0.01": {
          "tp": 2146,
          "fn": 54,
          "accuracy": 0.9754545454545455
        }
      },
      "auroc": 0.9431263257575757
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 815,
          "fn": 385,
          "accuracy": 0.6791666666666667
        },
        "0.01": {
          "tp": 764,
          "fn": 436,
          "accuracy": 0.6366666666666667
        }
      },
      "auroc": 0.70235
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2990,
          "fn": 410,
          "accuracy": 0.8794117647058823
        },
        "0.01": {
          "tp": 2910,
          "fn": 490,
          "accuracy": 0.8558823529411764
        }
      },
      "auroc": 0.8581464460784314
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1944,
          "fn": 256,
          "accuracy": 0.8836363636363637
        },
        "0.01": {
          "tp": 1693,
          "fn": 507,
          "accuracy": 0.7695454545454545
        }
      },
      "auroc": 0.9058935606060605
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 235,
          "fn": 965,
          "accuracy": 0.19583333333333333
        },
        "0.01": {
          "tp": 195,
          "fn": 1005,
          "accuracy": 0.1625
        }
      },
      "auroc": 0.3542012152777777
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2179,
          "fn": 1221,
          "accuracy": 0.6408823529411765
        },
        "0.01": {
          "tp": 1888,
          "fn": 1512,
          "accuracy": 0.5552941176470588
        }
      },
      "auroc": 0.7111786151960784
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 4119,
          "fn": 281,
          "accuracy": 0.9361363636363637
        },
        "0.01": {
          "tp": 3839,
          "fn": 561,
          "accuracy": 0.8725
        }
      },
      "auroc": 0.9245099431818181
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1050,
          "fn": 1350,
          "accuracy": 0.4375
        },
        "0.01": {
          "tp": 959,
          "fn": 1441,
          "accuracy": 0.39958333333333335
        }
      },
      "auroc": 0.5282756076388889
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 5169,
          "fn": 1631,
          "accuracy": 0.7601470588235294
        },
        "0.01": {
          "tp": 4798,
          "fn": 2002,
          "accuracy": 0.7055882352941176
        }
      },
      "auroc": 0.784662530637255
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 171,
          "fn": 29,
          "accuracy": 0.855
        }
      },
      "auroc": 0.927521875
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        },
        "0.01": {
          "tp": 91,
          "fn": 109,
          "accuracy": 0.455
        }
      },
      "auroc": 0.8683729166666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 262,
          "fn": 138,
          "accuracy": 0.655
        }
      },
      "auroc": 0.8979473958333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        }
      },
      "auroc": 0.920840625
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 106,
          "fn": 94,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        }
      },
      "auroc": 0.7723145833333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        }
      },
      "auroc": 0.8465776041666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 323,
          "fn": 77,
          "accuracy": 0.8075
        }
      },
      "auroc": 0.9241812500000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 261,
          "fn": 139,
          "accuracy": 0.6525
        },
        "0.01": {
          "tp": 145,
          "fn": 255,
          "accuracy": 0.3625
        }
      },
      "auroc": 0.8203437499999999
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 645,
          "fn": 155,
          "accuracy": 0.80625
        },
        "0.01": {
          "tp": 468,
          "fn": 332,
          "accuracy": 0.585
        }
      },
      "auroc": 0.8722625
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9395760416666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.37777083333333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 218,
          "fn": 182,
          "accuracy": 0.545
        },
        "0.01": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        }
      },
      "auroc": 0.6586734375
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        }
      },
      "auroc": 0.8853604166666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        },
        "0.01": {
          "tp": 19,
          "fn": 181,
          "accuracy": 0.095
        }
      },
      "auroc": 0.4169489583333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        },
        "0.01": {
          "tp": 153,
          "fn": 247,
          "accuracy": 0.3825
        }
      },
      "auroc": 0.6511546874999999
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 365,
          "fn": 35,
          "accuracy": 0.9125
        },
        "0.01": {
          "tp": 322,
          "fn": 78,
          "accuracy": 0.805
        }
      },
      "auroc": 0.9124682291666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 50,
          "fn": 350,
          "accuracy": 0.125
        },
        "0.01": {
          "tp": 37,
          "fn": 363,
          "accuracy": 0.0925
        }
      },
      "auroc": 0.39735989583333337
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 415,
          "fn": 385,
          "accuracy": 0.51875
        },
        "0.01": {
          "tp": 359,
          "fn": 441,
          "accuracy": 0.44875
        }
      },
      "auroc": 0.6549140625
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 167,
          "fn": 33,
          "accuracy": 0.835
        },
        "0.01": {
          "tp": 121,
          "fn": 79,
          "accuracy": 0.605
        }
      },
      "auroc": 0.8857979166666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 183,
          "accuracy": 0.085
        },
        "0.01": {
          "tp": 6,
          "fn": 194,
          "accuracy": 0.03
        }
      },
      "auroc": 0.45238541666666665
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        },
        "0.01": {
          "tp": 127,
          "fn": 273,
          "accuracy": 0.3175
        }
      },
      "auroc": 0.6690916666666668
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 116,
          "fn": 84,
          "accuracy": 0.58
        }
      },
      "auroc": 0.8815968750000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.2673270833333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 236,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 116,
          "fn": 284,
          "accuracy": 0.29
        }
      },
      "auroc": 0.5744619791666666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 329,
          "fn": 71,
          "accuracy": 0.8225
        },
        "0.01": {
          "tp": 237,
          "fn": 163,
          "accuracy": 0.5925
        }
      },
      "auroc": 0.8836973958333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 19,
          "fn": 381,
          "accuracy": 0.0475
        },
        "0.01": {
          "tp": 6,
          "fn": 394,
          "accuracy": 0.015
        }
      },
      "auroc": 0.35985625
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 452,
          "accuracy": 0.435
        },
        "0.01": {
          "tp": 243,
          "fn": 557,
          "accuracy": 0.30375
        }
      },
      "auroc": 0.6217768229166667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9384375
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 151,
          "fn": 49,
          "accuracy": 0.755
        },
        "0.01": {
          "tp": 119,
          "fn": 81,
          "accuracy": 0.595
        }
      },
      "auroc": 0.8733833333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 307,
          "fn": 93,
          "accuracy": 0.7675
        }
      },
      "auroc": 0.9059104166666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 146,
          "fn": 54,
          "accuracy": 0.73
        }
      },
      "auroc": 0.8949624999999999
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 35,
          "fn": 165,
          "accuracy": 0.175
        },
        "0.01": {
          "tp": 11,
          "fn": 189,
          "accuracy": 0.055
        }
      },
      "auroc": 0.6218083333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 188,
          "accuracy": 0.53
        },
        "0.01": {
          "tp": 157,
          "fn": 243,
          "accuracy": 0.3925
        }
      },
      "auroc": 0.7583854166666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 334,
          "fn": 66,
          "accuracy": 0.835
        }
      },
      "auroc": 0.9167000000000001
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 186,
          "fn": 214,
          "accuracy": 0.465
        },
        "0.01": {
          "tp": 130,
          "fn": 270,
          "accuracy": 0.325
        }
      },
      "auroc": 0.7475958333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 559,
          "fn": 241,
          "accuracy": 0.69875
        },
        "0.01": {
          "tp": 464,
          "fn": 336,
          "accuracy": 0.58
        }
      },
      "auroc": 0.8321479166666668
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9381625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 82,
          "fn": 118,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 34,
          "fn": 166,
          "accuracy": 0.17
        }
      },
      "auroc": 0.729034375
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 279,
          "fn": 121,
          "accuracy": 0.6975
        },
        "0.01": {
          "tp": 220,
          "fn": 180,
          "accuracy": 0.55
        }
      },
      "auroc": 0.8335984375000001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        },
        "0.01": {
          "tp": 97,
          "fn": 103,
          "accuracy": 0.485
        }
      },
      "auroc": 0.8670322916666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 13,
          "fn": 187,
          "accuracy": 0.065
        },
        "0.01": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        }
      },
      "auroc": 0.4418895833333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 230,
          "accuracy": 0.425
        },
        "0.01": {
          "tp": 99,
          "fn": 301,
          "accuracy": 0.2475
        }
      },
      "auroc": 0.6544609375
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 354,
          "fn": 46,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 283,
          "fn": 117,
          "accuracy": 0.7075
        }
      },
      "auroc": 0.9025973958333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 95,
          "fn": 305,
          "accuracy": 0.2375
        },
        "0.01": {
          "tp": 36,
          "fn": 364,
          "accuracy": 0.09
        }
      },
      "auroc": 0.5854619791666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 449,
          "fn": 351,
          "accuracy": 0.56125
        },
        "0.01": {
          "tp": 319,
          "fn": 481,
          "accuracy": 0.39875
        }
      },
      "auroc": 0.7440296875
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9212385416666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        },
        "0.01": {
          "tp": 59,
          "fn": 141,
          "accuracy": 0.295
        }
      },
      "auroc": 0.8157270833333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        },
        "0.01": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        }
      },
      "auroc": 0.8684828124999999
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        },
        "0.01": {
          "tp": 135,
          "fn": 65,
          "accuracy": 0.675
        }
      },
      "auroc": 0.8991364583333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        },
        "0.01": {
          "tp": 8,
          "fn": 192,
          "accuracy": 0.04
        }
      },
      "auroc": 0.6363479166666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 213,
          "fn": 187,
          "accuracy": 0.5325
        },
        "0.01": {
          "tp": 143,
          "fn": 257,
          "accuracy": 0.3575
        }
      },
      "auroc": 0.7677421875000001
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 366,
          "fn": 34,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 298,
          "fn": 102,
          "accuracy": 0.745
        }
      },
      "auroc": 0.9101874999999999
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 146,
          "fn": 254,
          "accuracy": 0.365
        },
        "0.01": {
          "tp": 67,
          "fn": 333,
          "accuracy": 0.1675
        }
      },
      "auroc": 0.7260374999999999
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 512,
          "fn": 288,
          "accuracy": 0.64
        },
        "0.01": {
          "tp": 365,
          "fn": 435,
          "accuracy": 0.45625
        }
      },
      "auroc": 0.8181125000000001
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8832427083333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        },
        "0.01": {
          "tp": 114,
          "fn": 86,
          "accuracy": 0.57
        }
      },
      "auroc": 0.8832427083333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        }
      },
      "auroc": 0.8677135416666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 156,
          "fn": 44,
          "accuracy": 0.78
        },
        "0.01": {
          "tp": 108,
          "fn": 92,
          "accuracy": 0.54
        }
      },
      "auroc": 0.8677135416666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        }
      },
      "auroc": 0.8754781250000001
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 325,
          "fn": 75,
          "accuracy": 0.8125
        },
        "0.01": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        }
      },
      "auroc": 0.8754781250000001
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9103812499999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 157,
          "fn": 43,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9103812499999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9030562499999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.9030562499999999
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        }
      },
      "auroc": 0.90671875
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 357,
          "fn": 43,
          "accuracy": 0.8925
        },
        "0.01": {
          "tp": 299,
          "fn": 101,
          "accuracy": 0.7475
        }
      },
      "auroc": 0.90671875
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.906965625
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        },
        "0.01": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        }
      },
      "auroc": 0.906965625
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.8777802083333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 100,
          "fn": 100,
          "accuracy": 0.5
        }
      },
      "auroc": 0.8777802083333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        }
      },
      "auroc": 0.8923729166666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 233,
          "fn": 167,
          "accuracy": 0.5825
        }
      },
      "auroc": 0.8923729166666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9277010416666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9277010416666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        }
      },
      "auroc": 0.8630541666666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 152,
          "fn": 48,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 94,
          "fn": 106,
          "accuracy": 0.47
        }
      },
      "auroc": 0.8630541666666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        }
      },
      "auroc": 0.8953776041666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 258,
          "fn": 142,
          "accuracy": 0.645
        }
      },
      "auroc": 0.8953776041666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.8966718750000001
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 138,
          "fn": 62,
          "accuracy": 0.69
        }
      },
      "auroc": 0.8966718750000001
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.8748937499999999
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 158,
          "fn": 42,
          "accuracy": 0.79
        },
        "0.01": {
          "tp": 122,
          "fn": 78,
          "accuracy": 0.61
        }
      },
      "auroc": 0.8748937499999999
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8857828125
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 68,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 260,
          "fn": 140,
          "accuracy": 0.65
        }
      },
      "auroc": 0.8857828125
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2040,
          "fn": 160,
          "accuracy": 0.9272727272727272
        },
        "0.01": {
          "tp": 1723,
          "fn": 477,
          "accuracy": 0.7831818181818182
        }
      },
      "auroc": 0.9159724431818181
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 537,
          "fn": 663,
          "accuracy": 0.4475
        },
        "0.01": {
          "tp": 327,
          "fn": 873,
          "accuracy": 0.2725
        }
      },
      "auroc": 0.6861123263888889
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2577,
          "fn": 823,
          "accuracy": 0.7579411764705882
        },
        "0.01": {
          "tp": 2050,
          "fn": 1350,
          "accuracy": 0.6029411764705882
        }
      },
      "auroc": 0.8348453431372549
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1840,
          "fn": 360,
          "accuracy": 0.8363636363636363
        },
        "0.01": {
          "tp": 1346,
          "fn": 854,
          "accuracy": 0.6118181818181818
        }
      },
      "auroc": 0.8850388257575759
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 220,
          "fn": 980,
          "accuracy": 0.18333333333333332
        },
        "0.01": {
          "tp": 94,
          "fn": 1106,
          "accuracy": 0.07833333333333334
        }
      },
      "auroc": 0.5261060763888888
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2060,
          "fn": 1340,
          "accuracy": 0.6058823529411764
        },
        "0.01": {
          "tp": 1440,
          "fn": 1960,
          "accuracy": 0.4235294117647059
        }
      },
      "auroc": 0.7583566789215685
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3880,
          "fn": 520,
          "accuracy": 0.8818181818181818
        },
        "0.01": {
          "tp": 3069,
          "fn": 1331,
          "accuracy": 0.6975
        }
      },
      "auroc": 0.9005056344696969
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 757,
          "fn": 1643,
          "accuracy": 0.3154166666666667
        },
        "0.01": {
          "tp": 421,
          "fn": 1979,
          "accuracy": 0.17541666666666667
        }
      },
      "auroc": 0.6061092013888888
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4637,
          "fn": 2163,
          "accuracy": 0.6819117647058823
        },
        "0.01": {
          "tp": 3490,
          "fn": 3310,
          "accuracy": 0.513235294117647
        }
      },
      "auroc": 0.7966010110294117
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9451677083333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9464380208333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 170,
          "fn": 30,
          "accuracy": 0.85
        },
        "0.01": {
          "tp": 148,
          "fn": 52,
          "accuracy": 0.74
        }
      },
      "auroc": 0.8459010416666666
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 348,
          "fn": 52,
          "accuracy": 0.87
        }
      },
      "auroc": 0.8968046875000001
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 370,
          "fn": 30,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        }
      },
      "auroc": 0.895534375
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 770,
          "fn": 30,
          "accuracy": 0.9625
        },
        "0.01": {
          "tp": 742,
          "fn": 58,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.9216213541666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        }
      },
      "auroc": 0.9369052083333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08432291666666669
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 205,
          "accuracy": 0.4875
        },
        "0.01": {
          "tp": 190,
          "fn": 210,
          "accuracy": 0.475
        }
      },
      "auroc": 0.5106140625000001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 79,
          "fn": 121,
          "accuracy": 0.395
        },
        "0.01": {
          "tp": 27,
          "fn": 173,
          "accuracy": 0.135
        }
      },
      "auroc": 0.7360802083333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.08150520833333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 80,
          "fn": 320,
          "accuracy": 0.2
        },
        "0.01": {
          "tp": 28,
          "fn": 372,
          "accuracy": 0.07
        }
      },
      "auroc": 0.40879270833333337
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 274,
          "fn": 126,
          "accuracy": 0.685
        },
        "0.01": {
          "tp": 217,
          "fn": 183,
          "accuracy": 0.5425
        }
      },
      "auroc": 0.8364927083333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.08291406250000001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 275,
          "fn": 525,
          "accuracy": 0.34375
        },
        "0.01": {
          "tp": 218,
          "fn": 582,
          "accuracy": 0.2725
        }
      },
      "auroc": 0.4597033854166666
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9420083333333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 44,
          "fn": 156,
          "accuracy": 0.22
        },
        "0.01": {
          "tp": 28,
          "fn": 172,
          "accuracy": 0.14
        }
      },
      "auroc": 0.3999958333333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 242,
          "fn": 158,
          "accuracy": 0.605
        },
        "0.01": {
          "tp": 222,
          "fn": 178,
          "accuracy": 0.555
        }
      },
      "auroc": 0.6710020833333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 183,
          "fn": 17,
          "accuracy": 0.915
        }
      },
      "auroc": 0.9376614583333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 198,
          "accuracy": 0.01
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.11203854166666669
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 203,
          "accuracy": 0.4925
        },
        "0.01": {
          "tp": 184,
          "fn": 216,
          "accuracy": 0.46
        }
      },
      "auroc": 0.5248499999999999
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        },
        "0.01": {
          "tp": 377,
          "fn": 23,
          "accuracy": 0.9425
        }
      },
      "auroc": 0.9398348958333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 354,
          "accuracy": 0.115
        },
        "0.01": {
          "tp": 29,
          "fn": 371,
          "accuracy": 0.0725
        }
      },
      "auroc": 0.2560171875
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 439,
          "fn": 361,
          "accuracy": 0.54875
        },
        "0.01": {
          "tp": 406,
          "fn": 394,
          "accuracy": 0.5075
        }
      },
      "auroc": 0.5979260416666665
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9453854166666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9462635416666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9458244791666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 174,
          "fn": 26,
          "accuracy": 0.87
        },
        "0.01": {
          "tp": 155,
          "fn": 45,
          "accuracy": 0.775
        }
      },
      "auroc": 0.8940572916666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.303659375
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        },
        "0.01": {
          "tp": 155,
          "fn": 245,
          "accuracy": 0.3875
        }
      },
      "auroc": 0.5988583333333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 352,
          "fn": 48,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9197213541666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 202,
          "fn": 198,
          "accuracy": 0.505
        },
        "0.01": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        }
      },
      "auroc": 0.6249614583333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 575,
          "fn": 225,
          "accuracy": 0.71875
        },
        "0.01": {
          "tp": 550,
          "fn": 250,
          "accuracy": 0.6875
        }
      },
      "auroc": 0.77234140625
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        },
        "0.01": {
          "tp": 192,
          "fn": 8,
          "accuracy": 0.96
        }
      },
      "auroc": 0.93568125
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 93,
          "fn": 107,
          "accuracy": 0.465
        }
      },
      "auroc": 0.7963125
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 328,
          "fn": 72,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 285,
          "fn": 115,
          "accuracy": 0.7125
        }
      },
      "auroc": 0.865996875
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 146,
          "accuracy": 0.27
        },
        "0.01": {
          "tp": 16,
          "fn": 184,
          "accuracy": 0.08
        }
      },
      "auroc": 0.6740572916666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.06545416666666669
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 54,
          "fn": 346,
          "accuracy": 0.135
        },
        "0.01": {
          "tp": 16,
          "fn": 384,
          "accuracy": 0.04
        }
      },
      "auroc": 0.3697557291666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 249,
          "fn": 151,
          "accuracy": 0.6225
        },
        "0.01": {
          "tp": 208,
          "fn": 192,
          "accuracy": 0.52
        }
      },
      "auroc": 0.8048692708333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 267,
          "accuracy": 0.3325
        },
        "0.01": {
          "tp": 93,
          "fn": 307,
          "accuracy": 0.2325
        }
      },
      "auroc": 0.4308833333333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 418,
          "accuracy": 0.4775
        },
        "0.01": {
          "tp": 301,
          "fn": 499,
          "accuracy": 0.37625
        }
      },
      "auroc": 0.6178763020833333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9468375
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 188,
          "fn": 12,
          "accuracy": 0.94
        }
      },
      "auroc": 0.9366197916666668
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 386,
          "fn": 14,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9417286458333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        }
      },
      "auroc": 0.9247
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 164,
          "accuracy": 0.18
        },
        "0.01": {
          "tp": 18,
          "fn": 182,
          "accuracy": 0.09
        }
      },
      "auroc": 0.4667864583333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 225,
          "fn": 175,
          "accuracy": 0.5625
        },
        "0.01": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        }
      },
      "auroc": 0.6957432291666668
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        },
        "0.01": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        }
      },
      "auroc": 0.93576875
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 232,
          "fn": 168,
          "accuracy": 0.58
        },
        "0.01": {
          "tp": 206,
          "fn": 194,
          "accuracy": 0.515
        }
      },
      "auroc": 0.701703125
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 621,
          "fn": 179,
          "accuracy": 0.77625
        },
        "0.01": {
          "tp": 580,
          "fn": 220,
          "accuracy": 0.725
        }
      },
      "auroc": 0.8187359375
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9309166666666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 196,
          "fn": 4,
          "accuracy": 0.98
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9309166666666667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.8892520833333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 166,
          "fn": 34,
          "accuracy": 0.83
        },
        "0.01": {
          "tp": 136,
          "fn": 64,
          "accuracy": 0.68
        }
      },
      "auroc": 0.8892520833333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        }
      },
      "auroc": 0.9100843750000001
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 313,
          "fn": 87,
          "accuracy": 0.7825
        }
      },
      "auroc": 0.9100843750000001
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.8966875
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 176,
          "fn": 24,
          "accuracy": 0.88
        },
        "0.01": {
          "tp": 139,
          "fn": 61,
          "accuracy": 0.695
        }
      },
      "auroc": 0.8966875
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.8526
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 150,
          "fn": 50,
          "accuracy": 0.75
        },
        "0.01": {
          "tp": 101,
          "fn": 99,
          "accuracy": 0.505
        }
      },
      "auroc": 0.8526
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        }
      },
      "auroc": 0.87464375
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 326,
          "fn": 74,
          "accuracy": 0.815
        },
        "0.01": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        }
      },
      "auroc": 0.87464375
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9447875
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9447875
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9462479166666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9462479166666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9419270833333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9419270833333333
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.7932697916666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 104,
          "fn": 96,
          "accuracy": 0.52
        },
        "0.01": {
          "tp": 43,
          "fn": 157,
          "accuracy": 0.215
        }
      },
      "auroc": 0.7932697916666667
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 301,
          "fn": 99,
          "accuracy": 0.7525
        },
        "0.01": {
          "tp": 236,
          "fn": 164,
          "accuracy": 0.59
        }
      },
      "auroc": 0.8675984375000001
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 301,
          "fn": 99,
          "accuracy": 0.7525
        },
        "0.01": {
          "tp": 236,
          "fn": 164,
          "accuracy": 0.59
        }
      },
      "auroc": 0.8675984375000001
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9101739583333335
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        },
        "0.01": {
          "tp": 164,
          "fn": 36,
          "accuracy": 0.82
        }
      },
      "auroc": 0.9101739583333335
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.8642687499999999
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 162,
          "fn": 38,
          "accuracy": 0.81
        },
        "0.01": {
          "tp": 142,
          "fn": 58,
          "accuracy": 0.71
        }
      },
      "auroc": 0.8642687499999999
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        }
      },
      "auroc": 0.8872213541666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 342,
          "fn": 58,
          "accuracy": 0.855
        },
        "0.01": {
          "tp": 306,
          "fn": 94,
          "accuracy": 0.765
        }
      },
      "auroc": 0.8872213541666667
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2136,
          "fn": 64,
          "accuracy": 0.9709090909090909
        },
        "0.01": {
          "tp": 2044,
          "fn": 156,
          "accuracy": 0.9290909090909091
        }
      },
      "auroc": 0.9347217803030303
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 772,
          "fn": 428,
          "accuracy": 0.6433333333333333
        },
        "0.01": {
          "tp": 701,
          "fn": 499,
          "accuracy": 0.5841666666666666
        }
      },
      "auroc": 0.6847803819444445
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2908,
          "fn": 492,
          "accuracy": 0.8552941176470589
        },
        "0.01": {
          "tp": 2745,
          "fn": 655,
          "accuracy": 0.8073529411764706
        }
      },
      "auroc": 0.8465071691176471
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1672,
          "fn": 528,
          "accuracy": 0.76
        },
        "0.01": {
          "tp": 1376,
          "fn": 824,
          "accuracy": 0.6254545454545455
        }
      },
      "auroc": 0.8598584280303029
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 212,
          "fn": 988,
          "accuracy": 0.17666666666666667
        },
        "0.01": {
          "tp": 168,
          "fn": 1032,
          "accuracy": 0.14
        }
      },
      "auroc": 0.3125574652777778
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1884,
          "fn": 1516,
          "accuracy": 0.5541176470588235
        },
        "0.01": {
          "tp": 1544,
          "fn": 1856,
          "accuracy": 0.4541176470588235
        }
      },
      "auroc": 0.6666933823529411
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3808,
          "fn": 592,
          "accuracy": 0.8654545454545455
        },
        "0.01": {
          "tp": 3420,
          "fn": 980,
          "accuracy": 0.7772727272727272
        }
      },
      "auroc": 0.8972901041666667
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 984,
          "fn": 1416,
          "accuracy": 0.41
        },
        "0.01": {
          "tp": 869,
          "fn": 1531,
          "accuracy": 0.3620833333333333
        }
      },
      "auroc": 0.4986689236111111
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 4792,
          "fn": 2008,
          "accuracy": 0.7047058823529412
        },
        "0.01": {
          "tp": 4289,
          "fn": 2511,
          "accuracy": 0.6307352941176471
        }
      },
      "auroc": 0.7566002757352941
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.946984375
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9473463541666667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.8553614583333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 363,
          "fn": 37,
          "accuracy": 0.9075
        }
      },
      "auroc": 0.9015348958333332
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 27,
          "accuracy": 0.9325
        },
        "0.01": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        }
      },
      "auroc": 0.9011729166666665
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 773,
          "fn": 27,
          "accuracy": 0.96625
        },
        "0.01": {
          "tp": 762,
          "fn": 38,
          "accuracy": 0.9525
        }
      },
      "auroc": 0.924440625
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9440833333333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.08256354166666668
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 202,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 194,
          "fn": 206,
          "accuracy": 0.485
        }
      },
      "auroc": 0.5133234375
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 133,
          "fn": 67,
          "accuracy": 0.665
        },
        "0.01": {
          "tp": 80,
          "fn": 120,
          "accuracy": 0.4
        }
      },
      "auroc": 0.8251427083333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.07971562500000001
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 266,
          "accuracy": 0.335
        },
        "0.01": {
          "tp": 81,
          "fn": 319,
          "accuracy": 0.2025
        }
      },
      "auroc": 0.45242916666666666
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": {
          "tp": 274,
          "fn": 126,
          "accuracy": 0.685
        }
      },
      "auroc": 0.8846130208333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        },
        "0.01": {
          "tp": 1,
          "fn": 399,
          "accuracy": 0.0025
        }
      },
      "auroc": 0.08113958333333336
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 332,
          "fn": 468,
          "accuracy": 0.415
        },
        "0.01": {
          "tp": 275,
          "fn": 525,
          "accuracy": 0.34375
        }
      },
      "auroc": 0.4828763020833334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.946559375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 55,
          "fn": 145,
          "accuracy": 0.275
        },
        "0.01": {
          "tp": 37,
          "fn": 163,
          "accuracy": 0.185
        }
      },
      "auroc": 0.4182208333333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 255,
          "fn": 145,
          "accuracy": 0.6375
        },
        "0.01": {
          "tp": 236,
          "fn": 164,
          "accuracy": 0.59
        }
      },
      "auroc": 0.6823901041666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 195,
          "fn": 5,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9450416666666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 1,
          "fn": 199,
          "accuracy": 0.005
        }
      },
      "auroc": 0.12093333333333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 201,
          "fn": 199,
          "accuracy": 0.5025
        },
        "0.01": {
          "tp": 196,
          "fn": 204,
          "accuracy": 0.49
        }
      },
      "auroc": 0.5329875
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9458005208333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 58,
          "fn": 342,
          "accuracy": 0.145
        },
        "0.01": {
          "tp": 38,
          "fn": 362,
          "accuracy": 0.095
        }
      },
      "auroc": 0.26957708333333336
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 456,
          "fn": 344,
          "accuracy": 0.57
        },
        "0.01": {
          "tp": 432,
          "fn": 368,
          "accuracy": 0.54
        }
      },
      "auroc": 0.6076888020833333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9464854166666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9470708333333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.946778125
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 184,
          "fn": 16,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 177,
          "fn": 23,
          "accuracy": 0.885
        }
      },
      "auroc": 0.9212020833333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 197,
          "accuracy": 0.015
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.3643291666666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 187,
          "fn": 213,
          "accuracy": 0.4675
        },
        "0.01": {
          "tp": 177,
          "fn": 223,
          "accuracy": 0.4425
        }
      },
      "auroc": 0.642765625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 383,
          "fn": 17,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 376,
          "fn": 24,
          "accuracy": 0.94
        }
      },
      "auroc": 0.93384375
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 203,
          "fn": 197,
          "accuracy": 0.5075
        },
        "0.01": {
          "tp": 199,
          "fn": 201,
          "accuracy": 0.4975
        }
      },
      "auroc": 0.6557
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 586,
          "fn": 214,
          "accuracy": 0.7325
        },
        "0.01": {
          "tp": 575,
          "fn": 225,
          "accuracy": 0.71875
        }
      },
      "auroc": 0.7947718749999999
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9428802083333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 51,
          "accuracy": 0.745
        },
        "0.01": {
          "tp": 110,
          "fn": 90,
          "accuracy": 0.55
        }
      },
      "auroc": 0.8192833333333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 347,
          "fn": 53,
          "accuracy": 0.8675
        },
        "0.01": {
          "tp": 308,
          "fn": 92,
          "accuracy": 0.77
        }
      },
      "auroc": 0.8810817708333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 101,
          "accuracy": 0.495
        },
        "0.01": {
          "tp": 49,
          "fn": 151,
          "accuracy": 0.245
        }
      },
      "auroc": 0.7858166666666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        },
        "0.01": {
          "tp": 0,
          "fn": 200,
          "accuracy": 0.0
        }
      },
      "auroc": 0.07411875000000002
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 99,
          "fn": 301,
          "accuracy": 0.2475
        },
        "0.01": {
          "tp": 49,
          "fn": 351,
          "accuracy": 0.1225
        }
      },
      "auroc": 0.42996770833333336
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 297,
          "fn": 103,
          "accuracy": 0.7425
        },
        "0.01": {
          "tp": 247,
          "fn": 153,
          "accuracy": 0.6175
        }
      },
      "auroc": 0.8643484375000001
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 149,
          "fn": 251,
          "accuracy": 0.3725
        },
        "0.01": {
          "tp": 110,
          "fn": 290,
          "accuracy": 0.275
        }
      },
      "auroc": 0.4467010416666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 446,
          "fn": 354,
          "accuracy": 0.5575
        },
        "0.01": {
          "tp": 357,
          "fn": 443,
          "accuracy": 0.44625
        }
      },
      "auroc": 0.6555247395833333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.9411708333333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 393,
          "fn": 7,
          "accuracy": 0.9825
        }
      },
      "auroc": 0.9444395833333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        },
        "0.01": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        }
      },
      "auroc": 0.938346875
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 46,
          "fn": 154,
          "accuracy": 0.23
        },
        "0.01": {
          "tp": 22,
          "fn": 178,
          "accuracy": 0.11
        }
      },
      "auroc": 0.488025
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 240,
          "fn": 160,
          "accuracy": 0.6
        },
        "0.01": {
          "tp": 211,
          "fn": 189,
          "accuracy": 0.5275
        }
      },
      "auroc": 0.7131859375
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 389,
          "fn": 11,
          "accuracy": 0.9725
        }
      },
      "auroc": 0.9430276041666668
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 245,
          "fn": 155,
          "accuracy": 0.6125
        },
        "0.01": {
          "tp": 215,
          "fn": 185,
          "accuracy": 0.5375
        }
      },
      "auroc": 0.7145979166666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 639,
          "fn": 161,
          "accuracy": 0.79875
        },
        "0.01": {
          "tp": 604,
          "fn": 196,
          "accuracy": 0.755
        }
      },
      "auroc": 0.8288127604166667
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9367989583333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        },
        "0.01": {
          "tp": 186,
          "fn": 14,
          "accuracy": 0.93
        }
      },
      "auroc": 0.9367989583333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9133677083333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 19,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 163,
          "fn": 37,
          "accuracy": 0.815
        }
      },
      "auroc": 0.9133677083333334
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        }
      },
      "auroc": 0.9250833333333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 374,
          "fn": 26,
          "accuracy": 0.935
        },
        "0.01": {
          "tp": 349,
          "fn": 51,
          "accuracy": 0.8725
        }
      },
      "auroc": 0.9250833333333333
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9226833333333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 189,
          "fn": 11,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 169,
          "fn": 31,
          "accuracy": 0.845
        }
      },
      "auroc": 0.9226833333333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.8956510416666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        },
        "0.01": {
          "tp": 145,
          "fn": 55,
          "accuracy": 0.725
        }
      },
      "auroc": 0.8956510416666666
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9091671875
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 362,
          "fn": 38,
          "accuracy": 0.905
        },
        "0.01": {
          "tp": 314,
          "fn": 86,
          "accuracy": 0.785
        }
      },
      "auroc": 0.9091671875
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9470885416666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9470885416666666
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9473984375
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9473984375
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9432395833333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9432395833333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        }
      },
      "auroc": 0.839634375
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 134,
          "fn": 66,
          "accuracy": 0.67
        },
        "0.01": {
          "tp": 75,
          "fn": 125,
          "accuracy": 0.375
        }
      },
      "auroc": 0.839634375
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        }
      },
      "auroc": 0.8914369791666666
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 331,
          "fn": 69,
          "accuracy": 0.8275
        },
        "0.01": {
          "tp": 272,
          "fn": 128,
          "accuracy": 0.68
        }
      },
      "auroc": 0.8914369791666666
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9248322916666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 190,
          "fn": 10,
          "accuracy": 0.95
        },
        "0.01": {
          "tp": 180,
          "fn": 20,
          "accuracy": 0.9
        }
      },
      "auroc": 0.9248322916666667
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.8970802083333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 178,
          "fn": 22,
          "accuracy": 0.89
        },
        "0.01": {
          "tp": 160,
          "fn": 40,
          "accuracy": 0.8
        }
      },
      "auroc": 0.8970802083333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.91095625
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 368,
          "fn": 32,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 340,
          "fn": 60,
          "accuracy": 0.85
        }
      },
      "auroc": 0.91095625
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2164,
          "fn": 36,
          "accuracy": 0.9836363636363636
        },
        "0.01": {
          "tp": 2122,
          "fn": 78,
          "accuracy": 0.9645454545454546
        }
      },
      "auroc": 0.940971590909091
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 803,
          "fn": 397,
          "accuracy": 0.6691666666666667
        },
        "0.01": {
          "tp": 738,
          "fn": 462,
          "accuracy": 0.615
        }
      },
      "auroc": 0.6925489583333332
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2967,
          "fn": 433,
          "accuracy": 0.8726470588235294
        },
        "0.01": {
          "tp": 2860,
          "fn": 540,
          "accuracy": 0.8411764705882353
        }
      },
      "auroc": 0.8532930147058824
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1874,
          "fn": 326,
          "accuracy": 0.8518181818181818
        },
        "0.01": {
          "tp": 1632,
          "fn": 568,
          "accuracy": 0.7418181818181818
        }
      },
      "auroc": 0.8960072916666667
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 226,
          "fn": 974,
          "accuracy": 0.18833333333333332
        },
        "0.01": {
          "tp": 187,
          "fn": 1013,
          "accuracy": 0.15583333333333332
        }
      },
      "auroc": 0.3304138888888889
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2100,
          "fn": 1300,
          "accuracy": 0.6176470588235294
        },
        "0.01": {
          "tp": 1819,
          "fn": 1581,
          "accuracy": 0.535
        }
      },
      "auroc": 0.6963860906862747
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 4038,
          "fn": 362,
          "accuracy": 0.9177272727272727
        },
        "0.01": {
          "tp": 3754,
          "fn": 646,
          "accuracy": 0.8531818181818182
        }
      },
      "auroc": 0.9184894412878787
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1029,
          "fn": 1371,
          "accuracy": 0.42875
        },
        "0.01": {
          "tp": 925,
          "fn": 1475,
          "accuracy": 0.3854166666666667
        }
      },
      "auroc": 0.5114814236111112
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 5067,
          "fn": 1733,
          "accuracy": 0.7451470588235294
        },
        "0.01": {
          "tp": 4679,
          "fn": 2121,
          "accuracy": 0.6880882352941177
        }
      },
      "auroc": 0.7748395526960784
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9471677083333332
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9474380208333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9474380208333333
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 800,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        }
      },
      "auroc": 0.9475731770833333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9472729166666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 191,
          "fn": 9,
          "accuracy": 0.955
        }
      },
      "auroc": 0.94071875
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9439958333333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.946390625
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 185,
          "fn": 15,
          "accuracy": 0.925
        },
        "0.01": {
          "tp": 173,
          "fn": 27,
          "accuracy": 0.865
        }
      },
      "auroc": 0.9171604166666667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 384,
          "fn": 16,
          "accuracy": 0.96
        },
        "0.01": {
          "tp": 371,
          "fn": 29,
          "accuracy": 0.9275
        }
      },
      "auroc": 0.9317755208333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9468317708333334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 382,
          "fn": 18,
          "accuracy": 0.955
        },
        "0.01": {
          "tp": 364,
          "fn": 36,
          "accuracy": 0.91
        }
      },
      "auroc": 0.9289395833333333
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 781,
          "fn": 19,
          "accuracy": 0.97625
        },
        "0.01": {
          "tp": 761,
          "fn": 39,
          "accuracy": 0.95125
        }
      },
      "auroc": 0.9378856770833334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9472729166666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.945665625
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9464692708333333
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.94303125
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 394,
          "fn": 6,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9453697916666667
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9474906249999999
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9443484375000001
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 797,
          "fn": 3,
          "accuracy": 0.99625
        },
        "0.01": {
          "tp": 790,
          "fn": 10,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.94591953125
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9472729166666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9469135416666666
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9470932291666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        },
        "0.01": {
          "tp": 193,
          "fn": 7,
          "accuracy": 0.965
        }
      },
      "auroc": 0.940465625
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9454729166666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 390,
          "fn": 10,
          "accuracy": 0.975
        }
      },
      "auroc": 0.9429692708333334
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        },
        "0.01": {
          "tp": 392,
          "fn": 8,
          "accuracy": 0.98
        }
      },
      "auroc": 0.9438692708333333
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9461932291666667
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 788,
          "fn": 12,
          "accuracy": 0.985
        }
      },
      "auroc": 0.94503125
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9472729166666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9455958333333333
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 396,
          "fn": 4,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9464343749999999
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.9474906249999999
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9466520833333334
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 799,
          "fn": 1,
          "accuracy": 0.99875
        },
        "0.01": {
          "tp": 796,
          "fn": 4,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9470713541666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9465197916666667
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9451302083333333
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9458249999999999
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9471140625
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 397,
          "fn": 3,
          "accuracy": 0.9925
        }
      },
      "auroc": 0.9464192708333334
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 798,
          "fn": 2,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 795,
          "fn": 5,
          "accuracy": 0.99375
        }
      },
      "auroc": 0.9467666666666668
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9457927083333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9457927083333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9404583333333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 194,
          "fn": 6,
          "accuracy": 0.97
        }
      },
      "auroc": 0.9404583333333333
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9431255208333332
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 391,
          "fn": 9,
          "accuracy": 0.9775
        }
      },
      "auroc": 0.9431255208333332
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9468791666666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 199,
          "fn": 1,
          "accuracy": 0.995
        }
      },
      "auroc": 0.9468791666666667
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.94729375
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        },
        "0.01": {
          "tp": 399,
          "fn": 1,
          "accuracy": 0.9975
        }
      },
      "auroc": 0.94729375
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 400,
          "fn": 0,
          "accuracy": 1.0
        }
      },
      "auroc": 0.9477083333333334
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9469239583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9469239583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9450239583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 198,
          "fn": 2,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 197,
          "fn": 3,
          "accuracy": 0.985
        }
      },
      "auroc": 0.9450239583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9459739583333333
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 398,
          "fn": 2,
          "accuracy": 0.995
        },
        "0.01": {
          "tp": 395,
          "fn": 5,
          "accuracy": 0.9875
        }
      },
      "auroc": 0.9459739583333333
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": {
          "tp": 2192,
          "fn": 8,
          "accuracy": 0.9963636363636363
        }
      },
      "auroc": 0.9473441287878788
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1195,
          "fn": 5,
          "accuracy": 0.9958333333333333
        },
        "0.01": {
          "tp": 1187,
          "fn": 13,
          "accuracy": 0.9891666666666666
        }
      },
      "auroc": 0.9460704861111111
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3395,
          "fn": 5,
          "accuracy": 0.9985294117647059
        },
        "0.01": {
          "tp": 3379,
          "fn": 21,
          "accuracy": 0.9938235294117647
        }
      },
      "auroc": 0.9468946078431373
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2191,
          "fn": 9,
          "accuracy": 0.9959090909090909
        },
        "0.01": {
          "tp": 2178,
          "fn": 22,
          "accuracy": 0.99
        }
      },
      "auroc": 0.9458039772727272
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1181,
          "fn": 19,
          "accuracy": 0.9841666666666666
        },
        "0.01": {
          "tp": 1157,
          "fn": 43,
          "accuracy": 0.9641666666666666
        }
      },
      "auroc": 0.9405930555555556
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3372,
          "fn": 28,
          "accuracy": 0.991764705882353
        },
        "0.01": {
          "tp": 3335,
          "fn": 65,
          "accuracy": 0.9808823529411764
        }
      },
      "auroc": 0.9439648284313725
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 4391,
          "fn": 9,
          "accuracy": 0.9979545454545454
        },
        "0.01": {
          "tp": 4370,
          "fn": 30,
          "accuracy": 0.9931818181818182
        }
      },
      "auroc": 0.9465740530303031
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 2376,
          "fn": 24,
          "accuracy": 0.99
        },
        "0.01": {
          "tp": 2344,
          "fn": 56,
          "accuracy": 0.9766666666666667
        }
      },
      "auroc": 0.9433317708333334
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 6767,
          "fn": 33,
          "accuracy": 0.9951470588235294
        },
        "0.01": {
          "tp": 6714,
          "fn": 86,
          "accuracy": 0.9873529411764705
        }
      },
      "auroc": 0.945429718137255
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2367,
          "fn": 33,
          "accuracy": 0.98625
        },
        "0.01": {
          "tp": 2299,
          "fn": 101,
          "accuracy": 0.9579166666666666
        }
      },
      "auroc": 0.9415170138888889
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2229,
          "fn": 171,
          "accuracy": 0.92875
        },
        "0.01": {
          "tp": 2079,
          "fn": 321,
          "accuracy": 0.86625
        }
      },
      "auroc": 0.9217153645833334
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4596,
          "fn": 204,
          "accuracy": 0.9575
        },
        "0.01": {
          "tp": 4378,
          "fn": 422,
          "accuracy": 0.9120833333333334
        }
      },
      "auroc": 0.9316161892361111
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2351,
          "fn": 49,
          "accuracy": 0.9795833333333334
        },
        "0.01": {
          "tp": 2259,
          "fn": 141,
          "accuracy": 0.94125
        }
      },
      "auroc": 0.9388111979166667
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1885,
          "fn": 515,
          "accuracy": 0.7854166666666667
        },
        "0.01": {
          "tp": 1657,
          "fn": 743,
          "accuracy": 0.6904166666666667
        }
      },
      "auroc": 0.8298227430555557
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4236,
          "fn": 564,
          "accuracy": 0.8825
        },
        "0.01": {
          "tp": 3916,
          "fn": 884,
          "accuracy": 0.8158333333333333
        }
      },
      "auroc": 0.884316970486111
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4718,
          "fn": 82,
          "accuracy": 0.9829166666666667
        },
        "0.01": {
          "tp": 4558,
          "fn": 242,
          "accuracy": 0.9495833333333333
        }
      },
      "auroc": 0.9401641059027778
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4114,
          "fn": 686,
          "accuracy": 0.8570833333333333
        },
        "0.01": {
          "tp": 3736,
          "fn": 1064,
          "accuracy": 0.7783333333333333
        }
      },
      "auroc": 0.8757690538194446
    },
    {
      "domain": "wiki",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 8832,
          "fn": 768,
          "accuracy": 0.92
        },
        "0.01": {
          "tp": 8294,
          "fn": 1306,
          "accuracy": 0.8639583333333334
        }
      },
      "auroc": 0.9079665798611113
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2344,
          "fn": 56,
          "accuracy": 0.9766666666666667
        },
        "0.01": {
          "tp": 2277,
          "fn": 123,
          "accuracy": 0.94875
        }
      },
      "auroc": 0.9369013020833334
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 277,
          "fn": 2123,
          "accuracy": 0.11541666666666667
        },
        "0.01": {
          "tp": 227,
          "fn": 2173,
          "accuracy": 0.09458333333333334
        }
      },
      "auroc": 0.22310069444444447
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2621,
          "fn": 2179,
          "accuracy": 0.5460416666666666
        },
        "0.01": {
          "tp": 2504,
          "fn": 2296,
          "accuracy": 0.5216666666666666
        }
      },
      "auroc": 0.5800009982638888
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1497,
          "fn": 903,
          "accuracy": 0.62375
        },
        "0.01": {
          "tp": 987,
          "fn": 1413,
          "accuracy": 0.41125
        }
      },
      "auroc": 0.7952658854166667
    },
    {
      "domain": "wiki",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3841,
          "fn": 959,
          "accuracy": 0.8002083333333333
        },
        "0.01": {
          "tp": 3264,
          "fn": 1536,
          "accuracy": 0.68
        }
      },
      "auroc": 0.86608359375
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2272,
          "fn": 128,
          "accuracy": 0.9466666666666667
        },
        "0.01": {
          "tp": 2129,
          "fn": 271,
          "accuracy": 0.8870833333333333
        }
      },
      "auroc": 0.928265798611111
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 711,
          "fn": 1689,
          "accuracy": 0.29625
        },
        "0.01": {
          "tp": 532,
          "fn": 1868,
          "accuracy": 0.22166666666666668
        }
      },
      "auroc": 0.46885616319444434
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2983,
          "fn": 1817,
          "accuracy": 0.6214583333333333
        },
        "0.01": {
          "tp": 2661,
          "fn": 2139,
          "accuracy": 0.554375
        }
      },
      "auroc": 0.6985609809027777
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2224,
          "fn": 176,
          "accuracy": 0.9266666666666666
        },
        "0.01": {
          "tp": 2033,
          "fn": 367,
          "accuracy": 0.8470833333333333
        }
      },
      "auroc": 0.9205585069444444
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 304,
          "fn": 2096,
          "accuracy": 0.12666666666666668
        },
        "0.01": {
          "tp": 242,
          "fn": 2158,
          "accuracy": 0.10083333333333333
        }
      },
      "auroc": 0.24672621527777777
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2528,
          "fn": 2272,
          "accuracy": 0.5266666666666666
        },
        "0.01": {
          "tp": 2275,
          "fn": 2525,
          "accuracy": 0.4739583333333333
        }
      },
      "auroc": 0.5836423611111111
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4496,
          "fn": 304,
          "accuracy": 0.9366666666666666
        },
        "0.01": {
          "tp": 4162,
          "fn": 638,
          "accuracy": 0.8670833333333333
        }
      },
      "auroc": 0.9244121527777778
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1015,
          "fn": 3785,
          "accuracy": 0.21145833333333333
        },
        "0.01": {
          "tp": 774,
          "fn": 4026,
          "accuracy": 0.16125
        }
      },
      "auroc": 0.3577911892361112
    },
    {
      "domain": "wiki",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5511,
          "fn": 4089,
          "accuracy": 0.5740625
        },
        "0.01": {
          "tp": 4936,
          "fn": 4664,
          "accuracy": 0.5141666666666667
        }
      },
      "auroc": 0.6411016710069444
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2380,
          "fn": 20,
          "accuracy": 0.9916666666666667
        },
        "0.01": {
          "tp": 2345,
          "fn": 55,
          "accuracy": 0.9770833333333333
        }
      },
      "auroc": 0.9440211805555555
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2105,
          "fn": 295,
          "accuracy": 0.8770833333333333
        },
        "0.01": {
          "tp": 1929,
          "fn": 471,
          "accuracy": 0.80375
        }
      },
      "auroc": 0.9022164930555556
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 390,
          "fn": 2010,
          "accuracy": 0.1625
        },
        "0.01": {
          "tp": 269,
          "fn": 2131,
          "accuracy": 0.11208333333333333
        }
      },
      "auroc": 0.45491484375
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2495,
          "fn": 2305,
          "accuracy": 0.5197916666666667
        },
        "0.01": {
          "tp": 2198,
          "fn": 2602,
          "accuracy": 0.4579166666666667
        }
      },
      "auroc": 0.6785656684027779
    },
    {
      "domain": "wiki",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4485,
          "fn": 315,
          "accuracy": 0.934375
        },
        "0.01": {
          "tp": 4274,
          "fn": 526,
          "accuracy": 0.8904166666666666
        }
      },
      "auroc": 0.9231188368055555
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2344,
          "fn": 56,
          "accuracy": 0.9766666666666667
        },
        "0.01": {
          "tp": 2294,
          "fn": 106,
          "accuracy": 0.9558333333333333
        }
      },
      "auroc": 0.9364995659722223
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1524,
          "fn": 876,
          "accuracy": 0.635
        },
        "0.01": {
          "tp": 1121,
          "fn": 1279,
          "accuracy": 0.46708333333333335
        }
      },
      "auroc": 0.7727931423611112
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3868,
          "fn": 932,
          "accuracy": 0.8058333333333333
        },
        "0.01": {
          "tp": 3415,
          "fn": 1385,
          "accuracy": 0.7114583333333333
        }
      },
      "auroc": 0.8546463541666666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1211,
          "fn": 1189,
          "accuracy": 0.5045833333333334
        },
        "0.01": {
          "tp": 708,
          "fn": 1692,
          "accuracy": 0.295
        }
      },
      "auroc": 0.7596213541666667
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 241,
          "fn": 2159,
          "accuracy": 0.10041666666666667
        },
        "0.01": {
          "tp": 206,
          "fn": 2194,
          "accuracy": 0.08583333333333333
        }
      },
      "auroc": 0.21848472222222226
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1452,
          "fn": 3348,
          "accuracy": 0.3025
        },
        "0.01": {
          "tp": 914,
          "fn": 3886,
          "accuracy": 0.19041666666666668
        }
      },
      "auroc": 0.48905303819444446
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3555,
          "fn": 1245,
          "accuracy": 0.740625
        },
        "0.01": {
          "tp": 3002,
          "fn": 1798,
          "accuracy": 0.6254166666666666
        }
      },
      "auroc": 0.8480604600694445
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1765,
          "fn": 3035,
          "accuracy": 0.36770833333333336
        },
        "0.01": {
          "tp": 1327,
          "fn": 3473,
          "accuracy": 0.2764583333333333
        }
      },
      "auroc": 0.4956389322916666
    },
    {
      "domain": "wiki",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 5320,
          "fn": 4280,
          "accuracy": 0.5541666666666667
        },
        "0.01": {
          "tp": 4329,
          "fn": 5271,
          "accuracy": 0.4509375
        }
      },
      "auroc": 0.6718496961805556
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2347,
          "fn": 53,
          "accuracy": 0.9779166666666667
        },
        "0.01": {
          "tp": 2274,
          "fn": 126,
          "accuracy": 0.9475
        }
      },
      "auroc": 0.9389547743055555
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2121,
          "fn": 279,
          "accuracy": 0.88375
        },
        "0.01": {
          "tp": 1938,
          "fn": 462,
          "accuracy": 0.8075
        }
      },
      "auroc": 0.9048006076388888
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4468,
          "fn": 332,
          "accuracy": 0.9308333333333333
        },
        "0.01": {
          "tp": 4212,
          "fn": 588,
          "accuracy": 0.8775
        }
      },
      "auroc": 0.9218776909722223
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2184,
          "fn": 216,
          "accuracy": 0.91
        },
        "0.01": {
          "tp": 1990,
          "fn": 410,
          "accuracy": 0.8291666666666667
        }
      },
      "auroc": 0.9126060763888889
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 676,
          "fn": 1724,
          "accuracy": 0.2816666666666667
        },
        "0.01": {
          "tp": 435,
          "fn": 1965,
          "accuracy": 0.18125
        }
      },
      "auroc": 0.5321787326388888
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2860,
          "fn": 1940,
          "accuracy": 0.5958333333333333
        },
        "0.01": {
          "tp": 2425,
          "fn": 2375,
          "accuracy": 0.5052083333333334
        }
      },
      "auroc": 0.722392404513889
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4531,
          "fn": 269,
          "accuracy": 0.9439583333333333
        },
        "0.01": {
          "tp": 4264,
          "fn": 536,
          "accuracy": 0.8883333333333333
        }
      },
      "auroc": 0.9257804253472222
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2797,
          "fn": 2003,
          "accuracy": 0.5827083333333334
        },
        "0.01": {
          "tp": 2373,
          "fn": 2427,
          "accuracy": 0.494375
        }
      },
      "auroc": 0.7184896701388889
    },
    {
      "domain": "wiki",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 7328,
          "fn": 2272,
          "accuracy": 0.7633333333333333
        },
        "0.01": {
          "tp": 6637,
          "fn": 2963,
          "accuracy": 0.6913541666666667
        }
      },
      "auroc": 0.8221350477430556
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2202,
          "fn": 198,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 1971,
          "fn": 429,
          "accuracy": 0.82125
        }
      },
      "auroc": 0.9137252604166668
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2202,
          "fn": 198,
          "accuracy": 0.9175
        },
        "0.01": {
          "tp": 1971,
          "fn": 429,
          "accuracy": 0.82125
        }
      },
      "auroc": 0.9137252604166668
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1968,
          "fn": 432,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 1632,
          "fn": 768,
          "accuracy": 0.68
        }
      },
      "auroc": 0.8786636284722222
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1968,
          "fn": 432,
          "accuracy": 0.82
        },
        "0.01": {
          "tp": 1632,
          "fn": 768,
          "accuracy": 0.68
        }
      },
      "auroc": 0.8786636284722222
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4170,
          "fn": 630,
          "accuracy": 0.86875
        },
        "0.01": {
          "tp": 3603,
          "fn": 1197,
          "accuracy": 0.750625
        }
      },
      "auroc": 0.8961944444444445
    },
    {
      "domain": "wiki",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4170,
          "fn": 630,
          "accuracy": 0.86875
        },
        "0.01": {
          "tp": 3603,
          "fn": 1197,
          "accuracy": 0.750625
        }
      },
      "auroc": 0.8961944444444445
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2099,
          "fn": 301,
          "accuracy": 0.8745833333333334
        },
        "0.01": {
          "tp": 1811,
          "fn": 589,
          "accuracy": 0.7545833333333334
        }
      },
      "auroc": 0.8946473090277779
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2099,
          "fn": 301,
          "accuracy": 0.8745833333333334
        },
        "0.01": {
          "tp": 1811,
          "fn": 589,
          "accuracy": 0.7545833333333334
        }
      },
      "auroc": 0.8946473090277779
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1893,
          "fn": 507,
          "accuracy": 0.78875
        },
        "0.01": {
          "tp": 1528,
          "fn": 872,
          "accuracy": 0.6366666666666667
        }
      },
      "auroc": 0.8654213541666665
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1893,
          "fn": 507,
          "accuracy": 0.78875
        },
        "0.01": {
          "tp": 1528,
          "fn": 872,
          "accuracy": 0.6366666666666667
        }
      },
      "auroc": 0.8654213541666665
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3992,
          "fn": 808,
          "accuracy": 0.8316666666666667
        },
        "0.01": {
          "tp": 3339,
          "fn": 1461,
          "accuracy": 0.695625
        }
      },
      "auroc": 0.8800343315972221
    },
    {
      "domain": "wiki",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3992,
          "fn": 808,
          "accuracy": 0.8316666666666667
        },
        "0.01": {
          "tp": 3339,
          "fn": 1461,
          "accuracy": 0.695625
        }
      },
      "auroc": 0.8800343315972221
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2308,
          "fn": 92,
          "accuracy": 0.9616666666666667
        },
        "0.01": {
          "tp": 2198,
          "fn": 202,
          "accuracy": 0.9158333333333334
        }
      },
      "auroc": 0.9336910590277777
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2308,
          "fn": 92,
          "accuracy": 0.9616666666666667
        },
        "0.01": {
          "tp": 2198,
          "fn": 202,
          "accuracy": 0.9158333333333334
        }
      },
      "auroc": 0.9336910590277777
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2228,
          "fn": 172,
          "accuracy": 0.9283333333333333
        },
        "0.01": {
          "tp": 2074,
          "fn": 326,
          "accuracy": 0.8641666666666666
        }
      },
      "auroc": 0.9222596354166667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2228,
          "fn": 172,
          "accuracy": 0.9283333333333333
        },
        "0.01": {
          "tp": 2074,
          "fn": 326,
          "accuracy": 0.8641666666666666
        }
      },
      "auroc": 0.9222596354166667
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4536,
          "fn": 264,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 4272,
          "fn": 528,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9279753472222222
    },
    {
      "domain": "wiki",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4536,
          "fn": 264,
          "accuracy": 0.945
        },
        "0.01": {
          "tp": 4272,
          "fn": 528,
          "accuracy": 0.89
        }
      },
      "auroc": 0.9279753472222222
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2284,
          "fn": 116,
          "accuracy": 0.9516666666666667
        },
        "0.01": {
          "tp": 2174,
          "fn": 226,
          "accuracy": 0.9058333333333334
        }
      },
      "auroc": 0.9283763888888888
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2284,
          "fn": 116,
          "accuracy": 0.9516666666666667
        },
        "0.01": {
          "tp": 2174,
          "fn": 226,
          "accuracy": 0.9058333333333334
        }
      },
      "auroc": 0.9283763888888888
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1557,
          "fn": 843,
          "accuracy": 0.64875
        },
        "0.01": {
          "tp": 1014,
          "fn": 1386,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.8169265625
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1557,
          "fn": 843,
          "accuracy": 0.64875
        },
        "0.01": {
          "tp": 1014,
          "fn": 1386,
          "accuracy": 0.4225
        }
      },
      "auroc": 0.8169265625
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3841,
          "fn": 959,
          "accuracy": 0.8002083333333333
        },
        "0.01": {
          "tp": 3188,
          "fn": 1612,
          "accuracy": 0.6641666666666667
        }
      },
      "auroc": 0.8726514756944445
    },
    {
      "domain": "wiki",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 3841,
          "fn": 959,
          "accuracy": 0.8002083333333333
        },
        "0.01": {
          "tp": 3188,
          "fn": 1612,
          "accuracy": 0.6641666666666667
        }
      },
      "auroc": 0.8726514756944445
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2159,
          "fn": 241,
          "accuracy": 0.8995833333333333
        },
        "0.01": {
          "tp": 1949,
          "fn": 451,
          "accuracy": 0.8120833333333334
        }
      },
      "auroc": 0.9068092881944444
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2159,
          "fn": 241,
          "accuracy": 0.8995833333333333
        },
        "0.01": {
          "tp": 1949,
          "fn": 451,
          "accuracy": 0.8120833333333334
        }
      },
      "auroc": 0.9068092881944444
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1989,
          "fn": 411,
          "accuracy": 0.82875
        },
        "0.01": {
          "tp": 1727,
          "fn": 673,
          "accuracy": 0.7195833333333334
        }
      },
      "auroc": 0.8738750000000002
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1989,
          "fn": 411,
          "accuracy": 0.82875
        },
        "0.01": {
          "tp": 1727,
          "fn": 673,
          "accuracy": 0.7195833333333334
        }
      },
      "auroc": 0.8738750000000002
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4148,
          "fn": 652,
          "accuracy": 0.8641666666666666
        },
        "0.01": {
          "tp": 3676,
          "fn": 1124,
          "accuracy": 0.7658333333333334
        }
      },
      "auroc": 0.8903421440972223
    },
    {
      "domain": "wiki",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 4148,
          "fn": 652,
          "accuracy": 0.8641666666666666
        },
        "0.01": {
          "tp": 3676,
          "fn": 1124,
          "accuracy": 0.7658333333333334
        }
      },
      "auroc": 0.8903421440972223
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 25106,
          "fn": 1294,
          "accuracy": 0.9509848484848484
        },
        "0.01": {
          "tp": 23721,
          "fn": 2679,
          "accuracy": 0.8985227272727273
        }
      },
      "auroc": 0.9275826309974747
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 21207,
          "fn": 5193,
          "accuracy": 0.8032954545454546
        },
        "0.01": {
          "tp": 17881,
          "fn": 8519,
          "accuracy": 0.6773106060606061
        }
      },
      "auroc": 0.8714750631313133
    },
    {
      "domain": "wiki",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 46313,
          "fn": 6487,
          "accuracy": 0.8771401515151516
        },
        "0.01": {
          "tp": 41602,
          "fn": 11198,
          "accuracy": 0.7879166666666667
        }
      },
      "auroc": 0.8995288470643938
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1596,
          "fn": 4,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9438722981770833
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1564,
          "fn": 36,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9374613606770833
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3160,
          "fn": 40,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9406668294270834
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9435734537760416
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1288,
          "fn": 312,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.8448600748697916
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2885,
          "fn": 315,
          "accuracy": 0.9015625
        },
        "0.01": null
      },
      "auroc": 0.8942167643229166
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3193,
          "fn": 7,
          "accuracy": 0.9978125
        },
        "0.01": null
      },
      "auroc": 0.9437228759765625
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2852,
          "fn": 348,
          "accuracy": 0.89125
        },
        "0.01": null
      },
      "auroc": 0.8911607177734374
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 6045,
          "fn": 355,
          "accuracy": 0.94453125
        },
        "0.01": null
      },
      "auroc": 0.9174417968750002
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1580,
          "fn": 20,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9406542643229167
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 1585,
          "accuracy": 0.009375
        },
        "0.01": null
      },
      "auroc": 0.11110696614583335
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 1605,
          "accuracy": 0.4984375
        },
        "0.01": null
      },
      "auroc": 0.525880615234375
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1378,
          "fn": 222,
          "accuracy": 0.86125
        },
        "0.01": null
      },
      "auroc": 0.8803991861979167
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 1598,
          "accuracy": 0.00125
        },
        "0.01": null
      },
      "auroc": 0.055766438802083346
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1380,
          "fn": 1820,
          "accuracy": 0.43125
        },
        "0.01": null
      },
      "auroc": 0.46808281249999995
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2958,
          "fn": 242,
          "accuracy": 0.924375
        },
        "0.01": null
      },
      "auroc": 0.9105267252604167
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 3183,
          "accuracy": 0.0053125
        },
        "0.01": null
      },
      "auroc": 0.08343670247395835
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2975,
          "fn": 3425,
          "accuracy": 0.46484375
        },
        "0.01": null
      },
      "auroc": 0.4969817138671874
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1564,
          "fn": 36,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9289801106770833
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1497,
          "fn": 103,
          "accuracy": 0.935625
        },
        "0.01": null
      },
      "auroc": 0.9178222005208334
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3061,
          "fn": 139,
          "accuracy": 0.9565625
        },
        "0.01": null
      },
      "auroc": 0.9234011555989583
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1559,
          "fn": 41,
          "accuracy": 0.974375
        },
        "0.01": null
      },
      "auroc": 0.9380350097656251
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1581,
          "fn": 19,
          "accuracy": 0.988125
        },
        "0.01": null
      },
      "auroc": 0.9396938151041667
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3140,
          "fn": 60,
          "accuracy": 0.98125
        },
        "0.01": null
      },
      "auroc": 0.9388644124348958
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1466,
          "fn": 134,
          "accuracy": 0.91625
        },
        "0.01": null
      },
      "auroc": 0.9093818196614584
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 37,
          "fn": 1563,
          "accuracy": 0.023125
        },
        "0.01": null
      },
      "auroc": 0.37110634765624995
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1503,
          "fn": 1697,
          "accuracy": 0.4696875
        },
        "0.01": null
      },
      "auroc": 0.6402440836588541
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3025,
          "fn": 175,
          "accuracy": 0.9453125
        },
        "0.01": null
      },
      "auroc": 0.9237084147135417
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1618,
          "fn": 1582,
          "accuracy": 0.505625
        },
        "0.01": null
      },
      "auroc": 0.6554000813802083
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 4643,
          "fn": 1757,
          "accuracy": 0.72546875
        },
        "0.01": null
      },
      "auroc": 0.789554248046875
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1570,
          "fn": 30,
          "accuracy": 0.98125
        },
        "0.01": null
      },
      "auroc": 0.9396448079427082
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1104,
          "fn": 496,
          "accuracy": 0.69
        },
        "0.01": null
      },
      "auroc": 0.7912864746093751
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2674,
          "fn": 526,
          "accuracy": 0.835625
        },
        "0.01": null
      },
      "auroc": 0.8654656412760416
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1140,
          "fn": 460,
          "accuracy": 0.7125
        },
        "0.01": null
      },
      "auroc": 0.8385023274739583
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 1600,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.06426993815104168
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1140,
          "fn": 2060,
          "accuracy": 0.35625
        },
        "0.01": null
      },
      "auroc": 0.45138613281249995
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2710,
          "fn": 490,
          "accuracy": 0.846875
        },
        "0.01": null
      },
      "auroc": 0.8890735677083335
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1104,
          "fn": 2096,
          "accuracy": 0.345
        },
        "0.01": null
      },
      "auroc": 0.42777820638020836
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3814,
          "fn": 2586,
          "accuracy": 0.5959375
        },
        "0.01": null
      },
      "auroc": 0.6584258870442709
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1589,
          "fn": 11,
          "accuracy": 0.993125
        },
        "0.01": null
      },
      "auroc": 0.94099873046875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1520,
          "fn": 80,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9271431477864583
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3109,
          "fn": 91,
          "accuracy": 0.9715625
        },
        "0.01": null
      },
      "auroc": 0.9340709391276042
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1547,
          "fn": 53,
          "accuracy": 0.966875
        },
        "0.01": null
      },
      "auroc": 0.9310439453125
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3136,
          "fn": 64,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.936021337890625
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1540,
          "fn": 60,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.93011875
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1540,
          "fn": 60,
          "accuracy": 0.9625
        },
        "0.01": null
      },
      "auroc": 0.93011875
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1509,
          "fn": 91,
          "accuracy": 0.943125
        },
        "0.01": null
      },
      "auroc": 0.9213327636718749
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1509,
          "fn": 91,
          "accuracy": 0.943125
        },
        "0.01": null
      },
      "auroc": 0.9213327636718749
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3049,
          "fn": 151,
          "accuracy": 0.9528125
        },
        "0.01": null
      },
      "auroc": 0.9257257568359374
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3049,
          "fn": 151,
          "accuracy": 0.9528125
        },
        "0.01": null
      },
      "auroc": 0.9257257568359374
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1455,
          "fn": 145,
          "accuracy": 0.909375
        },
        "0.01": null
      },
      "auroc": 0.9081666503906249
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1455,
          "fn": 145,
          "accuracy": 0.909375
        },
        "0.01": null
      },
      "auroc": 0.9081666503906249
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1295,
          "fn": 305,
          "accuracy": 0.809375
        },
        "0.01": null
      },
      "auroc": 0.8744080403645833
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1295,
          "fn": 305,
          "accuracy": 0.809375
        },
        "0.01": null
      },
      "auroc": 0.8744080403645833
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2750,
          "fn": 450,
          "accuracy": 0.859375
        },
        "0.01": null
      },
      "auroc": 0.8912873453776041
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2750,
          "fn": 450,
          "accuracy": 0.859375
        },
        "0.01": null
      },
      "auroc": 0.8912873453776041
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1589,
          "fn": 11,
          "accuracy": 0.993125
        },
        "0.01": null
      },
      "auroc": 0.938853515625
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1589,
          "fn": 11,
          "accuracy": 0.993125
        },
        "0.01": null
      },
      "auroc": 0.938853515625
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1518,
          "fn": 82,
          "accuracy": 0.94875
        },
        "0.01": null
      },
      "auroc": 0.9261655436197918
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1518,
          "fn": 82,
          "accuracy": 0.94875
        },
        "0.01": null
      },
      "auroc": 0.9261655436197918
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3107,
          "fn": 93,
          "accuracy": 0.9709375
        },
        "0.01": null
      },
      "auroc": 0.9325095296223959
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 3107,
          "fn": 93,
          "accuracy": 0.9709375
        },
        "0.01": null
      },
      "auroc": 0.9325095296223959
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1589,
          "fn": 11,
          "accuracy": 0.993125
        },
        "0.01": null
      },
      "auroc": 0.9406892089843749
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1589,
          "fn": 11,
          "accuracy": 0.993125
        },
        "0.01": null
      },
      "auroc": 0.9406892089843749
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 867,
          "fn": 733,
          "accuracy": 0.541875
        },
        "0.01": null
      },
      "auroc": 0.7872444986979167
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 867,
          "fn": 733,
          "accuracy": 0.541875
        },
        "0.01": null
      },
      "auroc": 0.7872444986979167
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2456,
          "fn": 744,
          "accuracy": 0.7675
        },
        "0.01": null
      },
      "auroc": 0.8639668538411458
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2456,
          "fn": 744,
          "accuracy": 0.7675
        },
        "0.01": null
      },
      "auroc": 0.8639668538411458
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1462,
          "fn": 138,
          "accuracy": 0.91375
        },
        "0.01": null
      },
      "auroc": 0.9109984700520833
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1462,
          "fn": 138,
          "accuracy": 0.91375
        },
        "0.01": null
      },
      "auroc": 0.9109984700520833
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1297,
          "fn": 303,
          "accuracy": 0.810625
        },
        "0.01": null
      },
      "auroc": 0.8718368815104166
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 1297,
          "fn": 303,
          "accuracy": 0.810625
        },
        "0.01": null
      },
      "auroc": 0.8718368815104166
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2759,
          "fn": 441,
          "accuracy": 0.8621875
        },
        "0.01": null
      },
      "auroc": 0.89141767578125
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 2759,
          "fn": 441,
          "accuracy": 0.8621875
        },
        "0.01": null
      },
      "auroc": 0.89141767578125
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 17093,
          "fn": 507,
          "accuracy": 0.9711931818181818
        },
        "0.01": null
      },
      "auroc": 0.9328192560369318
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 15111,
          "fn": 2489,
          "accuracy": 0.8585795454545454
        },
        "0.01": null
      },
      "auroc": 0.8910646055279356
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "none",
      "accuracy": {
        "0.05": {
          "tp": 32204,
          "fn": 2996,
          "accuracy": 0.9148863636363637
        },
        "0.01": null
      },
      "auroc": 0.9119419307824337
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1578,
          "fn": 22,
          "accuracy": 0.98625
        },
        "0.01": null
      },
      "auroc": 0.9394701822916667
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1483,
          "fn": 117,
          "accuracy": 0.926875
        },
        "0.01": null
      },
      "auroc": 0.9169108886718751
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3061,
          "fn": 139,
          "accuracy": 0.9565625
        },
        "0.01": null
      },
      "auroc": 0.9281905354817709
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1579,
          "fn": 21,
          "accuracy": 0.986875
        },
        "0.01": null
      },
      "auroc": 0.938587744140625
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1147,
          "fn": 453,
          "accuracy": 0.716875
        },
        "0.01": null
      },
      "auroc": 0.7976996256510417
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2726,
          "fn": 474,
          "accuracy": 0.851875
        },
        "0.01": null
      },
      "auroc": 0.8681436848958333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3157,
          "fn": 43,
          "accuracy": 0.9865625
        },
        "0.01": null
      },
      "auroc": 0.9390289632161459
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2630,
          "fn": 570,
          "accuracy": 0.821875
        },
        "0.01": null
      },
      "auroc": 0.8573052571614583
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 5787,
          "fn": 613,
          "accuracy": 0.90421875
        },
        "0.01": null
      },
      "auroc": 0.8981671101888021
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1515,
          "fn": 85,
          "accuracy": 0.946875
        },
        "0.01": null
      },
      "auroc": 0.9272341634114583
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 4,
          "fn": 1596,
          "accuracy": 0.0025
        },
        "0.01": null
      },
      "auroc": 0.08612112630208334
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1519,
          "fn": 1681,
          "accuracy": 0.4746875
        },
        "0.01": null
      },
      "auroc": 0.5066776448567709
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1164,
          "fn": 436,
          "accuracy": 0.7275
        },
        "0.01": null
      },
      "auroc": 0.83648134765625
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 1598,
          "accuracy": 0.00125
        },
        "0.01": null
      },
      "auroc": 0.051126123046875006
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1166,
          "fn": 2034,
          "accuracy": 0.364375
        },
        "0.01": null
      },
      "auroc": 0.4438037353515625
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2679,
          "fn": 521,
          "accuracy": 0.8371875
        },
        "0.01": null
      },
      "auroc": 0.8818577555338541
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 6,
          "fn": 3194,
          "accuracy": 0.001875
        },
        "0.01": null
      },
      "auroc": 0.06862362467447918
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2685,
          "fn": 3715,
          "accuracy": 0.41953125
        },
        "0.01": null
      },
      "auroc": 0.47524069010416664
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1408,
          "fn": 192,
          "accuracy": 0.88
        },
        "0.01": null
      },
      "auroc": 0.8896127115885417
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1278,
          "fn": 322,
          "accuracy": 0.79875
        },
        "0.01": null
      },
      "auroc": 0.8658944335937501
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2686,
          "fn": 514,
          "accuracy": 0.839375
        },
        "0.01": null
      },
      "auroc": 0.8777535725911458
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1457,
          "fn": 143,
          "accuracy": 0.910625
        },
        "0.01": null
      },
      "auroc": 0.9162146158854167
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1514,
          "fn": 86,
          "accuracy": 0.94625
        },
        "0.01": null
      },
      "auroc": 0.9220848470052084
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2971,
          "fn": 229,
          "accuracy": 0.9284375
        },
        "0.01": null
      },
      "auroc": 0.9191497314453123
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1379,
          "fn": 221,
          "accuracy": 0.861875
        },
        "0.01": null
      },
      "auroc": 0.8856228027343751
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 1573,
          "accuracy": 0.016875
        },
        "0.01": null
      },
      "auroc": 0.36598660481770834
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1406,
          "fn": 1794,
          "accuracy": 0.439375
        },
        "0.01": null
      },
      "auroc": 0.6258047037760417
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2836,
          "fn": 364,
          "accuracy": 0.88625
        },
        "0.01": null
      },
      "auroc": 0.9009187093098957
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1541,
          "fn": 1659,
          "accuracy": 0.4815625
        },
        "0.01": null
      },
      "auroc": 0.6440357259114584
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 4377,
          "fn": 2023,
          "accuracy": 0.68390625
        },
        "0.01": null
      },
      "auroc": 0.7724772176106772
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1508,
          "fn": 92,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.92640703125
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 779,
          "fn": 821,
          "accuracy": 0.486875
        },
        "0.01": null
      },
      "auroc": 0.7051520182291666
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2287,
          "fn": 913,
          "accuracy": 0.7146875
        },
        "0.01": null
      },
      "auroc": 0.8157795247395835
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 913,
          "fn": 687,
          "accuracy": 0.570625
        },
        "0.01": null
      },
      "auroc": 0.783234033203125
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 1600,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.061268619791666666
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 913,
          "fn": 2287,
          "accuracy": 0.2853125
        },
        "0.01": null
      },
      "auroc": 0.4222513264973959
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2421,
          "fn": 779,
          "accuracy": 0.7565625
        },
        "0.01": null
      },
      "auroc": 0.8548205322265625
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 779,
          "fn": 2421,
          "accuracy": 0.2434375
        },
        "0.01": null
      },
      "auroc": 0.38321031901041663
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 3200,
          "accuracy": 0.5
        },
        "0.01": null
      },
      "auroc": 0.6190154256184897
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1513,
          "fn": 87,
          "accuracy": 0.945625
        },
        "0.01": null
      },
      "auroc": 0.9235944986979165
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1312,
          "fn": 288,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.8837353678385418
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2825,
          "fn": 375,
          "accuracy": 0.8828125
        },
        "0.01": null
      },
      "auroc": 0.9036649332682292
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1433,
          "fn": 167,
          "accuracy": 0.895625
        },
        "0.01": null
      },
      "auroc": 0.9020165690104167
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2946,
          "fn": 254,
          "accuracy": 0.920625
        },
        "0.01": null
      },
      "auroc": 0.9128055338541665
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1381,
          "fn": 219,
          "accuracy": 0.863125
        },
        "0.01": null
      },
      "auroc": 0.8907374674479167
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1381,
          "fn": 219,
          "accuracy": 0.863125
        },
        "0.01": null
      },
      "auroc": 0.8907374674479167
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1335,
          "fn": 265,
          "accuracy": 0.834375
        },
        "0.01": null
      },
      "auroc": 0.8701138509114583
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1335,
          "fn": 265,
          "accuracy": 0.834375
        },
        "0.01": null
      },
      "auroc": 0.8701138509114583
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2716,
          "fn": 484,
          "accuracy": 0.84875
        },
        "0.01": null
      },
      "auroc": 0.8804256591796877
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2716,
          "fn": 484,
          "accuracy": 0.84875
        },
        "0.01": null
      },
      "auroc": 0.8804256591796877
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1249,
          "fn": 351,
          "accuracy": 0.780625
        },
        "0.01": null
      },
      "auroc": 0.8607180501302085
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1249,
          "fn": 351,
          "accuracy": 0.780625
        },
        "0.01": null
      },
      "auroc": 0.8607180501302085
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1065,
          "fn": 535,
          "accuracy": 0.665625
        },
        "0.01": null
      },
      "auroc": 0.8177323567708334
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1065,
          "fn": 535,
          "accuracy": 0.665625
        },
        "0.01": null
      },
      "auroc": 0.8177323567708334
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2314,
          "fn": 886,
          "accuracy": 0.723125
        },
        "0.01": null
      },
      "auroc": 0.8392252034505208
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2314,
          "fn": 886,
          "accuracy": 0.723125
        },
        "0.01": null
      },
      "auroc": 0.8392252034505208
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1496,
          "fn": 104,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9171891927083333
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1496,
          "fn": 104,
          "accuracy": 0.935
        },
        "0.01": null
      },
      "auroc": 0.9171891927083333
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1355,
          "fn": 245,
          "accuracy": 0.846875
        },
        "0.01": null
      },
      "auroc": 0.8885611816406249
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1355,
          "fn": 245,
          "accuracy": 0.846875
        },
        "0.01": null
      },
      "auroc": 0.8885611816406249
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2851,
          "fn": 349,
          "accuracy": 0.8909375
        },
        "0.01": null
      },
      "auroc": 0.9028751871744791
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2851,
          "fn": 349,
          "accuracy": 0.8909375
        },
        "0.01": null
      },
      "auroc": 0.9028751871744791
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1552,
          "fn": 48,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9281440104166666
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1552,
          "fn": 48,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9281440104166666
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 618,
          "fn": 982,
          "accuracy": 0.38625
        },
        "0.01": null
      },
      "auroc": 0.7233085937500001
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 618,
          "fn": 982,
          "accuracy": 0.38625
        },
        "0.01": null
      },
      "auroc": 0.7233085937500001
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2170,
          "fn": 1030,
          "accuracy": 0.678125
        },
        "0.01": null
      },
      "auroc": 0.8257263020833334
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2170,
          "fn": 1030,
          "accuracy": 0.678125
        },
        "0.01": null
      },
      "auroc": 0.8257263020833334
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1294,
          "fn": 306,
          "accuracy": 0.80875
        },
        "0.01": null
      },
      "auroc": 0.8737484049479165
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1294,
          "fn": 306,
          "accuracy": 0.80875
        },
        "0.01": null
      },
      "auroc": 0.8737484049479165
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1129,
          "fn": 471,
          "accuracy": 0.705625
        },
        "0.01": null
      },
      "auroc": 0.8236386393229166
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 1129,
          "fn": 471,
          "accuracy": 0.705625
        },
        "0.01": null
      },
      "auroc": 0.8236386393229166
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2423,
          "fn": 777,
          "accuracy": 0.7571875
        },
        "0.01": null
      },
      "auroc": 0.8486935221354166
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 2423,
          "fn": 777,
          "accuracy": 0.7571875
        },
        "0.01": null
      },
      "auroc": 0.8486935221354166
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 15951,
          "fn": 1649,
          "accuracy": 0.9063068181818181
        },
        "0.01": null
      },
      "auroc": 0.9084609389796402
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 13248,
          "fn": 4352,
          "accuracy": 0.7527272727272727
        },
        "0.01": null
      },
      "auroc": 0.8486537775213067
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "whitespace",
      "accuracy": {
        "0.05": {
          "tp": 29199,
          "fn": 6001,
          "accuracy": 0.8295170454545454
        },
        "0.01": null
      },
      "auroc": 0.8785573582504735
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1583,
          "fn": 17,
          "accuracy": 0.989375
        },
        "0.01": null
      },
      "auroc": 0.9387274251302083
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1446,
          "fn": 154,
          "accuracy": 0.90375
        },
        "0.01": null
      },
      "auroc": 0.9091450032552083
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3029,
          "fn": 171,
          "accuracy": 0.9465625
        },
        "0.01": null
      },
      "auroc": 0.9239362141927083
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1577,
          "fn": 23,
          "accuracy": 0.985625
        },
        "0.01": null
      },
      "auroc": 0.9368607096354167
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1055,
          "fn": 545,
          "accuracy": 0.659375
        },
        "0.01": null
      },
      "auroc": 0.7683674967447917
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2632,
          "fn": 568,
          "accuracy": 0.8225
        },
        "0.01": null
      },
      "auroc": 0.8526141031901043
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 3160,
          "fn": 40,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9377940673828125
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2501,
          "fn": 699,
          "accuracy": 0.7815625
        },
        "0.01": null
      },
      "auroc": 0.8387562500000001
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 5661,
          "fn": 739,
          "accuracy": 0.88453125
        },
        "0.01": null
      },
      "auroc": 0.8882751586914062
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1517,
          "fn": 83,
          "accuracy": 0.948125
        },
        "0.01": null
      },
      "auroc": 0.9257722330729167
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 1591,
          "accuracy": 0.005625
        },
        "0.01": null
      },
      "auroc": 0.097804931640625
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1526,
          "fn": 1674,
          "accuracy": 0.476875
        },
        "0.01": null
      },
      "auroc": 0.5117885823567708
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 904,
          "fn": 696,
          "accuracy": 0.565
        },
        "0.01": null
      },
      "auroc": 0.7870000651041666
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 1598,
          "accuracy": 0.00125
        },
        "0.01": null
      },
      "auroc": 0.054463834635416666
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 906,
          "fn": 2294,
          "accuracy": 0.283125
        },
        "0.01": null
      },
      "auroc": 0.42073194986979173
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2421,
          "fn": 779,
          "accuracy": 0.7565625
        },
        "0.01": null
      },
      "auroc": 0.8563861490885416
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 3189,
          "accuracy": 0.0034375
        },
        "0.01": null
      },
      "auroc": 0.07613438313802084
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2432,
          "fn": 3968,
          "accuracy": 0.38
        },
        "0.01": null
      },
      "auroc": 0.4662602661132813
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1399,
          "fn": 201,
          "accuracy": 0.874375
        },
        "0.01": null
      },
      "auroc": 0.8866906087239582
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1245,
          "fn": 355,
          "accuracy": 0.778125
        },
        "0.01": null
      },
      "auroc": 0.8589222819010417
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2644,
          "fn": 556,
          "accuracy": 0.82625
        },
        "0.01": null
      },
      "auroc": 0.8728064453125
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1468,
          "fn": 132,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9166403645833334
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1529,
          "fn": 71,
          "accuracy": 0.955625
        },
        "0.01": null
      },
      "auroc": 0.9271291829427084
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2997,
          "fn": 203,
          "accuracy": 0.9365625
        },
        "0.01": null
      },
      "auroc": 0.9218847737630207
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1312,
          "fn": 288,
          "accuracy": 0.82
        },
        "0.01": null
      },
      "auroc": 0.87426669921875
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 10,
          "fn": 1590,
          "accuracy": 0.00625
        },
        "0.01": null
      },
      "auroc": 0.2854617350260416
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1322,
          "fn": 1878,
          "accuracy": 0.413125
        },
        "0.01": null
      },
      "auroc": 0.5798642171223959
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2780,
          "fn": 420,
          "accuracy": 0.86875
        },
        "0.01": null
      },
      "auroc": 0.8954535319010416
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1539,
          "fn": 1661,
          "accuracy": 0.4809375
        },
        "0.01": null
      },
      "auroc": 0.606295458984375
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 4319,
          "fn": 2081,
          "accuracy": 0.67484375
        },
        "0.01": null
      },
      "auroc": 0.7508744954427082
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1502,
          "fn": 98,
          "accuracy": 0.93875
        },
        "0.01": null
      },
      "auroc": 0.925364599609375
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 572,
          "fn": 1028,
          "accuracy": 0.3575
        },
        "0.01": null
      },
      "auroc": 0.6464731770833333
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2074,
          "fn": 1126,
          "accuracy": 0.648125
        },
        "0.01": null
      },
      "auroc": 0.7859188883463541
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 683,
          "fn": 917,
          "accuracy": 0.426875
        },
        "0.01": null
      },
      "auroc": 0.7256279622395835
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 1600,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.05445174153645834
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 683,
          "fn": 2517,
          "accuracy": 0.2134375
        },
        "0.01": null
      },
      "auroc": 0.3900398518880208
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2185,
          "fn": 1015,
          "accuracy": 0.6828125
        },
        "0.01": null
      },
      "auroc": 0.8254962809244791
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 572,
          "fn": 2628,
          "accuracy": 0.17875
        },
        "0.01": null
      },
      "auroc": 0.35046245930989584
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2757,
          "fn": 3643,
          "accuracy": 0.43078125
        },
        "0.01": null
      },
      "auroc": 0.5879793701171875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1525,
          "fn": 75,
          "accuracy": 0.953125
        },
        "0.01": null
      },
      "auroc": 0.9241912109375001
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1266,
          "fn": 334,
          "accuracy": 0.79125
        },
        "0.01": null
      },
      "auroc": 0.8690772298177083
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2791,
          "fn": 409,
          "accuracy": 0.8721875
        },
        "0.01": null
      },
      "auroc": 0.8966342203776042
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1390,
          "fn": 210,
          "accuracy": 0.86875
        },
        "0.01": null
      },
      "auroc": 0.8951108561197916
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2915,
          "fn": 285,
          "accuracy": 0.9109375
        },
        "0.01": null
      },
      "auroc": 0.9096510335286458
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1426,
          "fn": 174,
          "accuracy": 0.89125
        },
        "0.01": null
      },
      "auroc": 0.897101904296875
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1426,
          "fn": 174,
          "accuracy": 0.89125
        },
        "0.01": null
      },
      "auroc": 0.897101904296875
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1303,
          "fn": 297,
          "accuracy": 0.814375
        },
        "0.01": null
      },
      "auroc": 0.8697697916666666
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1303,
          "fn": 297,
          "accuracy": 0.814375
        },
        "0.01": null
      },
      "auroc": 0.8697697916666666
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2729,
          "fn": 471,
          "accuracy": 0.8528125
        },
        "0.01": null
      },
      "auroc": 0.883435847981771
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2729,
          "fn": 471,
          "accuracy": 0.8528125
        },
        "0.01": null
      },
      "auroc": 0.883435847981771
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1141,
          "fn": 459,
          "accuracy": 0.713125
        },
        "0.01": null
      },
      "auroc": 0.8386661295572917
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1141,
          "fn": 459,
          "accuracy": 0.713125
        },
        "0.01": null
      },
      "auroc": 0.8386661295572917
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 895,
          "fn": 705,
          "accuracy": 0.559375
        },
        "0.01": null
      },
      "auroc": 0.7757380696614584
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 895,
          "fn": 705,
          "accuracy": 0.559375
        },
        "0.01": null
      },
      "auroc": 0.7757380696614584
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2036,
          "fn": 1164,
          "accuracy": 0.63625
        },
        "0.01": null
      },
      "auroc": 0.8072020996093752
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2036,
          "fn": 1164,
          "accuracy": 0.63625
        },
        "0.01": null
      },
      "auroc": 0.8072020996093752
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1482,
          "fn": 118,
          "accuracy": 0.92625
        },
        "0.01": null
      },
      "auroc": 0.9109866536458333
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1482,
          "fn": 118,
          "accuracy": 0.92625
        },
        "0.01": null
      },
      "auroc": 0.9109866536458333
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1293,
          "fn": 307,
          "accuracy": 0.808125
        },
        "0.01": null
      },
      "auroc": 0.8701663248697917
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1293,
          "fn": 307,
          "accuracy": 0.808125
        },
        "0.01": null
      },
      "auroc": 0.8701663248697917
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2775,
          "fn": 425,
          "accuracy": 0.8671875
        },
        "0.01": null
      },
      "auroc": 0.8905764892578125
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2775,
          "fn": 425,
          "accuracy": 0.8671875
        },
        "0.01": null
      },
      "auroc": 0.8905764892578125
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1536,
          "fn": 64,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9207981608072917
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1536,
          "fn": 64,
          "accuracy": 0.96
        },
        "0.01": null
      },
      "auroc": 0.9207981608072917
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 1227,
          "accuracy": 0.233125
        },
        "0.01": null
      },
      "auroc": 0.64580654296875
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 373,
          "fn": 1227,
          "accuracy": 0.233125
        },
        "0.01": null
      },
      "auroc": 0.64580654296875
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1909,
          "fn": 1291,
          "accuracy": 0.5965625
        },
        "0.01": null
      },
      "auroc": 0.7833023518880209
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1909,
          "fn": 1291,
          "accuracy": 0.5965625
        },
        "0.01": null
      },
      "auroc": 0.7833023518880209
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1213,
          "fn": 387,
          "accuracy": 0.758125
        },
        "0.01": null
      },
      "auroc": 0.8529601399739584
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1213,
          "fn": 387,
          "accuracy": 0.758125
        },
        "0.01": null
      },
      "auroc": 0.8529601399739584
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1034,
          "fn": 566,
          "accuracy": 0.64625
        },
        "0.01": null
      },
      "auroc": 0.7952384114583335
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 1034,
          "fn": 566,
          "accuracy": 0.64625
        },
        "0.01": null
      },
      "auroc": 0.7952384114583335
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2247,
          "fn": 953,
          "accuracy": 0.7021875
        },
        "0.01": null
      },
      "auroc": 0.8240992757161458
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 2247,
          "fn": 953,
          "accuracy": 0.7021875
        },
        "0.01": null
      },
      "auroc": 0.8240992757161458
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 15792,
          "fn": 1808,
          "accuracy": 0.8972727272727272
        },
        "0.01": null
      },
      "auroc": 0.9034454027580494
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 12009,
          "fn": 5591,
          "accuracy": 0.6823295454545455
        },
        "0.01": null
      },
      "auroc": 0.8213188831676136
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "upper_lower",
      "accuracy": {
        "0.05": {
          "tp": 27801,
          "fn": 7399,
          "accuracy": 0.7898011363636364
        },
        "0.01": null
      },
      "auroc": 0.8623821429628313
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1247,
          "fn": 353,
          "accuracy": 0.779375
        },
        "0.01": null
      },
      "auroc": 0.8503756184895834
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 603,
          "fn": 997,
          "accuracy": 0.376875
        },
        "0.01": null
      },
      "auroc": 0.6953298990885417
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1850,
          "fn": 1350,
          "accuracy": 0.578125
        },
        "0.01": null
      },
      "auroc": 0.7728527587890623
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1220,
          "fn": 380,
          "accuracy": 0.7625
        },
        "0.01": null
      },
      "auroc": 0.8397945638020833
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 269,
          "fn": 1331,
          "accuracy": 0.168125
        },
        "0.01": null
      },
      "auroc": 0.49849243164062484
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1489,
          "fn": 1711,
          "accuracy": 0.4653125
        },
        "0.01": null
      },
      "auroc": 0.6691434977213542
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2467,
          "fn": 733,
          "accuracy": 0.7709375
        },
        "0.01": null
      },
      "auroc": 0.8450850911458334
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 872,
          "fn": 2328,
          "accuracy": 0.2725
        },
        "0.01": null
      },
      "auroc": 0.5969111653645833
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3339,
          "fn": 3061,
          "accuracy": 0.52171875
        },
        "0.01": null
      },
      "auroc": 0.7209981282552084
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1507,
          "fn": 93,
          "accuracy": 0.941875
        },
        "0.01": null
      },
      "auroc": 0.922321875
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3,
          "fn": 1597,
          "accuracy": 0.001875
        },
        "0.01": null
      },
      "auroc": 0.06659560546875
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1510,
          "fn": 1690,
          "accuracy": 0.471875
        },
        "0.01": null
      },
      "auroc": 0.49445874023437497
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 312,
          "fn": 1288,
          "accuracy": 0.195
        },
        "0.01": null
      },
      "auroc": 0.5563296712239584
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 1598,
          "accuracy": 0.00125
        },
        "0.01": null
      },
      "auroc": 0.05136349283854167
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 314,
          "fn": 2886,
          "accuracy": 0.098125
        },
        "0.01": null
      },
      "auroc": 0.30384658203125
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1819,
          "fn": 1381,
          "accuracy": 0.5684375
        },
        "0.01": null
      },
      "auroc": 0.7393257731119791
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 5,
          "fn": 3195,
          "accuracy": 0.0015625
        },
        "0.01": null
      },
      "auroc": 0.05897954915364584
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1824,
          "fn": 4576,
          "accuracy": 0.285
        },
        "0.01": null
      },
      "auroc": 0.39915266113281245
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 718,
          "fn": 882,
          "accuracy": 0.44875
        },
        "0.01": null
      },
      "auroc": 0.7325108561197917
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 526,
          "fn": 1074,
          "accuracy": 0.32875
        },
        "0.01": null
      },
      "auroc": 0.6785389485677085
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1244,
          "fn": 1956,
          "accuracy": 0.38875
        },
        "0.01": null
      },
      "auroc": 0.70552490234375
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1520,
          "fn": 80,
          "accuracy": 0.95
        },
        "0.01": null
      },
      "auroc": 0.9269675944010416
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1227,
          "fn": 373,
          "accuracy": 0.766875
        },
        "0.01": null
      },
      "auroc": 0.8297408854166668
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2747,
          "fn": 453,
          "accuracy": 0.8584375
        },
        "0.01": null
      },
      "auroc": 0.8783542399088542
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 795,
          "fn": 805,
          "accuracy": 0.496875
        },
        "0.01": null
      },
      "auroc": 0.7220116861979167
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 1600,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.10863444010416667
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 795,
          "fn": 2405,
          "accuracy": 0.2484375
        },
        "0.01": null
      },
      "auroc": 0.4153230631510417
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 2315,
          "fn": 885,
          "accuracy": 0.7234375
        },
        "0.01": null
      },
      "auroc": 0.8244896402994792
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1227,
          "fn": 1973,
          "accuracy": 0.3834375
        },
        "0.01": null
      },
      "auroc": 0.4691876627604166
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 3542,
          "fn": 2858,
          "accuracy": 0.5534375
        },
        "0.01": null
      },
      "auroc": 0.646838651529948
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1489,
          "fn": 111,
          "accuracy": 0.930625
        },
        "0.01": null
      },
      "auroc": 0.9185552408854167
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 1492,
          "accuracy": 0.0675
        },
        "0.01": null
      },
      "auroc": 0.32700048828125
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 1603,
          "accuracy": 0.4990625
        },
        "0.01": null
      },
      "auroc": 0.6227778645833334
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 229,
          "fn": 1371,
          "accuracy": 0.143125
        },
        "0.01": null
      },
      "auroc": 0.5047435546875
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 1600,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.04656546223958335
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 229,
          "fn": 2971,
          "accuracy": 0.0715625
        },
        "0.01": null
      },
      "auroc": 0.2756545084635416
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1718,
          "fn": 1482,
          "accuracy": 0.536875
        },
        "0.01": null
      },
      "auroc": 0.7116493977864584
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 108,
          "fn": 3092,
          "accuracy": 0.03375
        },
        "0.01": null
      },
      "auroc": 0.18678297526041668
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1826,
          "fn": 4574,
          "accuracy": 0.2853125
        },
        "0.01": null
      },
      "auroc": 0.44921618652343753
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1188,
          "fn": 412,
          "accuracy": 0.7425
        },
        "0.01": null
      },
      "auroc": 0.8237597493489583
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 400,
          "fn": 1200,
          "accuracy": 0.25
        },
        "0.01": null
      },
      "auroc": 0.6193524739583334
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1588,
          "fn": 1612,
          "accuracy": 0.49625
        },
        "0.01": null
      },
      "auroc": 0.7215561116536457
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 792,
          "fn": 808,
          "accuracy": 0.495
        },
        "0.01": null
      },
      "auroc": 0.7375202636718751
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1980,
          "fn": 1220,
          "accuracy": 0.61875
        },
        "0.01": null
      },
      "auroc": 0.7806400065104167
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 849,
          "fn": 751,
          "accuracy": 0.530625
        },
        "0.01": null
      },
      "auroc": 0.7461816080729168
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 849,
          "fn": 751,
          "accuracy": 0.530625
        },
        "0.01": null
      },
      "auroc": 0.7461816080729168
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 644,
          "fn": 956,
          "accuracy": 0.4025
        },
        "0.01": null
      },
      "auroc": 0.6860059895833333
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 644,
          "fn": 956,
          "accuracy": 0.4025
        },
        "0.01": null
      },
      "auroc": 0.6860059895833333
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1493,
          "fn": 1707,
          "accuracy": 0.4665625
        },
        "0.01": null
      },
      "auroc": 0.716093798828125
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1493,
          "fn": 1707,
          "accuracy": 0.4665625
        },
        "0.01": null
      },
      "auroc": 0.716093798828125
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 547,
          "fn": 1053,
          "accuracy": 0.341875
        },
        "0.01": null
      },
      "auroc": 0.6404574869791666
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 547,
          "fn": 1053,
          "accuracy": 0.341875
        },
        "0.01": null
      },
      "auroc": 0.6404574869791666
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 311,
          "fn": 1289,
          "accuracy": 0.194375
        },
        "0.01": null
      },
      "auroc": 0.5535470703125001
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 311,
          "fn": 1289,
          "accuracy": 0.194375
        },
        "0.01": null
      },
      "auroc": 0.5535470703125001
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 858,
          "fn": 2342,
          "accuracy": 0.268125
        },
        "0.01": null
      },
      "auroc": 0.5970022786458333
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 858,
          "fn": 2342,
          "accuracy": 0.268125
        },
        "0.01": null
      },
      "auroc": 0.5970022786458333
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 812,
          "fn": 788,
          "accuracy": 0.5075
        },
        "0.01": null
      },
      "auroc": 0.7484031901041668
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 812,
          "fn": 788,
          "accuracy": 0.5075
        },
        "0.01": null
      },
      "auroc": 0.7484031901041668
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 435,
          "fn": 1165,
          "accuracy": 0.271875
        },
        "0.01": null
      },
      "auroc": 0.6522211588541669
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 435,
          "fn": 1165,
          "accuracy": 0.271875
        },
        "0.01": null
      },
      "auroc": 0.6522211588541669
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1247,
          "fn": 1953,
          "accuracy": 0.3896875
        },
        "0.01": null
      },
      "auroc": 0.7003121744791667
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1247,
          "fn": 1953,
          "accuracy": 0.3896875
        },
        "0.01": null
      },
      "auroc": 0.7003121744791667
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 941,
          "fn": 659,
          "accuracy": 0.588125
        },
        "0.01": null
      },
      "auroc": 0.7863192382812499
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 941,
          "fn": 659,
          "accuracy": 0.588125
        },
        "0.01": null
      },
      "auroc": 0.7863192382812499
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 1564,
          "accuracy": 0.0225
        },
        "0.01": null
      },
      "auroc": 0.40248064778645837
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 36,
          "fn": 1564,
          "accuracy": 0.0225
        },
        "0.01": null
      },
      "auroc": 0.40248064778645837
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 977,
          "fn": 2223,
          "accuracy": 0.3053125
        },
        "0.01": null
      },
      "auroc": 0.5943999430338541
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 977,
          "fn": 2223,
          "accuracy": 0.3053125
        },
        "0.01": null
      },
      "auroc": 0.5943999430338541
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 770,
          "fn": 830,
          "accuracy": 0.48125
        },
        "0.01": null
      },
      "auroc": 0.7030087727864582
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 770,
          "fn": 830,
          "accuracy": 0.48125
        },
        "0.01": null
      },
      "auroc": 0.7030087727864582
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 533,
          "fn": 1067,
          "accuracy": 0.333125
        },
        "0.01": null
      },
      "auroc": 0.6246237955729167
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 533,
          "fn": 1067,
          "accuracy": 0.333125
        },
        "0.01": null
      },
      "auroc": 0.6246237955729167
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1303,
          "fn": 1897,
          "accuracy": 0.4071875
        },
        "0.01": null
      },
      "auroc": 0.6638162841796875
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 1303,
          "fn": 1897,
          "accuracy": 0.4071875
        },
        "0.01": null
      },
      "auroc": 0.6638162841796875
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 11588,
          "fn": 6012,
          "accuracy": 0.6584090909090909
        },
        "0.01": null
      },
      "auroc": 0.7998964754971591
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 5833,
          "fn": 11767,
          "accuracy": 0.33142045454545455
        },
        "0.01": null
      },
      "auroc": 0.6325288500236741
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "synonym",
      "accuracy": {
        "0.05": {
          "tp": 17421,
          "fn": 17779,
          "accuracy": 0.49491477272727274
        },
        "0.01": null
      },
      "auroc": 0.7162126627604166
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1596,
          "fn": 4,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9435358398437499
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1548,
          "fn": 52,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9339798177083334
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3144,
          "fn": 56,
          "accuracy": 0.9825
        },
        "0.01": null
      },
      "auroc": 0.9387578287760416
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9429377278645833
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1246,
          "fn": 354,
          "accuracy": 0.77875
        },
        "0.01": null
      },
      "auroc": 0.829405322265625
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2841,
          "fn": 359,
          "accuracy": 0.8878125
        },
        "0.01": null
      },
      "auroc": 0.8861715250651041
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3191,
          "fn": 9,
          "accuracy": 0.9971875
        },
        "0.01": null
      },
      "auroc": 0.9432367838541666
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2794,
          "fn": 406,
          "accuracy": 0.873125
        },
        "0.01": null
      },
      "auroc": 0.8816925699869791
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 5985,
          "fn": 415,
          "accuracy": 0.93515625
        },
        "0.01": null
      },
      "auroc": 0.912464676920573
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1576,
          "fn": 24,
          "accuracy": 0.985
        },
        "0.01": null
      },
      "auroc": 0.9401094401041666
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 12,
          "fn": 1588,
          "accuracy": 0.0075
        },
        "0.01": null
      },
      "auroc": 0.10590501302083334
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1588,
          "fn": 1612,
          "accuracy": 0.49625
        },
        "0.01": null
      },
      "auroc": 0.5230072265625
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1265,
          "fn": 335,
          "accuracy": 0.790625
        },
        "0.01": null
      },
      "auroc": 0.8602591145833332
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 1598,
          "accuracy": 0.00125
        },
        "0.01": null
      },
      "auroc": 0.05498722330729167
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1267,
          "fn": 1933,
          "accuracy": 0.3959375
        },
        "0.01": null
      },
      "auroc": 0.4576231689453125
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2841,
          "fn": 359,
          "accuracy": 0.8878125
        },
        "0.01": null
      },
      "auroc": 0.9001842773437501
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 3186,
          "accuracy": 0.004375
        },
        "0.01": null
      },
      "auroc": 0.0804461181640625
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2855,
          "fn": 3545,
          "accuracy": 0.44609375
        },
        "0.01": null
      },
      "auroc": 0.4903151977539062
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1537,
          "fn": 63,
          "accuracy": 0.960625
        },
        "0.01": null
      },
      "auroc": 0.92200146484375
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1468,
          "fn": 132,
          "accuracy": 0.9175
        },
        "0.01": null
      },
      "auroc": 0.9081139160156251
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3005,
          "fn": 195,
          "accuracy": 0.9390625
        },
        "0.01": null
      },
      "auroc": 0.9150576904296875
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1559,
          "fn": 41,
          "accuracy": 0.974375
        },
        "0.01": null
      },
      "auroc": 0.9368159342447917
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1566,
          "fn": 34,
          "accuracy": 0.97875
        },
        "0.01": null
      },
      "auroc": 0.9358520670572916
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3125,
          "fn": 75,
          "accuracy": 0.9765625
        },
        "0.01": null
      },
      "auroc": 0.9363340006510417
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1438,
          "fn": 162,
          "accuracy": 0.89875
        },
        "0.01": null
      },
      "auroc": 0.901195703125
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 21,
          "fn": 1579,
          "accuracy": 0.013125
        },
        "0.01": null
      },
      "auroc": 0.32441694335937504
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1459,
          "fn": 1741,
          "accuracy": 0.4559375
        },
        "0.01": null
      },
      "auroc": 0.6128063232421875
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2997,
          "fn": 203,
          "accuracy": 0.9365625
        },
        "0.01": null
      },
      "auroc": 0.9190058186848957
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1587,
          "fn": 1613,
          "accuracy": 0.4959375
        },
        "0.01": null
      },
      "auroc": 0.6301345052083334
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 4584,
          "fn": 1816,
          "accuracy": 0.71625
        },
        "0.01": null
      },
      "auroc": 0.7745701619466145
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1569,
          "fn": 31,
          "accuracy": 0.980625
        },
        "0.01": null
      },
      "auroc": 0.9394175455729167
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 932,
          "fn": 668,
          "accuracy": 0.5825
        },
        "0.01": null
      },
      "auroc": 0.7496722167968751
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2501,
          "fn": 699,
          "accuracy": 0.7815625
        },
        "0.01": null
      },
      "auroc": 0.8445448811848957
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1050,
          "fn": 550,
          "accuracy": 0.65625
        },
        "0.01": null
      },
      "auroc": 0.8131974446614583
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 1600,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.060830338541666674
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1050,
          "fn": 2150,
          "accuracy": 0.328125
        },
        "0.01": null
      },
      "auroc": 0.43701389160156257
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2619,
          "fn": 581,
          "accuracy": 0.8184375
        },
        "0.01": null
      },
      "auroc": 0.8763074951171874
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 932,
          "fn": 2268,
          "accuracy": 0.29125
        },
        "0.01": null
      },
      "auroc": 0.4052512776692708
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3551,
          "fn": 2849,
          "accuracy": 0.55484375
        },
        "0.01": null
      },
      "auroc": 0.6407793863932292
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1580,
          "fn": 20,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.938007080078125
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1462,
          "fn": 138,
          "accuracy": 0.91375
        },
        "0.01": null
      },
      "auroc": 0.91554873046875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3042,
          "fn": 158,
          "accuracy": 0.950625
        },
        "0.01": null
      },
      "auroc": 0.9267779052734375
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1525,
          "fn": 75,
          "accuracy": 0.953125
        },
        "0.01": null
      },
      "auroc": 0.9245528483072917
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3105,
          "fn": 95,
          "accuracy": 0.9703125
        },
        "0.01": null
      },
      "auroc": 0.9312799641927083
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1512,
          "fn": 88,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9205182128906251
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1512,
          "fn": 88,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9205182128906251
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1447,
          "fn": 153,
          "accuracy": 0.904375
        },
        "0.01": null
      },
      "auroc": 0.9053955078125002
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1447,
          "fn": 153,
          "accuracy": 0.904375
        },
        "0.01": null
      },
      "auroc": 0.9053955078125002
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2959,
          "fn": 241,
          "accuracy": 0.9246875
        },
        "0.01": null
      },
      "auroc": 0.9129568603515625
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2959,
          "fn": 241,
          "accuracy": 0.9246875
        },
        "0.01": null
      },
      "auroc": 0.9129568603515625
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1387,
          "fn": 213,
          "accuracy": 0.866875
        },
        "0.01": null
      },
      "auroc": 0.8917462890625
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1387,
          "fn": 213,
          "accuracy": 0.866875
        },
        "0.01": null
      },
      "auroc": 0.8917462890625
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1196,
          "fn": 404,
          "accuracy": 0.7475
        },
        "0.01": null
      },
      "auroc": 0.8494861490885417
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1196,
          "fn": 404,
          "accuracy": 0.7475
        },
        "0.01": null
      },
      "auroc": 0.8494861490885417
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2583,
          "fn": 617,
          "accuracy": 0.8071875
        },
        "0.01": null
      },
      "auroc": 0.8706162190755208
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2583,
          "fn": 617,
          "accuracy": 0.8071875
        },
        "0.01": null
      },
      "auroc": 0.8706162190755208
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1567,
          "fn": 33,
          "accuracy": 0.979375
        },
        "0.01": null
      },
      "auroc": 0.9338846842447918
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1567,
          "fn": 33,
          "accuracy": 0.979375
        },
        "0.01": null
      },
      "auroc": 0.9338846842447918
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1484,
          "fn": 116,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.915212744140625
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1484,
          "fn": 116,
          "accuracy": 0.9275
        },
        "0.01": null
      },
      "auroc": 0.915212744140625
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3051,
          "fn": 149,
          "accuracy": 0.9534375
        },
        "0.01": null
      },
      "auroc": 0.9245487141927083
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 3051,
          "fn": 149,
          "accuracy": 0.9534375
        },
        "0.01": null
      },
      "auroc": 0.9245487141927083
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1585,
          "fn": 15,
          "accuracy": 0.990625
        },
        "0.01": null
      },
      "auroc": 0.9394080240885416
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1585,
          "fn": 15,
          "accuracy": 0.990625
        },
        "0.01": null
      },
      "auroc": 0.9394080240885416
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 735,
          "fn": 865,
          "accuracy": 0.459375
        },
        "0.01": null
      },
      "auroc": 0.7529739095052084
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 735,
          "fn": 865,
          "accuracy": 0.459375
        },
        "0.01": null
      },
      "auroc": 0.7529739095052084
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2320,
          "fn": 880,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.846190966796875
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2320,
          "fn": 880,
          "accuracy": 0.725
        },
        "0.01": null
      },
      "auroc": 0.846190966796875
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1414,
          "fn": 186,
          "accuracy": 0.88375
        },
        "0.01": null
      },
      "auroc": 0.8998559407552083
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1414,
          "fn": 186,
          "accuracy": 0.88375
        },
        "0.01": null
      },
      "auroc": 0.8998559407552083
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1238,
          "fn": 362,
          "accuracy": 0.77375
        },
        "0.01": null
      },
      "auroc": 0.8560856119791669
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 1238,
          "fn": 362,
          "accuracy": 0.77375
        },
        "0.01": null
      },
      "auroc": 0.8560856119791669
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2652,
          "fn": 548,
          "accuracy": 0.82875
        },
        "0.01": null
      },
      "auroc": 0.8779707763671876
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 2652,
          "fn": 548,
          "accuracy": 0.82875
        },
        "0.01": null
      },
      "auroc": 0.8779707763671876
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 16882,
          "fn": 718,
          "accuracy": 0.9592045454545455
        },
        "0.01": null
      },
      "auroc": 0.9277545868844697
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 14441,
          "fn": 3159,
          "accuracy": 0.8205113636363637
        },
        "0.01": null
      },
      "auroc": 0.8754009706439394
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "perplexity_misspelling",
      "accuracy": {
        "0.05": {
          "tp": 31323,
          "fn": 3877,
          "accuracy": 0.8898579545454546
        },
        "0.01": null
      },
      "auroc": 0.9015777787642045
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1534,
          "fn": 66,
          "accuracy": 0.95875
        },
        "0.01": null
      },
      "auroc": 0.931977490234375
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1448,
          "fn": 152,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.9130111165364583
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2982,
          "fn": 218,
          "accuracy": 0.931875
        },
        "0.01": null
      },
      "auroc": 0.9224943033854167
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1525,
          "fn": 75,
          "accuracy": 0.953125
        },
        "0.01": null
      },
      "auroc": 0.9289075846354167
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1327,
          "fn": 273,
          "accuracy": 0.829375
        },
        "0.01": null
      },
      "auroc": 0.883726220703125
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2852,
          "fn": 348,
          "accuracy": 0.89125
        },
        "0.01": null
      },
      "auroc": 0.9063169026692708
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3059,
          "fn": 141,
          "accuracy": 0.9559375
        },
        "0.01": null
      },
      "auroc": 0.9304425374348958
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2775,
          "fn": 425,
          "accuracy": 0.8671875
        },
        "0.01": null
      },
      "auroc": 0.8983686686197916
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 5834,
          "fn": 566,
          "accuracy": 0.9115625
        },
        "0.01": null
      },
      "auroc": 0.9144056030273437
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1462,
          "fn": 138,
          "accuracy": 0.91375
        },
        "0.01": null
      },
      "auroc": 0.915398974609375
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 299,
          "fn": 1301,
          "accuracy": 0.186875
        },
        "0.01": null
      },
      "auroc": 0.5971072753906249
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1761,
          "fn": 1439,
          "accuracy": 0.5503125
        },
        "0.01": null
      },
      "auroc": 0.756253125
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1407,
          "fn": 193,
          "accuracy": 0.879375
        },
        "0.01": null
      },
      "auroc": 0.8921973795572917
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2869,
          "fn": 331,
          "accuracy": 0.8965625
        },
        "0.01": null
      },
      "auroc": 0.9037981770833334
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1369,
          "fn": 231,
          "accuracy": 0.855625
        },
        "0.01": null
      },
      "auroc": 0.8860161295572916
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 763,
          "fn": 837,
          "accuracy": 0.476875
        },
        "0.01": null
      },
      "auroc": 0.7495551106770834
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2132,
          "fn": 1068,
          "accuracy": 0.66625
        },
        "0.01": null
      },
      "auroc": 0.8177856201171876
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1277,
          "fn": 323,
          "accuracy": 0.798125
        },
        "0.01": null
      },
      "auroc": 0.869812060546875
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 527,
          "fn": 1073,
          "accuracy": 0.329375
        },
        "0.01": null
      },
      "auroc": 0.6759503255208333
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1804,
          "fn": 1396,
          "accuracy": 0.56375
        },
        "0.01": null
      },
      "auroc": 0.772881193033854
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2646,
          "fn": 554,
          "accuracy": 0.826875
        },
        "0.01": null
      },
      "auroc": 0.8779140950520834
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1290,
          "fn": 1910,
          "accuracy": 0.403125
        },
        "0.01": null
      },
      "auroc": 0.7127527180989582
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 3936,
          "fn": 2464,
          "accuracy": 0.615
        },
        "0.01": null
      },
      "auroc": 0.7953334065755209
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1410,
          "fn": 190,
          "accuracy": 0.88125
        },
        "0.01": null
      },
      "auroc": 0.8979191080729167
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1444,
          "fn": 156,
          "accuracy": 0.9025
        },
        "0.01": null
      },
      "auroc": 0.909751953125
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1117,
          "fn": 483,
          "accuracy": 0.698125
        },
        "0.01": null
      },
      "auroc": 0.8331516927083333
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2561,
          "fn": 639,
          "accuracy": 0.8003125
        },
        "0.01": null
      },
      "auroc": 0.8714518229166666
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1263,
          "fn": 337,
          "accuracy": 0.789375
        },
        "0.01": null
      },
      "auroc": 0.8639337239583333
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 181,
          "fn": 1419,
          "accuracy": 0.113125
        },
        "0.01": null
      },
      "auroc": 0.53807978515625
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1444,
          "fn": 1756,
          "accuracy": 0.45125
        },
        "0.01": null
      },
      "auroc": 0.7010067545572918
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2707,
          "fn": 493,
          "accuracy": 0.8459375
        },
        "0.01": null
      },
      "auroc": 0.8868428385416668
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1298,
          "fn": 1902,
          "accuracy": 0.405625
        },
        "0.01": null
      },
      "auroc": 0.6856157389322917
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 4005,
          "fn": 2395,
          "accuracy": 0.62578125
        },
        "0.01": null
      },
      "auroc": 0.786229288736979
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1493,
          "fn": 107,
          "accuracy": 0.933125
        },
        "0.01": null
      },
      "auroc": 0.91874873046875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1377,
          "fn": 223,
          "accuracy": 0.860625
        },
        "0.01": null
      },
      "auroc": 0.8931591796875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2870,
          "fn": 330,
          "accuracy": 0.896875
        },
        "0.01": null
      },
      "auroc": 0.905953955078125
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1420,
          "fn": 180,
          "accuracy": 0.8875
        },
        "0.01": null
      },
      "auroc": 0.903007080078125
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 968,
          "fn": 632,
          "accuracy": 0.605
        },
        "0.01": null
      },
      "auroc": 0.8035158854166666
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2388,
          "fn": 812,
          "accuracy": 0.74625
        },
        "0.01": null
      },
      "auroc": 0.8532614827473959
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2913,
          "fn": 287,
          "accuracy": 0.9103125
        },
        "0.01": null
      },
      "auroc": 0.9108779052734376
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2345,
          "fn": 855,
          "accuracy": 0.7328125
        },
        "0.01": null
      },
      "auroc": 0.8483375325520832
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 5258,
          "fn": 1142,
          "accuracy": 0.8215625
        },
        "0.01": null
      },
      "auroc": 0.8796077189127605
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1292,
          "fn": 308,
          "accuracy": 0.8075
        },
        "0.01": null
      },
      "auroc": 0.8720829589843749
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1292,
          "fn": 308,
          "accuracy": 0.8075
        },
        "0.01": null
      },
      "auroc": 0.8720829589843749
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1224,
          "fn": 376,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.8573648274739585
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1224,
          "fn": 376,
          "accuracy": 0.765
        },
        "0.01": null
      },
      "auroc": 0.8573648274739585
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2516,
          "fn": 684,
          "accuracy": 0.78625
        },
        "0.01": null
      },
      "auroc": 0.8647238932291667
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2516,
          "fn": 684,
          "accuracy": 0.78625
        },
        "0.01": null
      },
      "auroc": 0.8647238932291667
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1304,
          "fn": 296,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.8756095540364583
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1304,
          "fn": 296,
          "accuracy": 0.815
        },
        "0.01": null
      },
      "auroc": 0.8756095540364583
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1207,
          "fn": 393,
          "accuracy": 0.754375
        },
        "0.01": null
      },
      "auroc": 0.8554114257812498
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1207,
          "fn": 393,
          "accuracy": 0.754375
        },
        "0.01": null
      },
      "auroc": 0.8554114257812498
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2511,
          "fn": 689,
          "accuracy": 0.7846875
        },
        "0.01": null
      },
      "auroc": 0.8655104899088542
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2511,
          "fn": 689,
          "accuracy": 0.7846875
        },
        "0.01": null
      },
      "auroc": 0.8655104899088542
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1499,
          "fn": 101,
          "accuracy": 0.936875
        },
        "0.01": null
      },
      "auroc": 0.916059765625
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1499,
          "fn": 101,
          "accuracy": 0.936875
        },
        "0.01": null
      },
      "auroc": 0.916059765625
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1423,
          "fn": 177,
          "accuracy": 0.889375
        },
        "0.01": null
      },
      "auroc": 0.9036638509114583
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1423,
          "fn": 177,
          "accuracy": 0.889375
        },
        "0.01": null
      },
      "auroc": 0.9036638509114583
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2922,
          "fn": 278,
          "accuracy": 0.913125
        },
        "0.01": null
      },
      "auroc": 0.9098618082682293
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2922,
          "fn": 278,
          "accuracy": 0.913125
        },
        "0.01": null
      },
      "auroc": 0.9098618082682293
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1528,
          "fn": 72,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9231534505208333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1528,
          "fn": 72,
          "accuracy": 0.955
        },
        "0.01": null
      },
      "auroc": 0.9231534505208333
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1331,
          "fn": 269,
          "accuracy": 0.831875
        },
        "0.01": null
      },
      "auroc": 0.87726669921875
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1331,
          "fn": 269,
          "accuracy": 0.831875
        },
        "0.01": null
      },
      "auroc": 0.87726669921875
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2859,
          "fn": 341,
          "accuracy": 0.8934375
        },
        "0.01": null
      },
      "auroc": 0.9002100748697917
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2859,
          "fn": 341,
          "accuracy": 0.8934375
        },
        "0.01": null
      },
      "auroc": 0.9002100748697917
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1320,
          "fn": 280,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.8799018391927083
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1320,
          "fn": 280,
          "accuracy": 0.825
        },
        "0.01": null
      },
      "auroc": 0.8799018391927083
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1221,
          "fn": 379,
          "accuracy": 0.763125
        },
        "0.01": null
      },
      "auroc": 0.8536480305989583
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 1221,
          "fn": 379,
          "accuracy": 0.763125
        },
        "0.01": null
      },
      "auroc": 0.8536480305989583
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2541,
          "fn": 659,
          "accuracy": 0.7940625
        },
        "0.01": null
      },
      "auroc": 0.8667749348958333
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 2541,
          "fn": 659,
          "accuracy": 0.7940625
        },
        "0.01": null
      },
      "auroc": 0.8667749348958333
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "paraphrase",
      "accuracy": {
        "0.05": {
          "tp": 14708,
          "fn": 2892,
          "accuracy": 0.8356818181818182
        },
        "0.01": null
      },
      "auroc": 0.8821028882575759
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1556,
          "fn": 44,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.9332238932291667
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1503,
          "fn": 97,
          "accuracy": 0.939375
        },
        "0.01": null
      },
      "auroc": 0.9242777506510417
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3059,
          "fn": 141,
          "accuracy": 0.9559375
        },
        "0.01": null
      },
      "auroc": 0.9287508219401042
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1548,
          "fn": 52,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9326753255208334
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1188,
          "fn": 412,
          "accuracy": 0.7425
        },
        "0.01": null
      },
      "auroc": 0.8195749511718751
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2736,
          "fn": 464,
          "accuracy": 0.855
        },
        "0.01": null
      },
      "auroc": 0.8761251383463542
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3104,
          "fn": 96,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.932949609375
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2691,
          "fn": 509,
          "accuracy": 0.8409375
        },
        "0.01": null
      },
      "auroc": 0.8719263509114583
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 5795,
          "fn": 605,
          "accuracy": 0.90546875
        },
        "0.01": null
      },
      "auroc": 0.9024379801432292
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1565,
          "fn": 35,
          "accuracy": 0.978125
        },
        "0.01": null
      },
      "auroc": 0.935522900390625
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 1586,
          "accuracy": 0.00875
        },
        "0.01": null
      },
      "auroc": 0.10610428059895835
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1579,
          "fn": 1621,
          "accuracy": 0.4934375
        },
        "0.01": null
      },
      "auroc": 0.5208135904947916
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1182,
          "fn": 418,
          "accuracy": 0.73875
        },
        "0.01": null
      },
      "auroc": 0.8330159505208333
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1,
          "fn": 1599,
          "accuracy": 0.000625
        },
        "0.01": null
      },
      "auroc": 0.054167903645833335
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1183,
          "fn": 2017,
          "accuracy": 0.3696875
        },
        "0.01": null
      },
      "auroc": 0.44359192708333334
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2747,
          "fn": 453,
          "accuracy": 0.8584375
        },
        "0.01": null
      },
      "auroc": 0.8842694254557291
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 3185,
          "accuracy": 0.0046875
        },
        "0.01": null
      },
      "auroc": 0.08013609212239582
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2762,
          "fn": 3638,
          "accuracy": 0.4315625
        },
        "0.01": null
      },
      "auroc": 0.4822027587890625
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1503,
          "fn": 97,
          "accuracy": 0.939375
        },
        "0.01": null
      },
      "auroc": 0.9142803548177083
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1400,
          "fn": 200,
          "accuracy": 0.875
        },
        "0.01": null
      },
      "auroc": 0.8968659830729167
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2903,
          "fn": 297,
          "accuracy": 0.9071875
        },
        "0.01": null
      },
      "auroc": 0.9055731689453125
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1547,
          "fn": 53,
          "accuracy": 0.966875
        },
        "0.01": null
      },
      "auroc": 0.9341905924479167
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1508,
          "fn": 92,
          "accuracy": 0.9425
        },
        "0.01": null
      },
      "auroc": 0.9208635416666666
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3055,
          "fn": 145,
          "accuracy": 0.9546875
        },
        "0.01": null
      },
      "auroc": 0.9275270670572916
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1387,
          "fn": 213,
          "accuracy": 0.866875
        },
        "0.01": null
      },
      "auroc": 0.8856536621093751
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 27,
          "fn": 1573,
          "accuracy": 0.016875
        },
        "0.01": null
      },
      "auroc": 0.33784685872395837
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1414,
          "fn": 1786,
          "accuracy": 0.441875
        },
        "0.01": null
      },
      "auroc": 0.6117502604166667
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2934,
          "fn": 266,
          "accuracy": 0.916875
        },
        "0.01": null
      },
      "auroc": 0.9099221272786457
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1535,
          "fn": 1665,
          "accuracy": 0.4796875
        },
        "0.01": null
      },
      "auroc": 0.6293552001953125
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 4469,
          "fn": 1931,
          "accuracy": 0.69828125
        },
        "0.01": null
      },
      "auroc": 0.7696386637369791
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1552,
          "fn": 48,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9333763509114583
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 908,
          "fn": 692,
          "accuracy": 0.5675
        },
        "0.01": null
      },
      "auroc": 0.734738916015625
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2460,
          "fn": 740,
          "accuracy": 0.76875
        },
        "0.01": null
      },
      "auroc": 0.8340576334635417
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 984,
          "fn": 616,
          "accuracy": 0.615
        },
        "0.01": null
      },
      "auroc": 0.78050693359375
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 1600,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.06207934570312501
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 984,
          "fn": 2216,
          "accuracy": 0.3075
        },
        "0.01": null
      },
      "auroc": 0.42129313964843756
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2536,
          "fn": 664,
          "accuracy": 0.7925
        },
        "0.01": null
      },
      "auroc": 0.8569416422526043
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 908,
          "fn": 2292,
          "accuracy": 0.28375
        },
        "0.01": null
      },
      "auroc": 0.398409130859375
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 3444,
          "fn": 2956,
          "accuracy": 0.538125
        },
        "0.01": null
      },
      "auroc": 0.6276753865559896
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1497,
          "fn": 103,
          "accuracy": 0.935625
        },
        "0.01": null
      },
      "auroc": 0.923152001953125
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1409,
          "fn": 191,
          "accuracy": 0.880625
        },
        "0.01": null
      },
      "auroc": 0.9042972819010416
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2906,
          "fn": 294,
          "accuracy": 0.908125
        },
        "0.01": null
      },
      "auroc": 0.9137246419270834
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1445,
          "fn": 155,
          "accuracy": 0.903125
        },
        "0.01": null
      },
      "auroc": 0.9098216796875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2942,
          "fn": 258,
          "accuracy": 0.919375
        },
        "0.01": null
      },
      "auroc": 0.9164868408203125
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1401,
          "fn": 199,
          "accuracy": 0.875625
        },
        "0.01": null
      },
      "auroc": 0.898965625
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1401,
          "fn": 199,
          "accuracy": 0.875625
        },
        "0.01": null
      },
      "auroc": 0.898965625
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1340,
          "fn": 260,
          "accuracy": 0.8375
        },
        "0.01": null
      },
      "auroc": 0.8854204101562501
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1340,
          "fn": 260,
          "accuracy": 0.8375
        },
        "0.01": null
      },
      "auroc": 0.8854204101562501
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2741,
          "fn": 459,
          "accuracy": 0.8565625
        },
        "0.01": null
      },
      "auroc": 0.8921930175781249
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2741,
          "fn": 459,
          "accuracy": 0.8565625
        },
        "0.01": null
      },
      "auroc": 0.8921930175781249
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1288,
          "fn": 312,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.8673046061197918
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1288,
          "fn": 312,
          "accuracy": 0.805
        },
        "0.01": null
      },
      "auroc": 0.8673046061197918
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1121,
          "fn": 479,
          "accuracy": 0.700625
        },
        "0.01": null
      },
      "auroc": 0.8280252278645832
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1121,
          "fn": 479,
          "accuracy": 0.700625
        },
        "0.01": null
      },
      "auroc": 0.8280252278645832
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2409,
          "fn": 791,
          "accuracy": 0.7528125
        },
        "0.01": null
      },
      "auroc": 0.8476649169921875
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2409,
          "fn": 791,
          "accuracy": 0.7528125
        },
        "0.01": null
      },
      "auroc": 0.8476649169921875
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1547,
          "fn": 53,
          "accuracy": 0.966875
        },
        "0.01": null
      },
      "auroc": 0.928351318359375
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1547,
          "fn": 53,
          "accuracy": 0.966875
        },
        "0.01": null
      },
      "auroc": 0.928351318359375
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1435,
          "fn": 165,
          "accuracy": 0.896875
        },
        "0.01": null
      },
      "auroc": 0.9103591145833334
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1435,
          "fn": 165,
          "accuracy": 0.896875
        },
        "0.01": null
      },
      "auroc": 0.9103591145833334
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2982,
          "fn": 218,
          "accuracy": 0.931875
        },
        "0.01": null
      },
      "auroc": 0.9193552164713542
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2982,
          "fn": 218,
          "accuracy": 0.931875
        },
        "0.01": null
      },
      "auroc": 0.9193552164713542
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1569,
          "fn": 31,
          "accuracy": 0.980625
        },
        "0.01": null
      },
      "auroc": 0.9338625
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1569,
          "fn": 31,
          "accuracy": 0.980625
        },
        "0.01": null
      },
      "auroc": 0.9338625
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 713,
          "fn": 887,
          "accuracy": 0.445625
        },
        "0.01": null
      },
      "auroc": 0.756830712890625
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 713,
          "fn": 887,
          "accuracy": 0.445625
        },
        "0.01": null
      },
      "auroc": 0.756830712890625
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2282,
          "fn": 918,
          "accuracy": 0.713125
        },
        "0.01": null
      },
      "auroc": 0.8453466064453125
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2282,
          "fn": 918,
          "accuracy": 0.713125
        },
        "0.01": null
      },
      "auroc": 0.8453466064453125
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1333,
          "fn": 267,
          "accuracy": 0.833125
        },
        "0.01": null
      },
      "auroc": 0.882794580078125
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1333,
          "fn": 267,
          "accuracy": 0.833125
        },
        "0.01": null
      },
      "auroc": 0.882794580078125
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1143,
          "fn": 457,
          "accuracy": 0.714375
        },
        "0.01": null
      },
      "auroc": 0.8373611490885415
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 1143,
          "fn": 457,
          "accuracy": 0.714375
        },
        "0.01": null
      },
      "auroc": 0.8373611490885415
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2476,
          "fn": 724,
          "accuracy": 0.77375
        },
        "0.01": null
      },
      "auroc": 0.8600778645833334
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 2476,
          "fn": 724,
          "accuracy": 0.77375
        },
        "0.01": null
      },
      "auroc": 0.8600778645833334
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 16358,
          "fn": 1242,
          "accuracy": 0.9294318181818182
        },
        "0.01": null
      },
      "auroc": 0.916820429391572
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 13698,
          "fn": 3902,
          "accuracy": 0.7782954545454546
        },
        "0.01": null
      },
      "auroc": 0.8596851044625947
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "number",
      "accuracy": {
        "0.05": {
          "tp": 30056,
          "fn": 5144,
          "accuracy": 0.8538636363636364
        },
        "0.01": null
      },
      "auroc": 0.8882527669270832
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.943348876953125
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1532,
          "fn": 68,
          "accuracy": 0.9575
        },
        "0.01": null
      },
      "auroc": 0.9326566080729166
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3127,
          "fn": 73,
          "accuracy": 0.9771875
        },
        "0.01": null
      },
      "auroc": 0.9380027425130208
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1589,
          "fn": 11,
          "accuracy": 0.993125
        },
        "0.01": null
      },
      "auroc": 0.9424935384114583
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1250,
          "fn": 350,
          "accuracy": 0.78125
        },
        "0.01": null
      },
      "auroc": 0.8388508463541667
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2839,
          "fn": 361,
          "accuracy": 0.8871875
        },
        "0.01": null
      },
      "auroc": 0.8906721923828126
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3184,
          "fn": 16,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9429212076822916
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2782,
          "fn": 418,
          "accuracy": 0.869375
        },
        "0.01": null
      },
      "auroc": 0.8857537272135417
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 5966,
          "fn": 434,
          "accuracy": 0.9321875
        },
        "0.01": null
      },
      "auroc": 0.9143374674479166
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1574,
          "fn": 26,
          "accuracy": 0.98375
        },
        "0.01": null
      },
      "auroc": 0.9398739908854167
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 14,
          "fn": 1586,
          "accuracy": 0.00875
        },
        "0.01": null
      },
      "auroc": 0.11024272460937501
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1588,
          "fn": 1612,
          "accuracy": 0.49625
        },
        "0.01": null
      },
      "auroc": 0.5250583577473958
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1371,
          "fn": 229,
          "accuracy": 0.856875
        },
        "0.01": null
      },
      "auroc": 0.8803102213541667
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 1598,
          "accuracy": 0.00125
        },
        "0.01": null
      },
      "auroc": 0.05561738281250002
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1373,
          "fn": 1827,
          "accuracy": 0.4290625
        },
        "0.01": null
      },
      "auroc": 0.4679638020833333
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2945,
          "fn": 255,
          "accuracy": 0.9203125
        },
        "0.01": null
      },
      "auroc": 0.9100921061197916
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 16,
          "fn": 3184,
          "accuracy": 0.005
        },
        "0.01": null
      },
      "auroc": 0.0829300537109375
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2961,
          "fn": 3439,
          "accuracy": 0.46265625
        },
        "0.01": null
      },
      "auroc": 0.49651107991536464
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1545,
          "fn": 55,
          "accuracy": 0.965625
        },
        "0.01": null
      },
      "auroc": 0.9242080891927082
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1452,
          "fn": 148,
          "accuracy": 0.9075
        },
        "0.01": null
      },
      "auroc": 0.909295703125
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2997,
          "fn": 203,
          "accuracy": 0.9365625
        },
        "0.01": null
      },
      "auroc": 0.9167518961588542
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1553,
          "fn": 47,
          "accuracy": 0.970625
        },
        "0.01": null
      },
      "auroc": 0.9373106608072916
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1574,
          "fn": 26,
          "accuracy": 0.98375
        },
        "0.01": null
      },
      "auroc": 0.9379803059895834
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3127,
          "fn": 73,
          "accuracy": 0.9771875
        },
        "0.01": null
      },
      "auroc": 0.9376454833984376
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1448,
          "fn": 152,
          "accuracy": 0.905
        },
        "0.01": null
      },
      "auroc": 0.906949658203125
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 47,
          "fn": 1553,
          "accuracy": 0.029375
        },
        "0.01": null
      },
      "auroc": 0.40106980794270836
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1495,
          "fn": 1705,
          "accuracy": 0.4671875
        },
        "0.01": null
      },
      "auroc": 0.6540097330729167
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3001,
          "fn": 199,
          "accuracy": 0.9378125
        },
        "0.01": null
      },
      "auroc": 0.9221301595052085
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1621,
          "fn": 1579,
          "accuracy": 0.5065625
        },
        "0.01": null
      },
      "auroc": 0.6695250569661457
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 4622,
          "fn": 1778,
          "accuracy": 0.7221875
        },
        "0.01": null
      },
      "auroc": 0.7958276082356772
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1569,
          "fn": 31,
          "accuracy": 0.980625
        },
        "0.01": null
      },
      "auroc": 0.9386255859375
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1056,
          "fn": 544,
          "accuracy": 0.66
        },
        "0.01": null
      },
      "auroc": 0.7851226074218751
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2625,
          "fn": 575,
          "accuracy": 0.8203125
        },
        "0.01": null
      },
      "auroc": 0.8618740966796875
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1081,
          "fn": 519,
          "accuracy": 0.675625
        },
        "0.01": null
      },
      "auroc": 0.8302679036458334
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 1600,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.06790509440104167
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1081,
          "fn": 2119,
          "accuracy": 0.3378125
        },
        "0.01": null
      },
      "auroc": 0.4490864990234375
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2650,
          "fn": 550,
          "accuracy": 0.828125
        },
        "0.01": null
      },
      "auroc": 0.8844467447916667
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1056,
          "fn": 2144,
          "accuracy": 0.33
        },
        "0.01": null
      },
      "auroc": 0.4265138509114583
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3706,
          "fn": 2694,
          "accuracy": 0.5790625
        },
        "0.01": null
      },
      "auroc": 0.6554802978515626
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1579,
          "fn": 21,
          "accuracy": 0.986875
        },
        "0.01": null
      },
      "auroc": 0.9381923828125001
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1469,
          "fn": 131,
          "accuracy": 0.918125
        },
        "0.01": null
      },
      "auroc": 0.9166932942708332
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3048,
          "fn": 152,
          "accuracy": 0.9525
        },
        "0.01": null
      },
      "auroc": 0.9274428385416666
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1517,
          "fn": 83,
          "accuracy": 0.948125
        },
        "0.01": null
      },
      "auroc": 0.9261952799479167
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3096,
          "fn": 104,
          "accuracy": 0.9675
        },
        "0.01": null
      },
      "auroc": 0.9321938313802084
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1535,
          "fn": 65,
          "accuracy": 0.959375
        },
        "0.01": null
      },
      "auroc": 0.925011083984375
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1535,
          "fn": 65,
          "accuracy": 0.959375
        },
        "0.01": null
      },
      "auroc": 0.925011083984375
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1475,
          "fn": 125,
          "accuracy": 0.921875
        },
        "0.01": null
      },
      "auroc": 0.9132152669270834
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1475,
          "fn": 125,
          "accuracy": 0.921875
        },
        "0.01": null
      },
      "auroc": 0.9132152669270834
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3010,
          "fn": 190,
          "accuracy": 0.940625
        },
        "0.01": null
      },
      "auroc": 0.9191131754557291
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3010,
          "fn": 190,
          "accuracy": 0.940625
        },
        "0.01": null
      },
      "auroc": 0.9191131754557291
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1434,
          "fn": 166,
          "accuracy": 0.89625
        },
        "0.01": null
      },
      "auroc": 0.9025862467447916
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1434,
          "fn": 166,
          "accuracy": 0.89625
        },
        "0.01": null
      },
      "auroc": 0.9025862467447916
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1262,
          "fn": 338,
          "accuracy": 0.78875
        },
        "0.01": null
      },
      "auroc": 0.8659289876302083
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1262,
          "fn": 338,
          "accuracy": 0.78875
        },
        "0.01": null
      },
      "auroc": 0.8659289876302083
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2696,
          "fn": 504,
          "accuracy": 0.8425
        },
        "0.01": null
      },
      "auroc": 0.8842576171875001
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2696,
          "fn": 504,
          "accuracy": 0.8425
        },
        "0.01": null
      },
      "auroc": 0.8842576171875001
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1566,
          "fn": 34,
          "accuracy": 0.97875
        },
        "0.01": null
      },
      "auroc": 0.934691552734375
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1566,
          "fn": 34,
          "accuracy": 0.97875
        },
        "0.01": null
      },
      "auroc": 0.934691552734375
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1462,
          "fn": 138,
          "accuracy": 0.91375
        },
        "0.01": null
      },
      "auroc": 0.9162248697916666
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1462,
          "fn": 138,
          "accuracy": 0.91375
        },
        "0.01": null
      },
      "auroc": 0.9162248697916666
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3028,
          "fn": 172,
          "accuracy": 0.94625
        },
        "0.01": null
      },
      "auroc": 0.9254582112630207
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 3028,
          "fn": 172,
          "accuracy": 0.94625
        },
        "0.01": null
      },
      "auroc": 0.9254582112630207
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1578,
          "fn": 22,
          "accuracy": 0.98625
        },
        "0.01": null
      },
      "auroc": 0.9384365234375001
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1578,
          "fn": 22,
          "accuracy": 0.98625
        },
        "0.01": null
      },
      "auroc": 0.9384365234375001
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 808,
          "fn": 792,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.7693672037760417
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 808,
          "fn": 792,
          "accuracy": 0.505
        },
        "0.01": null
      },
      "auroc": 0.7693672037760417
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2386,
          "fn": 814,
          "accuracy": 0.745625
        },
        "0.01": null
      },
      "auroc": 0.8539018636067707
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2386,
          "fn": 814,
          "accuracy": 0.745625
        },
        "0.01": null
      },
      "auroc": 0.8539018636067707
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1440,
          "fn": 160,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9070324544270834
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1440,
          "fn": 160,
          "accuracy": 0.9
        },
        "0.01": null
      },
      "auroc": 0.9070324544270834
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1293,
          "fn": 307,
          "accuracy": 0.808125
        },
        "0.01": null
      },
      "auroc": 0.8661705891927083
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 1293,
          "fn": 307,
          "accuracy": 0.808125
        },
        "0.01": null
      },
      "auroc": 0.8661705891927083
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2733,
          "fn": 467,
          "accuracy": 0.8540625
        },
        "0.01": null
      },
      "auroc": 0.8866015218098957
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 2733,
          "fn": 467,
          "accuracy": 0.8540625
        },
        "0.01": null
      },
      "auroc": 0.8866015218098957
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 16968,
          "fn": 632,
          "accuracy": 0.9640909090909091
        },
        "0.01": null
      },
      "auroc": 0.9299379498106062
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 14758,
          "fn": 2842,
          "accuracy": 0.8385227272727273
        },
        "0.01": null
      },
      "auroc": 0.8842199292732008
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "insert_paragraphs",
      "accuracy": {
        "0.05": {
          "tp": 31726,
          "fn": 3474,
          "accuracy": 0.9013068181818182
        },
        "0.01": null
      },
      "auroc": 0.9070789395419032
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1360,
          "fn": 240,
          "accuracy": 0.85
        },
        "0.01": null
      },
      "auroc": 0.8891505371093751
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 948,
          "fn": 652,
          "accuracy": 0.5925
        },
        "0.01": null
      },
      "auroc": 0.80301669921875
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2308,
          "fn": 892,
          "accuracy": 0.72125
        },
        "0.01": null
      },
      "auroc": 0.8460836181640625
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1344,
          "fn": 256,
          "accuracy": 0.84
        },
        "0.01": null
      },
      "auroc": 0.8787658040364583
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 669,
          "fn": 931,
          "accuracy": 0.418125
        },
        "0.01": null
      },
      "auroc": 0.71512607421875
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2013,
          "fn": 1187,
          "accuracy": 0.6290625
        },
        "0.01": null
      },
      "auroc": 0.7969459391276043
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2704,
          "fn": 496,
          "accuracy": 0.845
        },
        "0.01": null
      },
      "auroc": 0.8839581705729167
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1617,
          "fn": 1583,
          "accuracy": 0.5053125
        },
        "0.01": null
      },
      "auroc": 0.75907138671875
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4321,
          "fn": 2079,
          "accuracy": 0.67515625
        },
        "0.01": null
      },
      "auroc": 0.8215147786458334
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1531,
          "fn": 69,
          "accuracy": 0.956875
        },
        "0.01": null
      },
      "auroc": 0.9282960286458333
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 102,
          "fn": 1498,
          "accuracy": 0.06375
        },
        "0.01": null
      },
      "auroc": 0.31082739257812503
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1633,
          "fn": 1567,
          "accuracy": 0.5103125
        },
        "0.01": null
      },
      "auroc": 0.6195617106119792
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1414,
          "fn": 186,
          "accuracy": 0.88375
        },
        "0.01": null
      },
      "auroc": 0.8927918131510417
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 160,
          "fn": 1440,
          "accuracy": 0.1
        },
        "0.01": null
      },
      "auroc": 0.3442565104166667
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1574,
          "fn": 1626,
          "accuracy": 0.491875
        },
        "0.01": null
      },
      "auroc": 0.6185241617838542
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2945,
          "fn": 255,
          "accuracy": 0.9203125
        },
        "0.01": null
      },
      "auroc": 0.9105439208984374
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 262,
          "fn": 2938,
          "accuracy": 0.081875
        },
        "0.01": null
      },
      "auroc": 0.32754195149739584
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3207,
          "fn": 3193,
          "accuracy": 0.50109375
        },
        "0.01": null
      },
      "auroc": 0.6190429361979166
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 830,
          "fn": 770,
          "accuracy": 0.51875
        },
        "0.01": null
      },
      "auroc": 0.7648333170572916
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 751,
          "fn": 849,
          "accuracy": 0.469375
        },
        "0.01": null
      },
      "auroc": 0.7453773274739584
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1581,
          "fn": 1619,
          "accuracy": 0.4940625
        },
        "0.01": null
      },
      "auroc": 0.755105322265625
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1495,
          "fn": 105,
          "accuracy": 0.934375
        },
        "0.01": null
      },
      "auroc": 0.9138209798177084
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1130,
          "fn": 470,
          "accuracy": 0.70625
        },
        "0.01": null
      },
      "auroc": 0.8428262044270832
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2625,
          "fn": 575,
          "accuracy": 0.8203125
        },
        "0.01": null
      },
      "auroc": 0.8783235921223957
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1372,
          "fn": 228,
          "accuracy": 0.8575
        },
        "0.01": null
      },
      "auroc": 0.8813705729166668
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 334,
          "fn": 1266,
          "accuracy": 0.20875
        },
        "0.01": null
      },
      "auroc": 0.620946484375
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1706,
          "fn": 1494,
          "accuracy": 0.533125
        },
        "0.01": null
      },
      "auroc": 0.7511585286458333
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2867,
          "fn": 333,
          "accuracy": 0.8959375
        },
        "0.01": null
      },
      "auroc": 0.8975957763671873
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1464,
          "fn": 1736,
          "accuracy": 0.4575
        },
        "0.01": null
      },
      "auroc": 0.7318863444010417
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 4331,
          "fn": 2069,
          "accuracy": 0.67671875
        },
        "0.01": null
      },
      "auroc": 0.8147410603841145
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1518,
          "fn": 82,
          "accuracy": 0.94875
        },
        "0.01": null
      },
      "auroc": 0.9221151204427084
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 337,
          "fn": 1263,
          "accuracy": 0.210625
        },
        "0.01": null
      },
      "auroc": 0.6242597330729167
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1855,
          "fn": 1345,
          "accuracy": 0.5796875
        },
        "0.01": null
      },
      "auroc": 0.7731874267578125
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1242,
          "fn": 358,
          "accuracy": 0.77625
        },
        "0.01": null
      },
      "auroc": 0.8570552734375001
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 90,
          "fn": 1510,
          "accuracy": 0.05625
        },
        "0.01": null
      },
      "auroc": 0.41436494140625
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1332,
          "fn": 1868,
          "accuracy": 0.41625
        },
        "0.01": null
      },
      "auroc": 0.6357101074218751
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2760,
          "fn": 440,
          "accuracy": 0.8625
        },
        "0.01": null
      },
      "auroc": 0.8895851969401041
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 427,
          "fn": 2773,
          "accuracy": 0.1334375
        },
        "0.01": null
      },
      "auroc": 0.5193123372395834
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 3187,
          "fn": 3213,
          "accuracy": 0.49796875
        },
        "0.01": null
      },
      "auroc": 0.7044487670898437
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1189,
          "fn": 411,
          "accuracy": 0.743125
        },
        "0.01": null
      },
      "auroc": 0.8432456868489584
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 601,
          "fn": 999,
          "accuracy": 0.375625
        },
        "0.01": null
      },
      "auroc": 0.711068408203125
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1790,
          "fn": 1410,
          "accuracy": 0.559375
        },
        "0.01": null
      },
      "auroc": 0.7771570475260418
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1043,
          "fn": 557,
          "accuracy": 0.651875
        },
        "0.01": null
      },
      "auroc": 0.8156451822916666
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2232,
          "fn": 968,
          "accuracy": 0.6975
        },
        "0.01": null
      },
      "auroc": 0.8294454345703124
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 964,
          "fn": 636,
          "accuracy": 0.6025
        },
        "0.01": null
      },
      "auroc": 0.8018690755208333
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 964,
          "fn": 636,
          "accuracy": 0.6025
        },
        "0.01": null
      },
      "auroc": 0.8018690755208333
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 902,
          "fn": 698,
          "accuracy": 0.56375
        },
        "0.01": null
      },
      "auroc": 0.7788104329427084
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 902,
          "fn": 698,
          "accuracy": 0.56375
        },
        "0.01": null
      },
      "auroc": 0.7788104329427084
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1866,
          "fn": 1334,
          "accuracy": 0.583125
        },
        "0.01": null
      },
      "auroc": 0.7903397542317707
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1866,
          "fn": 1334,
          "accuracy": 0.583125
        },
        "0.01": null
      },
      "auroc": 0.7903397542317707
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1203,
          "fn": 397,
          "accuracy": 0.751875
        },
        "0.01": null
      },
      "auroc": 0.84933603515625
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1203,
          "fn": 397,
          "accuracy": 0.751875
        },
        "0.01": null
      },
      "auroc": 0.84933603515625
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1092,
          "fn": 508,
          "accuracy": 0.6825
        },
        "0.01": null
      },
      "auroc": 0.8251832194010418
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1092,
          "fn": 508,
          "accuracy": 0.6825
        },
        "0.01": null
      },
      "auroc": 0.8251832194010418
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2295,
          "fn": 905,
          "accuracy": 0.7171875
        },
        "0.01": null
      },
      "auroc": 0.8372596272786458
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 2295,
          "fn": 905,
          "accuracy": 0.7171875
        },
        "0.01": null
      },
      "auroc": 0.8372596272786458
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1027,
          "fn": 573,
          "accuracy": 0.641875
        },
        "0.01": null
      },
      "auroc": 0.81645322265625
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1027,
          "fn": 573,
          "accuracy": 0.641875
        },
        "0.01": null
      },
      "auroc": 0.81645322265625
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 895,
          "fn": 705,
          "accuracy": 0.559375
        },
        "0.01": null
      },
      "auroc": 0.7854715657552083
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 895,
          "fn": 705,
          "accuracy": 0.559375
        },
        "0.01": null
      },
      "auroc": 0.7854715657552083
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1922,
          "fn": 1278,
          "accuracy": 0.600625
        },
        "0.01": null
      },
      "auroc": 0.8009623942057292
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1922,
          "fn": 1278,
          "accuracy": 0.600625
        },
        "0.01": null
      },
      "auroc": 0.8009623942057292
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1147,
          "fn": 453,
          "accuracy": 0.716875
        },
        "0.01": null
      },
      "auroc": 0.8418066080729166
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1147,
          "fn": 453,
          "accuracy": 0.716875
        },
        "0.01": null
      },
      "auroc": 0.8418066080729166
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 775,
          "fn": 825,
          "accuracy": 0.484375
        },
        "0.01": null
      },
      "auroc": 0.7467203776041667
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 775,
          "fn": 825,
          "accuracy": 0.484375
        },
        "0.01": null
      },
      "auroc": 0.7467203776041667
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1922,
          "fn": 1278,
          "accuracy": 0.600625
        },
        "0.01": null
      },
      "auroc": 0.7942634928385418
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1922,
          "fn": 1278,
          "accuracy": 0.600625
        },
        "0.01": null
      },
      "auroc": 0.7942634928385418
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 999,
          "fn": 601,
          "accuracy": 0.624375
        },
        "0.01": null
      },
      "auroc": 0.8125046549479166
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 999,
          "fn": 601,
          "accuracy": 0.624375
        },
        "0.01": null
      },
      "auroc": 0.8125046549479166
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 855,
          "fn": 745,
          "accuracy": 0.534375
        },
        "0.01": null
      },
      "auroc": 0.7744706217447916
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 855,
          "fn": 745,
          "accuracy": 0.534375
        },
        "0.01": null
      },
      "auroc": 0.7744706217447916
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1854,
          "fn": 1346,
          "accuracy": 0.579375
        },
        "0.01": null
      },
      "auroc": 0.7934876383463542
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 1854,
          "fn": 1346,
          "accuracy": 0.579375
        },
        "0.01": null
      },
      "auroc": 0.7934876383463542
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 13263,
          "fn": 4337,
          "accuracy": 0.7535795454545454
        },
        "0.01": null
      },
      "auroc": 0.8530392060250949
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 11685,
          "fn": 5915,
          "accuracy": 0.6639204545454546
        },
        "0.01": null
      },
      "auroc": 0.8165147446141099
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "homoglyph",
      "accuracy": {
        "0.05": {
          "tp": 24948,
          "fn": 10252,
          "accuracy": 0.70875
        },
        "0.01": null
      },
      "auroc": 0.8347769753196022
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1590,
          "fn": 10,
          "accuracy": 0.99375
        },
        "0.01": null
      },
      "auroc": 0.9421195475260415
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1519,
          "fn": 81,
          "accuracy": 0.949375
        },
        "0.01": null
      },
      "auroc": 0.9268339518229166
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3109,
          "fn": 91,
          "accuracy": 0.9715625
        },
        "0.01": null
      },
      "auroc": 0.9344767496744791
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1584,
          "fn": 16,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9411344401041667
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1172,
          "fn": 428,
          "accuracy": 0.7325
        },
        "0.01": null
      },
      "auroc": 0.8071051106770833
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2756,
          "fn": 444,
          "accuracy": 0.86125
        },
        "0.01": null
      },
      "auroc": 0.874119775390625
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3174,
          "fn": 26,
          "accuracy": 0.991875
        },
        "0.01": null
      },
      "auroc": 0.9416269938151042
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2691,
          "fn": 509,
          "accuracy": 0.8409375
        },
        "0.01": null
      },
      "auroc": 0.8669695312500001
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 5865,
          "fn": 535,
          "accuracy": 0.91640625
        },
        "0.01": null
      },
      "auroc": 0.9042982625325521
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1564,
          "fn": 36,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9359377766927084
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 15,
          "fn": 1585,
          "accuracy": 0.009375
        },
        "0.01": null
      },
      "auroc": 0.11110696614583335
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1579,
          "fn": 1621,
          "accuracy": 0.4934375
        },
        "0.01": null
      },
      "auroc": 0.5235223714192708
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 891,
          "fn": 709,
          "accuracy": 0.556875
        },
        "0.01": null
      },
      "auroc": 0.7847898925781249
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 1598,
          "accuracy": 0.00125
        },
        "0.01": null
      },
      "auroc": 0.05583955078125001
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 893,
          "fn": 2307,
          "accuracy": 0.2790625
        },
        "0.01": null
      },
      "auroc": 0.4203147216796875
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2455,
          "fn": 745,
          "accuracy": 0.7671875
        },
        "0.01": null
      },
      "auroc": 0.8603638346354167
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 17,
          "fn": 3183,
          "accuracy": 0.0053125
        },
        "0.01": null
      },
      "auroc": 0.08347325846354167
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2472,
          "fn": 3928,
          "accuracy": 0.38625
        },
        "0.01": null
      },
      "auroc": 0.4719185465494792
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1453,
          "fn": 147,
          "accuracy": 0.908125
        },
        "0.01": null
      },
      "auroc": 0.9004323404947917
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1315,
          "fn": 285,
          "accuracy": 0.821875
        },
        "0.01": null
      },
      "auroc": 0.8754535807291666
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2768,
          "fn": 432,
          "accuracy": 0.865
        },
        "0.01": null
      },
      "auroc": 0.8879429606119792
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1543,
          "fn": 57,
          "accuracy": 0.964375
        },
        "0.01": null
      },
      "auroc": 0.9339337076822917
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1575,
          "fn": 25,
          "accuracy": 0.984375
        },
        "0.01": null
      },
      "auroc": 0.9372876464843749
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3118,
          "fn": 82,
          "accuracy": 0.974375
        },
        "0.01": null
      },
      "auroc": 0.9356106770833332
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1298,
          "fn": 302,
          "accuracy": 0.81125
        },
        "0.01": null
      },
      "auroc": 0.8683379720052085
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 1589,
          "accuracy": 0.006875
        },
        "0.01": null
      },
      "auroc": 0.27720639648437506
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1309,
          "fn": 1891,
          "accuracy": 0.4090625
        },
        "0.01": null
      },
      "auroc": 0.5727721842447917
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2841,
          "fn": 359,
          "accuracy": 0.8878125
        },
        "0.01": null
      },
      "auroc": 0.90113583984375
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1586,
          "fn": 1614,
          "accuracy": 0.495625
        },
        "0.01": null
      },
      "auroc": 0.6072470214843749
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 4427,
          "fn": 1973,
          "accuracy": 0.69171875
        },
        "0.01": null
      },
      "auroc": 0.7541914306640625
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1549,
          "fn": 51,
          "accuracy": 0.968125
        },
        "0.01": null
      },
      "auroc": 0.9335843587239583
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 927,
          "fn": 673,
          "accuracy": 0.579375
        },
        "0.01": null
      },
      "auroc": 0.746159716796875
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2476,
          "fn": 724,
          "accuracy": 0.77375
        },
        "0.01": null
      },
      "auroc": 0.8398720377604167
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 666,
          "fn": 934,
          "accuracy": 0.41625
        },
        "0.01": null
      },
      "auroc": 0.7263143717447917
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 1600,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.05714741210937501
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 666,
          "fn": 2534,
          "accuracy": 0.208125
        },
        "0.01": null
      },
      "auroc": 0.3917308919270833
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2215,
          "fn": 985,
          "accuracy": 0.6921875
        },
        "0.01": null
      },
      "auroc": 0.829949365234375
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 927,
          "fn": 2273,
          "accuracy": 0.2896875
        },
        "0.01": null
      },
      "auroc": 0.40165356445312506
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3142,
          "fn": 3258,
          "accuracy": 0.4909375
        },
        "0.01": null
      },
      "auroc": 0.61580146484375
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1571,
          "fn": 29,
          "accuracy": 0.981875
        },
        "0.01": null
      },
      "auroc": 0.9339088216145834
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1437,
          "fn": 163,
          "accuracy": 0.898125
        },
        "0.01": null
      },
      "auroc": 0.906942041015625
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3008,
          "fn": 192,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9204254313151041
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1453,
          "fn": 147,
          "accuracy": 0.908125
        },
        "0.01": null
      },
      "auroc": 0.9091003255208334
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 3024,
          "fn": 176,
          "accuracy": 0.945
        },
        "0.01": null
      },
      "auroc": 0.9215045735677084
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1480,
          "fn": 120,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9119449707031251
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1480,
          "fn": 120,
          "accuracy": 0.925
        },
        "0.01": null
      },
      "auroc": 0.9119449707031251
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1369,
          "fn": 231,
          "accuracy": 0.855625
        },
        "0.01": null
      },
      "auroc": 0.887776123046875
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1369,
          "fn": 231,
          "accuracy": 0.855625
        },
        "0.01": null
      },
      "auroc": 0.887776123046875
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2849,
          "fn": 351,
          "accuracy": 0.8903125
        },
        "0.01": null
      },
      "auroc": 0.8998605468750001
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2849,
          "fn": 351,
          "accuracy": 0.8903125
        },
        "0.01": null
      },
      "auroc": 0.8998605468750001
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1235,
          "fn": 365,
          "accuracy": 0.771875
        },
        "0.01": null
      },
      "auroc": 0.8562256673177082
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1235,
          "fn": 365,
          "accuracy": 0.771875
        },
        "0.01": null
      },
      "auroc": 0.8562256673177082
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 941,
          "fn": 659,
          "accuracy": 0.588125
        },
        "0.01": null
      },
      "auroc": 0.788711669921875
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 941,
          "fn": 659,
          "accuracy": 0.588125
        },
        "0.01": null
      },
      "auroc": 0.788711669921875
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2176,
          "fn": 1024,
          "accuracy": 0.68
        },
        "0.01": null
      },
      "auroc": 0.8224686686197917
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2176,
          "fn": 1024,
          "accuracy": 0.68
        },
        "0.01": null
      },
      "auroc": 0.8224686686197917
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1537,
          "fn": 63,
          "accuracy": 0.960625
        },
        "0.01": null
      },
      "auroc": 0.9244202311197915
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1537,
          "fn": 63,
          "accuracy": 0.960625
        },
        "0.01": null
      },
      "auroc": 0.9244202311197915
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1406,
          "fn": 194,
          "accuracy": 0.87875
        },
        "0.01": null
      },
      "auroc": 0.8950721842447917
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1406,
          "fn": 194,
          "accuracy": 0.87875
        },
        "0.01": null
      },
      "auroc": 0.8950721842447917
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2943,
          "fn": 257,
          "accuracy": 0.9196875
        },
        "0.01": null
      },
      "auroc": 0.9097462076822916
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2943,
          "fn": 257,
          "accuracy": 0.9196875
        },
        "0.01": null
      },
      "auroc": 0.9097462076822916
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1556,
          "fn": 44,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.92934677734375
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1556,
          "fn": 44,
          "accuracy": 0.9725
        },
        "0.01": null
      },
      "auroc": 0.92934677734375
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 475,
          "fn": 1125,
          "accuracy": 0.296875
        },
        "0.01": null
      },
      "auroc": 0.6731868326822917
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 475,
          "fn": 1125,
          "accuracy": 0.296875
        },
        "0.01": null
      },
      "auroc": 0.6731868326822917
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2031,
          "fn": 1169,
          "accuracy": 0.6346875
        },
        "0.01": null
      },
      "auroc": 0.8012668050130207
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2031,
          "fn": 1169,
          "accuracy": 0.6346875
        },
        "0.01": null
      },
      "auroc": 0.8012668050130207
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1286,
          "fn": 314,
          "accuracy": 0.80375
        },
        "0.01": null
      },
      "auroc": 0.8703166829427085
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1286,
          "fn": 314,
          "accuracy": 0.80375
        },
        "0.01": null
      },
      "auroc": 0.8703166829427085
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1072,
          "fn": 528,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.8058430013020833
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 1072,
          "fn": 528,
          "accuracy": 0.67
        },
        "0.01": null
      },
      "auroc": 0.8058430013020833
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2358,
          "fn": 842,
          "accuracy": 0.736875
        },
        "0.01": null
      },
      "auroc": 0.8380798421223958
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 2358,
          "fn": 842,
          "accuracy": 0.736875
        },
        "0.01": null
      },
      "auroc": 0.8380798421223958
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 16364,
          "fn": 1236,
          "accuracy": 0.9297727272727273
        },
        "0.01": null
      },
      "auroc": 0.9156518983783144
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 12470,
          "fn": 5130,
          "accuracy": 0.7085227272727272
        },
        "0.01": null
      },
      "auroc": 0.8323382176254736
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "article_deletion",
      "accuracy": {
        "0.05": {
          "tp": 28834,
          "fn": 6366,
          "accuracy": 0.8191477272727272
        },
        "0.01": null
      },
      "auroc": 0.873995058001894
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9435997233072916
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1545,
          "fn": 55,
          "accuracy": 0.965625
        },
        "0.01": null
      },
      "auroc": 0.934260546875
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3140,
          "fn": 60,
          "accuracy": 0.98125
        },
        "0.01": null
      },
      "auroc": 0.9389301350911459
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9431305338541667
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1252,
          "fn": 348,
          "accuracy": 0.7825
        },
        "0.01": null
      },
      "auroc": 0.8333748535156249
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2849,
          "fn": 351,
          "accuracy": 0.8903125
        },
        "0.01": null
      },
      "auroc": 0.8882526936848959
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3192,
          "fn": 8,
          "accuracy": 0.9975
        },
        "0.01": null
      },
      "auroc": 0.9433651285807292
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2797,
          "fn": 403,
          "accuracy": 0.8740625
        },
        "0.01": null
      },
      "auroc": 0.8838177001953126
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 5989,
          "fn": 411,
          "accuracy": 0.93578125
        },
        "0.01": null
      },
      "auroc": 0.9135914143880207
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1578,
          "fn": 22,
          "accuracy": 0.98625
        },
        "0.01": null
      },
      "auroc": 0.9403482584635416
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 9,
          "fn": 1591,
          "accuracy": 0.005625
        },
        "0.01": null
      },
      "auroc": 0.10397083333333333
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1587,
          "fn": 1613,
          "accuracy": 0.4959375
        },
        "0.01": null
      },
      "auroc": 0.5221595458984376
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1305,
          "fn": 295,
          "accuracy": 0.815625
        },
        "0.01": null
      },
      "auroc": 0.86492548828125
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2,
          "fn": 1598,
          "accuracy": 0.00125
        },
        "0.01": null
      },
      "auroc": 0.05542869466145834
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1307,
          "fn": 1893,
          "accuracy": 0.4084375
        },
        "0.01": null
      },
      "auroc": 0.4601770914713542
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2883,
          "fn": 317,
          "accuracy": 0.9009375
        },
        "0.01": null
      },
      "auroc": 0.9026368733723958
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 11,
          "fn": 3189,
          "accuracy": 0.0034375
        },
        "0.01": null
      },
      "auroc": 0.07969976399739584
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2894,
          "fn": 3506,
          "accuracy": 0.4521875
        },
        "0.01": null
      },
      "auroc": 0.49116831868489585
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1552,
          "fn": 48,
          "accuracy": 0.97
        },
        "0.01": null
      },
      "auroc": 0.9244583821614584
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1471,
          "fn": 129,
          "accuracy": 0.919375
        },
        "0.01": null
      },
      "auroc": 0.910969287109375
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3023,
          "fn": 177,
          "accuracy": 0.9446875
        },
        "0.01": null
      },
      "auroc": 0.9177138346354167
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1558,
          "fn": 42,
          "accuracy": 0.97375
        },
        "0.01": null
      },
      "auroc": 0.9378347330729166
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1579,
          "fn": 21,
          "accuracy": 0.986875
        },
        "0.01": null
      },
      "auroc": 0.9385390787760417
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3137,
          "fn": 63,
          "accuracy": 0.9803125
        },
        "0.01": null
      },
      "auroc": 0.9381869059244792
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1447,
          "fn": 153,
          "accuracy": 0.904375
        },
        "0.01": null
      },
      "auroc": 0.9041667968749998
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 25,
          "fn": 1575,
          "accuracy": 0.015625
        },
        "0.01": null
      },
      "auroc": 0.33229941406249996
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1472,
          "fn": 1728,
          "accuracy": 0.46
        },
        "0.01": null
      },
      "auroc": 0.61823310546875
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3005,
          "fn": 195,
          "accuracy": 0.9390625
        },
        "0.01": null
      },
      "auroc": 0.9210007649739583
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1604,
          "fn": 1596,
          "accuracy": 0.50125
        },
        "0.01": null
      },
      "auroc": 0.6354192464192709
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 4609,
          "fn": 1791,
          "accuracy": 0.72015625
        },
        "0.01": null
      },
      "auroc": 0.7782100056966146
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1569,
          "fn": 31,
          "accuracy": 0.980625
        },
        "0.01": null
      },
      "auroc": 0.9391298665364582
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1015,
          "fn": 585,
          "accuracy": 0.634375
        },
        "0.01": null
      },
      "auroc": 0.7640803059895833
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2584,
          "fn": 616,
          "accuracy": 0.8075
        },
        "0.01": null
      },
      "auroc": 0.8516050862630209
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1067,
          "fn": 533,
          "accuracy": 0.666875
        },
        "0.01": null
      },
      "auroc": 0.819113330078125
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 0,
          "fn": 1600,
          "accuracy": 0.0
        },
        "0.01": null
      },
      "auroc": 0.06095118815104166
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1067,
          "fn": 2133,
          "accuracy": 0.3334375
        },
        "0.01": null
      },
      "auroc": 0.4400322591145833
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2636,
          "fn": 564,
          "accuracy": 0.82375
        },
        "0.01": null
      },
      "auroc": 0.8791215983072916
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1015,
          "fn": 2185,
          "accuracy": 0.3171875
        },
        "0.01": null
      },
      "auroc": 0.41251574707031247
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3651,
          "fn": 2749,
          "accuracy": 0.57046875
        },
        "0.01": null
      },
      "auroc": 0.6458186726888021
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1588,
          "fn": 12,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9401070149739583
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1504,
          "fn": 96,
          "accuracy": 0.94
        },
        "0.01": null
      },
      "auroc": 0.9217291666666667
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3092,
          "fn": 108,
          "accuracy": 0.96625
        },
        "0.01": null
      },
      "auroc": 0.9309180908203125
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1533,
          "fn": 67,
          "accuracy": 0.958125
        },
        "0.01": null
      },
      "auroc": 0.9285103027343751
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3121,
          "fn": 79,
          "accuracy": 0.9753125
        },
        "0.01": null
      },
      "auroc": 0.9343086588541667
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1522,
          "fn": 78,
          "accuracy": 0.95125
        },
        "0.01": null
      },
      "auroc": 0.9261084635416665
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1522,
          "fn": 78,
          "accuracy": 0.95125
        },
        "0.01": null
      },
      "auroc": 0.9261084635416665
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1487,
          "fn": 113,
          "accuracy": 0.929375
        },
        "0.01": null
      },
      "auroc": 0.91574033203125
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1487,
          "fn": 113,
          "accuracy": 0.929375
        },
        "0.01": null
      },
      "auroc": 0.91574033203125
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3009,
          "fn": 191,
          "accuracy": 0.9403125
        },
        "0.01": null
      },
      "auroc": 0.9209243977864583
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3009,
          "fn": 191,
          "accuracy": 0.9403125
        },
        "0.01": null
      },
      "auroc": 0.9209243977864583
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1420,
          "fn": 180,
          "accuracy": 0.8875
        },
        "0.01": null
      },
      "auroc": 0.8993285807291667
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1420,
          "fn": 180,
          "accuracy": 0.8875
        },
        "0.01": null
      },
      "auroc": 0.8993285807291667
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1230,
          "fn": 370,
          "accuracy": 0.76875
        },
        "0.01": null
      },
      "auroc": 0.8581720540364582
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1230,
          "fn": 370,
          "accuracy": 0.76875
        },
        "0.01": null
      },
      "auroc": 0.8581720540364582
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2650,
          "fn": 550,
          "accuracy": 0.828125
        },
        "0.01": null
      },
      "auroc": 0.8787503173828124
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2650,
          "fn": 550,
          "accuracy": 0.828125
        },
        "0.01": null
      },
      "auroc": 0.8787503173828124
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1577,
          "fn": 23,
          "accuracy": 0.985625
        },
        "0.01": null
      },
      "auroc": 0.9361611002604167
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1577,
          "fn": 23,
          "accuracy": 0.985625
        },
        "0.01": null
      },
      "auroc": 0.9361611002604167
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1489,
          "fn": 111,
          "accuracy": 0.930625
        },
        "0.01": null
      },
      "auroc": 0.91920458984375
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1489,
          "fn": 111,
          "accuracy": 0.930625
        },
        "0.01": null
      },
      "auroc": 0.91920458984375
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3066,
          "fn": 134,
          "accuracy": 0.958125
        },
        "0.01": null
      },
      "auroc": 0.9276828450520834
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 3066,
          "fn": 134,
          "accuracy": 0.958125
        },
        "0.01": null
      },
      "auroc": 0.9276828450520834
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1584,
          "fn": 16,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9382943033854166
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1584,
          "fn": 16,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9382943033854166
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 731,
          "fn": 869,
          "accuracy": 0.456875
        },
        "0.01": null
      },
      "auroc": 0.7499461100260417
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 731,
          "fn": 869,
          "accuracy": 0.456875
        },
        "0.01": null
      },
      "auroc": 0.7499461100260417
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2315,
          "fn": 885,
          "accuracy": 0.7234375
        },
        "0.01": null
      },
      "auroc": 0.8441202067057292
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2315,
          "fn": 885,
          "accuracy": 0.7234375
        },
        "0.01": null
      },
      "auroc": 0.8441202067057292
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1439,
          "fn": 161,
          "accuracy": 0.899375
        },
        "0.01": null
      },
      "auroc": 0.9036600748697917
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1439,
          "fn": 161,
          "accuracy": 0.899375
        },
        "0.01": null
      },
      "auroc": 0.9036600748697917
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1271,
          "fn": 329,
          "accuracy": 0.794375
        },
        "0.01": null
      },
      "auroc": 0.860708642578125
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 1271,
          "fn": 329,
          "accuracy": 0.794375
        },
        "0.01": null
      },
      "auroc": 0.860708642578125
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2710,
          "fn": 490,
          "accuracy": 0.846875
        },
        "0.01": null
      },
      "auroc": 0.8821843587239583
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 2710,
          "fn": 490,
          "accuracy": 0.846875
        },
        "0.01": null
      },
      "auroc": 0.8821843587239583
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 16982,
          "fn": 618,
          "accuracy": 0.9648863636363636
        },
        "0.01": null
      },
      "auroc": 0.9299118637547349
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 14628,
          "fn": 2972,
          "accuracy": 0.8311363636363637
        },
        "0.01": null
      },
      "auroc": 0.8795079515861741
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "alternative_spelling",
      "accuracy": {
        "0.05": {
          "tp": 31610,
          "fn": 3590,
          "accuracy": 0.8980113636363637
        },
        "0.01": null
      },
      "auroc": 0.9047099076704544
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9443489583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9442866373697916
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9443177978515624
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9443489583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9441231282552083
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9442360432942708
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9443489583333333
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9442048828124999
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 6400,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9442769205729167
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1580,
          "fn": 20,
          "accuracy": 0.9875
        },
        "0.01": null
      },
      "auroc": 0.9413854329427083
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1550,
          "fn": 50,
          "accuracy": 0.96875
        },
        "0.01": null
      },
      "auroc": 0.931713916015625
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3130,
          "fn": 70,
          "accuracy": 0.978125
        },
        "0.01": null
      },
      "auroc": 0.9365496744791667
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9432791666666667
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1507,
          "fn": 93,
          "accuracy": 0.941875
        },
        "0.01": null
      },
      "auroc": 0.9219250488281249
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3102,
          "fn": 98,
          "accuracy": 0.969375
        },
        "0.01": null
      },
      "auroc": 0.9326021077473958
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3175,
          "fn": 25,
          "accuracy": 0.9921875
        },
        "0.01": null
      },
      "auroc": 0.9423322998046875
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3057,
          "fn": 143,
          "accuracy": 0.9553125
        },
        "0.01": null
      },
      "auroc": 0.9268194824218748
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 6232,
          "fn": 168,
          "accuracy": 0.97375
        },
        "0.01": null
      },
      "auroc": 0.9345758911132814
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9421925618489583
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1594,
          "fn": 6,
          "accuracy": 0.99625
        },
        "0.01": null
      },
      "auroc": 0.9413299153645833
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3191,
          "fn": 9,
          "accuracy": 0.9971875
        },
        "0.01": null
      },
      "auroc": 0.9417612386067707
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1550,
          "fn": 50,
          "accuracy": 0.96875
        },
        "0.01": null
      },
      "auroc": 0.9376461751302083
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1590,
          "fn": 10,
          "accuracy": 0.99375
        },
        "0.01": null
      },
      "auroc": 0.9421198567708332
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3140,
          "fn": 60,
          "accuracy": 0.98125
        },
        "0.01": null
      },
      "auroc": 0.9398830159505207
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1591,
          "fn": 9,
          "accuracy": 0.994375
        },
        "0.01": null
      },
      "auroc": 0.9411549153645833
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1577,
          "fn": 23,
          "accuracy": 0.985625
        },
        "0.01": null
      },
      "auroc": 0.9377834309895834
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3168,
          "fn": 32,
          "accuracy": 0.99
        },
        "0.01": null
      },
      "auroc": 0.9394691731770832
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3141,
          "fn": 59,
          "accuracy": 0.9815625
        },
        "0.01": null
      },
      "auroc": 0.9394005452473959
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3167,
          "fn": 33,
          "accuracy": 0.9896875
        },
        "0.01": null
      },
      "auroc": 0.9399516438802084
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 6308,
          "fn": 92,
          "accuracy": 0.985625
        },
        "0.01": null
      },
      "auroc": 0.9396760945638021
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1579,
          "fn": 21,
          "accuracy": 0.986875
        },
        "0.01": null
      },
      "auroc": 0.9411556640625
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1582,
          "fn": 18,
          "accuracy": 0.98875
        },
        "0.01": null
      },
      "auroc": 0.9389733723958333
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3161,
          "fn": 39,
          "accuracy": 0.9878125
        },
        "0.01": null
      },
      "auroc": 0.9400645182291666
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9437902669270832
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1591,
          "fn": 9,
          "accuracy": 0.994375
        },
        "0.01": null
      },
      "auroc": 0.939583349609375
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3191,
          "fn": 9,
          "accuracy": 0.9971875
        },
        "0.01": null
      },
      "auroc": 0.941686808268229
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3179,
          "fn": 21,
          "accuracy": 0.9934375
        },
        "0.01": null
      },
      "auroc": 0.9424729654947916
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3173,
          "fn": 27,
          "accuracy": 0.9915625
        },
        "0.01": null
      },
      "auroc": 0.9392783610026041
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 6352,
          "fn": 48,
          "accuracy": 0.9925
        },
        "0.01": null
      },
      "auroc": 0.9408756632486979
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1599,
          "fn": 1,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9435284016927084
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1599,
          "fn": 1,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9437824218749999
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3198,
          "fn": 2,
          "accuracy": 0.999375
        },
        "0.01": null
      },
      "auroc": 0.9436554117838543
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 2,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.943498388671875
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3197,
          "fn": 3,
          "accuracy": 0.9990625
        },
        "0.01": null
      },
      "auroc": 0.9435133951822916
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1568,
          "fn": 32,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9340048990885417
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1568,
          "fn": 32,
          "accuracy": 0.98
        },
        "0.01": null
      },
      "auroc": 0.9340048990885417
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1564,
          "fn": 36,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9333477701822916
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1564,
          "fn": 36,
          "accuracy": 0.9775
        },
        "0.01": null
      },
      "auroc": 0.9333477701822916
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3132,
          "fn": 68,
          "accuracy": 0.97875
        },
        "0.01": null
      },
      "auroc": 0.9336763346354167
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3132,
          "fn": 68,
          "accuracy": 0.97875
        },
        "0.01": null
      },
      "auroc": 0.9336763346354167
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 2,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9437674967447915
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1598,
          "fn": 2,
          "accuracy": 0.99875
        },
        "0.01": null
      },
      "auroc": 0.9437674967447915
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9434379231770833
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1597,
          "fn": 3,
          "accuracy": 0.998125
        },
        "0.01": null
      },
      "auroc": 0.9434379231770833
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3195,
          "fn": 5,
          "accuracy": 0.9984375
        },
        "0.01": null
      },
      "auroc": 0.9436027099609374
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3195,
          "fn": 5,
          "accuracy": 0.9984375
        },
        "0.01": null
      },
      "auroc": 0.9436027099609374
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9441291015625
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9441291015625
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9440990559895833
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9440990559895833
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9441140787760416
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9441140787760416
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9442457519531249
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9442457519531249
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9439346842447915
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1600,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9439346842447915
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9440902180989583
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3200,
          "fn": 0,
          "accuracy": 1.0
        },
        "0.01": null
      },
      "auroc": 0.9440902180989583
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9423955078124999
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1595,
          "fn": 5,
          "accuracy": 0.996875
        },
        "0.01": null
      },
      "auroc": 0.9423955078124999
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1589,
          "fn": 11,
          "accuracy": 0.993125
        },
        "0.01": null
      },
      "auroc": 0.9407726888020833
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 1589,
          "fn": 11,
          "accuracy": 0.993125
        },
        "0.01": null
      },
      "auroc": 0.9407726888020833
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3184,
          "fn": 16,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9415840983072917
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 3184,
          "fn": 16,
          "accuracy": 0.995
        },
        "0.01": null
      },
      "auroc": 0.9415840983072917
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 17466,
          "fn": 134,
          "accuracy": 0.9923863636363637
        },
        "0.01": null
      },
      "auroc": 0.9417090864701705
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 17528,
          "fn": 72,
          "accuracy": 0.9959090909090909
        },
        "0.01": null
      },
      "auroc": 0.9420903394294509
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "zero_width_space",
      "accuracy": {
        "0.05": {
          "tp": 34994,
          "fn": 206,
          "accuracy": 0.9941477272727273
        },
        "0.01": null
      },
      "auroc": 0.9418997129498106
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18430,
          "fn": 770,
          "accuracy": 0.9598958333333333
        },
        "0.01": null
      },
      "auroc": 0.9286458658854166
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 16739,
          "fn": 2461,
          "accuracy": 0.8718229166666667
        },
        "0.01": null
      },
      "auroc": 0.897597523328993
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 35169,
          "fn": 3231,
          "accuracy": 0.915859375
        },
        "0.01": null
      },
      "auroc": 0.9131216946072048
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18355,
          "fn": 845,
          "accuracy": 0.9559895833333333
        },
        "0.01": null
      },
      "auroc": 0.9261008653428819
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 13463,
          "fn": 5737,
          "accuracy": 0.7011979166666666
        },
        "0.01": null
      },
      "auroc": 0.7983921780056423
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 31818,
          "fn": 6582,
          "accuracy": 0.82859375
        },
        "0.01": null
      },
      "auroc": 0.8622465216742621
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 36785,
          "fn": 1615,
          "accuracy": 0.9579427083333333
        },
        "0.01": null
      },
      "auroc": 0.9273733656141494
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 30202,
          "fn": 8198,
          "accuracy": 0.7865104166666667
        },
        "0.01": null
      },
      "auroc": 0.8479948506673176
    },
    {
      "domain": "all",
      "model": "llama-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 66987,
          "fn": 9813,
          "accuracy": 0.8722265625
        },
        "0.01": null
      },
      "auroc": 0.8876841081407336
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18549,
          "fn": 651,
          "accuracy": 0.96609375
        },
        "0.01": null
      },
      "auroc": 0.9327379448784723
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 2046,
          "fn": 17154,
          "accuracy": 0.1065625
        },
        "0.01": null
      },
      "auroc": 0.2282172526041667
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 20595,
          "fn": 17805,
          "accuracy": 0.536328125
        },
        "0.01": null
      },
      "auroc": 0.5804775987413194
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 14188,
          "fn": 5012,
          "accuracy": 0.7389583333333334
        },
        "0.01": null
      },
      "auroc": 0.83431494140625
    },
    {
      "domain": "all",
      "model": "mpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 32737,
          "fn": 5663,
          "accuracy": 0.8525260416666667
        },
        "0.01": null
      },
      "auroc": 0.8835264431423611
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 16475,
          "fn": 2725,
          "accuracy": 0.8580729166666666
        },
        "0.01": null
      },
      "auroc": 0.884684743923611
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 15274,
          "fn": 3926,
          "accuracy": 0.7955208333333333
        },
        "0.01": null
      },
      "auroc": 0.8648663031684027
    },
    {
      "domain": "all",
      "model": "mpt-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 31749,
          "fn": 6651,
          "accuracy": 0.826796875
        },
        "0.01": null
      },
      "auroc": 0.874775523546007
    },
    {
      "domain": "all",
      "model": "gpt2",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 16343,
          "fn": 2857,
          "accuracy": 0.8511979166666667
        },
        "0.01": null
      },
      "auroc": 0.881502616373698
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 18418,
          "fn": 782,
          "accuracy": 0.9592708333333333
        },
        "0.01": null
      },
      "auroc": 0.9305940104166667
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 10437,
          "fn": 8763,
          "accuracy": 0.54359375
        },
        "0.01": null
      },
      "auroc": 0.7205058932834201
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 28855,
          "fn": 9545,
          "accuracy": 0.7514322916666667
        },
        "0.01": null
      },
      "auroc": 0.8255499518500434
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 11918,
          "fn": 7282,
          "accuracy": 0.6207291666666667
        },
        "0.01": null
      },
      "auroc": 0.7905239271375869
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 1862,
          "fn": 17338,
          "accuracy": 0.09697916666666667
        },
        "0.01": null
      },
      "auroc": 0.2022914347330729
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 13780,
          "fn": 24620,
          "accuracy": 0.35885416666666664
        },
        "0.01": null
      },
      "auroc": 0.4964076809353299
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 30336,
          "fn": 8064,
          "accuracy": 0.79
        },
        "0.01": null
      },
      "auroc": 0.8605589687771267
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 12299,
          "fn": 26101,
          "accuracy": 0.32028645833333336
        },
        "0.01": null
      },
      "auroc": 0.46139866400824653
    },
    {
      "domain": "all",
      "model": "mistral",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 42635,
          "fn": 34165,
          "accuracy": 0.5551432291666667
        },
        "0.01": null
      },
      "auroc": 0.6609788163926866
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 17911,
          "fn": 1289,
          "accuracy": 0.9328645833333333
        },
        "0.01": null
      },
      "auroc": 0.9159528591579861
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "yes",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 15356,
          "fn": 3844,
          "accuracy": 0.7997916666666667
        },
        "0.01": null
      },
      "auroc": 0.8677107286241319
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 33267,
          "fn": 5133,
          "accuracy": 0.866328125
        },
        "0.01": null
      },
      "auroc": 0.8918317938910588
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 16696,
          "fn": 2504,
          "accuracy": 0.8695833333333334
        },
        "0.01": null
      },
      "auroc": 0.893835226779514
    },
    {
      "domain": "all",
      "model": "mistral-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 34607,
          "fn": 3793,
          "accuracy": 0.9012239583333334
        },
        "0.01": null
      },
      "auroc": 0.90489404296875
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 16470,
          "fn": 2730,
          "accuracy": 0.8578125
        },
        "0.01": null
      },
      "auroc": 0.8878870849609375
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 16470,
          "fn": 2730,
          "accuracy": 0.8578125
        },
        "0.01": null
      },
      "auroc": 0.8878870849609375
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 15599,
          "fn": 3601,
          "accuracy": 0.8124479166666667
        },
        "0.01": null
      },
      "auroc": 0.8686910888671873
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 15599,
          "fn": 3601,
          "accuracy": 0.8124479166666667
        },
        "0.01": null
      },
      "auroc": 0.8686910888671873
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 32069,
          "fn": 6331,
          "accuracy": 0.8351302083333333
        },
        "0.01": null
      },
      "auroc": 0.8782890869140625
    },
    {
      "domain": "all",
      "model": "gpt3",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 32069,
          "fn": 6331,
          "accuracy": 0.8351302083333333
        },
        "0.01": null
      },
      "auroc": 0.8782890869140625
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 15261,
          "fn": 3939,
          "accuracy": 0.79484375
        },
        "0.01": null
      },
      "auroc": 0.8611593994140626
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 15261,
          "fn": 3939,
          "accuracy": 0.79484375
        },
        "0.01": null
      },
      "auroc": 0.8611593994140626
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 13212,
          "fn": 5988,
          "accuracy": 0.688125
        },
        "0.01": null
      },
      "auroc": 0.8196485161675348
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 13212,
          "fn": 5988,
          "accuracy": 0.688125
        },
        "0.01": null
      },
      "auroc": 0.8196485161675348
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 28473,
          "fn": 9927,
          "accuracy": 0.741484375
        },
        "0.01": null
      },
      "auroc": 0.8404039577907987
    },
    {
      "domain": "all",
      "model": "cohere",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 28473,
          "fn": 9927,
          "accuracy": 0.741484375
        },
        "0.01": null
      },
      "auroc": 0.8404039577907987
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 17299,
          "fn": 1901,
          "accuracy": 0.9009895833333333
        },
        "0.01": null
      },
      "auroc": 0.9041319607204861
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 17299,
          "fn": 1901,
          "accuracy": 0.9009895833333333
        },
        "0.01": null
      },
      "auroc": 0.9041319607204861
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 15795,
          "fn": 3405,
          "accuracy": 0.82265625
        },
        "0.01": null
      },
      "auroc": 0.877201848687066
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 15795,
          "fn": 3405,
          "accuracy": 0.82265625
        },
        "0.01": null
      },
      "auroc": 0.877201848687066
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 33094,
          "fn": 5306,
          "accuracy": 0.8618229166666667
        },
        "0.01": null
      },
      "auroc": 0.8906669047037761
    },
    {
      "domain": "all",
      "model": "chatgpt",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 33094,
          "fn": 5306,
          "accuracy": 0.8618229166666667
        },
        "0.01": null
      },
      "auroc": 0.8906669047037761
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 17765,
          "fn": 1435,
          "accuracy": 0.9252604166666667
        },
        "0.01": null
      },
      "auroc": 0.9137087131076389
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 17765,
          "fn": 1435,
          "accuracy": 0.9252604166666667
        },
        "0.01": null
      },
      "auroc": 0.9137087131076389
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9062,
          "fn": 10138,
          "accuracy": 0.47197916666666667
        },
        "0.01": null
      },
      "auroc": 0.7357555677625869
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 9062,
          "fn": 10138,
          "accuracy": 0.47197916666666667
        },
        "0.01": null
      },
      "auroc": 0.7357555677625869
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 26827,
          "fn": 11573,
          "accuracy": 0.6986197916666667
        },
        "0.01": null
      },
      "auroc": 0.8247321404351128
    },
    {
      "domain": "all",
      "model": "gpt4",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 26827,
          "fn": 11573,
          "accuracy": 0.6986197916666667
        },
        "0.01": null
      },
      "auroc": 0.8247321404351128
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 15565,
          "fn": 3635,
          "accuracy": 0.8106770833333333
        },
        "0.01": null
      },
      "auroc": 0.8699314602322048
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 15565,
          "fn": 3635,
          "accuracy": 0.8106770833333333
        },
        "0.01": null
      },
      "auroc": 0.8699314602322048
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 13675,
          "fn": 5525,
          "accuracy": 0.7122395833333334
        },
        "0.01": null
      },
      "auroc": 0.8258665052625868
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 13675,
          "fn": 5525,
          "accuracy": 0.7122395833333334
        },
        "0.01": null
      },
      "auroc": 0.8258665052625868
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 29240,
          "fn": 9160,
          "accuracy": 0.7614583333333333
        },
        "0.01": null
      },
      "auroc": 0.8478989827473957
    },
    {
      "domain": "all",
      "model": "cohere-chat",
      "decoding": "all",
      "repetition_penalty": "all",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 29240,
          "fn": 9160,
          "accuracy": 0.7614583333333333
        },
        "0.01": null
      },
      "auroc": 0.8478989827473957
    },
    {
      "domain": "all",
      "model": "all",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "all",
      "accuracy": {
        "0.05": {
          "tp": 160117,
          "fn": 51083,
          "accuracy": 0.7581297348484849
        },
        "0.01": null
      },
      "auroc": 0.8471188551777541
    }
  ],
  "thresholds": {
    "0.05": {
      "abstracts": 0.7296277777777777,
      "books": 0.905402777777778,
      "news": 0.8800000001074579,
      "poetry": 0.9200000000019712,
      "recipes": 0.8740845959595958,
      "reddit": 0.8584472222222224,
      "reviews": 0.8941010101010103,
      "wiki": 0.8700000000703443
    },
    "0.01": {
      "abstracts": 0.9171277777777775,
      "books": 0.9999999998284299,
      "news": 0.9797318181818184,
      "poetry": 0.953616161616162,
      "recipes": 0.9365845959595958,
      "reddit": 0.9365722222222224,
      "reviews": 0.9999999994655151,
      "wiki": 0.9500000000284348
    }
  },
  "fpr": {
    "0.05": {
      "abstracts": 0.050000000000000044,
      "books": 0.050000000000000044,
      "news": 0.04500000000000004,
      "poetry": 0.03500000000000003,
      "recipes": 0.050000000000000044,
      "reddit": 0.050000000000000044,
      "reviews": 0.050000000000000044,
      "wiki": 0.040000000000000036
    },
    "0.01": {
      "abstracts": 0.010000000000000009,
      "books": 0.015000000000000013,
      "news": 0.010000000000000009,
      "poetry": 0.010000000000000009,
      "recipes": 0.010000000000000009,
      "reddit": 0.010000000000000009,
      "reviews": 0.015000000000000013,
      "wiki": 0.0050000000000000044
    }
  }
}